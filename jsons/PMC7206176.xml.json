{
    "paper_id": "PMC7206176",
    "metadata": {
        "title": "Mobility Irregularity Detection with Smart Transit Card Data",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Xuesong",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "email": "xuesong.wang1@unsw.edu.au",
                "affiliation": {}
            },
            {
                "first": "Lina",
                "middle": [],
                "last": "Yao",
                "suffix": "",
                "email": "lina.yao@unsw.edu.au",
                "affiliation": {}
            },
            {
                "first": "Wei",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "email": "Wei.Liu@unsw.edu.au",
                "affiliation": {}
            },
            {
                "first": "Can",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "email": "can.li4@unsw.edu.au",
                "affiliation": {}
            },
            {
                "first": "Lei",
                "middle": [],
                "last": "Bai",
                "suffix": "",
                "email": "baisanshi@gmail.com",
                "affiliation": {}
            },
            {
                "first": "S.",
                "middle": [
                    "Travis"
                ],
                "last": "Waller",
                "suffix": "",
                "email": "s.waller@unsw.edu.au",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Smart public transit card is now widely used in cities all over the world. It brings massive convenience for both passengers and transit operators in daily lives. Online recharging and fare calculation can be automatically achieved with minimum labour cost involved. In this context, large volume of transit smart card data is continuously generated, which provides an unprecedented opportunity to profile passenger mobility patterns [8]. For example, the Opal card in the Greater Sydney area involves millions of users and records travel patterns of them on a daily basis. Uncovering recurrent patterns, and systematic variations, and irregular behavior patterns from these large datasets can aid in future transit route and stop planning or refinement, bus or train scheduling. Moreover, detecting irregular behavior patterns may also help alleviate or limit potential fraudulent behavior with smart transit card and subsequent loss.",
            "cite_spans": [
                {
                    "start": 435,
                    "end": 436,
                    "mention": "8",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The repetitiveness or regularity of travel patterns indeed allow planners and operators to optimally design public transport systems. However, in recent years, with a growing availability of individual mobility data, detecting irregularities regarding individual mobility in public transport system and quantifying the impacts of these become increasingly important, which can help the system operator and planner to more proactively accommodate both systematic behavior variations and stochasticity related to travel in public transit systems. At the same time, discovering and preventing potential fraudulent behavior with smart transit cards is also important in many cities. For example, the public transit card in cities such as Sydney, Melbourne, and Hong Kong is often linked to users\u2019 credit cards. Moreover, public transit cards in some cities (e.g., Octopus card in Hong Kong) can also be used to purchase daily goods from a variety of shops. Detecting irregular and potential fraudulent behavior with smart transit cards at early stages may help avoid considerable loss of travelers who lose their smart transit cards.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In order to address the problem of detecting irregular or fraudulent behavior in smart transit data, recent efforts have been made in spatial-temporal profile extraction and pattern comparison. First, convolutional-based or graph-based methods are adopted for describing spatial connections and layout for different stops [1, 5, 14]. Then a sequential layer is applied to model temporal correlations among historical transits. In this way, the model is able to predict a traffic-related value for a location at a timestamp. Finally, similarity-based algorithms are used to compare different passenger patterns. The similarity function can either be measured by a statistical distance metric between the normal and fraud data, or defined by a reconstruction error derived from an encoder-decoder structure [6, 13, 16]. Therefore, a reference passenger pattern can be discriminated from a potential irregular/fraudulent pattern.",
            "cite_spans": [
                {
                    "start": 323,
                    "end": 324,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 326,
                    "end": 327,
                    "mention": "5",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 329,
                    "end": 331,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 806,
                    "end": 807,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 809,
                    "end": 811,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 813,
                    "end": 815,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Despite promising success in excavating spatial-temporal profile based on morphological layout or traffic flow, passenger level information is often fused within a certain region, which is inadequate when distinguishing fine-grained passenger profile since everyone has their own mobility patterns. Besides, most of the existing fraud or irregularity detection methods only consider all passenger data in an aggregate manner and learn a decision boundary for common outlier patterns. However, these methods tend to fail when normal data has high intra-class variance, meaning that normal data is not compacted and the boundary between the normal and fraud data is blurry, or reconstructed errors are high even for normal data. This is exactly the case of mobility pattern where one passenger\u2019s normal record can be another one\u2019s abnormal data and totally confuses the aforementioned methods, thus calling for a personalised fraudulent behavior detection method.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we build a deep learning framework to detect irregular/fraudulent behavior with respect to smart transit card (note that we may simply use \u201cfraud\u201d or \u201cfraudulent\u201d to refer to \u201cirregular\u201d behaviors later on). We managed to extract passenger level spatial-temporal profile and present a personalised similarity learning for detection. More specifically, a route-to-stop embedding is first proposed to extract spatial correlations between different transit stops and routes for each passenger. Then attentive fusion is adopted to find spatial repetitive and time invariant pattern. Finally, a personalised similarity function is learned to evaluate the historical and recent mobility patterns. To summarize, this paper makes the following major contributions.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "\nWe propose a novel route-to-stop embedding to explore spatial correlations between routes and transit stops. It does not need to compute tens of thousands of global nodes in a graph. Instead, it only focuses on personalised nodes for each passenger, meanwhile maintain the ability to abstract node and edge correlations.We propose a learnable similarity function to measure the distance between repetitive invariant mobility pattern and recency pattern. Instead of integrating all the passenger data with high intra-class variance, the function directly applies to each passenger and makes personalised decision.We conduct experiments on a large-scale real-world dataset. Results demonstrated that using 20% of the total fraudulent data can achieve state-of-the art performance. With overall data, our model gains significant improvements on F1 and accuracy.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Recently a host of studies have investigated to extract spatial-temporal profile for a passenger. One typical approach is to use convolutional layers over a spatial map and to use recurrent layers over a time sequence. Lan et al. [7] and Chen et al. [3] adopted convolutional layers on the morphological layout of a city and LSTM to estimate travel time and predict urban air quality index. Recently convolutional graph networks have drawn more attention in helping discover non-linearity correlation between nodes and edges in a graph [5, 17]. Each node is represented by a vector while connections of each node are represented by an adjacency matrix or by fixed edge features. Nevertheless, most of existing approaches [1, 14] fuse passenger features in a node, while features on the same node can be dynamic for different passengers, since two passengers on the same stop may have different time slot preference. Also, usually nodes were built by segmenting city layout into hundreds of grids so as to compute adjacency matrix. However, in our scenario nodes are tens of thousands of transit stops, demanding highly expensive computation to build a traditional graph.",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 232,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 251,
                    "end": 252,
                    "mention": "3",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 537,
                    "end": 538,
                    "mention": "5",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 540,
                    "end": 542,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 722,
                    "end": 723,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 725,
                    "end": 727,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Related Works",
            "ref_spans": []
        },
        {
            "text": "In order to distinguish unique passenger profile from potential irregular or fraudulent behavior, distance-based and reconstruction-based detection methods have been widely used. The distance-based approaches assume that normal data are compact in distribution and far from sparse fraud. Conventional methods include isolation Forest [9], and One-Class Support Vector Machine. Present deep learning-based models try to learn a distinguished density space between the normal and fraud data [10, 11, 15]. DevNet [10] defines a deviation loss based on z-score of a prior Gaussian distribution and squeezes the outliers to the tail of the distribution. Perera et al. [11] present a compactness and a descriptiveness loss for one-class learning. Yoon et al. [15] propose a learnable scoring matrix to detecting incongruity between news headline and body text. The reconstruction based methods adopt encoder-decoder structures and claim that fraud can not be well-reconstructed through the structure. Cao et al. [2] and Xu et al. [13] propose variation-auto-encoder based models with extensive experiments. Zhang et al. [16] detect fraud on reconstructed residual correlation matrix. LSTM-NDT [6] and OmniAnomaly [12] adopt dynamic thresholding on reconstructed feature errors. Despite great effectiveness, these approaches tend to underperform in our task due to high intra-class variance of normal data, where one passenger\u2019s normal records can be irregular/fraudulent for others.",
            "cite_spans": [
                {
                    "start": 335,
                    "end": 336,
                    "mention": "9",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 490,
                    "end": 492,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 494,
                    "end": 496,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 498,
                    "end": 500,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 511,
                    "end": 513,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 664,
                    "end": 666,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 754,
                    "end": 756,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1007,
                    "end": 1008,
                    "mention": "2",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1025,
                    "end": 1027,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1115,
                    "end": 1117,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1188,
                    "end": 1189,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1208,
                    "end": 1210,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Related Works",
            "ref_spans": []
        },
        {
            "text": "Summary. The proposed method in this paper is fundamentally different from the literature in the following aspects. Firstly, the proposed route-to-stop embedding does not need to compute tens of thousands of node features in a graph. Instead, it only concentrates on personalised nodes for each passenger, meanwhile maintaining the ability of abstracting node and edge correlations. Furthermore, this study directly applies a personalised similarity function to learn the discrepancy between historical and new records for each passenger. In this way, the variance of data can be reduced by only focusing records of the same passenger. Therefore, the high intra-class variance problem of some existing approaches is avoided.",
            "cite_spans": [],
            "section": "Related Works",
            "ref_spans": []
        },
        {
            "text": "Data Description. We use public transit card of the Greater Sydney area (Opal card) in the case study. In total 187,000 passenger records were collected from April 1st to 30th, 2017. Each record is characterized with transit stop features and route features. Transit stop features consist of tap on/off time, longitude/latitude and postcode of stops, stop id and whether the tap is a transfer from another vehicle. Transit route features include vehicle type, vehicle id, duration, distance bands, journey costs and run direction. In this way, every record can be described as a starting stop to an ending stop through a route, and a passenger can be represented as a sequence of such records. As demonstrated in Fig. 1, there are four transit stops marked as red and three connecting routes for this passenger. Each stop has one or more records, representing the passenger pattern such as visiting frequency.\n",
            "cite_spans": [],
            "section": "Data Characteristics ::: Proposed Approach",
            "ref_spans": [
                {
                    "start": 718,
                    "end": 719,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "From Fig. 1, it is able to profile a passenger through spatial-temporal pattern. For instance, a route is made from Milsons Point Station to Town Hall Station nearly every weekday morning, indicating home/office location and potential occupation of the passenger. These repetitive stops together with less visiting stops, each with their own time invariant pattern, profile a unique passenger. We hereby aim to leverage this profile by excavating spatial correlations between transit stops and temporal preferences for each route, named \u201cpersonalised spatial-temporal profile\u201d for short. We expect to learn discriminative passenger profile, which can not only help transit operators provide more customised service, but can also alert passengers when irregular/fraudulent behavior occurs.",
            "cite_spans": [],
            "section": "Data Characteristics ::: Proposed Approach",
            "ref_spans": [
                {
                    "start": 10,
                    "end": 11,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Given a set of historical records \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X = \\{x_1, x_2, ..., x_N \\} $$\\end{document} with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_i \\in \\mathbb {R} ^D$$\\end{document} and a recent record \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{N+1}$$\\end{document} for a passenger, each record \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_i=(s_{i1},s_{i2},r_i)$$\\end{document} can be viewed as a route \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_i$$\\end{document} from a stop \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{i1}$$\\end{document} to a stop \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{i2}$$\\end{document} where (., .) denotes concatenation. The goal is to learn a personalised similarity function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi : (x_{N+1}, X) \\mapsto \\mathbb {R}^2 $$\\end{document} in a way that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{N+1}$$\\end{document} is similar to X if \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi (x_{N+1}, X)_{1} > \\phi (x_{N+1}, X)_{2}$$\\end{document}. Otherwise, it is a irregular/fraudulent behavior.",
            "cite_spans": [],
            "section": "Problem Statement ::: Proposed Approach",
            "ref_spans": []
        },
        {
            "text": "Figure 2 presents the overall framework of the personalised spatial-temporal similarity learning network. Historical pattern \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_N$$\\end{document} and the latest pattern \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{N+1}$$\\end{document} are extracted and compared from the corresponding data \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{ x_1, x_2, ..., x_{N} \\}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{N+1}$$\\end{document} to detect a irregular/fraudulent behavior. There are four major components in the framework: a route-to-stop embedding to describe the spatial mapping function from routes to stops, an attentive fusion to capture repetitive and time invariant pattern, a fully connected layer to extract recent pattern, and a similarity function to learn the discrepancy between the historical and recent patterns.\n",
            "cite_spans": [],
            "section": "The Proposed Framework ::: Proposed Approach",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Route-to-Stop Embedding (R2S). As mentioned before, every record is featured by two transit stops and a route. Instead of directly input raw features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_i=(s_{i1},s_{i2},r_i)$$\\end{document} into the model, we want to map two stop features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{i1},s_{i2}$$\\end{document} into the same space since starting and ending stops share the same raw input space. Also, stop and route features (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_i, r_i$$\\end{document}) have strong dependency correlations, for example, in Fig. 1, given route features like vehicle id, time duration and cost and starting stop features like tap on time and location, it is possible to speculate corresponding ending stop features. Hence, route features should also be projected to the same space as transit stops. Inspired by [4], a route-to-stop(R2S) embedding that is generalizable for each passenger is proposed to represent spatial correlation: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{$$\\end{document}route \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_i$$\\end{document}: starting stop \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{i1}$$\\end{document}\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mapsto $$\\end{document} ending stop \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{i2}$$\\end{document}\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\}$$\\end{document}:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} e_i = \\mathcal {E}(s_{i1}, s_{i2}, r_i) = (\\mathcal {F}[h(r_i) \\odot s_{i1}],\\mathcal {F}[h(r_i)\\odot s_{i2}]) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_i$$\\end{document} is the embedding of the i-th record. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {F}$$\\end{document} and h denote neural networks. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h(r_i)$$\\end{document} has the same dimension as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{i1}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{i2}$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\odot $$\\end{document} denotes element-wise multiplication. What R2S has achieved is to use h to project route features into stop space and to share a projection \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {F}$$\\end{document} between two stops.",
            "cite_spans": [
                {
                    "start": 1557,
                    "end": 1558,
                    "mention": "4",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "The Proposed Framework ::: Proposed Approach",
            "ref_spans": [
                {
                    "start": 1275,
                    "end": 1276,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The advantage of R2S layers lies in the ability to draw a customized spatial profile for each passenger. Different weights \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h(r_i)$$\\end{document} are assigned to stops. The starting and ending stops are treated equally with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {F}$$\\end{document}. In this way, two passengers who share the same starting stop will have distinguished weights on that stop during to other factors like duration or costs.",
            "cite_spans": [],
            "section": "The Proposed Framework ::: Proposed Approach",
            "ref_spans": []
        },
        {
            "text": "Repetitive and Time Invariant Pattern. A unique passenger profile lies in repetitive visiting stops and invariant transit time slot preference observed from historical records. To capture such repetitive and time invariant pattern \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_N$$\\end{document}, an LSTM layer followed by an attentive fusion is adopted:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\begin{matrix} u_i = LSTM(e_i, u_{i-1}),\\quad \\text {s.t.} \\quad 2\\le i \\le N, u_1 = \\mathbf {0} \\\\ a = softmax(\\mathbf {u}*W^u) \\in \\mathbb {R}^{N\\times 1}, \\qquad u_N = \\sum _{i=0}^N a_iu_i \\end{matrix}} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_i$$\\end{document} is the hidden layer of the i-th record defined by the R2S embedding \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_i$$\\end{document} and the hidden layer of the last timestamp \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{i-1}$$\\end{document}. Then, an attentive probability \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_i$$\\end{document} is calculated for each \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_i$$\\end{document} through attention matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W^u$$\\end{document}. The attention matrix is learned to focus on repetitive and time invariant stops so that the extracted pattern \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_N$$\\end{document} is distinguished from irregular/fraudulent behavior pattern.",
            "cite_spans": [],
            "section": "The Proposed Framework ::: Proposed Approach",
            "ref_spans": []
        },
        {
            "text": "Recency Mobility Pattern. For the latest record, the spatial profile \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_{N+1}$$\\end{document} is directly fed into a fully-connected layer to get recency mobility pattern \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{N+1}$$\\end{document}:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} u_{N+1} = FCN(e_{N+1}) \\end{aligned}$$\\end{document}The structure of hidden layers were chosen as same as the lstm hidden layers in order to learn a homogeneous projection as the historical pattern.",
            "cite_spans": [],
            "section": "The Proposed Framework ::: Proposed Approach",
            "ref_spans": []
        },
        {
            "text": "Personalised Similarity Learning. Given historical and recent record patterns \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_N$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{N+1}$$\\end{document}, A similarity function is modified from [15] aiming to provide personalised discrepancy decision:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} P_{fraud}&= (u_{N+1})^T M_1 u_{N} + b_1\\\\ P_{normal}&= (u_{N+1})^T M_2 u_{N} + b_2\\\\ P(u_{N+1}, u_N)&= softmax([p_{fraud}, p_{normal}]) \\end{aligned} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M_1, b_1, M_2, b_2$$\\end{document} are learnable parameters. If \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_{fraud} > p_{normal}$$\\end{document}, then it is irregular/fraudulent behavior. Using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u^TMu$$\\end{document} instead of directly adopting inner product \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u^Tu$$\\end{document} introduces a learnable matrix that can diminish differences between normal and historical data meanwhile enlarge the distributional gap between historical and fraud data. Hence, during training process, a pair of three records, i.e., (normal, irregular/fraudulent, historical) is built intentionally. The loss function is defined as:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} \\mathcal {L}(u_{N+1},u_{N}) = -\\mathbb {E}_{u_{N+1} \\sim u_N}\\log [P(u_{N+1}, u_N)]-\\mathbb {E }_{u_{N+1} \\not \\sim u_N}\\log [1-P(u_{N+1}, u_N)] \\end{aligned} \\end{aligned}$$\\end{document}\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{N+1} \\sim u_N$$\\end{document} refers to normal data and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{N+1} \\not \\sim u_N$$\\end{document} refers to fraud data. The reason why the problem is designed in a supervised manner is that a limited number of labeled anomalies can always provide critical prior knowledge for an unsupervised model [10]. However, in many real-world applications it is unable to collect massive labels. In order to address this issue, we generate labels based on the intuition that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{N+1}$$\\end{document} is a normal record if it comes from the same passenger as the historical records and it is an irregular record if it comes from other passengers. In this way, hand-crafted labels are not necessary.",
            "cite_spans": [
                {
                    "start": 705,
                    "end": 707,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 4221,
                    "end": 4223,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "The Proposed Framework ::: Proposed Approach",
            "ref_spans": []
        },
        {
            "text": "For the data set, records of 175,000 and 12,000 passengers were used as training and testing data. For each passenger, the number of historical records N is set to be 20. Embedding was adopted for categorical features. The overall dimension after the embedding is 322, with the dimension of stop1, stop2 and route features equalling 105, 105 and 112 respectively. For the R2S embedding \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {E}$$\\end{document}, hidden units of the neural network h is the same as the stop dimension 105, and the number of hidden units of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {F}$$\\end{document} is 50. The hidden states and output dimensions of LSTM are set to be 50 and 30. The hidden units of the FCN are 50 and 30.",
            "cite_spans": [],
            "section": "Experimental Settings ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Baseline Methods. We choose three fraud detection methods on general data: OCSVM, iForest and DevNet, and two fraud detection methods on time series data: LSTM-NDT, MSCRED as baseline models. Also, two variants of our method are compared to test efficiency.",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "\nOCSVM. The One-Class Support-Vector-Machine profiles normal data distribution boundary and claims \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{N+1}$$\\end{document} a fraud if it is outside the frontier.iForest [9]. The isolation Forest finds anomalies far from distributed-dense data. Each passenger data \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(x_{N+1}, X)$$\\end{document} is fit with an iForest model and a score of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{N+1}$$\\end{document} is given.DevNet [10]. A new deviation loss based on z-score is proposed for anomaly detection. The objective is to squeeze normal data into a small range and deviate outliers from this range using a network. Since it does not have sequential layers, all historical as well as the latest features were stacked to prevent input information loss.LSTM-NDT [6]. The LSTM with the Nonparametric Dynamic Thresholding uses LSTM for multivariate time series prediction and defines a reconstructed error threshold based on historical error mean and variance.MSCRED [16]. The Multi-Scale Convolutional Recurrent Encoder-Decoder encodes the correlation matrix for multivariate time series and uses the residual reconstructed matrix to detect a fraud.Conv-Sim. A variant of our approach that replaces R2S embedding with 1d convolution layers. A \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1 \\times 1$$\\end{document} kernel whose channel equals to input dimension slides over the 20 historical data. The motivation is that all historical data should share the same kernel.R2S-Net. A variant of our approach whose similarity function is a fully connected neural network. The input of the function is the concatenation of historical and recent data representation, and the output is irregular/fraudulent score.\n",
            "cite_spans": [
                {
                    "start": 439,
                    "end": 440,
                    "mention": "9",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1185,
                    "end": 1187,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1521,
                    "end": 1522,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1724,
                    "end": 1726,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Each experiment is given 10 runs and the mean and deviation values of the metrics are displayed in Table 1. The general fraud detection methods (OCSVM, DevNet) outperforms the sequential methods (LSTM-DNT and MSCRED). This is probably due to high intra-variance of the normal data, whereas a binary decision for a general method is easier to make than generating reconstructed data for a sequential model.",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 105,
                    "end": 106,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Overall, R2S-Sim as well as two variants significantly improves the state-of-the-art algorithms and is robust on different runs. It achieves high performance on all of the metrics other than a skewed precision. It gains 0.11 and 15.6% improvements on F1 and accuracy over DevNet. Conv-Sim is the closest to our method since attentive fusion compensates for the lack of temporal layers in 1d-convolution. The accuracy of R2S-Sim is around 2% higher than R2S-Net, verifying the effectiveness of the learned similarity function.\n",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Injection of fraud data. The original ratio of normal to fraudulent data is 1:1. We try to decrease the proportion of fraud training data. If 0\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\%$$\\end{document} fraud data is injected, then it becomes a completely unsupervised problem. Hence, we use different proportion of fraud training data: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{ 1\\%, 5\\%, 10\\%, 20\\%, 50\\%, 70\\%, 90\\%, 100\\% \\}$$\\end{document} to validate the performance. The ratio of test data is still 1:1 to keep consistency. Therefore if all the test data is treated as normal, the accuracy is 0.5 and the recall 1.0.",
            "cite_spans": [],
            "section": "Ablation Study ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Different Historical Window Size. Default window size is 20 in the setting. We assume that a passenger tap a card twice a day, then 20 is approximately a collection of weekly data. Different window sizes including \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{5, 10, 15 \\}$$\\end{document} were tested to see if passenger profile can be captured within a shorter time span.",
            "cite_spans": [],
            "section": "Ablation Study ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Figure 3 demonstrates the metrics with respect to data proportion and window size. All test data was regarded as normal using 1% fraud data. However, with 5% data, the performance of the model using a 20-time-step sequence begins to improve. A pattern is observed that the longer the sequence, the less training data is needed for better performance. With 20\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\%$$\\end{document} of the overall fraud data, all models can achieve the state-of-the-art accuracy of 0.7.\n\n",
            "cite_spans": [],
            "section": "Ablation Study ::: Experiments",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Remove R2S Embedding. Route-to-Stop embedding is removed and raw input features are input to attentive fusion. From Fig. 6, we can observe that except for precision, all the other metrics are improved with the embedding. Also, due to the reduced dimension of the input for the later LSTM layer, the overall training time is shorter than using raw features even with the added embedding.\n\n",
            "cite_spans": [],
            "section": "Ablation Study ::: Experiments",
            "ref_spans": [
                {
                    "start": 121,
                    "end": 122,
                    "mention": "6",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "In order to test if distinguishable patterns were learnt, a subset of the test data is visualized on raw features and learned features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_N, u_{N+1}$$\\end{document} (see Fig. 4). A t-sne model is trained with the pair (historical, normal, fraudulent). The relative vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {v}_{historical} - \\mathbf {v}_{fraudulent}, \\mathbf {v}_{historical} - \\mathbf {v}_{normal}$$\\end{document} are displayed rather than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {v}_{historical}$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {v}_{fraudulent}, \\mathbf {v}_{normal}$$\\end{document}. In this way, each data point in the plot is the difference between the historical and the latest representation of every passenger. Besides, test data was split into 40 groups and KL divergence was compared. Specifically, KL(normal||historical) and KL(fraudulent||historical) were compared and the violin plot is given in Fig. 5.",
            "cite_spans": [],
            "section": "Visualization on the Passenger Pattern ::: Experiments",
            "ref_spans": [
                {
                    "start": 443,
                    "end": 444,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1924,
                    "end": 1925,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "It is observed from Fig. 4(a) that the center of both normal and fraud cluster is around (0,0), meaning that the distribution of historical, normal and irregular/fraudulent raw data are homogeneous. This can also be concluded from Fig. 5(a) where difference between KL(normal||historical) and KL(fraud||historical) is negligible. After passenger pattern extraction, both differences are enlarged in Fig. 4(b) and Fig. 5(b), indicating that the model is pushing recent data away from historical one yet push irregular/fraudulent data harder so as to be distinctive. Also, there is a clear boundary between irregular/fraudulent and normal data.",
            "cite_spans": [],
            "section": "Visualization on the Passenger Pattern ::: Experiments",
            "ref_spans": [
                {
                    "start": 25,
                    "end": 26,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 236,
                    "end": 237,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 404,
                    "end": 405,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 418,
                    "end": 419,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "In this study, we build a deep learning framework to detect irregular or fraudulent behavior with respect to smart transit card usage. We managed to extract passenger-level spatial-temporal profile and present a personalised similarity learning for detection. More specifically, a route-to-stop embedding is first proposed to exploit spatial correlations between different transit stops and routes. Then attentive fusion is adopted to find spatial repetitive and time invariant pattern. Finally, a personalised similarity function is learned to discriminate the historical and recent mobility patterns. Experimental results on a large-scale dataset demonstrate that the proposed model outperforms baseline methods in terms of recall, F1 score, and accuracy. Moreover, with 20% of the total fraud/irregular data, we can achieve state-of-the-art performance. Visualization on learned patterns reveals that the method can learn distinctive features among the irregularity/fraud and the normal records.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Performance comparison\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Passenger spatial-temporal profile.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: The framework of personalised spatial-temporal similarity learning.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Performance w.r.t. fraud data ratio and window size. (a) Precision. (b) Recall. (c) F1. (d) Accuracy.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 4.: t-sne visualization on (a) raw features. (b) learned features.",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig. 5.: KL divergence between normal and fraud on (a) raw features. (b) learned features.",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Fig. 6.: Performance with/without R2S embedding.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Learning deep features for one-class classification",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perera",
                    "suffix": ""
                },
                {
                    "first": "VM",
                    "middle": [],
                    "last": "Patel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Image Process.",
            "volume": "28",
            "issn": "11",
            "pages": "5450-5463",
            "other_ids": {
                "DOI": [
                    "10.1109/TIP.2019.2917862"
                ]
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "Learning neural representations for network anomaly detection",
            "authors": [
                {
                    "first": "VL",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nicolau",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "McDermott",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Cybern.",
            "volume": "49",
            "issn": "8",
            "pages": "3074-3087",
            "other_ids": {
                "DOI": [
                    "10.1109/TCYB.2018.2838668"
                ]
            }
        },
        "BIBREF10": {
            "title": "Deep multi-task learning based urban air quality index modelling",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lyu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquit. Technol.",
            "volume": "3",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Passenger demographic attributes prediction for human-centered public transport",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "ST",
                    "middle": [],
                    "last": "Waller",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Neural Information Processing",
            "volume": "",
            "issn": "",
            "pages": "486-494",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}