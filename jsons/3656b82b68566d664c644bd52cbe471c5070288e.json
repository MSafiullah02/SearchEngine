{
    "paper_id": "3656b82b68566d664c644bd52cbe471c5070288e",
    "metadata": {
        "title": "MedLinker: Medical Entity Linking with Neural Representations and Dictionary Matching",
        "authors": [
            {
                "first": "Daniel",
                "middle": [],
                "last": "Loureiro",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "LIAAD -INESCTEC",
                    "location": {
                        "settlement": "Porto",
                        "country": "Portugal"
                    }
                },
                "email": "dloureiro@fc.up.pt"
            },
            {
                "first": "B",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "LIAAD -INESCTEC",
                    "location": {
                        "settlement": "Porto",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Al\u00edpio",
                "middle": [],
                "last": "M\u00e1rio",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "LIAAD -INESCTEC",
                    "location": {
                        "settlement": "Porto",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Jorge",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "LIAAD -INESCTEC",
                    "location": {
                        "settlement": "Porto",
                        "country": "Portugal"
                    }
                },
                "email": "amjorge@fc.up.pt"
            }
        ]
    },
    "abstract": [
        {
            "text": "Progress in the field of Natural Language Processing (NLP) has been closely followed by applications in the medical domain. Recent advancements in Neural Language Models (NLMs) have transformed the field and are currently motivating numerous works exploring their application in different domains. In this paper, we explore how NLMs can be used for Medical Entity Linking with the recently introduced MedMentions dataset, which presents two major challenges: (1) a large target ontology of over 2M concepts, and (2) low overlap between concepts in train, validation and test sets. We introduce a solution, MedLinker, that addresses these issues by leveraging specialized NLMs with Approximate Dictionary Matching, and show that it performs competitively on semantic type linking, while improving the state-of-the-art on the more fine-grained task of concept linking (+4 F1 on MedMentions main task).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Medical Entity Recognition and Linking remain challenging tasks at the intersection between Natural Language Processing (NLP) and Information Retrieval (IR). The main difficulty arises from the fact that annotated datasets are scarce and particularly expensive to collect (require domain expertise), while the ontologies used in this domain are also especially large. From the standpoint of NLP, the relatively small and low-coverage datasets are hard to model using current neural approaches, whereas IR is limited by the subtle semantics underlying the different concepts that constitute the ontologies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The recently introduced MedMentions [1] dataset provides the largest set of mention-level annotations targeting the UMLS (Unified Medical Language System) ontology. UMLS [6] is a compilation of several medical ontologies, making it the most comprehensive and broad, spanning a range of topics from viruses to biomedical occupations. Even though the MedMentions annotation effort was a substantial undertaking, it falls short of covering the full set of concepts comprising UMLS (\u223c1% coverage), as well as displaying low overlap between the set of concepts occurring in its training splits and the set of concepts occurring in the development and test splits (\u223c50% overlap). In order to overcome the challenges presented in this dataset, we propose a solution that's based on Neural Language Models (NLMs) but designed to fallback on Approximate Dictionary Matching (ADM) for zero-shot entity linking, taking advantage of the large lexicon provided with the UMLS Metathesaurus. Unlike previous approaches using NLMs in the medical domain [2] [3] [4] [5] , our solution decouples mention recognition and entity linking, leveraging NLMs for these subtasks in separate modules, and allowing for other methods, namely ADM, to take part in the linking process.",
            "cite_spans": [
                {
                    "start": 170,
                    "end": 173,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1045,
                    "end": 1048,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work we explore approaches using NLMs for the related task of Word Sense Disambiguation (WSD), particularly pooling methods for representing spans in NLM-space [9] . We show that the Semantic Type (STY) and Concept Unique Identifiers (CUI) embeddings learned in the process are useful for our linking tasks, and can be effectively combined with ADM for improved performance over previous solutions. While our solution, MedLinker 1 , is designed for MedMentions, the breadth of the target ontology makes it useful elsewhere.",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 171,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section we focus on previous works using NLMs for biomedical NER, or addressing MedMentions directly. While there are already several works using the latest Transformer-based NLMs for biomedical tasks [2] [3] [4] , these have, so far, focused only on the adaptation of the pretrained NLM for the medical domain, without considering how these can be leveraged with complementary approaches.",
            "cite_spans": [
                {
                    "start": 209,
                    "end": 212,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 213,
                    "end": 216,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 217,
                    "end": 220,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The authors of MedMentions reported results on a subset of their corpus (st21pv) using a popular method for biomedical NER called TaggerOne [8] . This method learns to jointly predict spans and link entities but relies mostly on discrete features. Also, MedMention's authors claimed that it took them several days to train their model with high-performance computing resources (e.g. 900 GB of RAM), raising tractability concerns about using TaggerOne.",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 143,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The first results on applying NLMs to MedMentions have been recently reported by [5] . Their solution showed strong performance for semantic type linking, but followed the standard approach for NER tasks also used by [2] [3] [4] .",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 84,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 217,
                    "end": 220,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 221,
                    "end": 224,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 225,
                    "end": 228,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The MedMentions dataset is provided in two variants, one targeting the full ontology of UMLS, and another targeting a subset of that ontology 2 selected by domain experts as particularly interesting for medical document retrieval. MedLinker is trained and evaluated on this subset (st21pv) of MedMentions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "Regarding the concept aliases present in UMLS, we found it useful to introduce some additional restrictions that improve the processing speed of our string matching methods while maintaining task performance. We discard aliases longer than 5 tokens and aliases that include punctuation (except dashes). Aliases are lowercased, along with the query strings used with dictionary methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "MedMentions uses the PubTator format, which annotates entities at the character-level. Since our methods require annotations at the token-level, we also preprocess the dataset with a tokenizer specialized for the medical domain. We use sciSpacy [7] for tokenization and sentence splitting, which is trained for the biomedical domain. Occasionally, sentence splitting errors incurred in misaligned mentions, and thus missing from training and evaluation. From a total of 203,282 annotations, this step produced 321 misalignments (0.16%).",
            "cite_spans": [
                {
                    "start": 245,
                    "end": 248,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "In this section we describe our methods for mention recognition, entity matching using string-based methods and contextual embeddings, and finally how the different matchers are combined into our final predictions. In Fig. 1 we show how the major components of our solution interact with each other. showing NER producing mentions that are matched to entities using independent approaches based on ngrams and contextual embeddings, which are combined in a post-processing step into the final entity predictions. Right: Detailed view into how we use NLMs to first derive spans from NER based on the last states of the NLM, then match entities based on a pooled representation of the predicted span (e.g. states for tokens t2, t3 and t4 at layers \u22121, \u22122). Mention Recognition Using NLMs. We follow the standard architecture for neural-based NER, using contextual embeddings from NLMs specialized for the medical domain [2] [3] [4] . This architecture is a BiLSTM that handles sequential encoding with long-term dependencies, together with a Conditional Random Field (CRF) which uses the BiLSTM's final states to improve dependencies between output labels. Similarly to [2] , this model also employs character-level embeddings, learned during training, to capture morphological information.",
            "cite_spans": [
                {
                    "start": 917,
                    "end": 920,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 921,
                    "end": 924,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 925,
                    "end": 928,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1167,
                    "end": 1170,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [
                {
                    "start": 218,
                    "end": 224,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Solution"
        },
        {
            "text": "Zero-Shot Linking with Approximate Dictionary Matching. Using Sim-String [10] , we represent our restricted set of aliases from UMLS as character n-grams, similarly to previous works [11, 12] . After experimenting with different sizes, we found that char n-grams of size 3 performed best. SimString matches strings (i.e. aliases) using the highly scalable CPMerge algorithm which is designed to find similar strings based on overlapping features (i.e. char ngrams).",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 77,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 183,
                    "end": 187,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 188,
                    "end": 191,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "Given aliases a \u2208 A UMLS , corresponding char n-gram features\u00e2, and a function map for mapping entities (concepts) e \u2208 E CUI/STY to aliases, we match query strings s, with char n-gram features\u015d, using the scoring function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "Linking by Similarity to Entity Embeddings. Considering that MedMentions includes a large set of annotated spans, similarly to WSD corpora, we replicate the pooling method used in [9] . Essentially, we represent STYs and CUIs as the average of all their corresponding contextual embeddings, which are, in turn, represented by the sum of the embeddings from the last 4 layers of the NLM. This results in 21 STY embeddings, and 18,425 CUI embeddings that can be matched using Nearest Neighbors (1NN, only most similar) in NLM-space.",
            "cite_spans": [
                {
                    "start": 180,
                    "end": 183,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "Given entities e \u2208 E CUI/STY , and corresponding precomputed embeddings \u2212 \u2192 e , we match query strings s, with embedding \u2212 \u2192 s , obtained by applying the same pooling procedure (with the full sentence), using the scoring function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "Linking by Classifying Contextual Embeddings. Even though the 1NN approach is very successful for WSD, we also take this opportunity to experiment with training a classifier using contextual embeddings from NLMs, instead of averaging all contextual embeddings grouped by entity. In particular, we train a minimal softmax classifier, without hidden layers (# parameters = # embedding dimensions * # entities), on the annotations of the training set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "Using the same notation defined previously, we match query spans to entities using the following scoring function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "The function f produces the output vector using weights learned during training with ADAM optimization and categorical cross-entropy loss. The output vector is processed by the softmax function to provide our class (entity) probabilities. We train for 100 epochs, with patience limit of 10, using a batch size of 64.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "Combining String and Contextual Matching. We effectively combine our matchers using the following straightforward ensembling method (see Sect. 5):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "scoreST R CT X(s, e) = max(scoreST R(s, e), scoreCT X(s, e)) (6) where scoreCT X can correspond either to score1NN or scoreCLF , depending on the configuration being tested. Still, this method exhibits an undesirable bias towards higher recall and lower precision. Therefore, we introduce a postprocessing (PP) step to minimize false positives. We train a Logistic Regression Binary Classifier, on the training set, using the following five features:",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 64,
                    "text": "(6)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "-max e\u2208E scoreST R(s, e) (max string).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "-max e\u2208E scoreCT X(s, e) (max context).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "-max e\u2208E (scoreST R(s, e), scoreCT X(s, e)) (max overall).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "-(max e\u2208E scoreST R(s, e) + max e\u2208E scoreCT X(s, e))/2 (average).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "-argmax e\u2208E scoreST R(s, e) == argmax e\u2208E scoreCT X(s, e) (agreement).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solution"
        },
        {
            "text": "Testing this classifier with different thresholds, we're able to determine the optimal thresholds for balanced Precision and Recall performance (see Fig. 2 ). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 149,
                    "end": 155,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Solution"
        },
        {
            "text": "We evaluate our solution on the test split of MedMentions st21pv, following the metrics described in the MedMentions paper [1] . We require that scoreST R, score1NN and scoreCLF have scores above 0.5 in order to reduce the incidence of false positives. In the event of ties, matches are sorted alphabetically.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 126,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation"
        },
        {
            "text": "Mention Recognition. As described in Sect. 4, our efforts on this task focus on experimenting with different NLMs specialized for the medical domain. On Table 1 we show that the various specialized NLMs we used to initialize the BiLSTM-CRF model produce comparable results on this task. Mention Linking. On Table 2 we present our results for Entity Linking using the predicted spans from the SciBERT (uncased) based NER model that performed best for Mention Recognition. These results show that our solution performs competitively, achieving state-of-the-art on CUI Linking, and comparable results to the state-of-the-art on STY Linking. Additionally, we also report performance using the different scoring functions covered in this paper. score1NN outperforms scoreCLF on CUI Linking, by a small margin, but on STY Linking scoreCLF substantially outperforms score1NN. We believe these differences can be explained from the fact that all STYs are represented in the training set, while the overlap between CUIs in the training and test sets is low. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 153,
                    "end": 160,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 307,
                    "end": 314,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Evaluation"
        },
        {
            "text": "A major issue in biomedical NLP or IR research is the fact that annotated datasets are scarce and expensive to collect. While that problem is unlikely to improve in the near future, this work has shown that there's still significant room for improvement by simply adapting existing approaches, such as endto-end neural models for NER or WSD, making them easier to integrate with previous works often unassociated to those approaches, such as ADM. From a more practical perspective, this work pushes the state-of-the-art on the new and challenging MedMentions dataset, while using a modular approach that can be further improved with the integration of additional IR methods in future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "MedMentions: a large biomedical corpus annotated with UMLS concepts",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mohan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "SciBERT: a pretrained language model for scientific text",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Bioinformatics",
            "volume": "36",
            "issn": "4",
            "pages": "1234--1240",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Extracting UMLS concepts from medical text using general and domain-specific deep learning models",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Nejadgholi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "C"
                    ],
                    "last": "Fraser",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "De Bruijn",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Laplante",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "Z"
                    ],
                    "last": "Abidine",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "The unified medical language system (UMLS): integrating biomedical terminology",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bodenreider",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Nucleic Acids Res",
            "volume": "32",
            "issn": "1",
            "pages": "267--270",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "ScispaCy: fast and robust models for biomedical natural language processing",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Neumann",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ammar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "TaggerOne: joint named entity recognition and normalization with semi-Markov models",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Leaman",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Bioinformatics",
            "volume": "32",
            "issn": "18",
            "pages": "2839--2846",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Language modelling makes sense: propagating representations through wordnet for full-coverage word sense disambiguation",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Loureiro",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Jorge",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Simple and efficient algorithm for approximate dictionary matching",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Okazaki",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tsujii",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "QuickUMLS: a fast, unsupervised approach for medical concept extraction",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Soldaini",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Goharian",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "MedIR Workshop, SIGIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Wide-scope biomedical named entity recognition and normalization with CRFs, fuzzy matching and character level modeling",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kaewphan",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hakala",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Miekka",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Salakoski",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ginter",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Database",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Left: Overview of our solution,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Performance variation on validation set of MedMentions st21pv (Left: STY, Right: CUI) using different thresholds on the decision filter. Vertical line (green) marks the threshold which resulted in the best balance between Precision and Recall (Color figure online).",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Mention recognition performance using different specialized NLMs.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Category Performance. Using gold spans to focus on linking performance, we notice 3 that types/categories such as 'T005-Virus' obtain 88 F1, while 'T022-Body System' obtains only 44 F1, on STY Linking. CUI Linking results show similar variations, although with a stronger tendency towards better performance on concepts belonging to narrower types (i.e. STYs encompassing fewer CUIs).",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Semantic Type (STY) and Concept (CUI) Linking performance comparison. \u2020 were produced using the same st21pv subset of UMLS release 2017 AA Active. -scoreST R CLF 59.23 67.81 63.23 40.70 59.59 48.37 -scoreST R CLF (PP, bal. thresh.) 63.13 63.69 63.41 48.43 50.07 49.24",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The research leading to these results has received funding from the European Union's Horizon 2020 -The EU Framework Programme for Research and Innovation 2014-2020, under grant agreement No. 733280.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}