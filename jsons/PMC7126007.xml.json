{
    "paper_id": "PMC7126007",
    "metadata": {
        "title": "A probabilistic approach to incorporating domain knowledge for closed-room people monitoring",
        "authors": [
            {
                "first": "Ji",
                "middle": [],
                "last": "Tao",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Yap-Peng",
                "middle": [],
                "last": "Tan",
                "suffix": "",
                "email": "eyptan@ntu.edu.sg",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "As automated video surveillance becomes increasingly important [9], [10], [18], we are endeavoring to develop a video-based system for closed-room people monitoring. In this paper, we present a promising system of our recent effort. The system consists of two modules: a video analysis module to detect and track people entering or leaving the only entrance/exit of a closed room (e.g., a research lab, class room or meeting room), and a people recognition module to correspond each observed person with a person previously entering the room or to identify him/her as a new person unseen before. The results obtained from the system allow the derivation of such useful information as to how long a person has stayed in the room, how many people are in the room during a certain period, and who they are. Potential applications of the system include, for example, identifying who could possibly be infected by a newly identified severe acute respiratory syndrome (SARS) victim [3], and characterizing the regular or abnormal human activities in a monitored work place.",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 66,
                    "mention": "[9]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 68,
                    "end": 72,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 74,
                    "end": 78,
                    "mention": "[18]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 976,
                    "end": 979,
                    "mention": "[3]",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Besides our proposed approach, there exist other solutions to the problems of interest. For example, biometrics have been successfully used for person recognition and very satisfactory results have been achieved. Representative work in biometric-based recognition ranges from fingerprint and iris identification to face and gait recognition [1], [4], [11], [20]. However, many of these methods involve intrusive data collection, i.e., require human proactive action and collaboration in the course of identification or authentication, and thus work mainly in well-controlled environments. Although gait recognition partly addresses this limitation by exploring human motion characteristics, gait information, by itself, has limited discriminating power and only works for people whose motion patterns have been well characterized. In another approach, Nakajima et al. [17] attempt to recognize people by color-based and shape-based low-level features with the use of multi-class support vector machine classifiers (SVMs), where the people under monitoring are restricted to their lab members, for whom a database for recognition can be constructed beforehand.",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 344,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 346,
                    "end": 349,
                    "mention": "[4]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 351,
                    "end": 355,
                    "mention": "[11]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 357,
                    "end": 361,
                    "mention": "[20]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 868,
                    "end": 872,
                    "mention": "[17]",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Regardless of the selection of feature representation, most of the existing approaches accomplish the recognition task based on some maximum likelihood classification rule [7], which is generally simple, effective and fast. Vega et al. [25] model people by their gait-based feature relational distribution and recognize a person by choosing the one with the smallest distribution distance from a test data set. McKenna et al. [14] attempt to recognize people after separation while tracking a group of people. They compare the color models of people after interaction with the models built before the interaction and correspond them to maximize a joint likelihood. It should be noted that these existing methods mainly perform recognition based on observations or features obtained at a single time instance or duration. The temporal correlation and constraint among observation sequence, however, are seldom utilized although they often exist in some specific contexts; for example, a person currently inside a room cannot be entering the room without first leaving it.",
            "cite_spans": [
                {
                    "start": 172,
                    "end": 175,
                    "mention": "[7]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 236,
                    "end": 240,
                    "mention": "[25]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 426,
                    "end": 430,
                    "mention": "[14]",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In comparison, we develop a systematic approach to exploit the inherent correlation and constraint among multiple people observations for improving the recognition accuracy. Specifically, an HMM-like, probabilistic inference framework is proposed to incorporate domain-specific knowledge (i.e., a closed-room with a single entrance/exit) for enhanced people recognition. To be unintrusive, we choose color histogram as the feature for people recognition for it can be easily and reliably extracted using a low-cost camera from a distance. Rather than using only a single observation, our method is capable of performing optimal recognition based on multiple observations acquired at different time instances to maximize a joint posterior probability. As a result, it can effectively enhance the limited discriminating power of low-level features, such as color histogram. Experimental results demonstrate that the proposed approach can obtain promising recognition rates, as compared with the existing maximum likelihood approaches.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this module, we extend the color-based people detection and tracking system developed in our previous work [13], [24]. To work in a more perceptually uniform color space, we choose the HSV (hue, saturation, and value) color space and represent the color of each pixel with a vector of three color attributes (H,S,V). To have a reference background for change detection, we use a background initialization scheme to construct the background model automatically from a scene that may contain moving objects or light flickers. Specifically, a pixel will not be deemed as a background pixel unless its color/intensity value remains stable for a sufficiently long period. The color attributes of each pixel during this period are then modeled with a multivariate Gaussian distribution as the pixel's background model.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 114,
                    "mention": "[13]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 116,
                    "end": 120,
                    "mention": "[24]",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "People detection & tracking ::: Video analysis",
            "ref_spans": []
        },
        {
            "text": "To accommodate scene change over time, we also update the background model adaptively based on the newly observed background pixels. The updating is carried out as follows:(1a)\u03bcx,t=(1-\u03bbx,t)\u03bcx,t-1+\u03bbx,tOx,\n(1b)\u03c3x,t2=(1-\u03bbx,t)\u03c3x,t-12+\u03bbx,t(Ox-\u03bcx,t)2,where x={H,S,V} and O=[OH,OS,OV]T denote the HSV values of each newly observed background pixel at time t, and \u03bbx,t is an updating parameter proportional to standard deviation \u03c3x,t-1. In our implementation, we set \u03bbx,t=0.5\u00d7\u03c3x,t-1, for the reason that the faster or larger a pixel changes, the faster its background model should be updated.",
            "cite_spans": [],
            "section": "People detection & tracking ::: Video analysis",
            "ref_spans": []
        },
        {
            "text": "Since the HSV chrominance information becomes less reliable when the scene is too dark, a value threshold ThrV and a saturation threshold ThrS are used to decide whether the observed hue information OH could be used for segmenting the foreground regions. (In our case, the foreground regions of interest are regions containing moving people.) Specifically, the following rules are applied to classify each pixel as either a background or a foreground pixel:\u2022If the hue information is reliable, the pixel is marked as a foreground pixel if |\u03bcH-OH|>3\u00d7\u03c3H; otherwise, it is marked as a background pixel.\u2022If the hue information is not reliable, the pixel is marked as a changed pixel if |\u03bcV-OV|>3\u00d7\u03c3V; otherwise, it is marked as a background pixel.Using the binary segmentation results, the module then localizes a moving person within a minimum bounding rectangle (MBR) and represents the person with a set of identity attributes, including a label, the size, and the centroid of the MBR.",
            "cite_spans": [],
            "section": "People detection & tracking ::: Video analysis",
            "ref_spans": []
        },
        {
            "text": "For people tracking, we extend the continuously adaptive mean shift (CAMSHIFT) algorithm [2] to work with the density function of the binary segmentation results. The extended CAMSHIFT algorithm searches within the current frame for the nearest foreground region of each moving person tracked in the previous frame. This is because each moving person in the current frame cannot move far away from his/her location in the previous frame in a video sequence of high frame rate\u2014an assumption that holds for many general applications.",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 92,
                    "mention": "[2]",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "People detection & tracking ::: Video analysis",
            "ref_spans": []
        },
        {
            "text": "During tracking, color histogram, a computationally easy color feature that is known to be rather invariant to changes of shapes and sizes [23], is acquired to represent the color appearance of each person. In our implementation, we quantize the coordinates of HSV color space uniformly into 12(hue), 4(saturation) and 4(value) bins, respectively, resulting in a total of 192 quantized colors. To account for varying person size, the color histogram Hp(k) for each tracked person p is normalized by the total number of foreground pixels of the person. Furthermore, to accommodate changes due to different lighting condition or camera view angle, the color histogram is updated as follows:(2)Hpt(k)=(1-l)\u00b7Hpt-1(k)+l\u00b7Hpnew(k),1\u2a7dk\u2a7d192,where l is a learning parameter, and Hpnew(k) is the color histogram obtained from the new frame acquired at time t. Note that as the color histogram of a tracked person could be severely affected by other pixels, such as the shadow pixels or noise clutters due to occlusion, we also make use of a shadow-removal scheme in the proposed system to enhance the quality of each extracted color histogram [24].",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 143,
                    "mention": "[23]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1132,
                    "end": 1136,
                    "mention": "[24]",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "People detection & tracking ::: Video analysis",
            "ref_spans": []
        },
        {
            "text": "The information acquired during the tracking of a person, including the size variation and motion trajectory of a tracked person, is then used to judge whether the person is entering or leaving a room as seen by the camera (Figs. 1\nand 2\n).",
            "cite_spans": [],
            "section": "People detection & tracking ::: Video analysis",
            "ref_spans": [
                {
                    "start": 224,
                    "end": 231,
                    "mention": "Figs. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 236,
                    "end": 237,
                    "mention": "2",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "Color histogram intersection has been widely used as a similarity measure for image recognition and retrieval [6], [8], [15]; however, direct application of this measure to gauge the likeness of people in our work poses some potential problems. For example, color histogram intersection of two different people is generally larger than zero for some of their appearance features, such as hair and skin, could share similar colors. On the other hand, the same person observed at different time instances may not have identical color histograms due to the differences in lighting conditions, camera view angles, and segmentation results. A function is therefore required to map the value of color histogram intersection of two observed people ci and cj, defined as I(Hi,Hj)=\u2211k=1192min(Hi(k),Hj(k)), onto a similarity probability, P(ci\u223ccj), indicating whether ci and cj correspond to the same person.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 113,
                    "mention": "[6]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 115,
                    "end": 118,
                    "mention": "[8]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 120,
                    "end": 124,
                    "mention": "[15]",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Similarity measure ::: Video analysis",
            "ref_spans": []
        },
        {
            "text": "Conceivably, the function P(I) needs to possess the following properties: (1) it should be nondecreasing; (2) P(I) should be approaching 0 and 1 as I takes values near the ends of the interval [0, 1], respectively; (3) the transition of P(I) from a low value to a high value should take place at where the value of I becomes evident to support that the two observed people are identical. After some subjective studies and comparisons, we select the sigmoid function [12] to perform the required mapping, which is shown in Fig. 3\nand can be expressed as(3)P[ci\u223ccj]=11+exp[-\u03b1(I(Hi,Hj)-\u03b2)].\n",
            "cite_spans": [
                {
                    "start": 466,
                    "end": 470,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Similarity measure ::: Video analysis",
            "ref_spans": [
                {
                    "start": 522,
                    "end": 528,
                    "mention": "Fig. 3",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "There are two parameters determining the shape of a sigmoid function curve: \u03b1 controlling the steepness of the transition and \u03b2 defining the center of the transition point. To assign proper values to the two parameters \u03b1 and \u03b2, we have examined a large number of people segmentation results with different similarity degrees; some of them are indicated in Fig. 3 using the crosses, whose x-coordinates are color histogram intersection values and y-coordinates are the similarity degrees judged by human observers. We have found that setting the parameter pair (\u03b1,\u03b2) to (16, 0.55) can fit the curve well.",
            "cite_spans": [],
            "section": "Similarity measure ::: Video analysis",
            "ref_spans": [
                {
                    "start": 356,
                    "end": 362,
                    "mention": "Fig. 3",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "It should be noted that other features, such as face or gait features, for which a similarity measure is defined can be used in our proposed people monitoring system. To make the system less intrusive, we have only made use of color histogram in the work reported in this paper.",
            "cite_spans": [],
            "section": "Similarity measure ::: Video analysis",
            "ref_spans": []
        },
        {
            "text": "As hidden Markov models (HMMs) could be used to integrate prior knowledge and incomplete observation data to perform probabilistic reasoning [16], [22], [26], our first attempt is to establish a suitable HMM for the recognition task in our problem. In general, an HMM can be fully characterized by a set of parameters \u03bb={A,B,\u03c0}, where A denotes the state transition probabilities, B, the observation symbol probabilities, and C, initial state probabilities. These parameters are usually pre-learned from a set of representative data based on a fixed number of states. This situation, however, is not applicable to our case, for the number of people as well as their activity patterns, i.e., the frequencies of entering and leaving, are generally different from place to place or time to time, and hard to estimate from prior data.",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 145,
                    "mention": "[16]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 147,
                    "end": 151,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 153,
                    "end": 157,
                    "mention": "[26]",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Problem formulation ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "To construct a framework that is well suited for the problem of our concern, we make use of the lattice structure and parameter setting of HMMs and formulate the problem of people recognition as follows. Assume that the room being monitored has only a single entrance/exit (referred to as a closed room) and is empty when the system is first activated. When a person is entering the room, we append a new state to the state set (database) to represent his/her identity; when a person leaves at time t, he/she will be recorded as a new observation Ot. Thus the states in the state set at time t correspond to the current people identities in the database (i.e., people that possibly stay in the room at time t), and are denoted as S(t)={S1\u22efSNt}. When an observation sequence O={O1\u22efOT} has been obtained over a period of time, the identities of the people leaving the room can be recognized from the state sets, S(1),S(2),\u2026, and S(T), recorded over the same period. Characterizing this reasoning framework with a time-variant parameter set \u03bb(t)={A(t),B(t),\u03c0(t)}, we can use the Viterbi algorithm [19] to find an optimal state sequence Q={q1\u22efqT} associated with the observation sequence to maximize a joint posterior probability P(Q,O|\u03bb(t)), so that each leaving person can correspond to one of those who are judged still inside the closed room.",
            "cite_spans": [
                {
                    "start": 1094,
                    "end": 1098,
                    "mention": "[19]",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Problem formulation ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "A recognition example of the proposed framework is illustrated in Fig. 4\n, where the optimal state sequence obtained is highlighted in bold (red). From this path,we can identify O1 as S1,O2 as S3, O3 as S1, etc. We can see that the framework has a lattice structure similar to HMMs; however, there are several key differences that distinguish our framework from conventional HMMs. First, and most importantly, the parameter set \u03bb(t) is time variant and needs to be derived at each observation time instance based on all the previous possible states and current observations rather than from some training data. Second, the number of states in our model is not fixed but can increase over time before a decision is made. Third, the states can be indefinite because more than one states could be associated with the same person, e.g., states S1 and S4 in the example shown in Fig. 4.",
            "cite_spans": [],
            "section": "Problem formulation ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 66,
                    "end": 72,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 874,
                    "end": 880,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "It should be noted that in our framework a state could represent the feature model and the identity of a person simultaneously. For clarity, we shall use si to denote a person's identity and Si his/her feature model.",
            "cite_spans": [],
            "section": "Problem formulation ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "It can be seen from our problem formulation that the main task in constructing the proposed framework lies in the estimation of the time-variant parameter set \u03bb(t). Once \u03bb(t) is known, we can find the optimal path indicating the recognized people by a \u201cturn-the-crank\u201d procedure given by the Viterbi algorithm [5]. Our solutions are given as follows.\u2022\nThe initial state distribution, \u03c0:According to the definition of the state set S(t)={S1\u22efSNt}, there are N1 states (people) in the database when the first observation of exit is made at time t=1. Without any other prior knowledge, we intuitively assume that everyone inside the room has an equal probability to leave the room. Hence,(4)\u03c0i=1N1,1\u2a7di\u2a7dN1.\n\u2022\nThe output probability of state\ni\nat\n \ntime\nt, bi(Ot):In analogy with the definition in HMMs, let bi(Ot) be the probability of observation Ot generated by state i. We regard this probability as how likely an observed Ot is due to person si, and simply approximate it by Eq. (3) using their feature similarity as(5)bi(Ot)=P[Ot\u223cSi],1\u2a7di\u2a7dNt.\n\u2022\nThe state transition probability, aij(t):Before proceeding to the next step, we define a set of probabilities between observation times t and t+1 as shown in Fig. 5\n. For concision, we shall refer to \u201cobservation time\u201d as \u201ctime\u201d hereafter without causing confusion. Let P[si,t+/-=1] be the probability of person si staying in the room at time instance t+/- (straightforwardly P[si,t+/-=0]=1-P[si,t+/-=1]), where t+ and t- denote the time instances right after and before the observation Ot being made. Let M be a likelihood matrix measuring the similarities between people entering between t and t+1 (i.e., SNt+1\u22efSNt+1) and the people staying in the room at time t (i.e., S1\u22efSNt), defined as(6)M={muv|muv=P[SNt+v\u223cSu]},1\u2a7du\u2a7dNt,1\u2a7dv\u2a7dNtt+1,where Nt is the number of states (people who possibly stay in the room) at time t and Ntt+1=Nt+1-Nt is the number of people entering between times t and t+1. With these auxiliary probabilities, we can now derive the transition probability using the following three steps:\n",
            "cite_spans": [
                {
                    "start": 310,
                    "end": 313,
                    "mention": "[5]",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 1202,
                    "end": 1208,
                    "mention": "Fig. 5",
                    "ref_id": "FIGREF6"
                }
            ]
        },
        {
            "text": "(i) Transition probability, aij(t):",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "The transition probability aij(t) measures the odds of person sj leaving the room at time t+1, given that person si left at time t, i.e., qt=si and qt+1=sj. Without other prior knowledge, it is reasonable to assume that the probability for sj to leave at time t+1 is proportional to that of his/her existence in the room at time instance t+1-. Conceivably, a person cannot leave a room if he/she is not in the room at all. Thus, the transition probability can be computed as(7)aij(t)=P[sj,t+1-=1|qt=si]\u2211jP[sj,t+1-=1|qt=si],1\u2a7di\u2a7dNt,1\u2a7dj\u2a7dNt+1,where the denominator is a normalization factor to make \u2211jaij(t)=1 for each i. The term in Eq. (7) can be further expanded for each particular state si^ by using Bayesian rules as(8)P[sj,t+1-=1|qt=si^]=\u2211allcondP[sj,t+1-=1|cond]\u00b7P[cond|qt=si^],where cond={s1,t+=\u03b81\u22efsNt,t+=\u03b8Nt} is one of the possible realizations of {s1,t+\u22efsNt,t+} over all \u03b8, and \u03b8i=1 or 0 represents the status of person si being in or out of the room. By assuming that the status of each person is statistically independent of the others, the second term on the right hand side of Eq. (8) can be expressed as(9)P[cond|qt=si^]=\u220fi=1NtP[si,t+=\u03b8i|qt=si^].\n",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "From Eqs. (7)\u2013(9), we can see that the transition probability ai^j(t) depends on two probabilities: P[sj,t+1-=1|cond] and P[si,t+=\u03b8i|qt=si^]; the former is the conditional odds of person sj staying in the room at time instance t+1- given the status of all the people observed up to time instance t+, and the latter is the probability of person si being status \u03b8i at time instance t+ given that person si^ leaves the room at time t. The calculation of these two probabilities is discussed below.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "(ii) Conditional probability, P[sj,t+1-=1|cond]:",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "We compute this conditional probability with the aid of the likelihood matrix M defined in Eq. (6). The specific calculation is given by(10)P[sj,t+1-=1|cond]=\u03b8j+\u2211vm\u02dcjv,1\u2a7dj\u2a7dNt,1-\u2211um\u02dcu(j-Nt),Nt\u2a7dj\u2a7dNt+1,where m\u02dcuv is the entry of a modified matrix M\u02dc which is derived from M to indicate how likely person sNt+v is in fact person su returning under a certain cond. We introduce the matrix M\u02dc in order to incorporate the domain knowledge in the context of the cond and improve our estimation. In particular, the value \u2211vm\u02dcjv can be considered as the gain of the existence odds for person sj due to newly entered people between times t and t+1. This gain, along with the existence odds of person sj at time instance t+, which is the binary value of \u03b8j, constitutes his/her existence odds at time instance t+1-. For those newly added states sj where Nt\u2a7dj\u2a7dNt+1, the value 1-\u2211um\u02dcu(j-Nt) can therefore be considered as the odds for person sj to exist as a new person. Note that in this framework we attempt to estimate these odds in an approximate manner, which is intuitive and has been found to be effective for improving the recognition rates.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "Specifically, m\u02dcuv is derived from muv as(11)m\u02dcuv=\u03b72D[\u03b5(\u03b8u)\u03c1(Su)muv],where \u03b72D is a 2-D normalization operation to be discussed later and(12)\u03b5(\u03b8u)=0if\u03b8u=1incond,1otherwise,\n(13)\u03c1(Su)=0ifSu\u2209PT(Sk,t),1otherwise.\n\u03b5(\u03b8u) and \u03c1(Su) are two indication factors incorporating the domain knowledge of a particular cond. Specifically, \u03b5(\u03b8u) shows that it is impossible for one to enter a closed room if he/she is currently inside, while \u03c1(Su) accounts for the fact that a person staying in the room could not enter the room again if he/she has not been observed leaving the room. PT(Si^,t) is the most likely path ending in state Si^ at time t, i.e., the most probable state sequence that is obtained by the Viterbi algorithm and includes all the people that have been observed leaving the room up to time t.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "To illustrate it, we provide in the following a numerical example, in which five people enter a room with three of them leaving, forming a process of three observations as shown in Fig. 6\n.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 181,
                    "end": 187,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ]
        },
        {
            "text": "For the time pair {t,t+1} shown in Fig. 6, suppose that the likelihood matrix obtained by Eq. (6) is equal toM=s4s5s10.20.8s20.50.3s30.40.2.Consider one of the possible realizations of people's status at time instance t+:cond1={s1,t+=0, s2,t+=0, s3,t+=1}; that is, {\u03b81=0,\u03b82=0,\u03b83=1} or all people are outside the room at time instance t+ except for person s3. The conditional probabilities that need to be computed are P[sj,t+1-=1|cond1],1\u2a7dj\u2a7d5.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 35,
                    "end": 41,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ]
        },
        {
            "text": "First, we incorporate the knowledge of the people status into the likelihood matrix M by using the two indication factors. From Eq. (12), \u03b5(\u03b83)=0 and the third row of M is set to zero. This is because if s3 is known to be inside the room, then neither s4 nor s5 can be s3 regardless of their similarities. The second indication factor relies on the most likely path ending in the state for which the transition probability needs to be calculated. Assume that S1 at time t is the state and the most likely path PT(S1,t) is as shown in Fig. 6. It can be seen that S3 is not on this optimal path, which means person s3 has not left the room since he/she entered. So none of s4 and s5 could be s3 and \u03c1(S3)=0 according to Eq. (13). With this domain knowledge, the original likelihood matrix M can be modified asM\u2032=s4s5s10.20.8s20.50.3s300.In this example, the two indication factors affect the same row of the likelihood matrix M. In general, the adjustments could be made on different rows depending on the cond as well as the state chosen for evaluating the transition probability.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 534,
                    "end": 540,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ]
        },
        {
            "text": "Next, the adjusted likelihood matrix M\u2032 is normalized so that the summations of the likelihoods corresponding to all possible circumstances are equal to one. To do this, we introduce a normalization operation, referred to as the correlated normalization operation or \u03b7 operation, which works as follows. Consider a person T and a group of candidates C consisting of N people, and define the similarity measure wn=P[Cn\u223cT]. Under the constraint that at most one person (or none) of C could be T, we can obtain the following new similarity measures(14)P[Cn\u223cT|constr]\u221dwn\u00b7\u220fj\u2260n(1-wj),1\u2a7dn\u2a7dN,\n(15)P[T\u2209C|constr]\u221d\u220fi=1N(1-wi).To make the probabilities corresponding to all possible circumstances sum up to one, the \u03b7 operation is defined to normalize the above new similarity measures as(16)\u03b7[wn]=wn\u00b7\u220fj\u2260n(1-wj)\u2211i=1Nwi\u220fj\u2260i(1-wj)+\u220fi=1N(1-wi),1\u2a7dn\u2a7dN.This normalization operation aims to correlate likelihoods that are measured independently by imposing the constraint mentioned before Eq. (14). It should be noted that this constraint can be applied to the adjusted likelihood matrix M\u2032 both row-wise and column-wise. For instance, at most one of s4 and s5 (or none of them) could be s1, while s4 could be at most one of s1, s2, and s3 (or none of them, i.e., s4 is a new person) in the example shown in Fig. 6. For simplicity, we apply a 2-D \u03b7 operation (\u03b72D), a row-wise normalization followed by a column-wise normalization, to normalize the likelihood matrix M\u2032 and obtain M\u02dc=s4s5s10.050.72s20.400.05s300.\n",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 1289,
                    "end": 1295,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ]
        },
        {
            "text": "From this normalized likelihood matrix M\u02dc, we can obtain useful information such as the existence odds of s1 is increased by 0.05+0.72 = 0.77, the odds that s4 is a new person is 1-0.05-0.40-0=0.55, etc. Consequently, the existence odds at time instance t+1- under cond1 can be obtained using Eq. (10) asP[s1,t+1-=1|cond1]=0+0.05+0.72=0.77,P[s2,t+1-=1|cond1]=0+0.40+0.05=0.45,P[s3,t+1-=1|cond1]=1+0+0=1,P[s4,t+1-=1|cond1]=1-0.05-0.40-0=0.55,P[s5,t+1-=1|cond1]=1-0.72-0.05-0=0.23.Applying the same procedure to all possible cond's, we can obtain all the conditional probabilities P[sj,t+1-=1|cond] required in Eq. (8), one of the two prerequisites for computing the transition probability as discussed in step (i).",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "(iii) Probability: P[si,t+=1|qt=si^]\n",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "The remaining unknown for calculating the transition probability ai^j(t) is P[si,t+=1|qt=si^], which is the existence odds of person si at time instance t+ given that the person leaving at time t is si^. To illustrate how to compute this unknown, we shall extend the example of Fig. 6 to time t+1 and show instead how to compute the unknown P[sj,t+1+=1|qt+1=sj^] by making use of the probabilities obtained so far and assuming the full knowledge of P[si,t+=1|qt=s(j)^], where S(j)^ is the previous state of Sj^ in the most likely path PT(Sj^,t+1). By the same procedure, P[si,t+=1|qt=si^] can be similarly estimated based on P[sh,t-1+=1|qt-1=s(i)^]. Note that, h,i, and j are indices of the states at times t-1, t, and t+1, respectively.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 278,
                    "end": 284,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ]
        },
        {
            "text": "As shown in Fig. 7\n, assume that person s1 leaves the room both at time t and at time t+1, i.e., (j^)=1 and j^=1 (denoted by the shaded circles). We manifest the values of P[si,t+=1|qt=s1], which are assumed to be known, on the right hand side of the observation made at time t, and the computed values of P[sj,t+1-=1|qt=s1] on the left hand side of the observation made at time t+1. The estimation process of the probabilities on the right hand side of the observation Ot+1 and the final results are shown in Table 1\n.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 510,
                    "end": 517,
                    "mention": "Table 1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We first examine the gain of existence odds from time instance t+ to t+1- for each person: \u03b3(j)=P[sj,t+1-=1|qt=s1]-P[sj,t+=1|qt=s1]. Clearly, \u2211j\u03b3(j)=2 because the gain in existence odds is due to the two newly entered people. However, if we know for sure that s1 leaves the room at time t+1, then he/she must stay in the room at time instance t+1-, and thus, \u03b3(1) should be equal to 1 rather than 0.77. In other words, the gain for each person needs to be re-adjusted (\u03b3\u02dc in the table) to incorporate the knowledge of this new assumption to ensure that \u03b3\u02dc(1)=1. That \u03b3\u02dc(1) is equal to one can be also explained as follows: since s1 leaves at time t and time t+1 successively, one of the entering persons s4 and s5 must be s1. As there is no reason to favor any person other than s1, our approach is to increase s1's existence odds from 0.77 to 1 and decrease the others\u2019 proportionally. This is sensible as once we know that the person who leaves the room at time t+1 is s1, then s4 and s5 should be more likely to be s1 than what we originally estimate, and as a result, less likely to be the other people. At time instance t+1+, the existence odds of s1 becomes zero due to his exit, and the others\u2019 can be obtained as the summation of their existence odds at time instance t+(P[sj,t+=1|qt=s1]) and the re-adjusted gain (\u03b3\u02dc(j)).",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "The above analysis and derivation can be generalized into the formulas below:(17)P[Sj,t+1+=1|qt+1=Sj^]=0ifj=j^,P[Sj,t+=1|qt=S(j)^]+\u03b3\u02dc(j)otherwise,\n(18)\u03b3\u02dc(j)=1ifj=j^,\u03b3(j)1-1-P[Sj^,t+1-=1|qt=S(j)^]\u2211j\u2260j^\u03b3(j)otherwise,\n(19)\u03b3(j)=P[Sj,t+1-=1|qt=S(j^)]-P[Sj,t+=1|qt=S(j)^].It can be seen from these formulas that the estimation of existence odds is recursive; that is, one's existence odds at time instance t+1+ depends on that at t+. To initialize the estimation, we set P[si,1+=1|q1=si^]=1 for all i\u2260i^ and P[si^,1+=1|q1=si^]=0 if si^ is the first person leaving the room (i.e. q1=si^), since all the people who have entered the room, except for si^, should stay inside at time instance 1+.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "It should also be noted that although the derivation may appear somewhat complex, it leads to an important underlying property of the proposed framework: the summation of the existence odds of all states at any time instance t+/-, i.e., \u2211iP[si,t+/-=1|qt=sj] for any t and j, is always equal to the number of people who stay in the room at that time.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "Batch recognition of observations (people who have left the room) can be performed at any time when necessary by retrieving the state sequence with the maximum score of joint posterior probability as the optimal state sequence.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "To use the proposed framework for recognizing people re-entering the room is equivalent to finding and merging those states associated with a same person in the optimal state sequence. This can be accomplished by the following local maximum likelihood scheme. Let qt=Si and search backward in the optimal state sequence for qt\u2032=Si, where t\u2032\u2208{1,\u2026,t-1} and t-t\u2032 is minimized. If such qt\u2032 exists, the person sj^ will be assumed as the person si re-entering, where j^ is obtained as(20)j^=argmaxj\u2208S(t)\u29f9S(t\u2032)P[Sj\u223cSi],and S(t) is the state set at time t. Then, Sj^ should be merged into Si and removed from all the state sets containing it thereafter. The backward search is performed from t=2 to the end of the optimal state sequence until all the states possibly representing a same person are disclosed and merged.",
            "cite_spans": [],
            "section": "Framework construction ::: Probabilistic reasoning framework",
            "ref_spans": []
        },
        {
            "text": "In this case, we assume that three people enter a room initially and then leave successively without any other new entries among them. The model established is shown in Fig. 8\n, where the likelihood matrices M are null as nobody enters between two observations.",
            "cite_spans": [],
            "section": "Case 1: Successive exits ::: Case studies",
            "ref_spans": [
                {
                    "start": 169,
                    "end": 175,
                    "mention": "Fig. 8",
                    "ref_id": "FIGREF9"
                }
            ]
        },
        {
            "text": "The transition matrix is first computed at time t=1 by using Eqs. (7)\u2013(19) derived in Section 3.2 and obtained as A(t=1)=01/21/21/201/21/21/20.The diagonal entry aii(t=1) is zero, which is in accordance with the intuitive analysis that if si leaves at time t=1 and nobody enters before the next observation time t=2, si could not leave at time t=2 again.",
            "cite_spans": [],
            "section": "Case 1: Successive exits ::: Case studies",
            "ref_spans": []
        },
        {
            "text": "Moving on to the next observation time t=2, we further assume that the previous states of S1, S2 and S3 in their own most likely paths are S2, S3 and S1 at time t=1, respectively, which can be obtained by using the Viterbi algorithm based on P[O1\u223cSi] and A(t=1). Hence, the transition matrix at time t=2 can be computed as A(t=2)=001100010.That means, from time t=2 to time t=3, each state has only one possible choice of transition. Because, for instance, if s1 leaves at time t=2 and its previous state s2 has already left at time t=1, the person who leaves the room at time t=3 must be s3. More specifically, to select which state at time t depends on not only the states at the previous time t-1 but also those prior to t-1.",
            "cite_spans": [],
            "section": "Case 1: Successive exits ::: Case studies",
            "ref_spans": []
        },
        {
            "text": "In this case, we shall assume that the ideal similarity measure between two people can be a binary value of either one (identical) or zero (distinct). In Fig. 9\n, two people enter first and one of them leaves afterward. After that one person enters again followed by one exit. We suppose that S3 looks the same as S1 but has different appearance from S2, i.e. P[S3\u223cS1]=1 and P[S3\u223cS2]=0 (likelihood matrix M=[10]T). The transition matrix can be computed as A(t=1)=1/21/201/201/2.\n",
            "cite_spans": [],
            "section": "Case 2: Ideal similarity measure ::: Case studies",
            "ref_spans": [
                {
                    "start": 154,
                    "end": 160,
                    "mention": "Fig. 9",
                    "ref_id": "FIGREF10"
                }
            ]
        },
        {
            "text": "We can interpret this process based on our general knowledge. If s1 leaves at time t=1, s3 will be viewed as s1 re-entering due to their same appearance. Therefore, at time instance t=2-, s1 and s2 must stay in the room while s3 cannot be a new person. s1 and s2 then take the equal probability to leave at time t=2, which is reflected in the transition matrix as {a11=a12=1/2,a13=0}. If s2 leaves at time t=1, it is interesting to see that s3 has to be a new person although he/she has the same appearance with S1 since s1 is already known to be inside. (One possibility is that s3 and s1 are identical twins wearing the same clothes.) In this case, s1 and s3 are equally possible to leave at time t=2 while s2 could not leave again. In the transition matrix, it is shown as {a21=a23=1/2,a22=0}. Once again, the proposed model obtains the same result as that derived by the intuitive analysis based on our general knowledge.",
            "cite_spans": [],
            "section": "Case 2: Ideal similarity measure ::: Case studies",
            "ref_spans": []
        },
        {
            "text": "We have tested the proposed people monitoring system with two videos acquired in a research laboratory using a camera to monitor the lab's only entrance. During 1-h monitoring period, video-I recorded eleven different people who were unaware of the experiment, of which four entered and left twice, another four entered and left only once, and three entered without leaving. Video-II simulated the process of people entering and leaving with the help of eight different students, among whom seven entered and left the lab for three times and another one twice. To further increase the test samples, a synthetic process generator is also designed to randomly re-arrange the entries and exits from the two videos based on the rule that one cannot enter unless he/she is outside the lab, and vice versa. This generator allows us to simulate a wide variety of combinations of entries and exits over time from the same group of people.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": []
        },
        {
            "text": "For comparison, we also implement a recognition approach based on maximum likelihood (ML) classification [14], [21] as follows. When a person is detected entering a closed room, he/she is compared with people who are in the system's database and currently labeled as \u2018out\u2019. If the maximum likelihood with respect to an \u2018out\u2019 person is larger than a threshold Tm (set to 0.7 in our implementation), they are considered the same person. Then, the observed person is labeled as \u2018in\u2019 and his/her corresponding color histogram in the database is updated with the latest one. Otherwise, the person is assumed to be new and then labeled as \u2018in\u2019 with a new identity label. When there are multiple exits without entries among them, the leaving people are recognized from the people with label \u2018in\u2019 by maximizing a joint likelihood.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 109,
                    "mention": "[14]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 111,
                    "end": 115,
                    "mention": "[21]",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Experimental results",
            "ref_spans": []
        },
        {
            "text": "To ensure that the system can perform as desired, we assume that each person has consistent color appearance (both frontal and back views) and it remains the same during the period of observation. This is because our proposed system uses only the color appearance of each person to perform recognition and the camera looks only at a fixed direction. This limitation can likely be circumvented by incorporating other possible features, such as face and thermal infrared imagery, or using more than one camera looking at different directions. We consider this as a potential future research direction.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": []
        },
        {
            "text": "\nFig. 10\nshows the sample images of the eight people (P1\u2013P8) recorded in video-II after foreground segmentation. Table 2\nshows an example sequence of entries and exits obtained by the synthetic process generator based on these eight people, where \u2018I\u2019 and \u2018O\u2019 represents in (enter) and out (exit), respectively. A total of 44 entries and exits are observed, among them 22 are entering while the other 22 are exiting.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": [
                {
                    "start": 1,
                    "end": 8,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 113,
                    "end": 120,
                    "mention": "Table 2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "The recognition results of the eight people (at observation time) are given in Table 3\n. The results obtained by our proposed approach and the ML approach are compared with the ground truth. In this particular process, our approach achieves 100 percent recognition rates, outperforming the ML approach. In the ML approach, the system wrongly recognizes P8 as P1 at time 7 and reversely at time 19. Furthermore, P7 is incorrectly identified as P3 at time 18 and on the contrary at time 20. The errors of recognizing different people as the same one are mainly due to that the similarities between the color histograms of these different people exceed the preset threshold Tm. On the contrary, the reason for the errors at time 10, 13 and 15, where P6 and P7 are identified as new people when they are in fact just re-entering, is that the similarities of their color histograms observed at different times are lower than Tm. In other words, the results of the ML approach are rather sensitive to the threshold Tm, inappropriate selection of which often causes such false recognitions as mentioned above. In contrast, our proposed approach benefits from using a threshold-free scheme; therefore, it is more robust to less-than-perfect segmentation results as well as changes in lighting conditions or view angles.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": [
                {
                    "start": 79,
                    "end": 86,
                    "mention": "Table 3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "At each observation time, there is a most likely path ending in each individual state with the probability of \u03b4t(i), which can be iteratively computed as \u03b4t+1(j)=[maxj\u03b4t(i)aij]bj(Ot+1) using the Viterbi algorithm. To make a recognition decision at time t is to choose from all the most likely paths the one with the largest value of \u03b4t(i). Hence, a confidence index can be defined in the range of [0, 1] to evaluate the reliability of a decision made at each time(21)Conf(t)=max(\u03b4t(i))\u2211i\u03b4t(i).The variation of the confidence index over time for the above example is shown in Fig. 11\n.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": [
                {
                    "start": 575,
                    "end": 582,
                    "mention": "Fig. 11",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "At the early monitoring stage, the system has only few choices for making decision so that the confidence index is usually high. With the increase of the state number, i.e., more possible paths to choose from, the reliability of a decision may decrease. However, the advantage of our approach lies in that it is capable of maintaining the decision reliability at a later observation time by collectively considering all the available observations. In Fig. 11, when the confidence index is lower than 0.8, the ML approach is likely to make a wrong decision at the corresponding time (as indicated by the (red) circles). Meanwhile, the proposed approach can still find the right choice since the probabilities (or scores) of the other paths are even lower. Note that as the recognition decision made at time t depends upon all the observations (not the decisions) obtained prior to time t in a sense of maximizing the joint posterior probability of multiple observations\u2014a merit of the Viterbi algorithm\u2014our proposed system can minimize the chance of error propagation. However, if there is a substantial change in the lighting condition or a person changes his/her color appearance (e.g., clothes) during the period of observation, the proposed framework may generate fallacious state set and degrade the recognition performance.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": [
                {
                    "start": 451,
                    "end": 458,
                    "mention": "Fig. 11",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Overall, 20 synthesized processes of entries and exits are generated from each of the two test videos. We set Tm to 0.7 to obtain the best results of the ML approach. The ground truth is obtained manually. It is evident from Table 4\nthat our proposed approach can improve the recognition rates notably.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": [
                {
                    "start": 225,
                    "end": 232,
                    "mention": "Table 4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "It should also be noted that the computational complexity of our algorithm may increase rapidly when more states are generated. However, it is possible for one to make a definite decision when the confidence index is sufficiently high so that states associated with the same person can be merged to reduce the total number of the states, e.g., at time 16 in Fig. 11. We shall investigate this in our future work.",
            "cite_spans": [],
            "section": "Experimental results",
            "ref_spans": [
                {
                    "start": 358,
                    "end": 365,
                    "mention": "Fig. 11",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "We have presented in this paper a novel probabilistic reasoning framework for people monitoring in a closed environment. The main contribution of our work lies in the development of a systematic way to exploit the temporal correlation and domain constraint of multiple people observations for improving the recognition accuracy compared with that of conventional maximum-likelihood approaches. Rather than identifying each single observation from a database, the framework is devised to recognize people based on multiple observations acquired at different time instances. Experimental results demonstrate that the proposed approach outperforms the conventional maximum-likelihood approach when using the same feature and being tested with the same test videos.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "It should also be noted that although we focus mainly on a system suitable for closed-room monitoring, we expect such correlation and constraint to be present in many application scenarios, e.g., an interconnected network of cameras monitoring multiple entrances/exits or adjacent areas of a large site, and our proposed approach can be readily extended to these applications.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table\u00a01: The estimation of the existence odds\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table\u00a02: An example sequence of entries (I) and exits (O) produced by the synthetic process generator using the eight people observed in video-II\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table\u00a03: Comparison of the recognition results obtained by the maximum likelihood (ML) approach and our proposed approach\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table\u00a04: Comparison of the recognition rates obtained by the maximum likelihood (ML) approach and our proposed approach\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig.\u00a01: System overview with two sample frames illustrating a person entering and leaving a closed room with a single entrance/exit.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig.\u00a010: The segmentation results of eight different people recorded in video-II, with their entering (frontal) images in the first row and leaving (backward) ones in the second row.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig.\u00a011: The confidence index over decision time.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig.\u00a02: Sample images of the people detection and tracking results: (a) a person is leaving the lab, and (b) another person is entering the lab.",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig.\u00a03: The Sigmoid function to map color histogram intersection onto the similarity probability of two observed people.",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Fig.\u00a04: A recognition example of the proposed probabilistic reasoning framework.",
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Fig.\u00a05: The likelihood matrix M between times t and t+1 and the probability of the existence of a person associated with a certain state at time instances t+ and t+1-.",
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Fig.\u00a06: An example illustrating the estimation of conditional probability.",
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Fig.\u00a07: An example to illustrate the estimation of existence odds.",
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Fig.\u00a08: A model illustrating the case in which three people enter and leave a closed room successively, where the dash arrows indicate the transitions that are not allowed.",
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Fig.\u00a09: A model illustrating the case in which S3 has perfect similarity with S1 and S2.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [
                {
                    "first": "B.",
                    "middle": [],
                    "last": "Bhanu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ju Han",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the Workshop on Motion and Video Computing",
            "volume": "",
            "issn": "",
            "pages": "145-150",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "W4: Real time surveillance of people and their activities",
            "authors": [
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Haritaoglu",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Harwood",
                    "suffix": ""
                },
                {
                    "first": "L.S.",
                    "middle": [],
                    "last": "Davis",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "22",
            "issn": "8",
            "pages": "809-830",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Kale",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Rajagopalan",
                    "suffix": ""
                },
                {
                    "first": "N.",
                    "middle": [],
                    "last": "Cuntoor",
                    "suffix": ""
                },
                {
                    "first": "V.",
                    "middle": [],
                    "last": "Kruger",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Proceedings of the ICFGR",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [
                {
                    "first": "B.L.",
                    "middle": [],
                    "last": "Kalman",
                    "suffix": ""
                },
                {
                    "first": "S.C.",
                    "middle": [],
                    "last": "Kwasny",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "vol. 4",
            "issn": "",
            "pages": "7-11",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Y.-P.",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "vol. 2",
            "issn": "",
            "pages": "137-140",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Tracking groups of people",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "McKenna",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Comput. Vision Image Understanding",
            "volume": "80",
            "issn": "",
            "pages": "42-56",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [
                {
                    "first": "F.",
                    "middle": [],
                    "last": "Moreno",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Tarrida",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Andrade-Cetto",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Sanfeliu",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the 16th International Conference on Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "368-371",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [
                {
                    "first": "V.",
                    "middle": [],
                    "last": "Nair",
                    "suffix": ""
                },
                {
                    "first": "J.J.",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the 15th Vision Interface Conference",
            "volume": "",
            "issn": "",
            "pages": "88-92",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "A bayesian computer vision system for modeling human interactions",
            "authors": [
                {
                    "first": "N.M.",
                    "middle": [],
                    "last": "Oliver",
                    "suffix": ""
                },
                {
                    "first": "B.",
                    "middle": [],
                    "last": "Rosario",
                    "suffix": ""
                },
                {
                    "first": "A.P.",
                    "middle": [],
                    "last": "Pentland",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "22",
            "issn": "8",
            "pages": "831-843",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "A tutorial on hidden Markov models and selected applications in speech recognition",
            "authors": [
                {
                    "first": "L.R.",
                    "middle": [],
                    "last": "Rabiner",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Proc. IEEE",
            "volume": "77",
            "issn": "22",
            "pages": "257-285",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Evaluation of automated biometrics-based identification and verification systems",
            "authors": [
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Surette",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Khanna",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Proc. IEEE",
            "volume": "85",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [
                {
                    "first": "J.M.",
                    "middle": [],
                    "last": "Siskind",
                    "suffix": ""
                },
                {
                    "first": "Q.",
                    "middle": [],
                    "last": "Morris",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Proceedings of ECCV",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [
                {
                    "first": "T.E.",
                    "middle": [],
                    "last": "Starner",
                    "suffix": ""
                },
                {
                    "first": "A.P.",
                    "middle": [],
                    "last": "Pentland",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Proceedings of the International Workshop on Automatic Face- and Gesture-Recognition",
            "volume": "",
            "issn": "",
            "pages": "189-194",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "Y.-P.",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the Fourth International Conference on Information, Communications & Signal Processing and Fourth Pacific-Rim Conference on Multimedia",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Statistical motion model based on the change of feature relationships: human gait-based recognition",
            "authors": [
                {
                    "first": "I.R.",
                    "middle": [],
                    "last": "Vega",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Sarkar",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "25",
            "issn": "10",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "Human action learning via hidden Markov model",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "S.C.",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Systems Man Cybern.",
            "volume": "27",
            "issn": "1",
            "pages": "34-44",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "Human and machine recognition of faces: a survey",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                },
                {
                    "first": "C.L.",
                    "middle": [],
                    "last": "Wilson",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Sirohey",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE",
            "volume": "83",
            "issn": "5",
            "pages": "705-741",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Church",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "",
            "volume": "vol. 2",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "An efficient color representation for image retrieval",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "B.S.",
                    "middle": [],
                    "last": "Manjunath",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Kenney",
                    "suffix": ""
                },
                {
                    "first": "M.S.",
                    "middle": [],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Image Process.",
            "volume": "10",
            "issn": "1",
            "pages": "140-147",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Duda",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Hart",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Stork",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Robust color histogram descriptors for video segment retrieval and identification",
            "authors": [
                {
                    "first": "A.M.",
                    "middle": [],
                    "last": "Ferman",
                    "suffix": ""
                },
                {
                    "first": "A.M.",
                    "middle": [],
                    "last": "Tekalp",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Mehrotra",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Image Process.",
            "volume": "11",
            "issn": "5",
            "pages": "497-508",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "Object recognition and tracking for remote video surveillance",
            "authors": [
                {
                    "first": "G.L.",
                    "middle": [],
                    "last": "Foresti",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Circuits Systems Video Technol.",
            "volume": "9",
            "issn": "7",
            "pages": "1045-1062",
            "other_ids": {
                "DOI": []
            }
        }
    }
}