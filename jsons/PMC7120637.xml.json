{
    "paper_id": "PMC7120637",
    "metadata": {
        "title": "News Timeline Generation: Accounting for Structural Aspects and Temporal Nature of News Stream",
        "authors": [
            {
                "first": "Leonid",
                "middle": [],
                "last": "Kalinichenko",
                "suffix": "",
                "email": "leonidandk@gmail.com",
                "affiliation": {}
            },
            {
                "first": "Yannis",
                "middle": [],
                "last": "Manolopoulos",
                "suffix": "",
                "email": "manolopo@csd.auth.gr",
                "affiliation": {}
            },
            {
                "first": "Oleg",
                "middle": [],
                "last": "Malkov",
                "suffix": "",
                "email": "malkov@inasan.ru",
                "affiliation": {}
            },
            {
                "first": "Nikolay",
                "middle": [],
                "last": "Skvortsov",
                "suffix": "",
                "email": "nskv@mail.ru",
                "affiliation": {}
            },
            {
                "first": "Sergey",
                "middle": [],
                "last": "Stupnikov",
                "suffix": "",
                "email": "sstupnikov@ipiran.ru",
                "affiliation": {}
            },
            {
                "first": "Vladimir",
                "middle": [],
                "last": "Sukhomlin",
                "suffix": "",
                "email": "sukhomlin@gmail.com",
                "affiliation": {}
            },
            {
                "first": "Mikhail",
                "middle": [],
                "last": "Tikhomirov",
                "suffix": "",
                "email": "tikhomirov.mm@gmail.com",
                "affiliation": {}
            },
            {
                "first": "Boris",
                "middle": [],
                "last": "Dobrov",
                "suffix": "",
                "email": "dobrov_bv@srcc.msu.ru",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Due to the explosive growth of the amount of content on the Internet, the problems of extraction and automatically summarizing useful information in the incoming data stream arises. One of such problems is the summarization of news articles on an event. The news story - is a set of news reports from various sources dedicated to describing an event. Such problems are often investigated and solved by news aggregators, for example, Google.News1 [17] or Yandex.News.2 This is due to the fact that to work with such problems the researcher needs a huge and diverse collection of news articles.",
            "cite_spans": [
                {
                    "start": 447,
                    "end": 449,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The typical \u201clifetime\u201d of the news story (the time of active discussion of the event) is usually a day or two, but not all events are so short. Some news stories have a \u201chistory\u201d in the form of a set of previous events that occurred at different moments and are more or less related to each other. Existing multi-document summarization approaches do not take into account the fact that the context, actors, geography and other event properties can vary over time.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The fact that journalists are returning to the same events, for example, with the appearance of new data, indicates that such events are important for the society. The need for a brief summary of the event raises the problem of forming a \u201ctimeline summary\u201d. Timeline summary is a type of multi-document summary, containing the essential details of the subject matter under discussion. The construction of such annotations is a complex task, performed by journalists or analysts manually. This implies that the automation of such a process is a urgent problem.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper we consider challenges and solutions for the automatic generation of temporal summaries. We consider this problem as a multi-document summarization on a query over a representative collection of news documents. The query in this case is the text of the news message. The situation corresponds to the scenario when a user would like to receive a timeline summary after reading the news document. The result should be a time-ordered list of descriptions of the key sub-events related to main event. The result consists of parts of existing sentences, since our solution refers to extractive summarization approaches.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A system was developed to automate the timeline summarization process. Experiments were conducted over a collection of 2 million Russian news for the first half of 2015. Three new factors were investigated to improve the results of constructing a timeline summary: query extension using pseudo-relevance feedback, accounting for the timing characteristics of news stories and the structure of the inverted pyramid.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "This is a follow-up study of timeline summarization problem reported in previous paper [25]. In this study, we expanded the collection of standard annotations three-fold. The evaluation process was improved by dividing the collection into a training and test parts. An optimization module was added for fitting the configurations. As a result, substantial progress was achieved. Taking into account the structure of the inverted pyramid showed a significant increase in the values of metrics, which was not achieved in the previous article.",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 90,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Currently, there are quite a number of methods for automatic text summarization [3]. Some methods that use large linguistic ontologies [12, 15], that may be automatically supplemented during the analysis. Other methods are based on the statistical properties of texts [16] or machine learning [13].",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 82,
                    "mention": "3",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 136,
                    "end": 138,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 140,
                    "end": 142,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 269,
                    "end": 271,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 294,
                    "end": 296,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Automatic Text Summarization Problem ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "During the generation of the annotations, the following problems occur [3, 7, 11]:Ensuring the completeness of the presentation of information, including the most up-to-date information.Decreasing of redundancy in the information provided.Ensuring the coherence and understandability of the information provided.\nTo ensure the completeness of the resulting annotation, it is often necessary to find links between sentences or documents [20].",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 73,
                    "mention": "3",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 75,
                    "end": 76,
                    "mention": "7",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 78,
                    "end": 80,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 437,
                    "end": 439,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Automatic Text Summarization Problem ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "To determine the redundancy in the generated annotations, various measures of similarity between sentences are used. One of the most common approaches is clustering - the selection of content groups of sentences [6]. Another approach to reduce redundancy is to compare candidate sentence with sentences that have already been included in the summary and to evaluate novel information. Example of such approach is the Maximal Marginal Relevance (MMR) [2].",
            "cite_spans": [
                {
                    "start": 213,
                    "end": 214,
                    "mention": "6",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 451,
                    "end": 452,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Automatic Text Summarization Problem ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "The problem of ensuring the coherence of information in the summary arises both in the methods of generating the annotation [18, 19], and in the methods of evaluation, because in order to assess the connectivity and linguistic qualities of the annotation, it is necessary to perform a manual evaluation.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 127,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 129,
                    "end": 131,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Automatic Text Summarization Problem ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "The problem of timeline summary construction has a number of differences from the standard summarization problem. For example, the temporal nature of events must be taken into account [9]. Also, to ensure completeness of the information provided, it is required to find documents from all sub-events of the topic under consideration.",
            "cite_spans": [
                {
                    "start": 185,
                    "end": 186,
                    "mention": "9",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Timeline Summary ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "When constructing timeline summary, data processing is mainly carried out over huge collections. In such collections, most of the information is not relevant to the user\u2019s request. This problem can be solved by using clustering methods [10, 14]. But the clustering methods have some issues. First, such a task should be solved many times over huge collections of documents, which affects the response time of the system. Secondly, the degree of closeness can be significantly smaller with standard measures of similarity for documents that describe far-in-time but related events. And, of course, it is required to identify the most characteristic objects [1, 9], for example, taking into account the structural features of the flow of documents [5, 8].",
            "cite_spans": [
                {
                    "start": 237,
                    "end": 239,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 241,
                    "end": 243,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 657,
                    "end": 658,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 660,
                    "end": 661,
                    "mention": "9",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 747,
                    "end": 748,
                    "mention": "5",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 750,
                    "end": 751,
                    "mention": "8",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Timeline Summary ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "The problem of constructing a timeline summary is a query-oriented. In the most general case, the user has a news document as a query. So further this problem will be considered as a problem of automatic creation of a summary on a query in the form of a text document. The output of the system is an annotation of n sentences. The connectivity between the sentences in this paper is not required. Figure 1 provides an example of a possible summary about the conflict on cemetery taken from the Interfax website.3\n\n",
            "cite_spans": [],
            "section": "General Description ::: Statement of the Problem",
            "ref_spans": [
                {
                    "start": 404,
                    "end": 405,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The aim of the work is to study the influence of various factors on the quality of the annotation.",
            "cite_spans": [],
            "section": "General Description ::: Statement of the Problem",
            "ref_spans": []
        },
        {
            "text": "The problem described above can be formalized in the following way. Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Q = \\left\\{ {q_{1} , q_{2} , \\ldots , q_{m} } \\right\\} $$\\end{document} be a set of queries and an associated set of reference annotations \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D_{g} = \\left\\{ {D_{g}^{{q_{1} }} , D_{g}^{{q_{2} }} , \\ldots , D_{g}^{{q_{m} }} } \\right\\} $$\\end{document} be an associated set of reference annotations. The system generates a set of summary \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D_{A} = \\left\\{ {D_{A}^{{q_{1} }} , D_{A}^{{q_{2} }} , \\ldots , D_{A}^{{q_{m} }} } \\right\\} $$\\end{document} in response to queries \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Q $$\\end{document} by algorithm \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ A $$\\end{document}. Then the problem is reduced to maximizing the following functional:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\frac{{\\mathop \\sum \\nolimits_{i = 1}^{i = \\left| Q \\right|} M\\left( {D_{A}^{{q_{i} }} , D_{g}^{{q_{i} }} } \\right)}}{\\left| Q \\right|} \\to max $$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ M $$\\end{document} is the proximity function between the annotations. Optimization is carried out for all parameters of the algorithm.",
            "cite_spans": [],
            "section": "Mathematical Statement of the Problem ::: Statement of the Problem",
            "ref_spans": []
        },
        {
            "text": "As mentioned earlier, the input collection contains 2 million news articles. It is not possible to work directly with such amount of information, therefore, it was decided to interact with the collection through a search engine. Search engine allows:Get a list of documents by text request.For a given document from collection, get the basic information: text, index, meta-information.\n",
            "cite_spans": [],
            "section": "Collection Processing ::: Approach",
            "ref_spans": []
        },
        {
            "text": "In this paper the following factors were investigated:Query extension strategy.Accounting for the temporal nature of news stories.Accounting for the structure of a news article in the form of an inverted pyramid.\n",
            "cite_spans": [],
            "section": "Studied Features ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Information that can be obtained from a query document is basically not enough to effectively build this type of annotation. This fact is a consequence of the fact that most news articles are not a general description of the event, but a discussion of some particular incident or fact.",
            "cite_spans": [],
            "section": "Query Extension Strategy ::: Approach",
            "ref_spans": []
        },
        {
            "text": "To avoid this problem, it is necessary to use the query extension techniques. The developed algorithm uses the idea of pseudo-relevance feedback, which is widely used in information retrieval problems [21]. For the query-document, the algorithm includes the following steps:The most significant K-terms are chosen on the basis of tf-idf weights forming thus the first-level query.Further on the basis of the first-level query documents are retrieved.The extracted cluster of documents is analyzed to find the most important terms forming thus the second-level query:\nFor each document, the most significant \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ T $$\\end{document} terms are considered.For each term, it is calculated how often it was in the top \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ T $$\\end{document} terms on all cluster.The list of terms is sorted by frequency, the best \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ M $$\\end{document} terms are selected.\nSteps 2\u20133 are repeated (A double query extension process that forms a third-level query).Output of the algorithm is a vector of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ N $$\\end{document} terms representing to some extent the semantics of the input document.\nNote that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ K, T, M, N $$\\end{document} are parameters of the algorithm and they must be configured. As an example of the work of the query extension module, consider the algorithm steps on a news article about the terrorist attack in Paris (Table 1).\n",
            "cite_spans": [
                {
                    "start": 202,
                    "end": 204,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Query Extension Strategy ::: Approach",
            "ref_spans": [
                {
                    "start": 2651,
                    "end": 2652,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "The table shows that a higher-level query has more significant terms for this event.",
            "cite_spans": [],
            "section": "Query Extension Strategy ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Since any event depends on time, the content of publications and their number also depend on the time. As an example, Fig. 2 shows a graph of the time dependence of publications on the \u201cEarthquake in Nepal\u201d event.\n",
            "cite_spans": [],
            "section": "Temporal Nature of News Stories ::: Approach",
            "ref_spans": [
                {
                    "start": 123,
                    "end": 124,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "To take into account this factor, for the set \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D $$\\end{document} of extracted documents the following procedure is undertaken:The entire timeline of the event is divided into days with labels \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ T = \\left\\{ {t_{1} , t_{2} , \\ldots , t_{n} } \\right\\} $$\\end{document}.Each document receives a label from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ T $$\\end{document} based on the publication date \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D_{i}^{t} $$\\end{document}.Documents published on days with a number of publications less then the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ NDoc_{tr} $$\\end{document} threshold (2) are discarded.\n2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ NDoc_{tr} = 0.2*MEAN_{top\\,3} \\left( D \\right) $$\\end{document}\nThe output is a sorted list of collections \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ C = \\left\\{ {C_{{t_{1} }} ,  C_{{t_{2} }} , \\ldots , C_{{t_{n} }} } \\right\\} $$\\end{document}, where each collection \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ C_{{t_{i} }} $$\\end{document} contains only documents with the label \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ t_{\\text{i}} $$\\end{document}.\n",
            "cite_spans": [],
            "section": "Temporal Nature of News Stories ::: Approach",
            "ref_spans": []
        },
        {
            "text": "This feature is taken into account in the following way:For a set of documents \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D $$\\end{document}, a similarity matrix between the upper and lower parts of the documents is constructed. If the specified similarity threshold is exceeded, it is considered that there is a link between the documents \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D_{i} $$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D_{j} $$\\end{document}.The importance of documents is calculated by using the LexRank algorithm over the constructed graph [4].For documents whose weight is greater than a certain threshold, the previously described procedure for expanding the query is performed.\n",
            "cite_spans": [
                {
                    "start": 1258,
                    "end": 1259,
                    "mention": "4",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Inter-Document Feature. ::: Inverted Pyramid ::: Approach",
            "ref_spans": []
        },
        {
            "text": "As a result, the output is a ranked list of documents \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D $$\\end{document} and a set of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Q_{D} $$\\end{document} new queries, which further, together with accounting for the temporal nature of the news story, will help in sentence ranking algorithm. Among other things, document weights will also be taken into account in the ranking functions.",
            "cite_spans": [],
            "section": "Inter-Document Feature. ::: Inverted Pyramid ::: Approach",
            "ref_spans": []
        },
        {
            "text": "To this feature into account for the following procedure is undertaken: during the ranking of sentences, the weight of the sentence is multiplied by a coefficient that lowers the weight of sentences in the middle of the document.",
            "cite_spans": [],
            "section": "Intra-Document Feature. ::: Inverted Pyramid ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Also, after described inter-document procedure, all constructed extended queries \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Q_{D} $$\\end{document} are mapped to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ t_{\\text{i}} $$\\end{document} labels from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ T $$\\end{document} (Fig. 4).\n",
            "cite_spans": [],
            "section": "Intra-Document Feature. ::: Inverted Pyramid ::: Approach",
            "ref_spans": [
                {
                    "start": 995,
                    "end": 996,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "At various stages of the algorithm, there are a number of points where the measure of closeness between sentences is calculated. For these purpose a cosine measure of similarity (3) is used in all cases.3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Sim_{cos} \\left( {S_{i} , S_{j} } \\right) = \\frac{{\\left( {S_{i} ,   S_{j} } \\right)}}{{\\left| {S_{i} \\left| * \\right|S_{j} } \\right|}} $$\\end{document}\n",
            "cite_spans": [],
            "section": "Similarity of Sentences ::: Approach",
            "ref_spans": []
        },
        {
            "text": "The choice of representation of a sentence plays an important role for calculating similarity. In this article we used the standard tf-idf representation. But to calculate the similarity between sentences when searching for links between documents, word2vec [24] representation was used. To achieve this, the resulting sentence vector is represented as a weighted mean of word2vec word representations. Weighing was carried out by tf-idf.",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 261,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Similarity of Sentences ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Word2vec model was trained on the entire collection of 2 million news articles. During preprocessing removal of stop words and lemmatization were applied. The width of the window was chosen to be 5, and the length of the vector was 100.",
            "cite_spans": [],
            "section": "Similarity of Sentences ::: Approach",
            "ref_spans": []
        },
        {
            "text": "This module deals with the ranking of sentences. The ranking is a modified version of the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ MMR $$\\end{document} algorithm \u2013 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ MMRT $$\\end{document} (4) taking into account all the factors described in Sect. 4.2:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ MMRT_{{s_{i}^{t} }} = INC_{{s_{i}^{t} }} - DEC_{{s_{i}^{t} }} $$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ INC_{{s_{i}^{t} }} $$\\end{document} \u2013 is a term describing the positive part of the formula, which depends on the similarity of the sentence to the query, the weight of the document from which the sentence is taken, and the sentence number in the document.5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ INC_{{s_{i}^{t} }} = \\left( {1 + \\alpha *I_{i} } \\right)* \\gamma * \\lambda *Sim\\left( {Q^{t} , S_{i}^{t} } \\right) $$\\end{document}\n6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\gamma = 1 - 0.5*{ \\sin }\\left( {\\frac{i* \\pi }{{\\left| {D_{s} } \\right|}}} \\right) $$\\end{document}\n",
            "cite_spans": [],
            "section": "Sentence Ranking Module ::: Approach",
            "ref_spans": []
        },
        {
            "text": "The parameters \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\alpha $$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\lambda $$\\end{document} are configurable parameters of the algorithm, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ I_{i} $$\\end{document} \u2013 is document weight \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ D_{s} $$\\end{document}, which includes a sentence under the index \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ i $$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ S_{i}^{t} $$\\end{document} \u2013 is estimated sentence under the index \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ i $$\\end{document} with label \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ t $$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Q^{t} $$\\end{document} \u2013 query for this time label, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\gamma $$\\end{document} \u2013 multiplier, which reduce the weight of sentences from the middle of the document.",
            "cite_spans": [],
            "section": "Sentence Ranking Module ::: Approach",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ DEC_{{s_{i}^{t} }} $$\\end{document} is the penalty term. It depends on the similarity to the already extracted sentences:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ DEC_{{s_{i}^{t} }} = \\left( {1 - \\lambda } \\right)*\\mathop {\\hbox{max} }\\nolimits_{{S_{j} \\in S}} Sim(S_{j} ,  S_{i}^{t} ) $$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ S_{j} $$\\end{document} is one of the extracted sentences, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ S $$\\end{document} is the set of all already extracted sentences.",
            "cite_spans": [],
            "section": "Sentence Ranking Module ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Processing of sentences occurs in chronological order with a restriction on the maximum number of sentences per day.",
            "cite_spans": [],
            "section": "Sentence Ranking Module ::: Approach",
            "ref_spans": []
        },
        {
            "text": "The features described in Subsect. 4.2 are realized at various stages of the system. The general scheme of the algorithm is shown on Fig. 5.\n",
            "cite_spans": [],
            "section": "System Diagram ::: Approach",
            "ref_spans": [
                {
                    "start": 138,
                    "end": 139,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "The system was evaluated using several metrics: ROUGE-1, ROUGE-2, and Sentence Recall \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ R^{sent} $$\\end{document}:8\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ ROUGE - N = \\frac{{\\left| {N_{A} \\cap N_{g} } \\right|}}{{\\left| {N_{g} } \\right|}} $$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ N_{A} $$\\end{document} is the set of n-grams for the constructed annotations, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ N_{g} $$\\end{document} is the set of n-grams for the reference (gold) annotations.9\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ R^{sent} = \\frac{{\\left| {S_{A} \\equiv S_{g} } \\right|}}{{\\left| {S_{g} } \\right|}}, $$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ S_{A} $$\\end{document} is the set of sentences from the constructed annotations, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ S_{g} $$\\end{document} is the set of sentences from the reference annotations. Operator \u2261 denotes the following: the result of the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\left| {S_{A} \\equiv S_{g} } \\right| $$\\end{document} is a subset of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ S_{A} $$\\end{document} such that their semantic equivalent is present in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ S_{g} $$\\end{document}.",
            "cite_spans": [],
            "section": "Metrics for Evaluation ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "Since a test set of annotations is required for evaluating procedure, in the course of the research, timeline summaries were manually prepared. The procedure for the formation of such a collection was as follows:At the first stage with the help of Wikipedia there high-profile events were selected, which were actively covered in the press for the beginning of 2015.Further, for most of the events on the site \u201cInterfax\u201d, the search for the corresponding story was carried out. On the basis of documents corresponding to the story, a timeline summary was created.If there is no corresponding story on the \u201cInterfax\u201d, the materials were studied on the topic and a timeline summary was created on the basis of the documents read.\n",
            "cite_spans": [],
            "section": "Data Preparation ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "As a result, 45 annotations on 15 news stories were created (Table 2).\n",
            "cite_spans": [],
            "section": "Data Preparation ::: Evaluation",
            "ref_spans": [
                {
                    "start": 67,
                    "end": 68,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Since the system contains a large number of parameters (total 23 parameters), some of which are presented in Table 3, there was a need to optimize the choice of the values of these parameters.\n",
            "cite_spans": [],
            "section": "Optimization of Algorithm Parameters ::: Evaluation",
            "ref_spans": [
                {
                    "start": 115,
                    "end": 116,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "To achieve this, the entire collection of the reference annotations was divided into train and test parts with the ratio 2 to 1. Further, the functional (1) was implemented in Python using an open hyperopt [22] package based on machine learning. This package uses the technique of Sequential model-based optimization (SMBO) [23] for the parameters selection. The parameters were trained on the training part. After that, the final evaluation of the configurations took place on the test part.",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 209,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 325,
                    "end": 327,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Optimization of Algorithm Parameters ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "In order to evaluate the contribution of the considered features, a fitting and evaluation of the following 6 configurations was made:baseline \u2013 a simple approach to summarization, without taking into account the factors considered, using MMR as ranking algorithm.querry-ex \u2013 adding a query extension strategy feature to baseline (Sect. 4.3), but without double query extension.double-ex \u2013 querry-ex + double query extension (Sect. 4.3).temporal \u2013 double-ex + accounting for the temporal nature of news stories (Sect. 4.4).importance \u2013 temporal + accounting for the structure of a news article in the form of an inverted pyramid, when tf-idf representation is used (Sect. 4.5).w2v-imp \u2013 importance, but using w2v for computing sentence similarity when accounting for the structure of a news article (Sect. 4.6).\n",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": []
        },
        {
            "text": "The result of evaluation of the configurations can be seen in Table 4. This table shows that each of the features considered gives a positive contribution to the quality of generation of timeline summary. As an example of the final annotation, one can consider a fragment of the annotation on the previously mentioned incident of the crash in Taiwan in Table 5.\n\n",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 68,
                    "end": 69,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 359,
                    "end": 360,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "In this article we presented an approach for building a timeline summary. The conducted research shows that the problem of constructing the timeline summary differs from the standard MDS problem. The effectiveness of using the following features was shown:Query extension strategy.Accounting for the temporal nature of news stories.Accounting for the structure of a news article in the form of an inverted pyramid.\n",
            "cite_spans": [],
            "section": "Conclusions and Future Work",
            "ref_spans": []
        },
        {
            "text": "Extending the query, as expected, has a positive effect on the event representation discussed in the document. But the interesting fact is that re-extension the query (double query extension) has a much greater effect. This is because the documents that are retrieved on the first-level query are not sufficient for a good presentation of the event.",
            "cite_spans": [],
            "section": "Conclusions and Future Work",
            "ref_spans": []
        },
        {
            "text": "The fact that accounting for the temporal nature of news stories improves the quality of the annotation is an obvious consequence of the fact that news stories and events have temporal characteristics.",
            "cite_spans": [],
            "section": "Conclusions and Future Work",
            "ref_spans": []
        },
        {
            "text": "Taking into account the structure of the inverted pyramid gives an improvement. Increase the values of metrics on the w2v-imp configuration means that the correctness of the recognized links between the documents plays a significant role. This fact raises challenges for future research.",
            "cite_spans": [],
            "section": "Conclusions and Future Work",
            "ref_spans": []
        },
        {
            "text": "Using structural features of news articles make it possible to obtain information, the use of which can significantly improve the quality of generated annotations.",
            "cite_spans": [],
            "section": "Conclusions and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table\u00a01.: Query extension algorithm stages example.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table\u00a02.: News stories on which the reference annotations are made.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table\u00a03.: Some system parameters.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table\u00a04.: Evaluation results.\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table\u00a05.: The generated timeline summary fragment about the plane crash in Taiwan.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig.\u00a01.: Timeline summary part about conflict at cemetery.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig.\u00a02.: Dependence of the number of publications per day for an event",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig.\u00a03.: Inverted pyramid on the example of an article. (https://themoscowtimes.com/articles/moscow-museum-takes-you-inside-north-korea-60240)",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig.\u00a04.: Query mapping.",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig.\u00a05.: Working scheme.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "A topic modeling based approach to novel document automatic summarization",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lei",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Expert Syst. Appl.",
            "volume": "84",
            "issn": "",
            "pages": "12-23",
            "other_ids": {
                "DOI": [
                    "10.1016/j.eswa.2017.04.054"
                ]
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "A semantic approach for text clustering using WordNet and lexical chains",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Bao",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Expert Syst. Appl.",
            "volume": "42",
            "issn": "4",
            "pages": "2264-2275",
            "other_ids": {
                "DOI": [
                    "10.1016/j.eswa.2014.10.023"
                ]
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Hyperopt: a python library for model selection and hyperparameter optimization",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bergstra",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Komer",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Eliasmith",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yamins",
                    "suffix": ""
                },
                {
                    "first": "DD",
                    "middle": [],
                    "last": "Cox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Comput. Sci. Discov.",
            "volume": "8",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1088/1749-4699/8/1/014008"
                ]
            }
        },
        "BIBREF15": {
            "title": "Sequential model-based optimization for general algorithm configuration",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                },
                {
                    "first": "Holger",
                    "middle": [
                        "H"
                    ],
                    "last": "Hoos",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Leyton-Brown",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Learning and Intelligent Optimization",
            "volume": "",
            "issn": "",
            "pages": "507-523",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "Lexrank: graph-based lexical centrality as salience in text summarization",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Erkan",
                    "suffix": ""
                },
                {
                    "first": "DR",
                    "middle": [],
                    "last": "Radev",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "J. Artif. Intell. Res.",
            "volume": "22",
            "issn": "",
            "pages": "457-479",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "Exploring the interactions of storylines from informative news events",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "ML",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "XY",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "J. Comput. Sci. Technol.",
            "volume": "29",
            "issn": "3",
            "pages": "502-518",
            "other_ids": {
                "DOI": [
                    "10.1007/s11390-014-1445-6"
                ]
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Introduction to the special issue on summarization",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Radev",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "McKeown",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Hovy",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Comput. Linguist.",
            "volume": "28",
            "issn": "4",
            "pages": "399-408",
            "other_ids": {
                "DOI": [
                    "10.1162/089120102762671927"
                ]
            }
        },
        "BIBREF23": {
            "title": "Connecting two (or less) dots: discovering structure in news articles",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shahaf",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Guestrin",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ACM Trans. Knowl. Discov. Data (TKDD)",
            "volume": "5",
            "issn": "4",
            "pages": "24-54",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Timeline summarization from relevant headlines",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alrifai",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Herder",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "245-256",
            "other_ids": {
                "DOI": []
            }
        }
    }
}