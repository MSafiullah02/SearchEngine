{
    "paper_id": "2e4725949ac016efc6a566edb088a3746109a117",
    "metadata": {
        "title": "LoPAD: A Local Prediction Approach to Anomaly Detection",
        "authors": [
            {
                "first": "Sha",
                "middle": [],
                "last": "Lu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of South Australia",
                    "location": {
                        "postCode": "5095",
                        "settlement": "Adelaide",
                        "region": "SA",
                        "country": "Australia"
                    }
                },
                "email": "sha.lu@mymail.unisa.edu.au"
            },
            {
                "first": "Lin",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of South Australia",
                    "location": {
                        "postCode": "5095",
                        "settlement": "Adelaide",
                        "region": "SA",
                        "country": "Australia"
                    }
                },
                "email": "lin.liu@unisa.edu.au"
            },
            {
                "first": "Jiuyong",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of South Australia",
                    "location": {
                        "postCode": "5095",
                        "settlement": "Adelaide",
                        "region": "SA",
                        "country": "Australia"
                    }
                },
                "email": "jiuyong.li@unisa.edu.au"
            },
            {
                "first": "Duy",
                "middle": [],
                "last": "Le",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of South Australia",
                    "location": {
                        "postCode": "5095",
                        "settlement": "Adelaide",
                        "region": "SA",
                        "country": "Australia"
                    }
                },
                "email": ""
            },
            {
                "first": "Jixue",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of South Australia",
                    "location": {
                        "postCode": "5095",
                        "settlement": "Adelaide",
                        "region": "SA",
                        "country": "Australia"
                    }
                },
                "email": "jixue.liu@unisa.edu.au"
            }
        ]
    },
    "abstract": [
        {
            "text": "Dependency-based anomaly detection methods detect anomalies by looking at the deviations from the normal probabilistic dependency among variables and are able to discover more subtle and meaningful anomalies. However, with high dimensional data, they face two key challenges. One is how to find the right set of relevant variables for a given variable from the large search space to assess dependency deviation. The other is how to use the dependency to estimate the expected value of a variable accurately. In this paper, we propose the Local Prediction approach to Anomaly Detection (LoPAD) framework to deal with the two challenges simultaneously. Through introducing Markov Blanket into dependencybased anomaly detection, LoPAD decomposes the high dimensional unsupervised anomaly detection problem into local feature selection and prediction problems while achieving better performance and interpretability. The framework enables instantiations with off-the-shelf predictive models for anomaly detection. Comprehensive experiments have been done on both synthetic and real-world data. The results show that LoPAD outperforms state-of-the-art anomaly detection methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "According to [7] , anomalies are patterns in data that do not conform to a welldefined notion of normal behavior. The mainstream methods for anomaly detection, e.g. LOF [5] , are based on proximity between objects. These methods evaluate the anomalousness of an object through its distance or density within its neighborhood. If an object stays far away from other objects or in a sparse neighborhood, it is more likely to be an anomaly [1] .",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 16,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 169,
                    "end": 172,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 437,
                    "end": 440,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Another research direction in anomaly detection is to exploit the dependency among variables, which has shown successful applications in various fields [1] . Dependency-based methods firstly discover variable dependency possessed by the majority of objects, then the anomalousness of objects is evaluated through how well they follow the dependency. The objects whose variable dependency significantly deviate from the normal dependency are flagged as anomalies.",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 155,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "These methods can detect certain anomalies that cannot be discovered through proximity because though these anomalies violate the dependency, they may still locate in a dense neighborhood.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A way to measure dependency deviation is to examine the difference between the observed value and the expected value of an object, where the expected value is estimated based on the underlying dependency [1] . Specifically, for an object, the expected value of a given variable is estimated using the values of a set of other variables of the object. Here, we call the given variable the target variable, and the set of other variables relevant variables.",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 207,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Relevant variable selection and expected value estimation are the two critical steps of dependency-based anomaly detection, as they play a decisive role in the performance of the detection. However, they have not been well addressed by existing methods. Relevant variable selection faces a dilemma in high dimensional data. On the one hand, it is expected that the complete dependency, i.e., the dependency between a target variable and all the other variables, is utilized to discover anomalies accurately. On the other hand, it is common that in real-world data, only some variables are relevant to the data generation mechanism for the target variable. Irrelevant variables have no or very little contribution to the anomaly score, and even have a negative impact on the effectiveness [18] . How to find the set of most relevant variables that can capture the complete dependency around a target variable is a challenge, especially in high dimensional data given the large number of possible subsets of variables.",
            "cite_spans": [
                {
                    "start": 788,
                    "end": 792,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A naive approach is to use all other variables as the relevant variables for a target variable, as the ALSO algorithm [12] does. However, doing so leads to two major problems. Firstly, it is computationally expensive to build prediction models in high dimensional data. Secondly, conditioning on all other variables means irrelevant variables can affect the detection accuracy. Another approach is to select a small set of relevant variables. COMBN [2] is a typical method falling in this category. COMBN uses the set of all direct cause variables of a target in a Bayesian network as the relevant variables. However, only selecting a small subset of variables may miss some important dependencies, resulting in poor detection performance too.",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 122,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 449,
                    "end": 452,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To deal with these problems, we propose an optimal attribute-wise method, LoPAD (Local Prediction approach to Anomaly Detection), which innovatively introduces Markov Blanket (MB) and predictive models to anomaly detection to enable the use off-the-shelf classification methods to solve high dimensional unsupervised anomaly detection problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "MB is a fundamental concept in the Bayesian network (BN) theory [13] . For any variable X in a BN, the MB of X, denoted as MB(X), comprises its parents (direct causes), children (direct effects) and spouses (the other parents of X's children). Given MB(X), X is conditionally independent of all the other variables, which means MB(X) encodes the complete dependency of X. So for LoPAD, we propose to use MB(X) as the relevant variables of X. As in high dimensional data, MB(X) usually has much lower dimensionality than that of the dataset, which enables LoPAD to deal with high dimensional data.",
            "cite_spans": [
                {
                    "start": 64,
                    "end": 68,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Moreover, using MB(X) LoPAD can achieve a more accurate estimation of the expected value of X. The study in [9] has shown that MB(X) is the optimal feature set for a prediction model of X in the sense of minimizing the amount of predictive information loss. Therefore, we propose to predict the expected value of X with a prediction model using MB(X) as the predictors. It is noted that LoPAD is not limited to a specific prediction algorithm, which means a variety of off-the-shelf prediction methods can be utilized and thus relax the restrictions on data distributions and data types.",
            "cite_spans": [
                {
                    "start": 108,
                    "end": 111,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In summary, by using MB of a variable, LoPAD simultaneously solves the two challenges in dependency-based anomaly detection, relevant variable selection and expected value estimation. The main contributions of this work are as below:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-Through introducing Markov Blanket into dependency-based anomaly detection, we decompose the high dimensional unsupervised anomaly detection problem into local feature selection and prediction problems, which also provide better interpretation of detected anomalies. -We develop an anomaly detection framework, LoPAD, to efficiently and effectively discover anomalies in high dimensional data of different types. -We present an instantiated algorithm based on the LoPAD framework and conduct extensive experiments on a range of synthetic and real-world datasets to demonstrate the effectiveness and efficiency of LoPAD.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we use an upper case letter, e.g. X to denote a variable; a lower case letter, e.g. x for a value of a variable; a boldfaced upper case letter, e.g. X = {X 1 , X 2 , . . . , X m } for a set of variables; and a boldfaced lower case letters, e.g. x = (x 1 , x 2 , . . . , x m ), for a value vector of a set of variables. We have reserved the letter D for a data matrix of n objects and m variables, x i for the i-th row vector (data point or object) of D, and x ij for the j-th element in x i . In LoPAD, the anomalousness of an object is evaluated based on the deviation of its observed value from the expected value. There are two types of deviations, value-wise deviation and vector-wise deviation as defined below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Notation and Definitions"
        },
        {
            "text": "Given an object x i , its value-wise deviation with respect to variable X j is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 (Value-wise Deviation)."
        },
        {
            "text": "where x ij is the observed value of X j in x i , and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 (Value-wise Deviation)."
        },
        {
            "text": "is the expected value of X j estimated using the function g() based on the values on other variables X \u2286 X \\ {X j }.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 (Value-wise Deviation)."
        },
        {
            "text": "x i is the aggregation of all its value-wise deviations calculated using a combination function as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 2 (Vector-wise Deviation). The vector-wise deviation of object"
        },
        {
            "text": "From the above definitions, we see that value-wise deviation evaluates how well an object follows the dependency around a specific variable, and vector-wise deviation evaluates how an object collectively follows the dependencies. Based on the definitions, we can now define the research problem of this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 2 (Vector-wise Deviation). The vector-wise deviation of object"
        },
        {
            "text": "Given a dataset D with n objects and a user specified parameter k, our goal is to detect the top-k ranked objects according to the descending order of vector-wise deviations as anomalies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 3 (Problem Definition)."
        },
        {
            "text": "To obtain value-wise deviation of an object, two problems need to be addressed. One is how to find the right set of relevant variables of a target variable, i.e. X in Eq. 2, which should completely and accurately represent the dependency of X j on other variables. For high dimensional data, it is more challenging as the number of subsets of X \\ {X j } increases exponentially with the number of variables in a dataset. The other problem is how to use the selected relevant variables to make an accurate estimation of the expected value.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The LoPAD Framework"
        },
        {
            "text": "The LoPAD framework adapts optimal feature selection technique and supervised machine learning technique to detect anomalies in three phases: (1) Relevant variable selection for each variable X j using the optimal feature select technique; (2) Estimation of the expected value of X j using the selected variables with a predictive model; (3) Anomaly score generation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The LoPAD Framework"
        },
        {
            "text": "In this phase, the goal is to select the optimal relevant variables for a target variable. We firstly introduce the concept of MB, then explain why MB is the set of optimal relevant variables.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Phase 1: Relevant Variable Selection."
        },
        {
            "text": "Markov Blankets are defined in the context of a Bayesian network (BN) [13] . A BN is a type of probabilistic graphical model used to represent and infer the dependency among variables. A BN is denoted as a pair of (G, P ), where G is a Directed Acyclic Graph (DAG) showing the structure of the BN and P is the joint probability of the nodes in G. Specifically, G = (V, E), where V is the set of nodes representing the random variables in the domain under consideration, and E \u2286 V \u00d7 V is the set of arcs representing the dependency among the nodes. X 1 \u2208 V is known as a parent of X 2 \u2208 V (or X 2 is a child of X 1 ) if there exists an arc X 1 \u2192 X 2 . In a BN, given all its parents, a node X is conditionally independent of all its non-descendant nodes, known as the Markov condition for a BN, based on which the joint probability distribution of V can be decomposed to the product of the conditional probabilities as follows:",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 74,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Phase 1: Relevant Variable Selection."
        },
        {
            "text": "where P a(X) is the set of all parents of X. For any variable X \u2208 V in a BN, its MB contains all the children, parents, and spouses of X, denoted as MB(X). Given MB(X), X is conditionally independent of all other variables in V, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Phase 1: Relevant Variable Selection."
        },
        {
            "text": "According to Eq. 5, MB(X) represents the information needed to estimate the probability of X by making X irrelevant to the remaining variables, which makes MB(X) is the minimal set of relevant variables to obtain the complete dependency of X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Phase 1: Relevant Variable Selection."
        },
        {
            "text": "This phase aims to estimate the expected value of a variable in an object (defined in Eq. 2) using the selected variables. The function g() in Eq. 2 is implemented with a prediction model. Specifically, for each variable, a prediction model is built to predict the expected value on the variable using the selected relevant variables as predictors. A large number of off-the-shelf prediction models can be chosen to suit the requirement of the data. By doing so, we decompose the anomaly detection problem into individual prediction/classification problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Phase 2: Expected Value Estimation."
        },
        {
            "text": "In this phase, the vector-wise deviation, i.e., anomaly score, is obtained by applying a combination function over value-wise deviations. Various combination functions can be used in the LoPAD framework, such as maximum function, averaging function, weighted summation. A detailed study on the impact of different combination functions on the performance of anomaly detection can be found in [10] .",
            "cite_spans": [
                {
                    "start": 392,
                    "end": 396,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Phase 3: Anomaly Score Generation."
        },
        {
            "text": "As shown in Algorithm 1, we present an instantiation of the LoPAD framework, i.e. the LoPAD algorithm. Given an input dataset D, for each variable, its relevant variable selection is done at Line 3, then a prediction model is built at Line 4. From Lines 5 to 8, value-wise deviations are computed for all the objects. In Line 10, value-wised deviation is normalized. With Lines 11 to 13, vector-wise deviations are obtained by combining value-wise deviations. At Line 14, top-k scored objects are output as identified anomalies. As anomalies are rare in a dataset, although LoPAD uses the dataset with anomalies to discover MBs and train the prediction models, the impact of anomalies on MB learning and model training is limited.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The LoPAD Algorithm"
        },
        {
            "text": "For the LoPAD algorithm, we use the fast-IAMB method [16] to learn MBs. For estimating expected values, we adopt CART regression tree [4] to enable the LoPAD algorithm to cope with both linear and non-linear dependency. It is noted that regression models are notorious for being affected by the outliers in the training set. We adopt Bootstrap aggregating (also known as bagging) [3] to mitigate this problem to achieve better prediction accuracy. Before computing vector-wise deviations, the obtained value-wised deviations need to be normalized. Specifically, for each object x i on each target variable X j , \u03b4 ij is normalized as the Z-score using the mean and standard deviation of \u03b4 j . After normalization, negative values represent the small deviations. As we are only interested in large deviations, the vector-wise deviation is obtained by summing up the positive normalized value-wise deviations as follows:",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 134,
                    "end": 137,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 380,
                    "end": 383,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "The LoPAD Algorithm"
        },
        {
            "text": "The time complexity of the LoPAD algorithm mainly comes from two sources, learning MB and building the prediction model. For a dataset with n objects and m variables, the complexity of the MB discovering using fast-IAMB is O(m 2 \u03bb) [15] , where \u03bb is the average size of MBs. The complexity of building m prediction models is O(m\u03bbnlogn) [4] . Therefore, the overall complexity of the LoPAD algorithm is O(m 2 \u03bb) + O(m\u03bbnlogn).",
            "cite_spans": [
                {
                    "start": 232,
                    "end": 236,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 336,
                    "end": 339,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "The LoPAD Algorithm"
        },
        {
            "text": "Data Generation. For synthetic data, 4 benchmark BNs from bnlearn repository [14] are used to generate linear Gaussian distributed datasets. For each BN, 20 datasets with 5000 objects are generated. Then the following process is followed to inject anomalies. Firstly, 1% objects and 10% variables are randomly selected. Then anomalous values are injected to the selected objects on these selected variables. The injected anomalous values are uniformly distributed values in the range of the minimum and maximum values of the selected variables.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 81,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In this way, the values of anomalies are still in the original range of the selected variables, but their dependency with other variables is violated. For each BN, the average ROC AUC (area under the ROC curve) of the 20 datasets is reported. For real-world data, we choose 13 datasets ( Table 1) that cover diverse domains, e.g., spam detection, molecular bioactivity detection, and image object recognition. AID362, backdoor, mnist and caltech16 are obtained from Kaggle dataset repository, and the others are retrieved from the UCI repository [8] . These datasets are often used in anomaly detection literature. We follow the common process to obtain the ground truth anomaly labels, i.e. using samples in a majority class as normal objects, and a small class, or down-sampling objects in a class as anomalies. Categorical features are converted into numeric ones by 1-of-encoding [6] . If the number of objects in the anomaly class is more than 1% of the number of normal objects, we randomly sample the latter number of objects from the anomaly class as anomalies. Experiments are repeated 20 times, and the average AUC is reported. If the ratio of anomalies is less than 1%, the experiment is conducted once, which is the case for the wine, AID362 and arrhythmia datasets.",
            "cite_spans": [
                {
                    "start": 546,
                    "end": 549,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 884,
                    "end": 887,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 288,
                    "end": 296,
                    "text": "Table 1)",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "Comparison Methods. The comparison methods include dependency-based methods, ALSO [12] and COMBN [2] ; and proximity-based methods, MBOM [17] , iForest [11] and LOF [5] . The major difference in LoPAD, ALSO and COMBN is the choice of relevant variables. ALSO uses all remaining variables, and COMBN uses parent variables, while LoPAD utilizes MBs. The effectiveness of using MB in LoPAD is validated by comparing LoPAD with ALSO. MBOM and iForest are proximity-based methods, which detect anomalies based on density in subspaces. LOF is a classic density-based method, which is used as the baseline method.",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 86,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 97,
                    "end": 100,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 137,
                    "end": 141,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 152,
                    "end": 156,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 165,
                    "end": 168,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In the experiments (including sensitivity tests), we adopt the commonly used or recommended parameters that are used in the original papers. For a fair comparison, both LoPAD and ALSO adopt CART regression tree [4] with bagging. In CART, the number of minimum objects to split is set to 20, and the minimum number of objects in a bucket is 7, the complexity parameter is set to 0.03. The number of CART trees in bagging is set to 25. In MBOM and LOF, the number of the nearest neighbor is set to 10. For iForest, the number of trees is set to 100 without subsampling.",
            "cite_spans": [
                {
                    "start": 211,
                    "end": 214,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "All algorithms are implemented in R 3.5.3 on a computer with 3.5 GHz (12 cores) CPU and 32 G memory.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Performance Evaluation. The experimental results are shown in Table 2 . If a method could not produce a result within 2 hour, we terminate the experiment. Such cases occur to COMBN and are shown as '-' in Table 2 . LoPAD yields 13 best results (out of 17) and LoPAD achieves the best average AUC of 0.859 with the smallest standard deviation of 0.027. Overall, dependency-based methods (LoPAD, ALSO and COMBN) perform better than proximity-based methods (MBOM, iForest and LOF). Compared with ALSO, LoPAD improves 4.2% on AUC, which is attributed to the use of MB. COMBN yields two best results, but its high time complexity makes it unable to produce results for several datasets. Comparing LoPAD with MBOM, LoPAD performs significantly better with a 15.3% AUC improvement. Although iForest has the best result among the proximity-based methods, LoPAD has a 9.1% AUC improvement over it. As to LOF, LoPAD has a 14.6% AUC improvement over it. The average size of the MB is much smaller than the original dimensionality on all datasets, which means that comparing to ALSO, LoPAD works based on much smaller dimensionality but still achieves the best results in most cases.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 62,
                    "end": 69,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 205,
                    "end": 212,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "We apply the Wilcoxon rank sum test to the results of the 17 datasets (4 synthetic and 13 real-world datasets) by pairing LoPAD with each of the other methods. The null hypothesis is that the result of LoPAD is generated from the distribution whose mean is greater than the compared method. The p-values are 0.0005 with ALSO, 0.0001 with MBOM, 0.0599 with COMBN, 0.0007 with iForest and 0.0002 with LOF. The p-value with COMBN is not reliable because of the small number of results (COMBN is unable to produce results for 5 out of 21 datasets). Except for COMBN, all the p-values are far less than 0.05, indicating that LoPAD performs significantly better than the other methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "The running time of these datasets is shown in Table 3 . Overall, dependencybased methods are slower because they need extra time to learn MBs or the BN and prediction models. COMBN is unable to produce results in 2 h on 5 datasets. Fig. 1 . In Fig. 1(a) , the number of variables with injected anomalous values ranges from 1 to 20, while the ratio of anomalies is fixed to 1%. In Fig. 1(b) , the ratio of anomalies ranges from 1% to 10%, while the number of anomalous variables is fixed to 10. In Fig. 1(c) , the dimension ranges from 100 to 1000, while the number of variables with injected anomalous values is 10 and the ratio of anomalies fixes to 1%. Overall, all methods follow similar trends in terms of their sensitivity to these parameters, and LoPAD shows consistent results which are better than comparison methods in most cases. Anomaly Interpretation. One advantage of LoPAD is the interpretability of detected anomalies. For a detected anomaly, the variables with high deviations can be utilized to explain detected anomalies. The difference between the expected and the observed values of these variables indicate the strength and direction of the deviation. We use the result of the mnist dataset as an example to show how to interpret an anomaly detected by LoPAD. In mnist, each object is a 28 * 28 grey-scaled image of a hand-writing digit. Each pixel is a variable, whose value ranges from 0 to 255. Zero corresponds to white, and 255 is black. In our experiment, 7 is the normal class, and 0 is the anomaly class. Figure 2(b) is the top-ranked anomaly by LoPAD (a digit 0) and Fig. 2(c) is its expected values. In Fig. 2(d) (which also shows the top-ranked anomaly as in Fig. 2(b) ), the pixels indicated with a red dot or a green cross are the top-100 deviated pixels (variables). The green pixels have negative deviations, i.e. their observed values are much smaller than their expected values, which means according to the underlying dependency, these pixels are expected to be darker. The red pixels have positive deviations, i.e. their observed values are much bigger than their expected values, which means they are expected to be much lighter.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 47,
                    "end": 54,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 233,
                    "end": 239,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 245,
                    "end": 254,
                    "text": "Fig. 1(a)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 381,
                    "end": 390,
                    "text": "Fig. 1(b)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 498,
                    "end": 507,
                    "text": "Fig. 1(c)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1535,
                    "end": 1546,
                    "text": "Figure 2(b)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1598,
                    "end": 1607,
                    "text": "Fig. 2(c)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1635,
                    "end": 1644,
                    "text": "Fig. 2(d)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1692,
                    "end": 1701,
                    "text": "Fig. 2(b)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "We can use these pixels or variables with high derivations to understand why this image is an anomaly as explained in the following. In Fig. 2(d) , the highly deviated pixels concentrate in the 3 areas in the blue ellipses. These areas visually are indeed the areas where the observed object ( Fig. 2(b) ) and its expected value (Fig. 2(c) ) differ the most. Comparing Fig. 2(d) with Fig. 2(a) , we can see the anomalousness mainly locates in the 3 areas: (1) in area 1, the stroke is not supposed to be totally closed; (2) the little 'tail' in area 2 is not expected; (3) the stroke in area 3 should move a little to the left.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 136,
                    "end": 145,
                    "text": "Fig. 2(d)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 294,
                    "end": 303,
                    "text": "Fig. 2(b)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 329,
                    "end": 339,
                    "text": "(Fig. 2(c)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 369,
                    "end": 378,
                    "text": "Fig. 2(d)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 384,
                    "end": 393,
                    "text": "Fig. 2(a)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "In summary, this example shows that the deviations from the normal dependency among variables can be used to explain the causes of anomalies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Dependency-based anomaly detection approach works under the assumption that normal objects follow the dependency among variables, while anomalies do not. The key challenge for applying approach is how to decide the predictors of a target variable, especially in high dimensional data. However, existing research has not paid attention to how to choose an optimal set of relevant variables. They either use all the other variables, such as ALSO [12] , or a small subset of variables, such as COMBN [2] . The inappropriate choice of predictors has a negative impact on the effectiveness and efficiency of anomaly detection, as indicated by the experiments in Sect. 3. In this paper, we innovatively tackle this issue by introducing MBs as relevant variables.",
            "cite_spans": [
                {
                    "start": 444,
                    "end": 448,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 497,
                    "end": 500,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Apart from dependency-based approach, the mainstream of anomaly detection methods is proximity-based, such as LOF [5] . These methods work under the assumption that normal objects are in a dense neighborhood, while anomalies stay far away from other objects or in a sparse neighborhood [7] . Building upon the different assumptions, the key difference between dependency-based and proximity-based approaches is that the former considers the relationship among variables, while the latter relies on the relationship among objects.",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 117,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 286,
                    "end": 289,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "A branch of proximity-based approach, subspace-based methods, partially utilizes dependency in anomaly detection. In high dimensional data, the distances among objects vanish with the increase of dimensionality (known as the curse of dimensionality). To address this problem, some subspace-based methods are proposed [18] to detect anomalies based on the proximity with respect to subsets of variables, i.e., subspaces. However, although subspace-based anomaly detection methods make use of variable dependency, they use the dependency to determine subspaces, instead of measuring anomalousness. Often these methods find a subset of correlated variables as a subspace, then still use proximity-based methods to detect outlier in each subspace. For example, with MBOM [17] , a subspace contains a variable and its MB, and LOF is used to evaluate anomalousness in each such a subspace. Another novel subspace-based anomaly detection method, iForest [11] , randomly selects subsets of variables as subspaces, which shows good performance in both effectiveness and efficiency.",
            "cite_spans": [
                {
                    "start": 317,
                    "end": 321,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 767,
                    "end": 771,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 947,
                    "end": 951,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this paper, we have proposed an anomaly detection method, LoPAD, which divides and conquers high dimensional anomaly detection problem with Markov Blanket learning and off-the-shelf prediction methods. Through using MB as the relevant variables of a target variable, LoPAD ensures that complete dependency is captured and utilized. Moreover, as MBs are the optimal feature selection sets for prediction tasks, LoPAD also ensures more accurate estimation of the expected values of variables. Introducing MB into dependency-based anomaly detection methods provides the sound theoretical support to the most critical steps of dependency-based methods. Additionally, the results of the comprehensive experiments conducted in this paper have demonstrated the superior performance and efficiency of LoPAD comparing to the state-of-the-art anomaly detection methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Outlier analysis",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Aggarwal",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Data Mining",
            "volume": "",
            "issn": "",
            "pages": "237--263",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-14142-8_8"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Mining causal outliers using Gaussian Bayesian networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Babbar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chawla",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "2012 Proceedings of ICTAI",
            "volume": "1",
            "issn": "",
            "pages": "97--104",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Bagging predictors",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Mach. Learn",
            "volume": "24",
            "issn": "2",
            "pages": "123--140",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Classification and Regression Trees",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Routledge",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "LOF: identifying density-based local outliers",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Breunig",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "P"
                    ],
                    "last": "Kriegel",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "T"
                    ],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sander",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "ACM SIGMOD Rec",
            "volume": "29",
            "issn": "",
            "pages": "93--104",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "O"
                    ],
                    "last": "Campos",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Data Min. Knowl. Discov",
            "volume": "30",
            "issn": "4",
            "pages": "891--927",
            "other_ids": {
                "DOI": [
                    "10.1007/s10618-015-0444-8"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Anomaly detection: a survey",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Chandola",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "ACM Comput. Surv. (CSUR)",
            "volume": "",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "UCI machine learning repository",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dua",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Graff",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Causal feature selection",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Guyon",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Aliferis",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Computational Methods of Feature Selection",
            "volume": "",
            "issn": "",
            "pages": "79--102",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Interpreting and unifying outlier scores",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "P"
                    ],
                    "last": "Kriegel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kroger",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Schubert",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zimek",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of SIAM",
            "volume": "",
            "issn": "",
            "pages": "13--24",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Isolation forest",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "T"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Ting",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "H"
                    ],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "ICDM 2008",
            "volume": "",
            "issn": "",
            "pages": "413--422",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "A decomposition of the outlier detection problem into a set of supervised learning problems",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Paulheim",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Meusel",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Mach. Learn",
            "volume": "100",
            "issn": "2-3",
            "pages": "509--531",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Causality: Models, Reasoning and Inference",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pearl",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "29",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Bayesian network repository",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Scutari",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Algorithms for large scale Markov blanket discovery",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Tsamardinos",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "F"
                    ],
                    "last": "Aliferis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "R"
                    ],
                    "last": "Statnikov",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Statnikov",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "FLAIRS Conference",
            "volume": "2",
            "issn": "",
            "pages": "376--380",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Speculative Markov blanket discovery for optimal feature selection",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yaramakala",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Margaritis",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "ICDM 2005",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Markov boundary-based outlier mining",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Neural Netw. Learn. Syst",
            "volume": "30",
            "issn": "4",
            "pages": "1259--1264",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A survey on unsupervised outlier detection in high-dimensional numerical data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zimek",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Schubert",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "P"
                    ],
                    "last": "Kriegel",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Stat. Anal. Data Min.: ASA Data Sci. J",
            "volume": "5",
            "issn": "5",
            "pages": "363--387",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "D, a dataset with n objects and a set of m variables, denoted as X; k, the number of anomalies to output Output: top k detected anomalies 1: initialize deviation matrix \u0394n\u00d7m 2: for each Xj \u2208 X, j \u2208 {1, . . . , m} do 3: discovery MB(Xj) using fast-IAMB algorithm //relevant variable selection 4: train a prediction (CART) model gj: Xj = gj(MB(Xj)) 5: for each xi \u2208 D, i \u2208 {1, . . . , n} do 6: predict xij with gj using Equation 2 7: compute \u03b4 ij using Equation 1 //value-wise deviation 8: end for 9: end for 10: normalize \u0394 //normalization 11: for each xi \u2208 D, i \u2208 {1, . . . , n} do 12: compute anomly score \u03b4i using Equation 6 //vector-wise deviation 13: end for 14: output top-k scored objects based on descending order of \u03b4i(i \u2208 {1, . . . , n})",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The results of sensitivity experiments",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The example of the interpretation of a detected anomaly",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "a) shows the average values of all the 1038 images in the dataset, which can be seen as a representation of the normal class (digit 7 here).",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The summary of 4 synthetic and 13 real-world datasets Normal and anomaly class labels are not applicable to synthetic datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Experimental results (ROC AUC)",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Average running time (in seconds)",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We acknowledge Australian Government Training Program Scholarship, and Data to Decisions CRC (D2DCRC), Cooperative Research Centres Programme for funding this research. The work has also been partially supported by ARC Discovery Project DP170101306.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements."
        }
    ]
}