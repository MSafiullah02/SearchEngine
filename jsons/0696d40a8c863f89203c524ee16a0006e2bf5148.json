{
    "paper_id": "0696d40a8c863f89203c524ee16a0006e2bf5148",
    "metadata": {
        "title": "Detection and Estimation of Local Signals",
        "authors": [
            {
                "first": "Xiao",
                "middle": [],
                "last": "Fang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Stanford University",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We study the maximum score statistic to detect and estimate local signals in the form of change-points in the level, slope, or other property of a sequence of observations, and to segment the sequence when there appear to be multiple changes. We find that when observations are serially dependent, the change-points can lead to upwardly biased estimates of autocorrelations, resulting in a sometimes serious loss of power. Examples involving temperature variations, the level of atmospheric greenhouse gases, suicide rates and daily incidence of COVID-19 illustrate the general theory.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "We consider a problem of detection, estimation, and segmentation of local nonlinear signals imbedded in a sequence of observations. As a model, we assume for u = 1, . . . , T",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "where t 1 < t 2 < . . . < t M define the locations and f the \"shape,\" of the local signals.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "For asymptotic analysis given below, we assume that the t k are scaled, so t k /T = t 0,k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "Initially we assume the X u are fixed, but for some applications they are random. The u are independent mean 0, normally distributed errors. For the moment we assume their variances are known and equal one, and discuss later how they should be estimated. The nuisance parameters \u00b5(X u ) depend on the variable X u and may be constant or a parameterized regression function. They might, for example, be the mean values for a control group, for which it is assumed that all of the \u03be j = 0. For our general theory we assume \u03c1 is an unknown constant, but we estimate \u03c1 differently in different problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "An important special case that we return to in examples below is X u = u and \u00b5 u = \u03b1 + \u03b2[(u \u2212 (T + 1)/2)/T ], so with this notation the model becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "It is occasionally convenient to simplify several basic calculations by considering an alternative continuous time model, for which (2) can be conveniently written",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "for 0 \u2264 u \u2264 T , where dW defines white noise residuals.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "Specific choices of the function f appear to be appropriate for a number of applications, some of which are discussed below. The special case f (x) = I{x > 0} is a frequently discussed \"change-point\" model, which has been applied to a variety of problems, usually with \u03c1 assumed equal to 0. See, for example, Olshen et al. (2004) , Robbins, Gallagher, and Lund (2016) and references given there. Fang, Li and Siegmund (2018) and Fryzlewicz (2014) address a version of the problem that involves the possibility of multiple changepoints, and a major goal is segmentation of the observations to identify their number and locations, while controlling the probability of false positive detections. In this paper we extend the methods of Fang, Li and Siegmund (2018) to deal with more general signals and with observations that may have autoregressive dependence. See also Baranowski, Chen, and Fryzlewicz (2019) .",
            "cite_spans": [
                {
                    "start": 309,
                    "end": 329,
                    "text": "Olshen et al. (2004)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 332,
                    "end": 367,
                    "text": "Robbins, Gallagher, and Lund (2016)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 396,
                    "end": 424,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 732,
                    "end": 760,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 867,
                    "end": 906,
                    "text": "Baranowski, Chen, and Fryzlewicz (2019)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "An interesting special case is a broken line regression model, where f (x) = x + , so the model is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "This model has a long history, again primarily for the case of independent observations and at most one break-point in a regression line. An application to monitor kidney function after transplant has been discussed in a series of papers by A. F. M. Smith and others (e.g. Smith and Cook (1980) ). Davies (1987) and Knowles and Siegmund (1989) give theoretical analyses for independent observations. Toms and Lesperance (2003) Shin et al. (2013) , Schwartzman et al. (2013) ). In some applications it is often appropriate to add a scale parameter \u03c4 , so at u the signal located at t is of the form \u03bef [(u \u2212 t)/(\u03c4 T )] (cf. Siegmund and Worsley (1995) ). The model of \"paired change-points\"",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 294,
                    "text": "Smith and Cook (1980)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 298,
                    "end": 311,
                    "text": "Davies (1987)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 316,
                    "end": 343,
                    "text": "Knowles and Siegmund (1989)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 427,
                    "end": 445,
                    "text": "Shin et al. (2013)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 448,
                    "end": 473,
                    "text": "Schwartzman et al. (2013)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 623,
                    "end": 650,
                    "text": "Siegmund and Worsley (1995)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "of Olshen et al. (2004) can be described similarly, with f (x) = I{0 < x \u2264 1}.",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 23,
                    "text": "Olshen et al. (2004)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "In the special case that \u00b5 is a constant, X u = Y u\u22121 , and f (x) is the indicator that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "x \u2264 0, (1) is the simplest example of a threshold autoregression, as introduced by Tong and studied by Chan and Tong and others (e.g., ) in numerous projects. In a still different context, X u may be a regressor, say a biomarker, and the local signal can be used to study whether a subset of individuals, defined by the value of that biomarker, differ from others in some respect, e.g., response to a treatment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "In some applications X u may be multidimensional or the noise distribution may not be normal, with the Poisson distribution representing a particularly interesting alternative.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "Although the focus or our paper is on segmentation of multiple signals, we also develop confidence regions for t and jointly for t and \u03be. After beginning with some basic calculations and a discussion of testing for at most one change in Section 2, segmentation is discussed in Section 3 and illustrated in 3.1. New approximations to control the false positive error rate given in Section 3, which combine smooth and not smooth stochastic processes, play 4 an important role. Sections 4 and 5 briefly consider a number of other special cases of our general framework. Additional theory and examples are contained in an online Supplement.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "We have used simple models to minimize issues of over-fitting and data re-use that could arguably complicate interpretation of our segmentations. Once segmented, the models become simple linear models, so we also consider a second stage analysis to estimate more accurately the nuisance parameters and the \u03be j , which then allows us to re-evaluate the accuracy of the segmentation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "Although large sample theory suggests use of maximum likelihood estimators to estimate the nuisance parameters \u03c1 and \u03c3 2 , we find that in some cases the signals themselves introduce large bias into those estimators. We discuss some ad hoc alternatives below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction."
        },
        {
            "text": "Change.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Calculations and the Case of at Most One"
        },
        {
            "text": "To introduce our notation and provide some basic results, we begin with the model given by (2), with M = 1 or 0, and we consider a test of the hypothesis \u03be = 0. The log likelihood is given by (\u03be, t, \u03b8)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Calculations and the Case of at Most One"
        },
        {
            "text": "where \u03b8 = (\u03b1, \u03b2, \u03c1) , and we consider a test based on the maximum with respect to t of the standardized score statistic",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Calculations and the Case of at Most One"
        },
        {
            "text": "where \u03be = d /d\u03be and\u03b8 is the maximum likelihood estimator of the nuisance parameters under the hypothesis \u03be = 0; and \u03c3 2 (t) is the asymptotic variance of the numerator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Calculations and the Case of at Most One"
        },
        {
            "text": "We begin with the standard large sample expansion of the numerator in (5) given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "where I \u00b7,\u00b7 denotes elements of the Fisher information matrix and by the law of large numbers all quantities on the right hand side are evaluated at \u03be = 0 and true values of the other parameters. This expansion is valid in large samples up to terms that are o(T ) in probability. To emphasize the structure of this approximation we find it convenient to use the notation E 0 to denote expectation under the hypothesis \u03be = 0 and write (6) in the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "is not random although it depends on t; \u03b8 does not depend on t, and under the hypothesis \u03be = 0 it has mean value 0 and covariance matrix A \u22121 = I \u03b8,\u03b8 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Remark. It may be shown that the decomposition given in (6) does not depend on the normality of the j , although we can no longer use the terminology of likelihood, efficient score, etc., and must rely on the central limit theorem to justify the asymptotic normality of probability calculations given below. The special case of Poisson observaions is discussed briefly in Section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Calculations yield a number of simple propositions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Proposition 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "In particular the variance of V t is \u03c3 2 (t) = \u03a3(t, t).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Remark. In the case the local signal contains a scale parameter, so it takes the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": ", the covariance is similar in its general formulation, but slightly more complicated to compute in examples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5"
        },
        {
            "text": "Writing E t,\u03be to denote expectation under the alternative, where t and \u03be can be vectors, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": ", and consider the test statistic max t |Z t |. The false positive probability for the test that detects a signal when max t |Z t | is large is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "To evaluate this probability, we distinguish essentially two cases, one where f is continuous and the second where it is discontinuous. The case where f is the indicator of a half line or an interval (of unspecified length) is discussed in Fang, Li and Siegmund (2018) .",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 268,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "Here we assume that f is continuous and piecewise differentiable. An approximation based on Rice's formula is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "where\u017b t denotes the derivative of Z at t. For the model where the local signal at u has the form \u03bef [(u \u2212 t)/\u03c4 T ], and we maximize over both t and a range of values of \u03c4 , a first order approximation becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "where\u017b = \u2207Z t,\u03c4 . This approximation can be improved by adding terms involving edge effects and corrections for curvature. The most important is the boundary correction at the minimum value of \u03c4 , which equals (8\u03c0) \u22121/2 \u03d5(b)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": ". It is often convenient when T is large to ignore edge effects, which simplifies integration over 7 [T 0 , T 1 ]. See Siegmund and Worsley (1995) or Adler and Taylor (2007) for more detailed approximations.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 146,
                    "text": "Siegmund and Worsley (1995)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 150,
                    "end": 173,
                    "text": "Adler and Taylor (2007)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "To evaluate (8) and (9), the following result is useful.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "Finally observe that by Propositions 1 and 2, we have the \"matched filter\" conditions, and consequently",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "Proposition 5. Given t (or t, \u03c4 ), the variable V t (or V t,\u03c4 ) is sufficient for \u03be.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "For the specific case of (4), it simplifies calculations somewhat to consider the continuous time version given in (3), which amounts to replacing certain sums by integrals, leading to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "As expected, this result does not depend on the nuisance parameters \u03b1, \u03b2. It is perhaps surprising that it also does not depend on \u03c1. See the online Supplement for details.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "It is possible to obtain a confidence region for the parameters t or a joint region for t and \u03be. Calculation shows that the expected value of V s equals \u03be times the covariance of V s and V t (cf. Proposition 2). An easy consequence is that Z t = V t /\u03c3(t) is sufficient for \u03be, so the conditional distribution of Z s given Z t is the same as it would be under the null hypothesis, \u03be = 0. Now we must consider two cases, depending on whether the process has a jump at t.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "The case of a jump was studied by Fang, Li and Siegmund (2018) , so here we consider 8 the continuous case. For a similar approach based on a different probability evaluation see Knowles, Siegmund and Zhang (1991) . We use the Kac-Slepian model process, which for a mean 0 unit variance stationary, differentiable, Gaussian process, say U s , gives a parabolic approximation for the process in a neighborhood of s 0 conditional on U s 0 assuming a large value. As a consequence of those arguments, in a neighborhood of s 0",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 62,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 179,
                    "end": 213,
                    "text": "Knowles, Siegmund and Zhang (1991)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "with errors that converge to 0 in probability. It follows that the conditional distribution of max s (Z 2 s \u2212 Z 2 t ) is asymptotically \u03c7 2 with one degree of freedom, so a 1 \u2212 \u03b1 confidence region is given by the set of all s that satisfy Z 2 s \u2265 Z 2 t \u2212 \u03c7 2 1\u2212\u03b1 , where \u03c7 2 1\u2212\u03b1 is the 1 \u2212 \u03b1 quantile of the \u03c7 2 distribution with one degree of freedom. This is in effect the same result one would obtain by inverting the log likelihood ratio statistic if Z were the log likelihood of a parameter satisfying standard regularity conditions. A joint confidence region for \u03be and t is also easily obtained. To the condition that Z 2 t must be within a given distance of max s Z 2 s , we also require that |Z t \u2212 \u03be\u03c3(t)| is sufficiently small. The pairs \u03be, t that satisfy max \u03b4 [Z 2 t+\u03b4 \u2212 Z 2 t ] + (Z t \u2212 \u03be\u03c3(t)) 2 \u2264 c 2 provide a joint confidence region having the confidence coefficient",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "which equals the distribution of a \u03c7 2 random variable with two degrees of freedom, again as if standard regularity conditions had been satisfied.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "Looking ahead to the possibility of finding joint confidence regions for two (or more) change-points t 1 < t 2 , we face a more subtle argument. First note that with t = (t 1 , t 2 ), and \u03be = (\u03be 1 , \u03be 2 ), a straightforward calculation extending the result given in Proposition",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": ". This produces by a second calculation the conclusion that the log",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": ". Hence the log likelihood ratio statistic, given t, for testing that \u03be is the 0 vector equals",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "whereZ t 1 ,t 2 = \u03a3(t 1 , t 2 ) \u22121/2 (V t 1 , V t 2 ) has a standard bivariate normal distribution when \u03be = 0, and || \u00b7 || denotes the Euclidean norm. It follows from sufficiency and the Kac-Slepian argument used above that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "has a \u03c7 2 distribution with two degrees of freedom and can be used as above to obtain a joint confidence region for (t 1 , t 2 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "A joint confidence region for t, \u03be follows by an argment similar to that given above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "Remark. The confidence regions for the change-points are easy to visualize and seem reasonable. The estimates of \u03be are more problematic, since selection of the change-point value t introduces bias into the estimation of \u03be which may be exacerbated by a biased estimate of \u03c1. In Section 3 we consider a different method for estimating the parameters \u03be. The maximum Z value is about 4.60, with \u03c1 estimated to be 0.16, and occurs in 1969.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Confidence Regions"
        },
        {
            "text": "The appropriate (two-sided) p-value is about 6 \u00d7 10 \u22125 . See Figure 1 , where it appears that there might also be earlier change-points. Although the mercury thermometer was relatively young in 1659, those early years are interesting since they appear to involve the \"little ice age.\" We return to these data below for our discussion of segmentation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 61,
                    "end": 69,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Numerical Examples"
        },
        {
            "text": "Meanwhile it may be interesting to note that an approximate 95% confidence region based on the detected change in 1969 is (1895, 1990) , which reflects the local shape of the plot in Figure 1 and suggests the possibility that an increase in temperature may have actually begun during the late 19th century. 2.3 Estimation of \u03c3 2 and \u03c1: A Simulated Example.",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 128,
                    "text": "(1895,",
                    "ref_id": null
                },
                {
                    "start": 129,
                    "end": 134,
                    "text": "1990)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 183,
                    "end": 191,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Numerical Examples"
        },
        {
            "text": "Although we have assumed the variance known in our theoretical calculations, in our numerical studies we have estimated it by the residual mean square under the null hypothesis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical Examples"
        },
        {
            "text": "Other possibilities when the data are independent are sums of squares of first or second",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical Examples"
        },
        {
            "text": ", which may be preferable. First order differences remove most of the effect of changing mean values, and second order differences also mitigate the effect of slope changes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical Examples"
        },
        {
            "text": "Regarding correlation of the observations, a useful model is one that helps to control false positive errors with minimal loss of power; and to that end our working model is a first order autoregression. Using a value of \u03c1 that is too small can lead to an increase in false positives. But when there are change-points, the maximum likelihood estimator of \u03c1 under the null hypothesis that \u03be = 0 can be upwardly biased, leading to a loss of power, as we can easily see from the simulated data described below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical Examples"
        },
        {
            "text": "The simulated example in Table 1 illustrates the problem of estimating \u03c1 and the effect on the power to detect a change.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 32,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Numerical Examples"
        },
        {
            "text": "For the data in the third and fourth rows of the Table, if we assume it known that \u03c1 = 0 and use second order differences to estimate \u03c3 2 , the estimated value is 0.89 and the maximum Z value is 6.77. In all cases, the maximizing value of t is 94. In most other cases as well the maximizing value of t provides a reasonable estimator of the true value.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 49,
                    "end": 55,
                    "text": "Table,",
                    "ref_id": null
                }
            ],
            "section": "Numerical Examples"
        },
        {
            "text": "It is clear from these examples and others not shown that the existence of a change-point causes the estimator of \u03c1 to be upwardly biased and results in a loss of power compared to using the true value of \u03c1. This is usually worse if there are multiple change-points. If a plot of the data indicates long stretches without change-points, one might use an estimator of \u03c1 from that segment. For the data in the next to last row of Table 1 , an estimate of \u03c1 based on 13 A second consideration in estimation of \u03c1 is robustness of our procedure against more complex forms of dependence. Without exploring this question in detail, consider again the next to last row of Table 1 A different approach (e.g., Robbins, Gallagher, and Lund (2016) ) for dealing with dependence in the standard change-point model (i.e., f (s) = I{s > 0}) is to assume that the estimated residuals from a least squares fit of the null model form a locally dependent stationary process and estimate the autocovariances accordingly. Weak convergence arguments typically indicate that a rescaled process converges weakly to a Gaussian process, often related to a Brownian Bridge, which can be studied to look for change-points.",
            "cite_spans": [
                {
                    "start": 700,
                    "end": 735,
                    "text": "Robbins, Gallagher, and Lund (2016)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 428,
                    "end": 435,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 664,
                    "end": 671,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Numerical Examples"
        },
        {
            "text": "Although this method has some advantages, we prefer our approach for facilitating quantitative statements in the form of hypothesis tests and confidence intervals after a minimal amount of \"data snooping\" to arrive at a useful model. If we are successful in detecting the signal locations, our model for the other parameters becomes a standard linear model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical Examples"
        },
        {
            "text": "In Section 3.1 we discuss analysis of this linear model as a test of our approach.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical Examples"
        },
        {
            "text": "We now turn to the problem of segmentation when there may be several change-points. It is helpful conceptually to think of two different cases. In the first the signals exhibit no particular pattern. In the second two or more changes are expected to produce a definite signature. For the problem of changes in levels of the process, paired changes frequently take the form of an abrupt departure from followed by a return to a baseline level.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "15"
        },
        {
            "text": "Here we initially concentrate on the case of signals having no apparent pattern and consider versions of the two methods suggested in Fang, Li and Siegmund (2018) for detection of jump changes. First we consider a pseudo-sequential procedure, called Seq below, where it is convenient to write Z(t, T ) to denote the statistic Z(t) when the interval of observation",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 162,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "15"
        },
        {
            "text": "We also use a minimum sample size m 0 and a minimum lag n 0 , both of which we often set equal to 5. Let t 0 = m 0 and let T be the smallest integer exceeding t 0 + n 0 such that for some t 0 < t < T \u2212 n 0 , the value of Z(t, T ) exceeds a threshold b to be determined by a constraint on the probability of false positive error. The first change-point is taken to be t 1 : equal to arg max |Z t |, or the smallest or largest value of t for that smallest T . (In numerical experiments we have been unable to find a consistent preference.) The process is then iterated starting from t 1 . The nuisance parameter \u03c1 is assumed to be known. In practice it is estimated from the data or more often from a subset of the data to mitigate the effect of the bias discussed above. The other nuisance parameters, are estimated from the current, evolving interval of observations, so that the nuisance parameters associated with one change-point do not confound detection of another. It would also be possible to estimate \u03c1 from the currently studied subset of the data, but this estimator appears to be unstable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "15"
        },
        {
            "text": "To control the global false positive error rate, we want an approximation for",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "where m is the number of observations and m 0 , n 0 represent minimal sample sizes, both of which we frequently take to be 5. To emphasize that T is a variable quantity, we also write",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "A derivation of the approximation (15) begins from the initial approximation of the probability of interest by a sum over T of the sum over t \u2264 T \u2212 n 0 of the integral over x from 0 to \u221e of the product of three factors:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": ". If we choose \u03b4, so that \u03b4b is very small, the integral is dominated by small values of x. The third factor is approximately P{max j S j \u2264 \u2212x}, for a suitable random walk. For small x, it is approximately P{max j S j < 0}. Putting these two observations together leads to the approximation given by (15).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "A slightly more complex calculation leads to a different and apparently preferable approximation, which is based on a similar analysis but incorporates in addition the conditional probability, given",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "T ) 2 and hence is an independent \u03c7 2 1 random variable. This leads to an expression similar to (15), but with the integrand in (15) replaced by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "In spite of its somewhat different appearance, this second approximation is slightly less conservative than the first, but gives very similar numerical values. For m 0 = n 0 = 3 and m = 250, the first approximation gives a 0.05 threshold of 4.08, while the second gives 3.97. Since these probabilities vary substantially for small values of m 0 , n 0 , and neighboring changes in slope seem difficult to detect and rarely to occur in the data we have studied, we typically use the values m 0 = n 0 = 5, which for this example would reduce the 0.05 threshold given by (16) from 3.97 to 3.89.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "The approximation (16) When this process detects a change in slope, for the smallest value of T there typically is a cluster of values of t where |Z(t, T )| exceeds the chosen threshold. In practice we might reasonably choose the smallest such t, or the value that maximizes |Z(t, T )|. We then iterate the process starting from the selected t. Upon iteration the background linear regression is estimated using the data beginning with that t. It may not be clear which 18 choice is better in a particular application.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "Remarks. (i) Although we use this \"pseudo-sequential\" method with a given amount of data, it can also be used for actual sequential detection, as in the problem of monitoring kidney transplants discussed in Smith and Cook (1980) .",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 228,
                    "text": "Smith and Cook (1980)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "(ii) Although sample paths of the process Z(t, T ) as a function of t are smooth, as a function of T they behave locally like a random walk. A consequence is that to obtain a mathematically precise derivation of the approximation (15), local perturbations of the paramter T must be scaled by a small factor, say \u2206, which converges to 0 at the rate of Fang, Li and Siegmund (2018) for the case of abrupt changes in the mean. A similar remark applies to (17) given below.",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 379,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "A second method of segmentation, called MS below (for Maximum Score Statistic), uses",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "An asymptotic approximation to the null probability that this expression exceeds b can be computed by a modification of the argument behind (16), and we obtain the approximating expression",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "where \u03b2 0 (t, T 0 ) is defined similarly to \u03b2(t, T ). The evaluation of (17) can be simplified by a summation by parts and the observation that the various functions of three variables, (T 0 , t, T 1 ), occurring in (17) are in fact functions of two variables:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "These methods determine a list of putative local signals together with generally overlapping backgrounds. We consider different algorithms for selecting a set of local signals and backgrounds from this list, with a preference for the additional constraint that no background overlap two local signals. In Fang, Li and Siegmund (2018) examples of particular interest were the shortest background, (cf. also Baranowski, Chen, and Fryzlewicz (2019)), and the largest Z value. For the second of these methods we have paid in advance for false positive errors, so we can also consider other methods for searching the list, e.g., subjective",
            "cite_spans": [
                {
                    "start": 305,
                    "end": 333,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "consistency with a plot of the data. We also find it convenient computationally to follow the suggestion of Fryzlewicz (2014) by searching a random subset of intervals, which seems to work very well. When it appears that the number of changes is small, as in the examples considered in this paper, one might also use a combination of methods, e.g., searching a small random subset of intervals at a low threshold to generate candidates, then sorting those by hand with tests to detect at least one change at the appropriate higher threshold.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "The methods discussed so far are \"bottom up\" methods in the sense that they attempt to identify one local signal at a time against a background appropriate for that signal. A popular \"top down\" method in the change-point literature scans all the data looking for a pair of change-points (e.g., Olshen et al. (2004) ). In some cases, especially if there is only one change to be detected, the method will \"detect\" two changes, as required, but it will put one of them near an end of the data sequence, where it can be recognized and ignored.",
            "cite_spans": [
                {
                    "start": 294,
                    "end": 314,
                    "text": "Olshen et al. (2004)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "After an initial discovery of one or two change-points, the sequence is broken into two or three parts by those change-points, and the process is iterated as long as new change-points are identified. Here we also consider the statistic max s<t\u2212h U s,t , where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "and h is a parameter that represents a minimum distance between changes that we find interesting (usually taken to be 5 or 10 in the examples below). An appropriate threshold may be determined from the approximation (closely related to (9))",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "This approximation is approximately linear in the length of the sequence searched, so iterating the process leads in a final step to roughly the same probability of a false positive as is present in the first step. If we assume that in early iterations, only true positives are detected, then the iterative process does not create a multiple comparisons issue. As we will see in the examples below, this method has very good power properties, although its use requires somewhat more thoughtful analysis than the other methods. Since the process Z t can have relatively large local correlations, when there is only one change to be detected in an interval, and hence we expect to find a second \"fake\" change near an end-point of the interval searched, that \"fake\" change may be somewhat distant from the end-point, so it is not clear without additional analysis whether it should be ignored. Use of a linear analysis as in the following section can be very helpful.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "16"
        },
        {
            "text": "A search algorithm based on the maximum of (18) If we use (18) The plot of Z t (Figure 2 ) contains a broad peak and a local maximum in the late 20th century, which suggests the possibility of more than one change. A 95% confidence interval based on the theory suggested above is [1866, 1972] , which leaves open the possibility of a second change. Seq and MS confirm a 19th century change, but they fail to detect one in the 20th century even if the autocorrelation is set to 0. With the estimated autocorrelation of 0.24, the statistic (18) detects changes in both 1897 and 1980. A linear analysis with these two change points yields and R 2 of 0.33 and an autocorrelation of 0.03, which does not test to be different from 0. Temperature anomalies of several other countries in Europe exhibit similar, but not identical behavior. In these cases the test to detect a single change will sometimes pick out a local maximum in the 19th century and sometimes in the 20th, while the statistic (18) ",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 286,
                    "text": "[1866,",
                    "ref_id": null
                },
                {
                    "start": 287,
                    "end": 292,
                    "text": "1972]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 79,
                    "end": 88,
                    "text": "(Figure 2",
                    "ref_id": null
                }
            ],
            "section": "16"
        },
        {
            "text": "We consider again the model (2), now with \u00b5 u an unknown constant and f (u) = 1{u > 0}, so the t k are change-points in the level of the observed process. For some applications it may seem more appropriate to use f (u) = 1{0 < u/\u03c4 \u2264 1}, so the length of a local interval where the mean changes is an unknown constant \u03c4 , which may vary from one interval to another when there are multiple changepoints. Since the case \u03c1 = 0 has been widely studied, here we concentrate on time series, where the first order autoregressive dependence discussed above may be pertinent. Again it turns out that change-points introduce bias into the estimation of \u03c1, so we consider estimators based on subsets of the data that appear to be free of change-points.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "26"
        },
        {
            "text": "We consider primarily methods taken from Fang, Li and Siegmund (2018), which, as noted above, motivate the structure of the bottom up segmentation methods suggested in this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "26"
        },
        {
            "text": "We also consider the top down method of circular binary segmentation (CBS) (Olshen et al. (2004) ), which may have advantages when a change that increases (decreases) the mean value tends to be followed by a change that decreases (increases) the mean, since it directly models such paired change-points. And finally we consider the analogue of (18), which was not studied in Fang, Li and Siegmund (2018) . To that end we take \u03c1 = 0 for simplicity and let S t = t 1 Y u . Then V t = (tS m /m \u2212 S t ) , so U s,t is defined by the equation (18) for the approriately modified covariance matrix with entries s(1 \u2212 t/m) for s \u2264 t. In this section we continue to refer to this as the statistic (18). Two interesting climate related time series are AMO (Atlantic Multidecadal Oscillation), for which there are data beginning in 1856, and PDO (Pacific Decadal Oscillation) with data from 1900. In both cases the data have been recorded monthly, but we average 27 the monthly data to get annual time series with 163 and 118 values, respectively.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 96,
                    "text": "(Olshen et al. (2004)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 375,
                    "end": 403,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "26"
        },
        {
            "text": "A visual inspection of the AMO data suggests that there is no change-point before about 1900, so we estimate \u03c1 from the first 45 observations to get the value 0.38. All methods agree that there is a decrease in the mean at 46 (1901) , an increase at 70 (1925), a decrease about at 108 (1963) , and the latest increase at 139 (1995) . A linear analysis with these change-points returns an R 2 of about 0.72 and an auto-correlation of 0. It may be interesting to observe that the changes in slope detected in the Northern Hemisphere ocean temperature anomalies (Example 6 in Section 3.1) fall very close to the mid-points of these three intervals.",
            "cite_spans": [
                {
                    "start": 223,
                    "end": 232,
                    "text": "46 (1901)",
                    "ref_id": null
                },
                {
                    "start": 285,
                    "end": 291,
                    "text": "(1963)",
                    "ref_id": null
                },
                {
                    "start": 325,
                    "end": 331,
                    "text": "(1995)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "26"
        },
        {
            "text": "The PDO data are similar, but changes appear to be more frequent, as the name suggests, and a cumulative sum plot appears very noisy. For these data level changes seem to alternate positive and negative directions, so CBS and related methods that look for paired changes appear to have an advantage. Although the pseudo-sequential method detected the same changes in the PDO data as CBS, its detections at 76 and 114 were borderline.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "26"
        },
        {
            "text": "It is relatively straightforward to extend the analysis given above to generalized linear models, at least for independent observations. For example, suppose that Y u has the log likelihood",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "The efficient score to test the hypothesis \u03be = 0 or segment the sequence is much as before, although computation of \u03a8(t) is more complicated. Fortunately at least for the following examples, it seems plausible to assume independent (quasi)Poisson observations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "A perhaps more flexible model than (20) is an asymptotic quasi-likelihood model, e.g., Heyde (1997) , where there is the possibility of different models for the mean and variance. A very similar model to the one we developed in Section 2 is obtained by setting \u03c8(\u03b8 u ) = \u03b8 2 u /2 in (20), but we interpret \u03b8 u as the conditional expectation of Y u given the observations up to u \u2212 1, so the efficient score to detect a local signal is a sum of martingale differences. It is then straightforward to extend the methods developed above. We have studied several score statistics to detect local signals in the form of changes of level, slope, or (in the online Supplement) the autoregressive coefficient. To segment the observations, we consider two \"bottom up\" methods patterned after the methods of multiple change-point segmentation in Fang, Li and Siegmund (2018) and one \"top down\" method, defined in (18).",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 99,
                    "text": "Heyde (1997)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 835,
                    "end": 863,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "Estimation of the nuisance parameters, \u03c1 and \u03c3 2 , pose special problems. For the bottom up methods, in examining an interval of observations for possible change-points, intercept, level, slope and variance of the process are estimated locally, i.e., using only the data from the interval under consideration. For the autoregressive coefficient, our theory suggests using the (global) maximum likelihood estimator estimator under the hypothesis of no change. Since this estimator can be badly biased and result in a loss of power when there are change-points, we also consider ad hoc methods based on examination of different parts of the data. For use of (18), which estimates of \u03c3 2 based on the (usually long) interval being searched, a possible method for dealing with variance heterogeneity arising from the early part of the data is to discard initial segments of varying length and consider the stability of the results obtained.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "The multiple regression analsis suggested in Section 3.1, based on the assumption that detected break-points are correct, allows us to see if our segmentation is reasonable, estimate the magnitude of the detected signals, and reconsider our chosen value of \u03c1 to see if we have used one that is too small. In most examples, it appears that we have used a larger value than necessary, which may result in a loss of power in the case of a large discrepancy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "A different approach would be to use initially the value \u03c1 = 0, or some other small value to detect change points, apply the multiple regression analysis to discard changes that seem to be superfluous, and then iterate the process with the value of \u03c1 suggested by the regression analysis. We have not chosen this path because our goal has been to use a value of \u03c1 obtained with at most a small amount of \"data snooping,\" in order to claim that due to minimal re-use of the data the false positive error rate has been adequately controlled.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "Because our emphasis has been on detection of local signals, we have admittedly been For problems involving sparse, bump like changes as discussed briefly the Supplement, some superficial analysis suggests that our methods work well since most of the data are consistent with the null model, and hence global estimation of nuisance parameters does not pose a serious problem. In view of the ubiquity of genomic applications where \"bumps\" have different signatures suggested by a combination of science and experimental technique, and where the background may involve use of a control group it seems worthwhile to pursue a more systematic study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "Here we have considered signals in one-dimensional processes, where the number of possible \"shapes\" of the signals is relatively small. In view of the much larger variety of possible multi-dimensional signal shapes and the variety of approaches already existing in the literature a systematic comparative study of that problem may be valuable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "28"
        },
        {
            "text": "In this appendix we record basic formulas we have used in the paper. Assume the model is given by (2) ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "or alternatively the solution of (3) with Y 0 = 0 is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "Hence, under the null hypothesis that \u03be = 0 we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "For the special case of broken line regression in continuous time the first and second coor-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "The matrix A \u22121 = E( \u03b8 \u03b8 ) is straightforward to compute and somewhat tedious to invert. A very useful result is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "It follows that the numerator of Z t , has covariance function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "which does not depend on \u03b1, \u03b2, nor on \u03c1. In particular",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "To derive (15) and (17) (see below), we must consider T as variable and study the quantities \u03c3 2 , \u03a8, etc., as functions of both t and T . It is obvious from (25) that \u2202\u03c3 2 (t, T )/\u2202T does not depend on nuisance parameters. Since \u03a8(t, T ) depends on the nuisance parameters (\u03b1, \u03b2, \u03c1) only in its third coordinate, if we first differentiate (23) with respect to T , then multiply on the right by \u03a8(t, T ) it follows from (23) that neither (\u2202\u03a8/\u2202T ) A T \u03a8(t, T ) nor \u03a8(t, T ) (\u2202A T /\u2202T )\u03a8(t, T ) depends on nuisance paramters. The approximation (17) requires that we introduce T 0 < t as a third parameter. For broken line regression we also require",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "To study the behavior of Z t when \u03be is different from 0, and to help with the interpretation of a plot of Z t , we put t = (t 1 , . . . , t k ), \u03be = (\u03be 1 , . . . , \u03be k ) and use the notation E t,\u03be (\u00b7). By combining the results given above we find that Figure 6 ). Since estimates of \u03c1 based on segments of the data that appear not to involve changes were no more than 0.9, we tried using Seq with that value of suggests there may be a slope decrease in 1943, which together with the two other changes detected increases R 2 to 0.997. This change may be interesting insofar as it appears to coincide roughly with the hiatus in temperature increases in Greenland shown in Example 3 and the dip in northern hemisphere ocean anomalies mentioned in Example 10.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 252,
                    "end": 260,
                    "text": "Figure 6",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "Appendix C: Bump Hunting: fixed shape, variable amplitude and scale.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "In a variety of problems, the local signal to be detected occurs in a departure from, followed by a return to, a baseline value. In many cases the shape of the local signal arises naturally from the scientific context. An important example is inherited copy number variation, where there is an abrupt increase or decrease from the baseline value of two, which is followed quickly by a return to the baseline (e.g., Zhang et al. (2010) ). Other examples are ChIP-Seq, where a shape to expect is roughly triangular or double exponential (e.g., Shin et al. (2013) ) , or differential methylation, where a normal probability density function or an integral thereof might be reasonable (cf. ).",
            "cite_spans": [
                {
                    "start": 415,
                    "end": 434,
                    "text": "Zhang et al. (2010)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 542,
                    "end": 560,
                    "text": "Shin et al. (2013)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "We modify (2) to include a scale paramter \u03c4 , to obtain the log likelihood function",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "Here f is a positive, symmetric integrable function, e.g., the square root of (i) a standard normal density function, (ii) a triangular probability density on [\u22121, 1], (iii) a double exponential probability density, or (iv) a uniform probability density function on [\u22121/2, 1/2].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "Consider the case of at most one bump and assume that the search interval is long enough relative to the scale parameter that it is reasonable to ignore end effects. We define our standardized statistic as above, but we now write it as Z t,\u03c4 to reflect the unknown location t and width \u03c4 of the bump. An approximation to its false positive error probability is given in display (9). (For (iv) above a different approximation is required because of the discontinuities in f . See Fang, Li and Siegmund (2018) .)",
            "cite_spans": [
                {
                    "start": 479,
                    "end": 507,
                    "text": "Fang, Li and Siegmund (2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "Often bumps are relatively sparse, which makes estimation of \u03c1 and \u03c3 2 relatively easy, so one can see intervals of the data that can be used for estimating autocorrelation and other nuisance parameters without serious bias.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "Although a serious study of these methods is warranted by the variety of potential applications, we do not discuss these in detail here, which would require a lengthy investigation of the specific scientific features of those applications. In both the South Korean and the China data, the net slope change is close to zero. It may be interesting to consider a (quasi)Poisson log linear analysis, which usually agrees with the linear least squares analysis. The dispersion parameter often indicates large overdispersion. We have also considered a detection strategy using a square root transformation of the dependent variable, which would be expected to stabilize Poisson like variability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "Simulations suggest that this technique can be useful, but also that heteroscedasticity is usually not a serious problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL Appendix A: Some Basic Calculations"
        },
        {
            "text": "As a final illustration of our methods we consider a simple case of Threshold Autoregression, which has been studied in a number of papers by Tong and colleagues.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "For a model assume",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "To test \u03be = 0, in the notation of Section 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "and asymptotically under the null hypothesis we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "and A \u22121 is the 2 \u00d7 2 matrix with entries a 11 = T, a 12 = T E(Y ), a 22 = T E(Y 2 ), where Y denotes the stationary distribution of Y u under the null hypothesis (and the stationarity assumption that |\u03c1| < 1). Hence",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "tions show that the covariance of the standardized score statistics Z s and Z t are given by [G(min(s, t) ) \u2212 \u03a8 (s)A\u03a8(t)]/\u03c3(s)\u03c3(t). Hence locally for small \u03b4",
            "cite_spans": [
                {
                    "start": 93,
                    "end": 105,
                    "text": "[G(min(s, t)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "where the functions G, \u03a8 and\u0120 are evaluated at t, and slightly more generally Cov[(Z t+\u03b4 1 , Z t+\u03b4 2 )|Z t ] \u2248 min(\u03b4 1 , \u03b4 2 )[G \u2212 \u03a8 A\u03a8] \u22121\u0120 . This allows us to calculate an approximation to P{max t |Z t | \u2265 b}. The methods of, e.g., Woodroofe (1976) or Yakir (2013) lead after some calculation to",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 250,
                    "text": "Woodroofe (1976)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "We have evaluated (31) by numerical integration, and the p-values given below come from that evaluation with the observed mean value and standard deviation. We have also implemented an empirical version of this approximation, where the entire computation is based on the appropriately estimated quantities. These two approximations are in rough agreement for our examples, where the p-value is very small. Simulations suggest that the Type I error control of (31) is adequate under the model The power also seems reasonable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "In principle, one should learn something from the maximizing point of the statistic Z s relative to the estimated values of \u00b5 and of \u03c3, but in simulations the maximizing value of shows more variability than we can easily interpret. A calculation similar to that giving (30) indicates that in the case that \u03be = 0 the expected value of Z t is approximately \u03beT 1/2 \u03c3(t), as it would be for likelihood theory with standard regularity conditions. This approximation seems relatively stable and suggests a rough approximation for \u03be by equating the approximation to the observed value of max s Z s (or min s Z s ).",
            "cite_spans": [
                {
                    "start": 269,
                    "end": 273,
                    "text": "(30)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "With a slight modification in principle, but substantially more detailed calculation in application, one can consider higher order autoregressions. Suppose, for example, the model is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "Now X t is a two-dimensional vector, while G(t), \u03a8(t), and \u03c3 2 (t) = \u03a3 t , say, are 2 \u00d7 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "matrices. An appropriate test statistic has the form max t V t \u03a3 \u22121 t V t . Putting Z t = \u03a3 \u22121/2 V t , we can control the false positive probability based on the approximation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "where e = (cos(\u03c9), sin(\u03c9)) , and where \u03c0 \u22121 times the integral over \u03c9 equals the trace of \u03a3 \u22121 t\u0120 (t).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "Simulations suggest that the Type I error control of this procedure is adequate under the model. The power also seems reasonable. In principle, one should learn something from the maximizing point of the statistic Z t relative to the estimated values of \u00b5 and of \u03c3, but in simulations the maximizing value of t shows more variability than we can easily interpret.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "Examples. As illustrative applications, we follow in considering the Canadian lynx data and Nicholson's blowfly data. We do not, however, try to give a complete discussion of these well studied data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "The Canadian lynx data consist of annual counts over a 114 year period, which are considered a surrogate for the size of the lynx population. The scientific reasoning behind a two-phase model is the hypothesis that when the lynx population is small, it finds a more 48 than adequate food supply and increases until it reaches a size where the food supply is inadequate, when it then decreases until the cycle begins again. Empirical observations suggest that the decrease in times of food shortage is more rapid than the increase in times of abundance. If we use a first order auto regressive model, these fluctuations suggest a positive autoregressive parameter in times of food abundance, which decreases when there is a food shortage. Although there are clear outliers in the data, which has lead others to consider transformations to shorten the tails, we analyze the original data. Employing the first order autoregressive model with a possible change in the autoregressive parameter, as suggested above yields null estimators of E(Y ) and Var(Y ) of 1538 and 1560, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "The maximum value of Z t is 3.89, which occurs at about t = 3500, and the resulting (two-sided) p-value based on (31) is approximately 0.004.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        },
        {
            "text": "Nicholson's blowfly data (Brillinger (et al. 1980 ) and the web site www.stat.berkeley.edu/ brill/blowfly97I.html contains 361 observations in four columns, labeled respectively \"births,\" \"nonemerging,\" \"emerging,\" and \"deaths.\" The first, third, and fourth columns vary from numbers close to 0 to numbers in the several thousands. The second column typically involves substantially smaller numbers. For the first column. the estimated mean value, autocorrelation, and standard deviation under the null model are 1438, 0.6 and 1670, respectively. The maximum Z value is about 4.73, which occurs for t \u2248 5300. The p-value for the hypothesis of no change is approximately 2 \u00d7 10 \u22124 . For \"emerging\" the maximum Z-value is about 5.22, which gives a p-value of about 6 \u00d7 10 \u22125 . For these data the change in the autocorrelation is positive.",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 49,
                    "text": "(Brillinger (et al. 1980",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Appendix E: Threshold Autoregression"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Random Fields and Geometry",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Adler",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Taylor",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Narrowest-over-threshold detection of multiple change-points and change-point-like features",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Baranowsk",
                    "suffix": ""
                },
                {
                    "first": "Yining",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fryzlewicz",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Roy. Statist. Soc. B",
            "volume": "89",
            "issn": "",
            "pages": "649--672",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "On likelihood ratio tests for threshold autoregression JRSSB",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "S"
                    ],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Tong",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "",
            "volume": "52",
            "issn": "",
            "pages": "469--476",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Hypothesis testing when a nuisance parameter is present only under the alternative",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Davies",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "Biometrika",
            "volume": "74",
            "issn": "",
            "pages": "33--43",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Segmentation and estimation of change-point models: false positive control and confidence regions Arkiv",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "O"
                    ],
                    "last": "Siegmund",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Wild binary segmentation for multiple change-pont detection",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fryzlewicz",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Ann. Statist",
            "volume": "42",
            "issn": "",
            "pages": "2243--2281",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Quasi-Likelihood and Its Application",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Heyde",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Significance analysis and statistical dissection of variably methylated regions",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Jaffe",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Feinberg",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Irizarry",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Leek",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Biostatistics",
            "volume": "13",
            "issn": "",
            "pages": "166--178",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Bump hunting to identify differentially methylated regions in epigenetic epidemiology studies",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Jaffe",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Murakami",
                    "suffix": ""
                },
                {
                    "first": "Hwajin",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Leek",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "Daniele"
                    ],
                    "last": "Fallin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Feinberg",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Irizarry",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Int. J. of Epidemiology",
            "volume": "41",
            "issn": "",
            "pages": "200--209",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Morice Hemispheric and large-scale land surface air temperature variations: An extesive revision and an update to 2010",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "D"
                    ],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "H"
                    ],
                    "last": "Lister",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "J"
                    ],
                    "last": "Osborn",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Harpham",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Salmon",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "P"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "J. Geophys. Res",
            "volume": "117",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1029/2011JD017139"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "On Hotelling's approach to testing for a nonlinear parameter in regression Int",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Knowles",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Siegmund",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Statist. Rev",
            "volume": "57",
            "issn": "",
            "pages": "205--220",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Confidence regions in semilinear regression",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Knowles",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Siegmund",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "P"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "Biometrika",
            "volume": "79",
            "issn": "",
            "pages": "15--31",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A log-linear model for a Poisson process change-point",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Loader",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Ann. Statist",
            "volume": "20",
            "issn": "",
            "pages": "1391--1411",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Central England temperatures:monthly means 1659 to 1973 Quarterly",
            "authors": [
                {
                    "first": "Gordon",
                    "middle": [],
                    "last": "Manley",
                    "suffix": ""
                }
            ],
            "year": 1974,
            "venue": "J. Roy. Meteorological Soc",
            "volume": "100",
            "issn": "",
            "pages": "389--405",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Testing with a nuisance parameter present only under the alternative: a score based approach with application to segmented modeling",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "M"
                    ],
                    "last": "Muggeo",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "J. Statistical Computation and Simulation",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1080/00949655.2016.1149855"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Circular binary segmentation for the analysis of array-based DNA copy number data",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Olshen",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "S"
                    ],
                    "last": "Venkatraman",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Lucito",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wigler",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Biostatistics",
            "volume": "5",
            "issn": "",
            "pages": "557--572",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Bayesian analysis of a Poisson process with a change-point",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Raftery",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "E"
                    ],
                    "last": "Akman",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "Biometrika",
            "volume": "73",
            "issn": "",
            "pages": "85--80",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A general regression change-point test for time series data",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Robbins",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Gallagher",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Lund",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Jour. Amer. Statist. Assoc",
            "volume": "111",
            "issn": "",
            "pages": "670--683",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Multiple testing of local maxima for detection of peaks in ChIP-seq data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Schwartzman",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jaffe",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gavrilov",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "E"
                    ],
                    "last": "Meyer",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Ann. Appl. Statist",
            "volume": "7",
            "issn": "",
            "pages": "471--494",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Computational methodology for ChIP-seq analysis",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [
                        "S"
                    ],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Quantitative Biology",
            "volume": "1",
            "issn": "",
            "pages": "54--70",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Testing for a signal with unknown location and scale in a stationary Gaussian random field",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "O"
                    ],
                    "last": "Siegmund",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Worsley",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Ann. Statist",
            "volume": "23",
            "issn": "",
            "pages": "608--639",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Strait lines with a change-point: an analysis of some renal transplant data",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "F M"
                    ],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "G"
                    ],
                    "last": "Cook",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "J. Roy. Statist. Soc. Series C",
            "volume": "29",
            "issn": "",
            "pages": "180--189",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Piecewise regression: a tool for identifying ecological thresholds",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Toms",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Lesperance",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Ecology",
            "volume": "84",
            "issn": "",
            "pages": "2034--2041",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Detecting simultaneous changepoints in multiple sequences. With supplementary data available online",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "R"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "O"
                    ],
                    "last": "Siegmund",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Z"
                    ],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Biometrika",
            "volume": "97",
            "issn": "",
            "pages": "631--645",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Empirical modelling of population time series data: the case of age and density dependent vital rates",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "References",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Brillinger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Guckenheimer",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Guttorp",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Oster",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "Lectures on Mathematics in the Life Sciences",
            "volume": "13",
            "issn": "",
            "pages": "65--90",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Percentage points of the likelihood ratio test for threshold autoregression JRSSB",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "S"
                    ],
                    "last": "Chan",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "",
            "volume": "53",
            "issn": "",
            "pages": "691--696",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "On likelihood ratio tests for threshold autoregression JRSSB",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "S"
                    ],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Tong",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "",
            "volume": "52",
            "issn": "",
            "pages": "469--476",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Sea level rise from the late 19th to the early 21st century Surv",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Church",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "J"
                    ],
                    "last": "White",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Geophysis",
            "volume": "32",
            "issn": "",
            "pages": "585--602",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Significance analysis and statistical dissection of variably methylated regions",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Jaffe",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Feinberg",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Irizarry",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Leek",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Biostatistics",
            "volume": "13",
            "issn": "",
            "pages": "166--178",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Bump hunting to identify differentially methylated regions in epigenetic epidemiology studies",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Jaffe",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Murakami",
                    "suffix": ""
                },
                {
                    "first": "Hwajin",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Leek",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "Daniele"
                    ],
                    "last": "Fallin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Feinberg",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Irizarry",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Int. J. of Epidemiology",
            "volume": "41",
            "issn": "",
            "pages": "200--209",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "provides an analysis and ecological applications. Several of the examples discussed below involve climatological time series, or day by day newly confirmed cases of COVID-19. Although alternative models are possible, a broken line model provides a conceptual framework that allows us to fit an easy to understand model and to ask whether the breakpoints represent changes of scientific importance. In a variety of genomic applications t denotes a genomic location, and different applications suggest functions f having different characteristic shapes. For example, for detection of copy number variations (CNV) the indicator of an interval or a half line may be appropriate. For detection of differentially methylated genomic regions Jaffe, et al. (2012a) suggests a \"bump\" function. See simple examples below and elaborations of this model involving covariates in Jaffe, et al. (2012b). For ChIP-Seq analysis steeply decaying symmetric functions like (1 \u2212 |u|) + , exp(\u2212|u|), or a normal probability density function are different possibilities (cf.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Plot of Z 2 t for central england temperature data,1659-2017.  12",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": ", but where there is a second order autoregressive coefficient of -0.2 and \u03be = \u22120.06. A simulation of this case gives an estimator of \u03c1 equal to 0.6 based on all the data, with a maximuom Z-value of 3.32, and an estimator equal to 0.42 based on the second half of the data, with a maximum Z-value of 4.85. Other simulations, not given here, suggest that at least for higher order autoregressive dependence, the first order model we have suggested provides reasonable protection against an inflated false positive error rate.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "may be used to assign a p-value to the the individual detections. For example, suppose we use the threshold b = 4.09, which for m 0 = n 0 = 5 and m = 500 gives a global false positive error of 0.05. Suppose also that we detect a change at t = 50 with T = 70 and Z(50, 70) = 4.49. The approximation (16) for m = 70 and b = 4.49, which equals 0.001, can be regarded as a p-value for that detection. This argument can be applied iteratively to obtain p-values for a second and subsequent detections. In view of the lag between a putative change-point t and the value of T when detection occurs, it is possible for the sum of p-values to exceed the global significance value, although numerical experimentation indicates that this rarely occurs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "is easily adapted to obtain joint confidence regions for a pair of change points along the lines discussed toward the end of Section 2.1.An advantage of the top down feature is that the intervals searched are usually relatively long, so it may be reasonable to estimate different values of \u03c1 for the different intervals searched. If we assume that large changes are detected first, this method would presumably reduce the estimated value of \u03c1, in subsequent searches, but the problem of bias would remain as long as subsequent searches involve intervals containing changes. For the sake of consistency in our examples we have not explored this option.In this section we consider a number of data sets, where broken line regression is arguably an appropriate model, or at least where it allows one to address questions of interest regarding the times of changes in long term trends. Since all these examples involve observational time series, an assumption of independence seems inappropriate, although in several cases the estimated value of \u03c1 is small enough to be assumed equal to zero.After employing a detection method, we assume that the detections are in fact correct,so our model becomes a standard linear model. Estimation of the parameters of the linear model gives us an idea of the size and importance of different detected changes, the adequacy of the model as judged by the value of R 2 and\u03c1, and the reasonableness of our choice for the value of \u03c1 in our initial segmentation.Example 3: Greenland temperatures 1901-2016. An example from a site sponsored by the World Bank, https://climateknowledgeportal.worldbank.org/download-data is the average annual temperature of Greenland, which may be particularly interesting in view of the large amount of water contained in its glaciers. Data are provided for the years 1901-2016. SeeFigure 4in the online Supplement for a plot of Z t , which suggests a small increase followed by a decrease early in the 20th century, then a second increase near the end of the century. If we use all 116 observations to estimate \u03c1, we obtain\u03c1 = 0.13 and the test to detect at least one change detects a decrease in slope occurring in about 1929. If we use only the first half or the second half of the data to estimate \u03c1, we obtain a value slightly less than 0.1 and with this value detect an increase in 1993. With the smaller estimate of \u03c1, Seq detects three changes: an increase in 1919, a decrease in 1929, and a second increase that seems to occur about 1973, although 1984 gives essentially the same observed value of the statistic. With MS, we detect changes in 1929 and 1984. If we set \u03c1 = 0, we also barely detect a change in 1919. A linear model incorporating these three changes begins with an initially negative slope, with the first two changes essentially canceling each other, and the third change about three times as large (in magnitude) as the initial negative slope. The effect of putting these changes into the model is to increase R 2 from an initial value of 0.28 to approximately 0.7, while reducing the estimated value of \u03c1 from 0.13 to 0.02, which does not test to be different from 0. Although the change at 1919 is borderline, omitting it from the model provides a less satisfactory result (details omitted).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "detects two changes. This is perhaps not surprising since the method was designed for such data. For Switzerland, a 90 % joint confidence region for the two change-points with \u03c1 set equal to 0 contains about 2000 pairs of points. Typical examples near the extremes are(1829,1988), (1888,1962), (1890,1995).Example 2 (continued): Central England Average Annual Temperature. A challenging case is the central England temperature data for 360 years beginning in 1659. A plot of the data suggests possible changes very early in the series with a much larger change 2-3 hundred years later. The first 80-100 years may be less reliable, due to the primitive thermometers available at that time, but the early data are nonetheless interesting in view of the so-called \"little ice age,\" which overlapped these years. As reported above, a test of at least one change produces a significant increase in slope in 1970 with an estimated autocorrelation of 0.159, and a confidence interval going back about a hundred years, reflecting the increase in the plot of Z 2 t , inFigure 2, that begins in the late 19th century. If we use Seq with the threshold 4.00 and the autocorrelation 0.159, we find an increase in 1693, a decrease about 1708, and a larger increase about 1890. For MS with a threshold of 4.81 and the same value for \u03c1, we detect only the change in 1890. If we lower the detection standard to control the global false positive rate at 0.10 and set \u03c1 = 0.12, which is the estimate obtained from the first half of the data or from the second half of the data, we obtain a change in 1696 and a Plot either 1890 or 1988, but without a compelling reason to choose one or the other.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "suggests that either (or both) could be correct. A linear analysis indicates that the model with only one change at either 1890 or 1988 leads to an R 2 of approxmately 0.26 and a value of \u03c1 of about 0.12. Incorporating the two earlier changes detected by Seq raises R 2 to about 0.31 and reduces \u03c1 to about 0.1 for either choice of the third change. Including all four changes raises R 2 to about 0.33 and decreases\u03c1 to about 0.08. The statistic (20) to detect two changes at a time detects changes in 1697, 1704, 1730, and 1886; but a linear analysis indicates that the change in 1730 need not be included. The two earlier changes are detected on the initial search, and hence this represents an example where the statistic (20), when faced with an interval where there is only one change, may place a second change sufficiently far from an end-point of the interval searched that it is not obvious that it should be disregarded. Example 5: Age specific suicide rates in the United States. A small sample size example is suicide rates in the US from 1990 through 2017, which can be found on the web site ourworldindata.org. Autocorrelation appears to be very small, so we assume it is 0. All methods detect a substantial slope increase about 2000. For MS this is the only change detected, and it produces an R 2 of 0.91. Seq detects in addition, a second increase about 2014. The statistic (18) detects three changes, a decrease in slope about 1995 before a larger increase in 1999, and a third increase in 2014. A linear analysis with all three changes yields an R 2 of 0.97.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "If we estimate \u03c1 by all the observations, we get 0.55 and detect no changes. If we use the observations from 1 to 45, we get\u03c1 = 0.30 and detect changes at 48, 76, 99, and 114 by the pseudo-sequential method and by CBS, while the stricter segmentation method that uses all possible background intervals detects changes at 48, 76, and 114. The statistic (18) detects only the first two change-points. A linear analysis with the four detected change-points produces an R 2 of 0.48 and an estimate for \u03c1 of 0.27.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Rain Storms in New Zealand An example modeled as jump changes in a Poisson mean are heavy rain storms in New Zealand since 1900, which have been catalogued at https://hwe.niwa.co.nz. Both Seq and (18) suggest that there are two periods of increased storm activityfrom about 1920 to approximately 1949 and from 1996 to 2010. The statistic MS misses the decrease in level in 1949, but detects the other three changes. A generalized linear model analysis indicates that the four change model is preferable to three changes, but not surprisingly gives a relatively small z-score to the change in 1949. A series of interesting, but often challenging examples of contemporary interest are the slope changes occuring in the new cases day by day of COVID-19 occurring in various locations. We have used data from ourworldindata.org . Example 7: COVID-19 in Santa Clara County, California. We consider the 72 day history by day ending on 12 April 2020. The statistic (20), initially at a constant mean value and allowing for a slope change and for overdispersion, detects an increase in slope on the 34th day. The quasilikelihood statistic with \u03c8(\u03b8) = \u03b8 2 /2, autocorrelation of 0 and constant variance for the residuals, i.e., effectively just the single change statistic of Section 2, detects a slope increase on the same day. Both a linear least squares analysis and a loglinear (Poisson) analysis confirm this change and indicate 0 autocorrelation. The dispersion paramter is estimated to be about 20. Example 8: COVID-19 in South Korea. South Korea provides an example (84 observations as of 12 April, 2020). which involves multiple changes. We use a quasi-likelihood analysis, again with 0 autocorrelation. The statistic Seq detects a slope increase at 27, a decrease at 44, and another increase at 54, all of which seem evident from a plot of the data. The statistic (18) suggests slope changes at observations 32, 41, and 54 while the statistic MS also detects three changes: at 32,44, and 55. Remark. Additional examples and discussion of COVID-19 data are contained in the online supplement.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "selective in our use of examples and for the most part have limited our subsequent linear analysis to what it can tell us about our detection methods. In some examples it seems interesting to ask if there are explanations for the local changes: e.g., what is the evidence that a temperatures began to increase in the late 19th century, perhaps accompanied by an hiatus in the early-mid 20th century, before accelerating about 1980? In other examples it appears that our methods may lead to a reasonable regression fit without suggesting that the detected change-points need an explanation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "with M = 1 and \u00b5 u = \u03b1 + \u03b2[(u \u2212 (T + 1)/2)/T ]. The expressions given below are asymptotic for T >> 1; and for some specific examples, the results used are computed in continuous time. Hence, for example, T u=1 f [(u \u2212 t)/T ] may be evaluated as T 1 0 f (u \u2212 t/T )du, and E(Y u ) may be computed after solving the differential equation (3). In the change-point problems of (Fang, Li and Siegmund (2018)), where the function f is discontinuous, use of a continuous time model would result in considerable loss of accuracy, but it seems much less consequential here where for many of our examples, in particular for broken line regression, the stochastic processes under consideration have piecewise differentiable sample paths. Let \u03b3 = 1 \u2212 \u03c1. Then",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Reconstruction of Global Sea Levels. We consider the reconstruction of Church and White (2011) of global sea levels beginning in 1880. An updated version of the data can be found at https://github.com/datasets/sea-level-rise/blob/master/archive/ Our test for at least one change detects a slope increase in 1991. See Figure 5, where the broad peak of the plot of Z t suggests there may be more than one change-point. The estimated autocorrelation based on all the data is 0.34, and with this estimate Seq detects slope increases in 1938 and 1999. With the same estimate of \u03c1 MS detects only the change in 1999. However, in the first 100 observations the estimated autocorrelation is only 0.14. With an autocorrelation of 0.25 for MS, slope changes are detected in 1935 and 2000. A linear model incorporating these two changes yields an R 2 greater than 0.99 and an estimate for \u03c1 of 0.06. With \u03c1 = 0.34 the test (18) to detect two changes at a time indicates in addition to slope increases in 1932 and 1991 a decrease in 1915. With these three changes a linear analysis gives approximately the same R 2 and \u03c1 as the model with only two changes. Example 10: Northern Hemisphere Ocean Temperature Anomalies. An interesting example is provided by time series of ocean temperature. Both NOAA and the Hadley Center provide relevant data. The northern hemisphere temperatures appear to be an interesting illustration of our methods since a plot of the data suggests the possibility of multiple changes during the 20th century. We first consider the NOAA monthly anomalies, with m = 1665. ftp://ftp.ncdc.noaa.gov/pub/data/noaaglobaltemp/operational/timeseries/ Our test for at most one change gives a p-value about 0.001 and an estimated autocorrelation of 0.94 (cf.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "and a threshold equal to the nominal 0.05 threshold of 4.40. At these settings an increase was detected at observation 367 months (1912), a decrease at 756 (1943), and a second increase at 1133 (1974). For the m = 139 average annual anomalies, where \u03c1 appears to be substantially smaller, at a threshold of b = 3.75, m 0 = 4, and \u03c1 set equal to 0.5, positive changes are detected in 1910 and 1975, with an intervening negative trend in 1943. At the conventional level 0.05, MS detects the first and the third of these change-points, but misses the second. For the annual average northern hemisphere ocean temperature anomalies a linear model without changes has an R 2 of 0.88. Incorporating the three slope changes detected by Seq into the model produces only a small increase in R 2 to 0.90, but a large decrease in the estimated value of \u03c1 from 0.80 to 0.52. The statistic (18) also suggests three changes in slope, at 1909,1941, and 1974. For these changes the linear model again gives and R 2 of 0.90 and estimates \u03c1 to be 0.47. Example 11: Atmospheric Greenhouse Gases. In view of their connection with global warming, it is interesting to consider atmospheric greenhouse gases. Systematic atmo-Plot of Z t for reconstructed sea level rise from 1880 to 2014. Plot Z t , NOAA northern hemisphere ocean anomalies. spheric measurements began relatively recently, although reconstructions from surrogate measurements go back centuries. Atmospheric measurements for CO 2 show a number of increases in slope since about 1960. To illustrate our methods, we first consider global average methane emissions for 35 years beginning in 1984, reported at ftp://aftp.cmdl.noaa.gov/products/trends/ch4/. A linear model without changes shows a positive slope with an R 2 of 0.95 and an autocorrelation of zero. MS detects a slowing of the rate of increase in 1993 followed by a large increase in 2010. Incorporating these changes into a linear model produces an R 2 greater than 0.99 and an autocorrelation of zero. The statistic (18) detects the same two changes, although it places the initial decrease about 1996. An example of surrogate measurements of atmospheric CO 2 is provided by the web site of the Institute for Atmospheric and Climate Science (IAC) at the ETH-Z\u00fcrich. https://www.co2.earth/historical-co2-datasets To provide a comparable time frame to Example 3, we consider the 260 years 1755-2014. Unlike other examples, where changes are difficult to detect because of large variability, the variability in these data (and in the recent atmospheric measurements) is very small, so there appear to be many small changes, that do not help us to understand larger patterns that may have environment impact. The top down statistic (18) focuses on relatively few, large changes. The null estimate of \u03c1 is 0.39. With this value the first two changes suggested by (18) are slope increases occurring in 1871 and 1959. A linear model with these change-points produces an R 2 of 0.995 and a estimated value of \u03c1 of 0.01. The statistic MS with the same value of \u03c1 and T 0 = 20 to deter detection of small changes, detects the large increase in 1959 and an earlier detection in 1853, which has roughly the same effect on the linear model as the change in 1871 detected by (18). Both statistics suggest that there 42 are additional small changes in the first hundred years. The statistic (18), but not MS,",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Whereas both the Hadley Center and NOAA provide temperature data, their beginning dates are in the mid to late 19th century, and hence make it difficult to detect slope changes that may have occurred during the 19th century. The Berkeley Earth web site at http://berkeleyearth.org/data/ provides measurements going back to the mid 18th century. The results for several European countries are quite similar and suggest that at least in Europe increasing temperatures began in the late 19th century before the recent acceleration that began about 1980. As an example we consider Swiss temperature anomalies from 1753 to 2012. Our test for at least one change identifies a slope increase in 1890 with an estimated autocorrelation of 0.24.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Appendix D: Additional COVID-19 Examples.Example 11. COVID-19 in Italy. An interesting example is Italy, which had substantial difficulty in controlling the spread of the virus. Using data for 74 days up to(including)    12 April 2020, we find that MS detects changes an increase at day 34 and a substantial decrease at day 53. A linear least squares analysis and a (quasi) Poisson analysis suggest that these changes are reasonable.Example 12. COVID-19 in Hong Kong An extremely well organized and informative web site is chp-dashboard.geodata.gov.hk/covid-19/en.html . For 80 days of data up to 12 April, 2020, the method MS detects an increase in new cases on the 52nd day, followed by a decrease on the 66th. Both a least squares linear analysis and a (quasi)Poisson analysis suggest these are reasonable, with no day to day autocorrelation.Results for Taiwan are similar.Example13. COVID-19 in China In addition to South Korea, discussed above, China is another country that has apparently managed to stop the epidemic, but it poses a special challenge because the data are very noisy and contain one huge outlier in the center of the sequence of observations. To mitigate the effect of this outlier, we assumed that there had been a reporting delay and arbitrarily distributed many of those cases between the preceding two days. For the 97 days before 6 April 2020, Seq detects three changes at days 20, 46, and 55, while (18) would place them at 24, 45, and 54. Both the least squares linear analysis and a generalized linear (quasi)Poisson analysis seem to approve of these results.MS also detects three changes, at slightly different locations. Results for Taiwan are quite similar.The COVID-19 data typically start with a small number of cases showing relatively little day to day variability. When the number of cases is larger, the variability also becomes larger. In some cases it is enormous, apparenly partly due to reporting delays that can lead to huge fluctuations in a one or two days interval. The bottom up methods, Seq and MS seem to control the false positive rate reasonably well, because they estimate the variance of the data locally. In contrast (18) estimates the variance globally-according to the interval searched, so it may make false positive errors by using a variance that is too small. However, its stability can be checked by looking for consistency when the starting point of the sequence is varied. There are limitations, since starting the search too late in the sequence makes it difficult to detect an early change. To avoid some of the reporting incongruities, it is tempting to use a moving average of the day to day data. This reduces the variance but increases the autocorrelation quite substantially. In a small number of numerical experiments, it did not seem to offer consistent advantages. An interesting feature of our linear least squares analysis of the detected changes is that the estimated slopes are additive, so we have some information about the danger of another flare-up.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}