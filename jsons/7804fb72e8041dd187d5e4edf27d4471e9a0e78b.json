{
    "paper_id": "7804fb72e8041dd187d5e4edf27d4471e9a0e78b",
    "metadata": {
        "title": "Translating Diffusion, Wavelets, and Regularisation into Residual Networks",
        "authors": [
            {
                "first": "Tobias",
                "middle": [],
                "last": "Alt",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Joachim",
                "middle": [],
                "last": "Weickert",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Pascal",
                "middle": [],
                "last": "Peter",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Convolutional neural networks (CNNs) often perform well, but their stability is poorly understood. To address this problem, we consider the simple prototypical problem of signal denoising, where classical approaches such as nonlinear diffusion, wavelet-based methods and regularisation offer provable stability guarantees. To transfer such guarantees to CNNs, we interpret numerical approximations of these classical methods as a specific residual network (ResNet) architecture. This leads to a dictionary which allows to translate diffusivities, shrinkage functions, and regularisers into activation functions, and enables a direct communication between the four research communities. On the CNN side, it does not only inspire new families of nonmonotone activation functions, but also introduces intrinsically stable architectures for an arbitrary number of layers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In view of the undeniable success of deep learning approaches in all areas of data science (LeCun et al., 1998; 2015; Schmidhuber, 2015; Goodfellow et al., 2016) , there is a strong need to put them on a solid ground by establishing their mathematical foundations.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 111,
                    "text": "(LeCun et al., 1998;",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 112,
                    "end": 117,
                    "text": "2015;",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 118,
                    "end": 136,
                    "text": "Schmidhuber, 2015;",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 137,
                    "end": 161,
                    "text": "Goodfellow et al., 2016)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "An analytic way towards this ambitious goal is to express successful CNN architectures in terms of well-founded mathematical concepts. However, deep learning offers a plethora of design possibilities, and modern architectures may involve hundreds of layers and millions of parameters. Thus, this way of reducing a highly complex system to a simple and transparent mathematical model is not only very burdensome, but also bears the danger to lose performance critical CNN features along the way.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "An alternative, synthetic way uses well-established models that offer deep mathematical insights to build simple components of neural architectures which inherit these qualities. While this constructive road seems less stony, it has been explored surprisingly little.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we follow the road less taken and drive its simplicity to the extreme. Thus, at this point it is not our intention to design highly sophisticated architectures that produce state-of-the-art results in benchmarks. We do not even present any experiments at all.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our goal is to gain theoretical insights that can be useful to suggest neural architectures that are simpler, more compact, involve less parameters, and benefit from provable stability guarantees.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Our Contribution"
        },
        {
            "text": "We establish a comprehensive framework that for the first time allows to translate diffusion methods, wavelet approaches, and variational techniques simultaneously into a specific CNN architecture. The reason for choosing three denoising techniques lies in the intrinsic stability of denoising: Noise is a perturbation of the input data, which is not supposed to change the denoised output substantially. To maximise transparency and notational simplicity, we restrict ourselves to the 1D setting and choose particularly simple representatives in each class: Perona-Malik diffusion, Haar wavelet shrinkage, smooth first order variational models, and a single ResNet block. We show that discrete formulations of all three denoising approaches can be expressed as a specific ResNet block. It inherits its stability directly from the three denoising algorithms. Thus, a ResNet consisting only of these blocks is stable for any number of layers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Our Contribution"
        },
        {
            "text": "Whereas typical CNNs learn convolution weights and fix the nonlinear activation function to a simple design, we proceed in the opposite way: We fix the convolution kernels and study various nonlinear activation functions that are inspired by the diffusivities, shrinkage functions, and variational regularisers. For researchers from the diffusion, wavelet or variational communities, this introduces a dictionary that allows them to translate their methods directly into CNN architectures. Deep learning researchers will find hitherto unexplored nonmonotone activation functions and new motivations for existing ones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Our Contribution"
        },
        {
            "text": "We see our paper in the tradition of a number of recent contributions to the mathematical foundations of deep learning. For a detailed survey that covers results until 2017, we refer to . The following review focuses on works that are relevant for our paper and does not discuss other interesting aspects such as expressiveness (Poggio et al., 2017; Rolnick & Tegmark, 2018; Gribonval et al., 2019) and information theoretic interpretations (Shwartz-Ziv & Tishby, 2017) of CNNs.",
            "cite_spans": [
                {
                    "start": 328,
                    "end": 349,
                    "text": "(Poggio et al., 2017;",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 350,
                    "end": 374,
                    "text": "Rolnick & Tegmark, 2018;",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 375,
                    "end": 398,
                    "text": "Gribonval et al., 2019)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The seminal work of (Bruna & Mallat, 2013) employs wavelet operations to come up with scattering networks that perform well on classification problems. This has been the starting point for a number of wavelet-inspired CNNs; see e.g. (Wiatowski & B\u00f6lcskei, 2017; Fujieda et al., 2018; Williams & Li, 2018) and the references therein. Usually they exploit the spectral information or the multiscale nature of wavelets. Our work, however, focuses on iterated shift-invariant wavelet shrinkage on a single scale and utilises its connection to diffusion processes (Mr\u00e1zek et al., 2005) . Gaining insights into CNNs is possible by studying their energy landscapes (Nguyen & Hein, 2017; Chaudhari et al., 2018; Li et al., 2018; Draxler et al., 2018) , their optimality conditions (Haeffele & Vidal, 2017) , and by interpreting them as regularising architectures (Kuka\u010dka et al., 2017; Ulyanov et al., 2018; Dittmer et al., 2019; Kobler et al., 2020) . They can also be connected to incremental proximal gradient methods (Kobler et al., 2017; Bibi et al., 2019) , that are adequate for nonsmooth optimisation problems. In the present paper we advocate an interpretation in terms of iterated smooth energy minimisation problems. We do not require proximal steps and can exploit direct connections to diffusion filters Radmoser et al., 2000) .",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 42,
                    "text": "(Bruna & Mallat, 2013)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 233,
                    "end": 261,
                    "text": "(Wiatowski & B\u00f6lcskei, 2017;",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 262,
                    "end": 283,
                    "text": "Fujieda et al., 2018;",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 284,
                    "end": 304,
                    "text": "Williams & Li, 2018)",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 559,
                    "end": 580,
                    "text": "(Mr\u00e1zek et al., 2005)",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 658,
                    "end": 679,
                    "text": "(Nguyen & Hein, 2017;",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 680,
                    "end": 703,
                    "text": "Chaudhari et al., 2018;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 704,
                    "end": 720,
                    "text": "Li et al., 2018;",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 721,
                    "end": 742,
                    "text": "Draxler et al., 2018)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 773,
                    "end": 797,
                    "text": "(Haeffele & Vidal, 2017)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 855,
                    "end": 877,
                    "text": "(Kuka\u010dka et al., 2017;",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 878,
                    "end": 899,
                    "text": "Ulyanov et al., 2018;",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 900,
                    "end": 921,
                    "text": "Dittmer et al., 2019;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 922,
                    "end": 942,
                    "text": "Kobler et al., 2020)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1013,
                    "end": 1034,
                    "text": "(Kobler et al., 2017;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1035,
                    "end": 1053,
                    "text": "Bibi et al., 2019)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1309,
                    "end": 1331,
                    "text": "Radmoser et al., 2000)",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "A prominent avenue to establish mathematical foundations of CNNs is through their analysis in terms of stability. This can be achieved by studying their invertibility properties (Behrmann et al., 2018; Chang et al., 2018) , by exploiting sparse coding concepts (Romano et al., 2019) , and by interpreting deep learning as a parameter identification or optimal control problem for ordinary differential equations (Haber & Ruthotto, 2017; Thorpe & van Gennip, 2018; Zhang & Schaeffer, 2019) . CNNs can also be connected to flows of diffeomorphisms (Rousseau et al., 2019) and to parabolic or hyperbolic PDEs (Li & Shi, 2017; Arridge & Hauptmann, 2019; Smets et al., 2020) , where it is possible to transfer L 2 stability results (Ruthotto & Haber, 2019) . Our work focuses on diffusion PDEs. They allow us to establish stricter stability notions such as L \u221e stability and sign stability.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 201,
                    "text": "(Behrmann et al., 2018;",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 202,
                    "end": 221,
                    "text": "Chang et al., 2018)",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 261,
                    "end": 282,
                    "text": "(Romano et al., 2019)",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 412,
                    "end": 436,
                    "text": "(Haber & Ruthotto, 2017;",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 437,
                    "end": 463,
                    "text": "Thorpe & van Gennip, 2018;",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 464,
                    "end": 488,
                    "text": "Zhang & Schaeffer, 2019)",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 546,
                    "end": 569,
                    "text": "(Rousseau et al., 2019)",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 606,
                    "end": 622,
                    "text": "(Li & Shi, 2017;",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 623,
                    "end": 649,
                    "text": "Arridge & Hauptmann, 2019;",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 650,
                    "end": 669,
                    "text": "Smets et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 727,
                    "end": 751,
                    "text": "(Ruthotto & Haber, 2019)",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "We argue for shifting the focus of CNN models towards more sophisticated and also nonmonotone activation functions. The CNN literature offers only few examples of training activation functions (Kligvasser et al., 2018) or designing them in a well-founded and flexible way (Unser, 2019) . Nonmonotone activation functions have been suggested already before the advent of deep learning (De Felice et al., 1993; Meilijson & Ruppin, 1994) , but fell into oblivion afterwards. We revitalise this idea by providing a natural justification from the theory of diffusion filtering.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 218,
                    "text": "(Kligvasser et al., 2018)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 272,
                    "end": 285,
                    "text": "(Unser, 2019)",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 384,
                    "end": 408,
                    "text": "(De Felice et al., 1993;",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 409,
                    "end": 434,
                    "text": "Meilijson & Ruppin, 1994)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Our paper is structured as follows. We review general formulations of nonlinear diffusion, wavelet shrinkage, variational regularisation, and residual networks in Section 2. In Section 3, we present numerical approximations for three instances of the classical models. We interpret them in terms of a specific residual network architecture, for which we derive explicit stability guarantees. This leads to a dictionary for translating the nonlinearities of the three classical methods to activation functions, which is presented in Section 4. For a selection of the most popular nonlinearities, we derive their counterparts and discuss the results in detail. Finally, we summarise our conclusions in Section 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Organisation of the Paper"
        },
        {
            "text": "This section sketches nonlinear diffusion, wavelet shrinkage, variational regularisation, and residual networks in a general fashion. For each model, we highlight and discuss its central design choice.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Approaches"
        },
        {
            "text": "To ensure a consistent notation, all models in this section produce an output signal u from an input signal f . We define continuous one-dimensional signals u, f as mappings from a signal domain \u2126 = [a, b] to a codomain [c, d]. We employ reflecting boundary conditions on the signal domain boundaries a and b. The discrete signals u, f \u2208 R N are obtained by sampling the continuous functions at N equidistant positions with grid size h.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Approaches"
        },
        {
            "text": "In nonlinear diffusion (Perona & Malik, 1990) , filtered versions u(x, t) of an initial signal f (x) are computed as solutions of the nonlinear diffusion equation",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 45,
                    "text": "(Perona & Malik, 1990)",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Nonlinear Diffusion"
        },
        {
            "text": "with initial condition u(x, 0) = f (x) and diffusion time t. The evolution creates gradually simplified versions of f . The central design choice lies in the nonnegative and bounded diffusivity g(r) which controls the amount of smoothing depending on the local structure of the evolving signal. Choosing the constant diffusivity g(r) = 1 (Iijima, 1962) leads to a homogeneous diffusion process that smoothes the signal equally at all locations. A more sophisticated diffusivity such as the exponential Perona-Malik diffusivity g(r) = exp \u2212 r 2 2\u03bb 2 (Perona & Malik, 1990) inhibits smoothing around discontinuities where |\u2202 x u| is larger than the contrast parameter \u03bb. This allows discontinuity-preserving smoothing.",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 352,
                    "text": "(Iijima, 1962)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 549,
                    "end": 571,
                    "text": "(Perona & Malik, 1990)",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Nonlinear Diffusion"
        },
        {
            "text": "Classical discrete wavelet shrinkage (Donoho & Johnstone, 1994) manipulates a discrete signal f within a wavelet basis. With the following three-step framework, one obtains a filtered signal u:",
            "cite_spans": [
                {
                    "start": 37,
                    "end": 63,
                    "text": "(Donoho & Johnstone, 1994)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Wavelet Shrinkage"
        },
        {
            "text": "1. Analysis: One transforms the input signal f to wavelet and scaling coefficients by a transformation W .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Wavelet Shrinkage"
        },
        {
            "text": "A scalar shrinkage function S(r) with a threshold parameter \u03b8 is applied component-wise to the wavelet coefficients. The scaling coefficients remain unchanged.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Shrinkage:"
        },
        {
            "text": "3. Synthesis: One applies a back-transformationW to the manipulated coefficients to obtain the result u:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Shrinkage:"
        },
        {
            "text": "Besides the choice of the wavelet basis, the result is strongly influenced by the shrinkage function S(r). The hard shrinkage function (Mallat, 1999) eliminates all coefficients with a magnitude smaller than the threshold parameter, while the soft shrinkage function (Donoho, 1995) additionally modifies the remaining coefficients equally.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 149,
                    "text": "(Mallat, 1999)",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 267,
                    "end": 281,
                    "text": "(Donoho, 1995)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Shrinkage:"
        },
        {
            "text": "The classical wavelet transformation is not shift-invariant: Transforming a shifted input signal changes the resulting set of coefficients. To this end, cycle spinning was proposed in (Coifman & Donoho, 1995) , which averages the results of wavelet shrinkage for all possible shifts of the input signal. This shift-invariant wavelet transformation will serve as a basis for connecting wavelet shrinkage to nonlinear diffusion.",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 208,
                    "text": "(Coifman & Donoho, 1995)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Shrinkage:"
        },
        {
            "text": "Variational regularisation (Whittaker, 1923; Tikhonov, 1963) pursues the goal of finding a function u(x) that minimises an energy functional. A general formulation of such a functional is given by",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 44,
                    "text": "(Whittaker, 1923;",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 45,
                    "end": 60,
                    "text": "Tikhonov, 1963)",
                    "ref_id": "BIBREF53"
                }
            ],
            "ref_spans": [],
            "section": "Variational Regularisation"
        },
        {
            "text": "where a data term D(u, f ) drives u towards some input data f , and a regularisation term R(u) enforces smoothness conditions on u. We can control the balance between both terms by the regularisation parameter \u03b1 > 0. A solution minimising the energy is found via the Euler-Lagrange equations (Gelfand & Fomin, 2000) . These are PDEs that describe necessary conditions for a minimiser.",
            "cite_spans": [
                {
                    "start": 292,
                    "end": 315,
                    "text": "(Gelfand & Fomin, 2000)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Variational Regularisation"
        },
        {
            "text": "A simple yet effective choice for the regularisation term is a first order regularisation of the form R(u) = \u03a8 (\u2202 x u) with a regulariser \u03a8(r). The nonlinear function \u03a8 penalises variations in \u2202 x u to enforce a certain smoothness condition on u. Choosing e.g. \u03a8(r) = r 2 is called Whittaker-Tikhonov regularisation (Whittaker, 1923; Tikhonov, 1963) .",
            "cite_spans": [
                {
                    "start": 316,
                    "end": 333,
                    "text": "(Whittaker, 1923;",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 334,
                    "end": 349,
                    "text": "Tikhonov, 1963)",
                    "ref_id": "BIBREF53"
                }
            ],
            "ref_spans": [],
            "section": "Variational Regularisation"
        },
        {
            "text": "Residual networks (He et al., 2016) are a popular CNN architecture as they are easy to train, even for a high number of network layers. They consist of chained residual blocks. A residual block is made up of two convolutional layers with biases and nonlinear activation functions after each layer. Each block computes the output signal u from an input signal f by",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 35,
                    "text": "(He et al., 2016)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Residual Networks"
        },
        {
            "text": "with discrete convolution matrices W 1 , W 2 , activation functions \u03c3 1 , \u03c3 2 and bias vectors b 1 , b 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Residual Networks"
        },
        {
            "text": "The main difference to general feed-forward CNNs lies in the skip-connection which adds the original input signal f to the result of the inner activation function. This helps with the vanishing gradient problem and substantially improves training performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Residual Networks"
        },
        {
            "text": "The crucial difference between residual networks and the three previous approaches is the design focus: The three classical methods consider complex nonlinear modelling functions, while CNNs mainly focus on learning convolution weights and use simple activation functions. We will see that by permitting more general activation functions, we can relate all four methods within a unifying framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Residual Networks"
        },
        {
            "text": "Now we are in a position to present numerical approximations for the three classical models that allow to interpret them in terms of a residual network architecture.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Translation into Residual Networks"
        },
        {
            "text": "In practice, the continuous diffusion process is discretised and iterated to approximate the continuous solution u(x, T ) for a stopping time T . With the help of the flux function \u03a6(r) = g(r) r we rewrite the diffusion equation (1) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "For this equation, we perform a standard discretisation in the spatial and the temporal domain. This yields an explicit scheme which can be iterated. Starting with an initial signal",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": ", the evolving signal u k at a time step k is used to compute u k+1 at the next step by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "Here the temporal derivative is discretised by a forward difference with time step size \u03c4 . We apply a forward difference to implement the inner spatial derivative operator and a backward difference for the outer spatial derivative operator. Both can be realised with a simple convolution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "To obtain a scheme which is stable in the L \u221e norm, one can show that the time step size must fulfil",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "where g max is the maximum value that the diffusivity g(r) = \u03a6(r) r can attain (Weickert, 1998) . This guarantees a maximum-minimum principle, stating that the values of the filtered signal u k do not lie outside the range of the original signal f .",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 95,
                    "text": "(Weickert, 1998)",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "To achieve a substantial filter effect, one often needs a a diffusion time T that exceeds the limit in (7). Then one concatenates m explicit steps such that m\u03c4 = T and \u03c4 satisfies (7).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "In order to translate diffusion into residual networks, we rewrite the explicit scheme (6) in matrix-vector form:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "where D + h and D \u2212 h are convolution matrices denoting forward and backward difference operators with grid size h, respectively. In this notation, the resemblance to a residual block becomes apparent:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "and the bias vectors b 1 , b 2 are set to 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "We see that the convolutions implement forward and backward difference operators. Crucially, the inner activation function \u03c3 1 corresponds to the flux function \u03a6. The effect of the skip connection in the residual block also becomes clear now: It is the central ingredient to realise a time discretisation. We call a block of this form a diffusion block. A consequence of this equivalence is that a residual network chaining m diffusion blocks with activation function \u03a6(r) and time step size \u03c4 approximates a nonlinear diffusion process with stopping time T = m\u03c4 and diffusivity g(r) = \u03a6(r) r . These insights enable us to translate a diffusivity g directly into an activation function \u03a6:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "While this translation from a diffusion step to a residual block appears simple, it will serve as the Rosetta stone in our dictionary: It also allows to connect wavelet methods and variational regularisation to residual networks, since both paradigms can be related to diffusion (Mr\u00e1zek et al., 2005; . To keep our paper selfcontained, let us now sketch these correspondences.",
            "cite_spans": [
                {
                    "start": 279,
                    "end": 300,
                    "text": "(Mr\u00e1zek et al., 2005;",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "From Nonlinear Diffusion to Residual Networks"
        },
        {
            "text": "To explore the connection between wavelet shrinkage and nonlinear diffusion, (Mr\u00e1zek et al., 2005) consider shiftinvariant Haar wavelet shrinkage on the finest scale. The ",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 98,
                    "text": "(Mr\u00e1zek et al., 2005)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "From Wavelet Shrinkage to Nonlinear Diffusion"
        },
        {
            "text": "missing multiscale structure is compensated by iterating this shrinkage. They show that one step with shrinkage function S(r) is equivalent to an explicit diffusion step with diffusivity g(r), grid size h = 1, and time step size \u03c4 if",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Wavelet Shrinkage to Nonlinear Diffusion"
        },
        {
            "text": "The L \u221e stability condition (7) from the diffusion case translates into a condition on the shrinkage function \u2212r \u2264 S(r) \u2264 r for r > 0,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Wavelet Shrinkage to Nonlinear Diffusion"
        },
        {
            "text": "which is less restrictive than the typical design principle 0 \u2264 S(r) \u2264 r for r > 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Wavelet Shrinkage to Nonlinear Diffusion"
        },
        {
            "text": "Mr\u00e1zek et al. show that the latter one leads to a sign stable process in the sense of (Schoenberg, 1930) , i.e. the resulting signal shows not more sign changes than the input signal. This is a stronger stability notion than L \u221e stability. It limits the time step size to \u03c4 \u2264 h 2 4gmax . This is half the bound of (7). consider an energy functional with a quadratic data term and a regulariser \u03a8(r):",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 104,
                    "text": "(Schoenberg, 1930)",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "From Wavelet Shrinkage to Nonlinear Diffusion"
        },
        {
            "text": "The corresponding Euler-Lagrange equation for a minimiser u of the functional reads",
            "cite_spans": [],
            "ref_spans": [],
            "section": "From Variational Models to Nonlinear Diffusion"
        },
        {
            "text": "This can be regarded as a fully implicit time discretisation for a nonlinear diffusion process with stopping time T = \u03b1 and diffusivity g(r) = \u03a8 \u2032 (r) 2r . This process can also be approximated by m explicit diffusion steps of type (6) with time step size \u03c4 = \u03b1 m (Radmoser et al., 2000) , where m is chosen such that the stability condition (7) holds.",
            "cite_spans": [
                {
                    "start": 264,
                    "end": 287,
                    "text": "(Radmoser et al., 2000)",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "From Variational Models to Nonlinear Diffusion"
        },
        {
            "text": "The connections established so far imply direct stability guarantees for networks consisting of diffusion blocks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stability Guarantees for Our Residual Network"
        },
        {
            "text": "Theorem 2. A residual network chaining any number of diffusion blocks with time step size \u03c4 , grid size h, and antisymmetric activation function \u03a6(r) with finite Lipschitz constant L is stable in the L \u221e norm if",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stability Guarantees for Our Residual Network"
        },
        {
            "text": "It is also sign stable if the bound is chosen half as large.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stability Guarantees for Our Residual Network"
        },
        {
            "text": "Since \u03a6(r) = g(r) r and g is a symmetric diffusivity with bound g max , it follows that L = g max . Thus, (16) is the network analogue of the stability condition (7) for an explicit diffusion step. In the same way, the diffusion block inherits its sign stability from the sign stability condition of wavelet shrinkage. Stability of the full network follows by induction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stability Guarantees for Our Residual Network"
        },
        {
            "text": "Note that our results in terms of L \u221e or sign stability are stricter stability notions than the L 2 stability in (Ruthotto & Haber, 2019) where more general convolution filters are investigated: An L 2 stable network can still produce overshoots which violate L \u221e stability.",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 137,
                    "text": "(Ruthotto & Haber, 2019)",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "Stability Guarantees for Our Residual Network"
        },
        {
            "text": "Contrary to (Ruthotto & Haber, 2019) , our stability result does not require activation functions to be monotone. We will see that widely used diffusivites and shrinkage functions naturally lead to nonmonotone activation functions. Table 2 . Function plots for selected diffusivities, regularisers, shrinkage functions, and activation functions. The names of known functions are written above the graphs. Bold font indicates the best known function for each row. Axes and parameters are individually scaled for optimal qualitative inspection.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 36,
                    "text": "(Ruthotto & Haber, 2019)",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [
                {
                    "start": 232,
                    "end": 239,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Stability Guarantees for Our Residual Network"
        },
        {
            "text": "Regulariser \u03a8(r) Shrinkage Function S(r) Activation Function \u03a6(r)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Diffusivity g(r)"
        },
        {
            "text": "Whittaker-Tikhonov Identity Table 3 . Formulas for the function plots in Table 2 . The names of known functions are written above the equations. Bold font indicates the best known function for each row.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 28,
                    "end": 35,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 73,
                    "end": 80,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Constant"
        },
        {
            "text": "Diffusivity g(r)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Constant"
        },
        {
            "text": "Regulariser \u03a8(r) Shrinkage Function S(r) Activation Function \u03a6(r)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Constant"
        },
        {
            "text": "Whittaker-Tikhonov Identity",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Constant"
        },
        {
            "text": "Exploiting the Equations (10), (11), and (15), we are now in the position to present a general dictionary which can be used to translate arbitrary diffusivities, wavelet shrinkage functions, and variational regularisers into activation functions. This dictionary is displayed in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 279,
                    "end": 286,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Main Result"
        },
        {
            "text": "On one hand, our dictionary provides a blueprint for researchers acquainted with diffusion, wavelet shrinkage or regularisation to build a residual network for a desired model while preserving important theoretical properties. This can help them to develop rapid prototypes of the corresponding filters without the need to pay attention to implementational details. Also parallelisation for GPUs is readily available. Last but not least, these methods can be gradually refined by learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result"
        },
        {
            "text": "On the other hand, also CNN researchers can benefit. The dictionary shows how to restrict CNN architectures or parts thereof to models which are well-motivated, provably stable, and can benefit from the rich research results for diffusion, wavelet shrinkage and regularisation. Lastly, it can inspire CNN practitioners to use more sophisticated activation functions, in particular nonmonotone ones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Result"
        },
        {
            "text": "Let us now apply our general dictionary to prominent diffusivities, shrinkage functions, and regularisers in order to identify their activation functions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "What can We Learn from Popular Methods?"
        },
        {
            "text": "We visualise these functions in Table 2 and display their mathematical formulas in Table 3 . For our examples, we choose a grid size of h = 1. As we have g max = 1 for all cases considered, we set \u03c4 = \u03b1 = 1 4 . This fulfils the sign stability condition (16).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 32,
                    "end": 39,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 83,
                    "end": 90,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "What can We Learn from Popular Methods?"
        },
        {
            "text": "We generally observe that all resulting activation functions \u03a6(r) = g(r) r are antisymmetric, since g(r) = g(\u2212r). This is very natural in the diffusion case, where the argument of the flux function is the signal derivative \u2202 x u. It reflects a desired invariance axiom of denoising: Signal negation and filtering are commutative. Tables 2 and 3 fall into two classes: The first class comprises diffusion filters with constant (Iijima, 1962) and Charbonnier diffusivities (Charbonnier et al., 1994) , as well as soft wavelet shrinkage (Donoho, 1995) which involves a Huber regulariser (Huber, 1973 ) and a truncated total variation (TV) diffusivity (Rudin et al., 1992; Andreu et al., 2001) . These methods have strictly convex regularisers, and their shrinkage functions do not approximate the identity function for r \u2192 \u00b1\u221e. Most importantly, their activation functions are monotonically increasing. This is compatible with the stan-dard scenario in deep learning where the ReLU activation function dominates (Nair & Hinton, 2010) . On the diffusion side, the corresponding increasing flux functions act contrast reducing. Strictly convex regularisers have unique minimisers, and popular minimisation algorithms such as gradient descent converge globally.",
            "cite_spans": [
                {
                    "start": 426,
                    "end": 440,
                    "text": "(Iijima, 1962)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 471,
                    "end": 497,
                    "text": "(Charbonnier et al., 1994)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 534,
                    "end": 548,
                    "text": "(Donoho, 1995)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 584,
                    "end": 596,
                    "text": "(Huber, 1973",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 648,
                    "end": 668,
                    "text": "(Rudin et al., 1992;",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 669,
                    "end": 689,
                    "text": "Andreu et al., 2001)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1008,
                    "end": 1029,
                    "text": "(Nair & Hinton, 2010)",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [
                {
                    "start": 330,
                    "end": 344,
                    "text": "Tables 2 and 3",
                    "ref_id": null
                }
            ],
            "section": "What can We Learn from Popular Methods?"
        },
        {
            "text": "The second class is much more exciting. Its representatives are given by Perona-Malik diffusion (Perona & Malik, 1990) and two wavelet shinkage methods: garrote shrinkage (Gao, 1998) and hard shrinkage (Mallat, 1999) .",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 118,
                    "text": "(Perona & Malik, 1990)",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 171,
                    "end": 182,
                    "text": "(Gao, 1998)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 202,
                    "end": 216,
                    "text": "(Mallat, 1999)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Our six examples in"
        },
        {
            "text": "Garrote shrinkage corresponds to the truncated balanced forward-backward (BFB) diffusivity of (Keeling & Stollberger, 2002) , while hard shrinkage has a truncated quadratic regulariser which is used in the weak string model of (Geman & Geman, 1984) . Approaches of the second class have nonconvex regularisers, which may lead to multiple energy minimisers. Their shrinkage functions converge to the identity function for r \u2192 \u00b1\u221e. The flux function of the diffusion filter is nonmonotone. While this was considered somewhat problematic for continuous diffusion PDEs, it has been shown that their discretisations are well-posed (Weickert & Benhamouda, 1997) , in spite of the fact that they may act contrast enhancing. Since the activation function is equivalent to the flux function, it is also nonmonotone. This is very unusual for CNN architectures. Although there were a few very early proposals in the neural network literature arguing that such networks offer a larger storage capacity (De Felice et al., 1993) and have some optimality properties (Meilijson & Ruppin, 1994) , they had no impact on modern CNNs. Our results motivate their ideas from a different perspective. Since it is wellknown that nonconvex variational methods can outperform convex ones, it appears promising to incorporate nonmonotone activations into CNNs in spite of some challenges that have to be mastered.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 123,
                    "text": "(Keeling & Stollberger, 2002)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 227,
                    "end": 248,
                    "text": "(Geman & Geman, 1984)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 625,
                    "end": 654,
                    "text": "(Weickert & Benhamouda, 1997)",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 989,
                    "end": 1013,
                    "text": "(De Felice et al., 1993)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1050,
                    "end": 1076,
                    "text": "(Meilijson & Ruppin, 1994)",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Our six examples in"
        },
        {
            "text": "We have seen that CNNs and classical methods have much more in common than most people would expect: Focusing on three classical denoising approaches in a 1D setting and on a ResNet architecture with simple convolutions, we have established a dictionary that allows to translate diffusivities, shrinkage functions, and regularisers into activation functions. This does not only yield strict stability results for specific ResNets with an arbitrary number of layers, but also suggests to invest more efforts into the design of activation functions. In particular, nonmonotone activation functions warrant more attention.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Needless to say, our restrictions to 1D, to a single scale, and to denoising methods have been introduced mainly for didactic reasons. We see our work as an entry ticket to guide also other researchers with an expertise on PDEs, wavelets, and variational approaches into the CNN universe. As a result, we envision numerous generalisations, including extensions to higher dimensions and manifold-valued data. Of course, also additional key features of CNNs deserve to be analysed, for instance pooling operations. Our longterm vision is that this line of research will help to bridge the performance gap as well as the theory gap between model-driven and data-driven approaches, for their mutual benefit.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Minimizing total variation flow",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Andreu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ballester",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Caselles",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Maz\u00f3n",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Differential and Integral Equations",
            "volume": "14",
            "issn": "3",
            "pages": "321--360",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Networks for nonlinear diffusion problems in imaging",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Arridge",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hauptmann",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Mathematical Imaging and Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Analysis of invariance and robustness via invertibility of ReLU-networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Behrmann",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dittmer",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fernsel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Maass",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1806.09730"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Deep layers as stochastic solvers",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bibi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ghanem",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Koltun",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ranftl",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proc. 7th International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Invariant scattering convolution networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bruna",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mallat",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "35",
            "issn": "8",
            "pages": "1872--1886",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Reversible architectures for arbitrarily deep residual neural networks",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Haber",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ruthotto",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Begert",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Holtham",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "2811--2818",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Two deterministic half-quadratic regularization algorithms for computed imaging",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Charbonnier",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Blanc-F\u00e9raud",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Aubert",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Barlaud",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Proc. 1994 IEEE International Conference on Image Processing",
            "volume": "2",
            "issn": "",
            "pages": "168--172",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Deep relaxation: partial differential equations for optimizing deep neural networks",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Chaudhari",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oberman",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Osher",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Soatto",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Carlier",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Research in the Mathematical Sciences",
            "volume": "5",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Translation invariant denoising",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "R"
                    ],
                    "last": "Coifman",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Donoho",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Wavelets in Statistics",
            "volume": "",
            "issn": "",
            "pages": "125--150",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Dynamics of neural networks with nonmonotone activation function",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "De Felice",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Marangi",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Nardulli",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pasquariello",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Tedesco",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Network: Computation in Neural Systems",
            "volume": "4",
            "issn": "1",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Regularization by architecture: A deep prior approach for inverse problems",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dittmer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kluth",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Maass",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "O"
                    ],
                    "last": "Baguer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Mathematical Imaging and Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "De-noising by soft thresholding",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "L"
                    ],
                    "last": "Donoho",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "41",
            "issn": "",
            "pages": "613--627",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Ideal spatial adaptation by wavelet shrinkage",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "L"
                    ],
                    "last": "Donoho",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "M"
                    ],
                    "last": "Johnstone",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Biometrica",
            "volume": "81",
            "issn": "3",
            "pages": "425--455",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Towards efficient intra prediction based on image inpainting methods",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Draxler",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Veschgini",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Salmhofer",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hamprecht",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. 35th International Conference on Machine Learning",
            "volume": "80",
            "issn": "",
            "pages": "1309--1318",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Wavelet convolutional neural networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fujieda",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Takayama",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hachisuka",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1805.08620"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Wavelet shrinkage denoising using the nonnegative garrote",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Journal of Computational and Graphical Statistics",
            "volume": "7",
            "issn": "4",
            "pages": "469--488",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Calculus of Variations",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "M"
                    ],
                    "last": "Gelfand",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "V"
                    ],
                    "last": "Fomin",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Geman",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Geman",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "6",
            "issn": "",
            "pages": "721--741",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Deep Learning",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Approximation spaces of deep neural networks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gribonval",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kutyniok",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nielsen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Voigtlaender",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.01208"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Stable architectures for deep neural networks",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Haber",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ruthotto",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Inverse Problems",
            "volume": "34",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Global optimality in neural network training",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "D"
                    ],
                    "last": "Haeffele",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vidal",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. 2017 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "7331--7339",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proc. 2016 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Robust regression: Asymptotics, conjectures and Monte Carlo",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Huber",
                    "suffix": ""
                }
            ],
            "year": 1973,
            "venue": "The Annals of Statistics",
            "volume": "1",
            "issn": "5",
            "pages": "799--821",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Basic theory on normalization of pattern (in case of typical one-dimensional pattern)",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Iijima",
                    "suffix": ""
                }
            ],
            "year": 1962,
            "venue": "Bulletin of the Electrotechnical Laboratory",
            "volume": "26",
            "issn": "",
            "pages": "368--388",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Nonlinear anisotropic diffusion filters for wide range edge sharpening",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "L"
                    ],
                    "last": "Keeling",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Stollberger",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Inverse Problems",
            "volume": "18",
            "issn": "",
            "pages": "175--190",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Learning a spatial activation function for efficient image restoration",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kligvasser",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "R"
                    ],
                    "last": "Shaham",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Michaeli",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Xunit",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. 2018 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2433--2442",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Variational networks: Connecting variational methods and deep learning",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kobler",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Klatzer",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hammernik",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pock",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Pattern Recognition",
            "volume": "10496",
            "issn": "",
            "pages": "281--293",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Total deep variation for linear inverse problems",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kobler",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Effland",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kunisch",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pock",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2001.05005"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Regularization for deep learning: A taxonomy",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kuka\u010dka",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Golkov",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cremers",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1710.10686"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Gradientbased learning applied to document recognition. Proceedings of the IEEE",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Haffner",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "",
            "volume": "86",
            "issn": "",
            "pages": "2278--2324",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Deep learning. Nature",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "521",
            "issn": "",
            "pages": "436--444",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Visualizing the loss landscape of neural nets",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Taylor",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Studer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Goldstein",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wallach",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Larochelle",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Grauman",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Cesa-Bianchi",
                    "suffix": ""
                },
                {
                    "first": "Garnett",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. 32nd Annual Conference on Neural Information Processing Systems",
            "volume": "31",
            "issn": "",
            "pages": "6389--6399",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Deep residual learning and PDEs on manifolds",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1708.05115"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "A Wavelet Tour of Signal Processing",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mallat",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Optimal signalling in attractor neural networks",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Meilijson",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ruppin",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Proc. 7th Annual Conference on Neural Information Processing Systems",
            "volume": "7",
            "issn": "",
            "pages": "485--492",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Diffusion-inspired shrinkage functions and stability results for wavelet denoising",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Mr\u00e1zek",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weickert",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Steidl",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "International Journal of Computer Vision",
            "volume": "64",
            "issn": "2/3",
            "pages": "171--186",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Rectified linear units improve restricted Boltzmann machines",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nair",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proc. 27th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "807--814",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "The loss surface of deep and wide neural networks",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hein",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. 34th International Conference on Machine Learning",
            "volume": "70",
            "issn": "",
            "pages": "2603--2612",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Scale space and edge detection using anisotropic diffusion",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perona",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "12",
            "issn": "",
            "pages": "629--639",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Why and when can deep -but not shallownetworks avoid the curse of dimensionality: A review",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Poggio",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mhaskar",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Rosasco",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Miranda",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liao",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Journal of Automation and Computing",
            "volume": "14",
            "issn": "5",
            "pages": "503--519",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Scale-space properties of nonstationary iterative regularization methods",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Radmoser",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Scherzer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weickert",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Journal of Visual Communication and Image Representation",
            "volume": "11",
            "issn": "2",
            "pages": "96--114",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "The power of deeper networks for expressing natural functions",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rolnick",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tegmark",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. 6th International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Adversarial noise attacks of deep learning architectures: Stability analysis via sparse-modeled signals",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Romano",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Aberdam",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sulam",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Elad",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Mathematical Imaging and Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Residual networks as flows of diffeomorphisms",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Rousseau",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Drumetz",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fablet",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Mathematical Imaging and Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Nonlinear total variation based noise removal algorithms",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "I"
                    ],
                    "last": "Rudin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Osher",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fatemi",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Physica D",
            "volume": "60",
            "issn": "1-4",
            "pages": "259--268",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Deep neural networks motivated by partial differential equations",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ruthotto",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Haber",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Mathematical Imaging and Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Relations between regularization and diffusion filtering",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Scherzer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weickert",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Journal of Mathematical Imaging and Vision",
            "volume": "12",
            "issn": "1",
            "pages": "43--63",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Deep learning in neural networks: An overview",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Neural Networks",
            "volume": "61",
            "issn": "",
            "pages": "85--117",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "\u00dcber variationsvermindernde lineare Transformationen",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Schoenberg",
                    "suffix": ""
                }
            ],
            "year": 1930,
            "venue": "Mathematische Zeitschrift",
            "volume": "32",
            "issn": "",
            "pages": "321--328",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Opening the black box of deep neural networks via information",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Shwartz-Ziv",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tishby",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1703.00810"
                ]
            }
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Deep limits of residual neural networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Thorpe",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Van Gennip",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.11741"
                ]
            }
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Solution of incorrectly formulated problems and the regularization method",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Tikhonov",
                    "suffix": ""
                }
            ],
            "year": 1963,
            "venue": "Soviet Mathematics Doklady",
            "volume": "4",
            "issn": "",
            "pages": "1035--1038",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Deep image prior",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ulyanov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lempitsky",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. 2018 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "9446--9454",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "A representer theorem for deep neural networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Unser",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Machine Learning Research",
            "volume": "20",
            "issn": "110",
            "pages": "1--30",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Mathematics of deep learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vidal",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bruna",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Giryes",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Soatto",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1712.04741"
                ]
            }
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Anisotropic Diffusion in Image Processing",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weickert",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Teubner",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "A semidiscrete nonlinear scale-space theory and its relation to the Perona-Malik paradox",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weickert",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Benhamouda",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Advances in Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "A new method of graduation",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "T"
                    ],
                    "last": "Whittaker",
                    "suffix": ""
                }
            ],
            "year": 1923,
            "venue": "Proceedings of the Edinburgh Mathematical Society",
            "volume": "41",
            "issn": "",
            "pages": "65--75",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "A mathematical theory of deep convolutional neural networks for feature extraction",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wiatowski",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "B\u00f6lcskei",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "64",
            "issn": "3",
            "pages": "1845--1866",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Wavelet pooling for convolutional neural networks",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. 6th International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Forward stability of ResNet and its variants",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Schaeffer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Journal of Mathematical Imaging and Vision",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Diffusion block for one explicit nonlinear diffusion step (6) with flux or activation function \u03a6(r) and time step size \u03c4 . Stencils denote discrete convolution weights centred in position i.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "visualizes such a diffusion block. Graph nodes contain the current state of the signal at position i, while edges describe operations which are applied to proceed from one node to the next.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Dictionary for diffusivities g(r), regularisers \u03a8(r), wavelet shrinkage functions S(r), and activation functions \u03a6(r). A nonlinearity from a row can be translated into a nonlinearity from a column with the respective equation.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This work has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no. 741215, ERC Advanced Grant INCOVID). We thank Michael Ertel for checking our mathematical formulas.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ]
}