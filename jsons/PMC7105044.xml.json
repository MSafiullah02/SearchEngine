{
    "paper_id": "PMC7105044",
    "metadata": {
        "title": "Assessment of first-year post-graduate residents: Usefulness of multiple tools",
        "authors": [
            {
                "first": "Ying-Ying",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Fa-Yauh",
                "middle": [],
                "last": "Lee",
                "suffix": "",
                "email": "fylee@vghtpe.gov.tw",
                "affiliation": {}
            },
            {
                "first": "Hui-Chi",
                "middle": [],
                "last": "Hsu",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Chin-Chou",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Jaw-Wen",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Hao-Min",
                "middle": [],
                "last": "Cheng",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Wen-Shin",
                "middle": [],
                "last": "Lee",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Chiao-Lin",
                "middle": [],
                "last": "Chuang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Ching-Chih",
                "middle": [],
                "last": "Chang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Chia-Chang",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "The outbreak of the severe acute respiratory syndrome (SARS) epidemic that occurred during 2003 exposed serious deficiencies in Taiwan,s medical care and public healthcare systems, as well as its medical education system. The Department of Health, Executive Yuan of Taiwan, R.O.C., has had no efforts in promoting its \u201cProject of Reforming Taiwan,s Medical Care and Public Healthcare System\u201d since the spread of SARS was controlled. The reform of the medical care system aims to provide holistic medical treatment to people. Its strategies and methods include strengthening the improvement of resident education and quality of medical care. A project titled \u201cPost-graduate General Medical Training Program\u201d was announced by the Department of Health in August 2003. The evaluation of internal medicine first-year post-graduate (PGY1) residents usually consists of the Objective Structured Clinical Examination (OSCE) because it combines reliability with validity by using multiple testing in a standardized set of appropriate clinical scenarios in a practical and efficient format.1 The multiple-choice Internal Medicine in Training Examination (IM-ITE\u00ae) is a written test that is believed to be an alternative to performance testing such as the test by OSCE.2, 3 The reliability of the IM-ITE\u00ae is known to be good, with less testing time required.2 The IM-ITE\u00ae, covering knowledge in physical examination, laboratory, technical, and communication skills, is relatively cheap and easier to administer compared with an OSCE.4, 5 However, a paper-and-pencil knowledge test will overemphasize the cognitive aspects of clinical skills if the test does not require a resident to actually demonstrate these skills. Direct observation of procedural skills (DOPS) involves direct observation of a resident performing a variety of technical skills.6 A combination of the OSCE with the IM-ITE\u00ae and DOPS could bypass individual undesirable effects of each method and increase the completeness of assessment.5, 7, 8\n",
            "cite_spans": [
                {
                    "start": 1080,
                    "end": 1081,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1258,
                    "end": 1259,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1261,
                    "end": 1262,
                    "mention": "3",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1347,
                    "end": 1348,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1522,
                    "end": 1523,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1525,
                    "end": 1526,
                    "mention": "5",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1838,
                    "end": 1839,
                    "mention": "6",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1995,
                    "end": 1996,
                    "mention": "5",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1998,
                    "end": 1999,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 2001,
                    "end": 2002,
                    "mention": "8",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "High-stakes, large scale-OSCEs are used to assess clinical competence at the performance level of a \u201cshow how\u201d method based on Miller,s competency pyramid.9 The format of the OSCE is designed with a circuit of multiple stations in which the candidates accomplish specific tasks within a required time period.9, 10, 11 Replacing some OSCE stations with a written test might save resources and increase overall test reliability.4 It could offer an adequate compromise between the demands of reliability and feasibility. In post-graduate curriculum, designing a mixed-method assessment is often advised.12 Additionally, different content, multiple assessors, and a sufficient assessment time seem to be the fundamentals of a reliable assessment in clinical rotations. The 360-degree evaluation (multisource feedback) assesses general aspects of competence, including communication skills, clinical abilities, medical knowledge, technical skills, and teaching abilities of PGY1 residents.13 In general, different evaluation tools, including high-stakes, large-scale OSCE, DOPS, IM-ITE\u00ae, and 360-degree evaluations have their own particular roles in the assessment of learning outcomes. Thus, the purpose of our study was to determine the reliability of using a small-scale OSCE combined with other tools (DOPS and IM-ITE\u00ae) or a 360-degree evaluation to thoroughly evaluate PGY1 residents.",
            "cite_spans": [
                {
                    "start": 155,
                    "end": 156,
                    "mention": "9",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 308,
                    "end": 309,
                    "mention": "9",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 311,
                    "end": 313,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 315,
                    "end": 317,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 426,
                    "end": 427,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 600,
                    "end": 602,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 984,
                    "end": 986,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Between 2007 and 2010, 209 PGY1 residents (trainees) were evaluated by a small-scale OSCE before and after finishing 3 months of PGY1 internal medicine residency courses of Taipei Veteran General Hospital at Taiwan (Taipei VGH). Taipei VGH is a regional medical center that provides primary and tertiary care to active-duty and retired military members and their dependents. Taipei VGH serves as the primary teaching hospital for its internal medicine residency program. All the raters and senior physicians were recruited from among the clinical faculty and were teachers for the Department of Internal Medicine. The well-trained, non-physician experts for DOPS were independent from the Department of Internal Medicine of Taipei VGH.",
            "cite_spans": [],
            "section": "Study population ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The content of the small-scale OSCE, DOPS, IM-ITE\u00ae, and 360-degree evaluation were designed by a committee of expert physicians from our system who created the content blueprint and wrote the test questions according to well-established principles of examination construction. The committee members were regularly rotated.",
            "cite_spans": [],
            "section": "Study setting ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The small-scale OSCE consisted of six 15-minute stations. The OSCE consisted of six clinical problems that were made up of six core competencies defined by the Accreditation Council for Graduate Medical Education [ACGME (Appendix 1, Appendix 2)]. The content of each clinical problem consisted of physical examination skills, interpersonal skills, technical skills, problem-solving abilities, decision-making abilities, and patient treatment skills.14 The examination took place simultaneously at two different sites. At each site, there were two sessions, and the raters at each station changed between the two sessions. Thus, for each station, there were a total of four different raters during the test day. The small-scale OSCE had neither written a component nor a technical skills station, but it was entirely performance-based. At some stations, standardized patients were used to mimic the clinical problems of actual patients. A faculty rater graded each PGY1 resident according to a given set of 10\u201312 predetermined items presented in the form of a checklist. The score of checklists included items with a dichotomous scoring, yes/no, and an overall trichotomous scoring of pass/borderline/fail. All faculty raters attended serial training sessions that included extensive instruction on how to use the checklist in practice rating sessions. At each OSCE station, the raters acted as passive evaluators and were instructed not to guide or prompt the PGY1 residents. The summary score of each station was the sum of all the checklist items. The residents\u2019 performance score for each OSCE station was obtained by calculating the percentage of checklist items he or she obtained. The OSCE was performed before the training (OSCEbefore) and at the end of 3 months of training program (OSCE3rd month). Finally, average OSCE scores were calculated by averaging the three monthly scores for further analysis.",
            "cite_spans": [
                {
                    "start": 449,
                    "end": 451,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Small-scale OSCE ::: Methods",
            "ref_spans": []
        },
        {
            "text": "All PGY1 performed a series of standardized technical skills. For each skill, PGY1 residents were examined by the direct observation of experts and senior physicians using the technical skill-specific checklist.15 Four technical skills, including advanced cardiac life support (ACLS), lumbar puncture, central venous catheter insertion, and endotracheal tube insertion, were assessed regularly. Experts and senior physicians were provided with an identical checklist for the four technical skills before the test day and were asked to familiarize themselves with the checklist. In addition, they received a 30-minute orientation session just before examination. The DOPS checklist included items on communication skills, technical performance, and some theoretical questions, including knowledge of the indications, contraindications, potential complications, and different routes for the procedure that related to the task.16 All of these items were developed from the 11 domains of the DOPS in presented Appendix 3. Finally, the DOPS scores of each PGY1 resident were the averages of the ratings from the four experts and senior physicians for the four technical skills.",
            "cite_spans": [
                {
                    "start": 211,
                    "end": 213,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 924,
                    "end": 926,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "DOPS ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The 360-degree evaluations were made during the interval between the administration of the small-scale OSCE and DOPS. The 360-degree evaluation assessed general aspects of competence, including communication skills, clinical abilities, medical knowledge, technical skills, and teaching abilities that are shown in Appendix 4, Appendix 5. The Spearman-Brown prophecy formula was used to calculate the number of individuals needed to obtain a reliable rating.13, 16, 17 Our preliminary study found that the number of raters to achieve a reliability of 0.7 was 4. Five additional raters were needed to achieve a reliability of 0.8. Accordingly, the results of five raters of 360-degree evaluations were included for final analysis.",
            "cite_spans": [
                {
                    "start": 457,
                    "end": 459,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 461,
                    "end": 463,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 465,
                    "end": 467,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Monthly 360-degree evaluation ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The 12-item, one-page 360-degree evaluation forms were made by the faculty members, including one chief resident, one visiting physician, one chief physician, one nurse, and one head nurse of each of the services that residents rotated through monthly. In other words, every PGY1 resident received five evaluations by the five raters. The monthly 360-degree score was the average of scores from the five raters. Finally, the average 360-degree evaluation scores was calculated by averaging the three monthly scores (360-degree evaluation1st month 360-degree evaluation2nd month 360-degree evaluation3rd month) for further analysis.",
            "cite_spans": [],
            "section": "Monthly 360-degree evaluation ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The IM-ITE\u00ae is designed by the American College of Physicians (ACP) to give residents an opportunity for self-assessment, to give program directors the opportunity to evaluate their programs, and to identify areas in which residents need extra assistance.2, 18 Our multiple-choice IM-ITE\u00ae is a modification of the ACP\u2019s IM-ITE\u00ae. Our IM-ITE\u00ae was developed to test required knowledge that PGY1 residents most frequently encounter during their in-patient rotation. Initially, our IM-ITE\u00ae was composed of 80 items. After a first validation of the tool, 50 items were chosen based on experts\u2019 and residents\u2019 comments and validated again with a group of experts who confirmed the quality of the selected 50 items for our assessment purposes.",
            "cite_spans": [
                {
                    "start": 255,
                    "end": 256,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 258,
                    "end": 260,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "IM-ITE\u00ae ::: Methods",
            "ref_spans": []
        },
        {
            "text": "At the end of the course, all PGY1 residents were instructed to complete the DOPS and IM-ITE\u00ae as if they were the regular tests, even though the DOPS and IM-ITE\u00ae scores had no influence on pass/fail decisions of the OSCE. Additionally, the 12-item, 360-degree evaluation was completed for each PGY1 for each month. Our research used the averaged 360-degree evaluations, DOPS, IM-ITE\u00ae, and averaged small-scale OSCE scores, which had been collected as part of the routine procedure of the Department of Internal Medicine of Taipei VGH.",
            "cite_spans": [],
            "section": "Certification system ::: Methods",
            "ref_spans": []
        },
        {
            "text": "For the trainees who failed the DOPS and small-scale OSCE, special programs were designed according to their defects by senior physicians. Then, these trainees were re-evaluated until they passed all these tests. For those who failed the IM-ITE\u00ae and 360-degree evaluation, special training classes were conducted to re-educate them, program directors monitored their performance in the following 3-year residency (e.g., internal medicine, family medicine, surgery, pediatrics, dermatology, ophthalmology) course.",
            "cite_spans": [],
            "section": "Certification system ::: Methods",
            "ref_spans": []
        },
        {
            "text": "To ensure equal weighting of all evaluations formats, which was needed for further analysis, the scores of separate/averaged 360-degree evaluation, DOPS, IM-ITE\u00ae, and separate/averaged small-scale OSCE were transformed onto a similar 100% scale. The borderline group method was used to set the standard of \u201cpass\u201d for 360-degree evaluation, DOPS, IM-ITE\u00ae, and small-scale OSCE scores. Each station\u2019s \u201cpass\u201d score was the mean of the scores of PGY1 residents whose scores were rated \u201cborderline.\u201d19, 20 To estimate the reliability of the 360-degree evaluation, DOPS, IM-ITE\u00ae, and the small-scale OSCE separately, Cronback\u2019s alpha (\u03b1) coefficient were calculated for each evaluation. Kappa statistics were used to check the inter-rater agreement between expert and senior physician for the four procedure stations of DOPS. An \u03b1 of < 0.05 was accepted as statistically significant.",
            "cite_spans": [
                {
                    "start": 494,
                    "end": 496,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 498,
                    "end": 500,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Statistic analysis of data ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The descriptive statistics of the mean scores and standard deviations for each examination tool were analyzed with one sample or two-sample student\u2019s t test or analysis of variance when appropriate. Additionally, the correlations between the average OSCE and 360-degree evaluation score, small-scale OSCE + DOPS-composited score, and average 360-degree evaluation score, small-scale OSCE + DOPS + IM-ITE\u00ae score and average 360-degree evaluation score were analyzed by Pearson\u2019s correlation methods (Version 10.1, SPSS Inc., Chicago, Ill., USA). Comparisons between two correlation coefficients from paired measurements were carried out using the formula created by Kleinbaum and colleagues.21\n",
            "cite_spans": [
                {
                    "start": 690,
                    "end": 692,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Statistic analysis of data ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Our study included the analysis of the internal reliability of all our evaluation methods. The results showed that the reliability of the different evaluative methods was varied. The before-training OSCE (OSCEbefore) had a reliability of 0.73, the after-training OSCE (OSCE3rd month) 0.662, DOPS 0.82, IM-ITE\u00ae 0.69, 360-degree evaluation1st month 0.89, 360-degree evaluation2nd month 0.9, and 360-degree evaluation3rd month 0.79 (Table 1\n). Additionally, the inter-rater reliabilities between the expert and senior physicians for DOPS were good (ACLS: Kappa 0.71; lumbar puncture: Kappa 0.69, central venous catheter insertion: Kappa 0.75 and endotracheal tube insertion: Kappa 0.78).",
            "cite_spans": [],
            "section": "Reliability ::: Results",
            "ref_spans": [
                {
                    "start": 430,
                    "end": 437,
                    "mention": "Table 1",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "Before further correlation studies, the re-evaluation reliability of the small-scale OSCE and 360-degree evaluation were assessed. As seen in Fig. 1\n, OSCEbefore and OSCE3rd month scores were closely correlated (r = 0.64, p < 0.01). Meanwhile, the 360-degree evaluation1st month, 360-degree evaluation2nd month and 360-degree evaluation3rd month scores were well correlated, ranging from a low correlation of 0.54 between 360-degree evaluation1st month and 360-degree evaluation2nd month scores and a high correlation of 0.94 between 360-degree evaluation2nd month and 360-degree evaluation3rd month scores.",
            "cite_spans": [],
            "section": "Consistency of evaluations ::: Results",
            "ref_spans": [
                {
                    "start": 142,
                    "end": 148,
                    "mention": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "\nTable 2\n19 shows that average small-scale OSCE scores was significantly correlated with average 360-degree evaluation scores (r = 0.37, p < 0.05). Interestingly, the addition of DOPS scores to average small-scale OSCE scores significantly increased its (small scale-OSCE + DOPS-composited score) correlation with the average 360-degree evaluation scores (r = 0.72, p < 0.01). Furthermore, a combination of the IM-ITE\u00ae scores with small-scale OSCE + DOPS scores (small scale-OSCE + DOPS + IM-ITE\u00ae scores) markedly enhanced their correlation with 360-degree evaluation scores (r = 0.83, p < 0.01).",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 11,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Correlations ::: Results",
            "ref_spans": [
                {
                    "start": 1,
                    "end": 8,
                    "mention": "Table 2",
                    "ref_id": "TABREF6"
                }
            ]
        },
        {
            "text": "Next, we searched for the points that needed to be further improved in the design of the training program. The pass rates and the mean scores were significantly improved after 3 months of internal medicine training course [OSCEbefore: 36% and OSCE3rd month: 52%, p < 0.05 (Fig. 2\nand Table 3\n)]. The pass rate of the DOPS scores was around 70%. Meanwhile, the pass rate of the 360-degree evaluation scores was also progressively improved among three months of internal medicine training program (360-degree evaluation1st month: 57% 360-degree evaluation2nd month: 59% and 360-degree evaluation3rd month: 69%, p < 0.05). Although the overall pass rates varied between different evaluative methods, the differences did not reach significance level.",
            "cite_spans": [],
            "section": "Difficulty and efficiency of training ::: Results",
            "ref_spans": [
                {
                    "start": 273,
                    "end": 279,
                    "mention": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 284,
                    "end": 291,
                    "mention": "Table 3",
                    "ref_id": "TABREF7"
                }
            ]
        },
        {
            "text": "The objective of medical education is to produce excellent medical professionals and performance. To achieve this objective, Taipei VGH introduced and implemented the small-scale OSCE, DOPS, IM-ITE\u00ae, and 360-degree evaluations. Previous study suggested that the term \u201ccompetence\u201d is often used broadly to incorporate the domains of knowledge, skills, and attitudes.1 No single assessment method can successfully evaluate the clinical competence of PGY1 residents in internal medicine. It has been reported that the reliability of medical education performance increases with the addition of each different reliable measure.22 Thus, educators need to be cognizant of the most appropriate application tool. Our study explored whether a combination of assessment tools provides the best opportunity to evaluate and educate PGY1 resident in Taiwan.",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 366,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 623,
                    "end": 625,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "It is not clear whether lengthening the written test component (such as IM-ITE\u00ae) compensates for the loss of validity due to the use of fewer stations in the OSCE.4 Nonetheless, the reliability of the OSCE is partly determined by the testing time, and a large-scale OSCE is time- and money- consuming. Accordingly, an expensive large-scale OSCE should still be part of the assessment program.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 164,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "The 360-degree evaluation have been widely used in several medical and surgical residency training programs, and their usefulness has been very positive.13, 16 Our study observed the increase in rating scores with more months of training (Table 3), which supports the general validity of the 360-degree evaluation in assessing PGY1 resident competence including knowledge, skills, and attitudes.12, 16, 23 For formative purposes, the 360-degree evaluation helps a resident understand how other members of their team view his or her knowledge and attitudes. Thus, the 360-degree evaluation scores also help residents develop an action plan and improve their behavior as part of their training. In our study, we used 360-degree evaluation scores as a standard to assess the efficiency of different methods, or a combination of them in evaluating the performance of all PGY1 residents.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 155,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 157,
                    "end": 159,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 395,
                    "end": 397,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 399,
                    "end": 401,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 403,
                    "end": 405,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Discussion",
            "ref_spans": [
                {
                    "start": 239,
                    "end": 246,
                    "mention": "Table 3",
                    "ref_id": "TABREF7"
                }
            ]
        },
        {
            "text": "Nevertheless, the reliability of 360-degree evaluation in our study was different between the 3 months of the training program. This finding can be explained by the fact that the residents are not working in a stable environment. They change rotation frequently, and there are new raters at the new sites. It is also possible that PGY1 residents became less homogenous in their abilities during the 3 months of the training program. In fact, the 360-degree evaluation is a method that only provides global rating regarding of the PGY1 residents, performance; it will not demonstrate the details. In other words, the 360-degree evaluation is a tool for assessing the change of knowledge, skills, and attitude rather than physical examination skills. Actually, a complete evaluation of the PGY1 performance should include a 360-degree evaluation and an OSCE focusing on physical examination skills.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "The reliabilities of the DOPS, IM-ITE\u00ae, and 360-degree evaluation were good, indicating a high degree of internal consistency of these assessments. The pass rates of all methods were between 61% and 81% (Fig. 2). In comparison with other tools, the reliability of the small-scale OSCE was not acceptable. Meanwhile, the pass rate was not very high for the OSCE of our study. These results indicate that the structure of the small-scale OSCE used in our study should modify to improve the pass rate in the future. Nevertheless, average small scale-OSCE and 360-degree evaluation scores were still significantly well correlated (r = 0.37, p < 0.05), suggesting a high reliability of the overall program.1 Further, we combined the small-scale OSCE with other tools to improve its reliability and reflect the real performance of PGY1 residents as seen in Table 2. Notably, the correlation between small-scale OSCE + DOPS-composited scores and 360-degree evaluation scores was increased (r = 0.72, p < 0.01). Finally, a further markedly increase in the correlation between OSCE + DOPS + IM-ITE\u00ae and 360-degree evaluation scores was observed (r = 0.85, p < 0.01). These results can also be explained by the fact that small scale-OSCE, DOPS, and IM-ITE\u00ae assess different areas of knowledge and skills. Accordingly, adding all of the three scores showed a high correlation with the 360-degree evaluation because more items were being sampled.",
            "cite_spans": [
                {
                    "start": 701,
                    "end": 702,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Discussion",
            "ref_spans": [
                {
                    "start": 204,
                    "end": 210,
                    "mention": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 851,
                    "end": 858,
                    "mention": "Table 2",
                    "ref_id": "TABREF6"
                }
            ]
        },
        {
            "text": "There are some limitations to our study. First, this was a retrospective study of a single residency program with a relatively small sample size. However, our results are strengthened by the completeness of our data over a 3-year period. The series, small-scale OSCE, DOPS, IM-ITE\u00ae, and 360-degree evaluations were 3 years apart in time. This is a long period in a learning environment, and many confounding variables can have an impact on the learning of PGY1 residents. However, there is always \u201cnoise\u201d in educational measurement, and we can postulate that the impact of these confounding variables may be found to be equally distributed among the observed scores of the four evaluations and could explain the results. Despite the noise and 3-year time interval, we still observed a relatively strong correlation among the variables under study.",
            "cite_spans": [],
            "section": "Limitations",
            "ref_spans": []
        },
        {
            "text": "Second, no long-term follow-up, small-scale OSCE, DOPS, and IM-ITE\u00ae measurements during the 3 years of the residents\u2019 training were obtained (to evaluate the validity of these tools), and we did not address the durability of the small-scale OSCE and DOPS. Nevertheless, our study showed a strong correlation between the 360-degree evaluation and small-scale OSCE + DOPS + IM-ITE\u00ae scores. Accordingly, the following of the core competencies of trainees regularly by IM-ITE\u00ae and 360-degree evaluation in our system may be valid on the program level. In OSCE, it was not possible to blind faculty raters to the PGY1 resident\u2019s identity. Our study was included OSCE before and after 3 months of internal medicine training course. In order to avoid the bias coming from the fact that PGY1 residents with a weaker OSCE performance might have tended to prepare more diligently for their next post-course OSCE, the raters of small-scale OSCE in our study did not give any feedback to PGY1 residents before they completed the post-course OSCE. Meanwhile, the trainees knew their OSCEbefore and OSCE3month scores only after finishing the entire testing sequence.",
            "cite_spans": [],
            "section": "Limitations",
            "ref_spans": []
        },
        {
            "text": "Third, only four practical consideration stations were included in the DOPS of our study. Previous study had suggested that if the DOPS were to be used for certification, a greater number of skills stations should be included where the consequences of an erroneous pass/fail judgment were serious.21 Nonetheless, we arranged two raters (both an expert and a senior physician) to increase the reliability by the multisource evaluation. Notably, the inter-rater agreements were quite good for the four technical skills of DOPS in our study. Use of the experts for the DOPS evaluation can also avoid the \u201chalo effect\u201d due to having previous experience with the PGY1 resident, which could introduce positive or negative bias in scoring.",
            "cite_spans": [
                {
                    "start": 297,
                    "end": 299,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Limitations",
            "ref_spans": []
        },
        {
            "text": "Finally, previous studies have shown that the reliability of the 360-degree evaluation can be elevated by increasing the number of raters. Our current study only did a rough estimation about the number of raters needed for the reliability of the 360-degree evaluation to reach 0.7\u20130.8. In fact, a detailed analysis of heterogeneity of raters and PGY1 residents should also be considered, along with analyses by G-theory, in the future.",
            "cite_spans": [],
            "section": "Limitations",
            "ref_spans": []
        },
        {
            "text": "In conclusion, the strong correlations between the 360-degree evaluation and the small-scale OSCE + DOPS + IM-ITE\u00ae scores suggests that both methods measure the same quality. In the future, a small-scale OSCE associated with DOPS and the IM-ITE\u00ae could be an important assessment method in evaluating the performance of PGY1 residents.",
            "cite_spans": [],
            "section": "Limitations",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "a: The content of small-scale Objective Structural Clinical Examination stations of PGY1 residents.\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 1: Various scores of all PGY1 residents (n = 209).\n",
            "type": "table"
        },
        "TABREF6": {
            "text": "Table 2: Correlations between evaluative measures.\n",
            "type": "table"
        },
        "TABREF7": {
            "text": "Table 3: Reliability of various methods.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig.\u00a01: Correlation between (A) OSCEbefore and OSCE3rd month; (B) monthly 360-degree evaluations. * p < 0.05 vs. correlation coefficients of 360-degree evaluation1st month and 360-degree evaluation2nd month.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig.\u00a02: The overall pass rate (pass students/total students*100%) of (A) OSCE; (B) DOPS and IM-ITE\u00ae; (C) 360-degree evaluation of all PGY1 residents. *p < 0.05 vs. OSCEbefore and 360-degree evaluation1st month. DOPS = direct observation of procedure skills; IM-ITE = Internal Medicine in Training Examination (IM-ITE\u00ae); OSCE = Objective Structural Clinical Examination.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Methods for evaluating the clinical competence of residents in internal medicine: a review",
            "authors": [
                {
                    "first": "E.S.",
                    "middle": [],
                    "last": "Holmboe",
                    "suffix": ""
                },
                {
                    "first": "R.E.",
                    "middle": [],
                    "last": "Hawkins",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Ann Intern Med",
            "volume": "129",
            "issn": "",
            "pages": "43-48",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "The ACGME competencies: substance or form?",
            "authors": [
                {
                    "first": "D.C.",
                    "middle": [],
                    "last": "Leach",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "J Am Coll Surg",
            "volume": "192",
            "issn": "",
            "pages": "396-398",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Assessing the ACGME general competencies: general considerations and assessment methods",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Swing",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Acad Emerg Med",
            "volume": "9",
            "issn": "",
            "pages": "1278-1288",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Watts",
                    "suffix": ""
                },
                {
                    "first": "W.B.",
                    "middle": [],
                    "last": "Feldman",
                    "suffix": ""
                }
            ],
            "year": 1985,
            "venue": "Assessing Clinical Competence",
            "volume": "",
            "issn": "",
            "pages": "259-274",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "360-degree feedback: possibilities for assessment of the ACGME core competencies for emergency medicine residents",
            "authors": [
                {
                    "first": "K.G.",
                    "middle": [],
                    "last": "Rodgers",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Manifold",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Acad Emerg Med",
            "volume": "9",
            "issn": "",
            "pages": "1300-1304",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Assessment of clinical competence of medical students using the objective structured clinical examination: first 2 years experience in Taipei Veterans General Hospital",
            "authors": [
                {
                    "first": "C.C.",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C.Y.",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "C.L.",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y.L.",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "H.W.",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C.C.",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J Chin Med Assoc",
            "volume": "73",
            "issn": "",
            "pages": "589-595",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Objective structured assessment of technical skill (OSATS) for surgical residents",
            "authors": [
                {
                    "first": "J.A.",
                    "middle": [],
                    "last": "Martin",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Regehr",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Reznick",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Macrae",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Murnaghan",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Hutchison",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Br J Surg",
            "volume": "84",
            "issn": "",
            "pages": "273-278",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Implementing the ACGME general competencies in a cardiothoracic surgery residency program using 360-degree feedback",
            "authors": [
                {
                    "first": "R.S.",
                    "middle": [],
                    "last": "Higgins",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Bridges",
                    "suffix": ""
                },
                {
                    "first": "J.M.",
                    "middle": [],
                    "last": "Burke",
                    "suffix": ""
                },
                {
                    "first": "M.A.",
                    "middle": [],
                    "last": "O\u2019Donnell",
                    "suffix": ""
                },
                {
                    "first": "N.M.",
                    "middle": [],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "S.B.",
                    "middle": [],
                    "last": "Wilkes",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Ann Thorac Surg",
            "volume": "77",
            "issn": "",
            "pages": "12-17",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Reliability of a 360-degree evaluation to assess resident competence",
            "authors": [
                {
                    "first": "T.L.",
                    "middle": [],
                    "last": "Massagli",
                    "suffix": ""
                },
                {
                    "first": "J.D.",
                    "middle": [],
                    "last": "Carline",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Am J Phys Med Rehab",
            "volume": "86",
            "issn": "",
            "pages": "845-852",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "The foundation programme assessment tools: an opportunity to enhance feedback to trainees?",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Carr",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Postgrad Med J",
            "volume": "82",
            "issn": "",
            "pages": "576-579",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "Using borderline methods to compare passing standards for OSCE at graduation across three medical schools",
            "authors": [
                {
                    "first": "K.A.M.",
                    "middle": [],
                    "last": "Boursuicot",
                    "suffix": ""
                },
                {
                    "first": "T.E.",
                    "middle": [],
                    "last": "Roberts",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Peel",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Med Educ",
            "volume": "41",
            "issn": "",
            "pages": "1024-1031",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "The in-training examination in internal medicine: an analysis of resident performance over time",
            "authors": [
                {
                    "first": "R.A.",
                    "middle": [],
                    "last": "Garibaldi",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Subhiyah",
                    "suffix": ""
                },
                {
                    "first": "M.E.",
                    "middle": [],
                    "last": "Moore",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Waxman",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Annal Int Med",
            "volume": "137",
            "issn": "",
            "pages": "505-510",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Quantity assurance data for residents\u2019 global performance ratings",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Littlefield",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Pankert",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Schoolfield",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Acad Med",
            "volume": "76",
            "issn": "",
            "pages": "S102-S104",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [
                {
                    "first": "D.G.",
                    "middle": [],
                    "last": "Kleinbaum",
                    "suffix": ""
                },
                {
                    "first": "L.I.",
                    "middle": [],
                    "last": "Kupper",
                    "suffix": ""
                },
                {
                    "first": "K.E.",
                    "middle": [],
                    "last": "Muller",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Nizam",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Assessment methods in medical education",
            "authors": [
                {
                    "first": "J.J.",
                    "middle": [],
                    "last": "Norcini",
                    "suffix": ""
                },
                {
                    "first": "D.W.",
                    "middle": [],
                    "last": "Mckinley",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Teach Teacher Educ",
            "volume": "23",
            "issn": "",
            "pages": "239-250",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Testing technical skill via an innovative \"bench station\" examination",
            "authors": [
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Reznnick",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Regehr",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Mac Rae",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Martin",
                    "suffix": ""
                },
                {
                    "first": "W.",
                    "middle": [],
                    "last": "McCulloch",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Am J Surg",
            "volume": "173",
            "issn": "",
            "pages": "226-230",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "A written test as an alternative to performance testing",
            "authors": [
                {
                    "first": "C.P.M.",
                    "middle": [],
                    "last": "Vleuten",
                    "suffix": ""
                },
                {
                    "first": "S.J.",
                    "middle": [],
                    "last": "van Luijk",
                    "suffix": ""
                },
                {
                    "first": "H.J.M.",
                    "middle": [],
                    "last": "Beckers",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Med Educ",
            "volume": "23",
            "issn": "",
            "pages": "97-107",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "The effect on reliability of adding a separate written assessment component to an objective structured clinical examination",
            "authors": [
                {
                    "first": "B.H.",
                    "middle": [],
                    "last": "Verhoeven",
                    "suffix": ""
                },
                {
                    "first": "J.G.H.C.",
                    "middle": [],
                    "last": "Hamers",
                    "suffix": ""
                },
                {
                    "first": "A.J.J.A.",
                    "middle": [],
                    "last": "Scherpbier",
                    "suffix": ""
                },
                {
                    "first": "R.J.I.",
                    "middle": [],
                    "last": "Hoogenboom",
                    "suffix": ""
                },
                {
                    "first": "C.P.M.",
                    "middle": [],
                    "last": "Vamder Vleuten",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Med Educ",
            "volume": "34",
            "issn": "",
            "pages": "525-529",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "The assessment of clinical skills/competence/performance",
            "authors": [
                {
                    "first": "G.E.",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Acad Med",
            "volume": "65",
            "issn": "",
            "pages": "S63-S67",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "Are commonly used resident measurements associated with procedural skills in internal medicine residency training?",
            "authors": [
                {
                    "first": "S.J.",
                    "middle": [],
                    "last": "Durning",
                    "suffix": ""
                },
                {
                    "first": "L.J.",
                    "middle": [],
                    "last": "Cation",
                    "suffix": ""
                },
                {
                    "first": "J.L.",
                    "middle": [],
                    "last": "Jackson",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J Gen Intern Med",
            "volume": "22",
            "issn": "",
            "pages": "357-361",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "Do ratings on the American Board of Internal Medicine resident evaluation form detect differences in clinical competence?",
            "authors": [
                {
                    "first": "R.J.",
                    "middle": [],
                    "last": "Haber",
                    "suffix": ""
                },
                {
                    "first": "A.L.",
                    "middle": [],
                    "last": "Avins",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "J Gen Intern Med",
            "volume": "9",
            "issn": "",
            "pages": "140-145",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Assessing residents\u2019 clinical performance: cumulative results of a four-year study with the Objective Structured Clinical Examination",
            "authors": [
                {
                    "first": "R.W.",
                    "middle": [],
                    "last": "Schwartz",
                    "suffix": ""
                },
                {
                    "first": "D.B.",
                    "middle": [],
                    "last": "Witzke",
                    "suffix": ""
                },
                {
                    "first": "M.B.",
                    "middle": [],
                    "last": "Donnelly",
                    "suffix": ""
                },
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Stratton",
                    "suffix": ""
                },
                {
                    "first": "A.V.",
                    "middle": [],
                    "last": "Blue",
                    "suffix": ""
                },
                {
                    "first": "D.V.",
                    "middle": [],
                    "last": "Sloan",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Surgery",
            "volume": "124",
            "issn": "",
            "pages": "307-312",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Factor analysis can be a useful standard-setting tool in a high-stakes OSCE assessment",
            "authors": [
                {
                    "first": "A.M.S.",
                    "middle": [],
                    "last": "Chesser",
                    "suffix": ""
                },
                {
                    "first": "M.R.",
                    "middle": [],
                    "last": "Laing",
                    "suffix": ""
                },
                {
                    "first": "Z.H.",
                    "middle": [],
                    "last": "Miedzybrodzka",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Brittenden",
                    "suffix": ""
                },
                {
                    "first": "S.D.",
                    "middle": [],
                    "last": "Heys",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Med Educ",
            "volume": "38",
            "issn": "",
            "pages": "825-831",
            "other_ids": {
                "DOI": []
            }
        }
    }
}