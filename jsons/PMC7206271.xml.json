{
    "paper_id": "PMC7206271",
    "metadata": {
        "title": "Connecting the Dots: Hypotheses Generation by Leveraging Semantic Shifts",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Menasha",
                "middle": [],
                "last": "Thilakaratne",
                "suffix": "",
                "email": "menasha.thilakaratne@adelaide.edu.au",
                "affiliation": {}
            },
            {
                "first": "Katrina",
                "middle": [],
                "last": "Falkner",
                "suffix": "",
                "email": "katrina.falkner@adelaide.edu.au",
                "affiliation": {}
            },
            {
                "first": "Thushari",
                "middle": [],
                "last": "Atapattu",
                "suffix": "",
                "email": "thushari.atapattu@adelaide.edu.au",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Due to the massive influx of research publications, examining the published literature and constructing a novel research hypothesis in a sensible time-frame has almost become an unachievable endeavour for researchers. For example, consider a researcher who is interested in researching about dementia. To formulate a novel research hypothesis in the field, the researcher requires to comprehensively analyse and understand the existing body of knowledge. Currently, a simple search in PubMed alone for dementia results in more than 150,000 records. Even though techniques such as text summarisation would assist the users to glean high-level overview of the field, they fail to elicit implicit interesting connections in disparate and seemingly independent facts that have the potential in developing novel knowledge.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To this end, Literature-Based Discovery (LBD) (a.k.a. Hypotheses Generation) which is a sub-discipline of text mining aims to infer such interesting cross-silo associations that bridge uncorrelated fragments of information to provide novel, actionable and meaningful insights in the field. For instance, consider two disjoint topics of interest (A and C); a therapeutic substance (e.g., fish oil) and a disease (e.g., raynaud) where the LBD process attempts to elicit novel conceptual bridges [3] (e.g., blood viscosity) that meaningfully connect the two knowledge fragments (Fig. 1). Hence, the ultimate motive of LBD research is to give new impetus to deduce new knowledge that will conclusively accelerate scientific productivity and research innovation. Discovering such conceptual bridges in a cross-disciplinary manner is the crux of the problem that we intend to address.\n",
            "cite_spans": [
                {
                    "start": 494,
                    "end": 495,
                    "mention": "3",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": [
                {
                    "start": 581,
                    "end": 582,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Despite the significant advances gained so far in the discipline, almost all the LBD systems suffer from two major drawbacks; 1) Domain dependency: the existing LBD studies are mainly restricted to the medical domain and rely on medical-related knowledge resources (e.g., MeSH, UMLS) throughout their workflow. Some of the LBD studies are not generalisable within the medical domain itself due to the usage of highly specialised resources [8]. As a result, extending these techniques in other non-medical LBD settings (e.g., computer science domain) is infeasible. Consequently, LBD research outside of the medical domain is in a nascent stage [8], and 2) Static domains: the prior LBD studies are based on the assumption that the domains remain static. This clearly hinders the model\u2019s performance in recommending time-aware novel knowledge linkages as the domains are changing dynamically, and new knowledge is being added to each domain every single day. Otherwise stated, the contribution of temporal cues in eliciting novel knowledge has been overlooked in the discipline [3].",
            "cite_spans": [
                {
                    "start": 440,
                    "end": 441,
                    "mention": "8",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 645,
                    "end": 646,
                    "mention": "8",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1078,
                    "end": 1079,
                    "mention": "3",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To overcome the first limitation, a recent study of Sebastian et al. [7] has attempted to use WordNet in their LBD process. While this is encouraging, WordNet typically covers everyday English and limited in terms of scientific terminology. Considering the issues of domain-dependency in LBD studies arose the questions; how to identify potential knowledge discovery cues whose success does not depend on domain-dependent resources such as MeSH and UMLS? and what are the domain-independent resources that have a wider coverage than WordNet to perform the initial preprocessing of the literature?",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 71,
                    "mention": "7",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "More recently, a few studies [3, 4, 12] have attempted to mitigate the second limitation, which is considering the domains to be static through the infusion of temporal dimension into the LBD process. Even though these studies undoubtedly invigorate the traditional LBD setting, they still contain several inherent limitations. Firstly, the temporal analysis component of these studies is fairly shallow. For example, Xun et al. [12] have only considered the first and last values of the temporal sequence to measure the trend of implicit associations by ignoring the patterns in the overall sequence. Secondly, as of most of the existing LBD literature, these studies rely on one or two temporal characteristics to discover potential new knowledge linkages. This is limiting as such methodologies may excessively be picking only one type of novel knowledge. We believe that the novel knowledge is in different forms in the literature and thus, should fulfil multiple factors to broadly discover them. We observed a similar conclusion from the ARROWSMITH study [11] initiated by the pioneers in LBD discipline and from a recent LBD review [9]. To alleviate the aforementioned limitations, this study attempts to answer the following questions; does analysing whole time-series in a greater detail benefits in eliciting new knowledge?, and does providing a holistic solution that combines the complementary strengths of multiple characteristics (e.g., multiple semantic shifts) yield better predictive effects?",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 31,
                    "mention": "3",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 33,
                    "end": 34,
                    "mention": "4",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 36,
                    "end": 38,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 430,
                    "end": 432,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1062,
                    "end": 1064,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1140,
                    "end": 1141,
                    "mention": "9",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In summary, our contributions are; 1) proposing a generalisable LBD framework that can be easily adaptable to non-medical LBD settings, 2) quantifying semantic change of topics in conjunction with temporal trajectories and word embedding techniques to capture subtle cues that are robust and highly predictive in suggesting novel knowledge, 3) scrutinising the effect of temporal dynamics with high level of granularity in differentiating the potential connections from the false positives, and 4) integrating machine learning techniques to amalgamate the semantic shift measures to recommend the new knowledge.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The early work of Swanson demonstrated the potentiality of logically connecting independent information nuggets dispersed across the literature to generate new practical knowledge [9]. Even though these studies formed the groundwork in the discipline, the underlying knowledge synthesis was performed manually requiring a substantial amount of time and effort. Subsequently, several studies [8] have attempted to automate Swanson\u2019s manual process by incorporating frequency-based statistical measures. The major limitation of these methods is their excessive dependency on highly frequent topics. Consequently, RaJoLink LBD system [8] followed the notion of rarity by only favouring the low frequent topics. Nevertheless, reliance on high or rare frequencies were progressive, they do not necessarily capture semantically meaningful connections.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 182,
                    "mention": "9",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 392,
                    "end": 393,
                    "mention": "8",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 632,
                    "end": 633,
                    "mention": "8",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "In the meantime, several studies have experimented semantic predications (subject-predicate-object) using more specialised medical resources such as SemRep [6]. Despite being descriptive, the applications of these LBD systems are highly restrictive due to the following reasons. Firstly, they require to have a prior knowledge about all the potential predicates related to the problem and ignore the topics that are outside of the specified predicates. Secondly, the availability of such specialised resources is highly limited to certain problems [8]. Subsequently, another line of research [6] has integrated graph theory to the LBD process by analysing graph properties at; macro-level (e.g., shortest path), meso-level (e.g., clustering coefficient), and micro-level (e.g., centrality measures). Even though the graph-theoretic methods remain more successful, they fail to capture implicit linkages due to their rigid schema [4].",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 158,
                    "mention": "6",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 549,
                    "end": 550,
                    "mention": "8",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 593,
                    "end": 594,
                    "mention": "6",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 930,
                    "end": 931,
                    "mention": "4",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Notwithstanding the research progress gained so far in the discipline, almost all the research studies suffer from the following two major limitations; 1) over-reliance of domain-dependent resources that restricts the model\u2019s applications, and 2) neglecting temporal dimension by assuming the literature to be static. Hence, in this study we extend the state-of-the-art techniques by proposing a novel domain-independent temporal methodology to unwind new signals to detect intriguing novel knowledge linkages. Some of the inspiration for this study was emanated from the recent LBD studies that strived to model temporal dimension in LBD process [3, 4, 12]. Nevertheless, we differ from these studies in multiple ways as discussed in Sect. 1.",
            "cite_spans": [
                {
                    "start": 648,
                    "end": 649,
                    "mention": "3",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 651,
                    "end": 652,
                    "mention": "4",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 654,
                    "end": 656,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "This section provides a high-level overview of the proposed model by outlining the key functionalities of main phases. Recall that the input to our model is two topics of interest (A and C) and date T, where the goal is to analyse the literature up to time T and to detect latent top k conceptual bridges that are most likely to connect the two topics in future (i.e. in time T + 1).",
            "cite_spans": [],
            "section": "Overview of the Proposed Model",
            "ref_spans": []
        },
        {
            "text": "To facilitate this, a literature corpus collected up to the time T is required. In this regard, we consider two types of corpora namely; 1) local corpus: this is the query specific literature retrieved using the input (i.e. topic A and C) to obtain the local topics, and 2) global corpus: this is the entire literature set in the literature database that enables the analysis of local topics in a global scale. Subsequently, the global corpus is split into equivalent sized time-slices to obtain a time-specific global corpus that supports evolutionary analysis (see Fig. 2).\n",
            "cite_spans": [],
            "section": "Overview of the Proposed Model",
            "ref_spans": [
                {
                    "start": 572,
                    "end": 573,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "In the initial phase of our model, the local corpus is preprocessed (i.e. concept extraction and filtering) to identify the topics that are local to the input. Subsequently, the evolutionary analysis is performed by considering semantic change as the primary temporal setting. To quantify the semantic change, we construct latent embedding spaces for each time window using the global corpus and analyse the temporal trajectories of local topics in a global context. In this regard, we propose three broader classes of evolutionary measures; individual semantic shifts, relative semantic shifts and relative semantic shifts extended.",
            "cite_spans": [],
            "section": "Overview of the Proposed Model",
            "ref_spans": []
        },
        {
            "text": "Next, the derived semantically infused temporal trajectories of each local topic are analysed using time-series analysis techniques. To this end, we consider two types of models; Feature-based Time-series Model (FTM) and Dedicated Time-series Model (DTM). FTM utilises features extracted from each trajectory that detect patterns in the time-series. Lastly, the features are articulated to recommend the novel knowledge linkages. DTM follows a similar analysis as FTM where we consider the recent advances in deep learning, particularly Long Short-Term Memory (LSTM) to learn the patterns from the temporal trajectories. Unlike handcrafted features, such models offer the opportunity in discovering unforeseen structures of novel knowledge.",
            "cite_spans": [],
            "section": "Overview of the Proposed Model",
            "ref_spans": []
        },
        {
            "text": "The first challenge we faced was identifying a suitable multi-domain knowledgebase that has a broader coverage than WordNet [7] to facilitate the initial preprocessing. In this regard, we selected DBpedia which is the largest multi-domain ontology lying at the heart of Linked Open Data (LOD) cloud [10] as the primary structured knowledgebase. DBpedia is also a multilingual resource which allows this initial preprocessing extendable not only to literature in other domains but also in other languages1. To date, DBpedia supports 134 languages. The English version of DBpedia alone includes 1.7 billion facts. We mapped the typical preprocessing steps used in LBD workflow [9] by using the properties of DBpedia as summarised in Table 1. The justification for each DBpedia property selection (in Table 1) and how it is aligned with the LBD workflow are described in Supplementary material Section B. The remaining phases of our methodology do not require any knowledge inferences from outside resources, thereby fulfilling our intention of proposing a generalisable LBD model.\n",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 126,
                    "mention": "7",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 300,
                    "end": 302,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 676,
                    "end": 677,
                    "mention": "9",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Initial Preprocessing ::: Methodology",
            "ref_spans": [
                {
                    "start": 737,
                    "end": 738,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 804,
                    "end": 805,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "The focus of this section is to discuss how we quantified the semantic change of local topics (concepts) to recommend the novel conceptual bridges.",
            "cite_spans": [],
            "section": "Semantic Shifts ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Skip Gram with Negative Sampling (SGNS): Since word embeddings can be viewed as a potential diachronic tool [2], we learnt distributed representation of concepts in each distant time-slices of global corpus to analyse how the concepts semantically changed over time. To this end, we utilised the popular neural word embedding; word2vec [5] (more specifically SGNS) to construct the vector space for each snapshot of global corpus. In these representations each concept wi has a vector representation w(t) at each time-slice.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 110,
                    "mention": "2",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 337,
                    "end": 338,
                    "mention": "5",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Semantic Shifts ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Embedding Alignment: Due to the stochastic nature of SGNS, the constructed word vectors for each time period could be in arbitrary orthogonal transformations. Hence, it is important to align the word vectors to the same co-ordinate axes to facilitate semantic comparison of a same concept across time (e.g., for measures such as global semantic shifts). Defining a matrix of word embeddings trained at time period t as\n\n, the orthogonal procrustes alignment was performed across time-periods using Eq. 1 where\n\n. The solution corresponds to the best rotational alignment while preserving cosine similarity [2].1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathbf{R} ^{(\\textit{t})} = \\mathop {{{\\,\\mathrm{arg\\,min}\\,}}}\\limits _{{\\mathbf {Q}^\\top \\mathbf {Q}=\\mathbf {I}}}{\\parallel }{\\mathbf {QW}}^{(\\textit{t})}-{\\mathbf {W}^{(\\textit{t+1})}}{\\parallel }^{\\textit{F}} \\end{aligned}$$\\end{document}Measuring Semantic Change: This section outlines how we disentangled multiple types of semantic changes using three broad classes of evolutionary measures to distinguish potential novel knowledge linkages2.",
            "cite_spans": [
                {
                    "start": 607,
                    "end": 608,
                    "mention": "2",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Semantic Shifts ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Individual Semantic Shifts (ISS): In this category, we propose two different ways to measure the semantic shift of an individual concept.",
            "cite_spans": [],
            "section": "Semantic Shifts ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "\nGlobal Semantic Shift: The global semantic shift quantifies how far a concept has moved in semantic space between two consecutive time periods. For this purpose, we simply measure the cosine distance of the word vectors of the concept in the aligned vector spaces of time periods t and t + 1 as in Eq. 2. This measure is sensitive to subtle usage drifts and other global effects [1]. 2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\textit{d}}^{\\text {ISS-G}}({w^{(\\textit{t})}_{i}},{w^{(\\textit{t+1})}_{i}}) = \\text {cos-dist}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t+1})}_{i}) \\end{aligned}$$\\end{document}\nLocal Semantic Shift: The local semantic shift measures the change of the concept\u2019s local neighbourhood. Thus, this measure is sensitive to drastic shifts in core meaning and less sensitive to global shifts. Since, the measure is based on the local semantic neighbours, initially, the concept\n\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {K}$$\\end{document} nearest neighbours at time t are obtained (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathcal {N}_{\\mathcal {K}}}(w^{(\\textit{t})}_{i})$$\\end{document}). Subsequently, to quantify the change between the two time-periods t and t + 1, a second-order similarity vector for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w^{(\\textit{t})}_{i}$$\\end{document} is computed from these nearest neighbour sets as defined in Eq. 3. The computed vectors for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w^{(\\textit{t})}_{i}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w^{(\\textit{t+1})}_{i}$$\\end{document} are used to quantify the local neighbourhood change as in Eq. 4 (see [1] for details). 3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathbf {s}^{(\\textit{t})}(\\textit{j}) = \\text {cos-sim}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t})}_{j}) {\\text { where }} {{\\forall }w^{j}} {\\text { }}{\\in }{\\text { }} {\\mathcal {N}_{\\mathcal {K}}}(w^{(\\textit{t})}_{i}) {\\cup } {\\mathcal {N}_{\\mathcal {K}}}(w^{(\\textit{t+1})}_{i}) \\end{aligned}$$\\end{document}\n4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\textit{d}{^{ \\text {ISS-L}}}({w^{(\\textit{t})}_{i}},{w^{(\\textit{t+1})}_{i}}) = \\text {cos-dist}(\\mathbf {s}^{(\\textit{t})}_{i}, \\mathbf {s}^{(\\textit{t+1})}_{i}) \\end{aligned}$$\\end{document}\nRelative Semantic Shifts (RSS): In this category, we measure the semantic shifts of the concepts relative to the input topic A and C.",
            "cite_spans": [
                {
                    "start": 381,
                    "end": 382,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 3040,
                    "end": 3041,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Semantic Shifts ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "\nPairwise Semantic Displacement: This measure quantifies how the semantic similarity of a concept changes over the time relatively to the A and C topics. Thus, this measure verifies if there is a growing semantic similarity of the concept towards topic A and C (see Eq. 5). 5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\textit{s}{^{\\text { RSS-S}}}({w^{(\\textit{t})}_{i}},{w^{(\\textit{t})}_{A}},{w^{(\\textit{t})}_{C}}) = \\text {avg}( \\text {cos-sim}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t})}_{A}), \\text {cos-sim}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t})}_{C}) \\text {)} \\end{aligned}$$\\end{document}\nPairwise Distance Proximity: This measure verifies whether a concept\u2019s temporal trajectory is leaning towards to a close proximity of both the input topic A and C (Eq. 6). i.e. whether the concept\u2019s trajectory is not favouring A or C individually, but both at the same time. The intuition for this measure is that we are seeking for conceptual bridges that implicitly connects A and C, thus, the trajectory should favour both the topics. 6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned}&\\textit{d}{^{\\text { RSS-D}}}({w^{(\\textit{t})}_{i}},{w^{(\\textit{t})}_{A}},{w^{(\\textit{t})}_{C}}) = \\text {max(cos-dist}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t})}_{A}),\\text {cos-dist}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t})}_{C})\\text {)} \\\\ \\nonumber&\\qquad \\qquad \\, +\\,\\beta \\mid \\text {cos-dist}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t})}_{A})-\\text {cos-dist}(\\mathbf {w}^{(\\textit{t})}_{i}, \\mathbf {w}^{(\\textit{t})}_{})\\mid \\text { where } \\beta \\ge 0 \\end{aligned}$$\\end{document}\nRelative Semantic Shifts Extended (RSSEx): In this category, we extend the two measures proposed in RSS category using the recent neighbours of topic A and C namely; Neighbourhood Semantic Displacement and Neighbourhood Semantic Proximity. The recent neighbours of topic A (NA) and C (NC) in a time window \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {W}$$\\end{document} are calculated as in Eq. 7.7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} N_A =\\bigcap \\limits _{t=T-\\mathcal {W}}^T {\\mathcal {N}_{\\mathcal {K}}}(w^{(\\textit{t})}_{A}), \\text { } N_C =\\bigcap \\limits _{t=T-\\mathcal {W}}^T {\\mathcal {N}_{\\mathcal {K}}}(w^{(\\textit{t})}_{C}) \\end{aligned}$$\\end{document}\n",
            "cite_spans": [],
            "section": "Semantic Shifts ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "For each local topic in the corpus, we compute the six semantic shift measures discussed in Sect. 4.2. i.e. every local topic has six temporal trajectories that showcase how the topic semantically changed over the time. The derived semantically infused temporal trajectories are analysed at two levels;",
            "cite_spans": [],
            "section": "Semantically Infused Temporal Trajectories ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Feature-based Time-series Model (FTM): This model employs descriptive statistics of each temporal trajectory such as variance, median as the main temporal features3. Considering the variations of semantic shifts, we consider two types of FTM models; 1) FTM-D: This type considers ISS and RSS as the key temporal trajectories, and 2) FTM-Ex: This type employs ISS and RSS-Ex with the intention of evaluating the contribution of local neighbourhood in the relative measures. The potentially of the knowledge linkage is decided by the estimated probability of FTM when the knowledge linkage is in the testing slice.",
            "cite_spans": [],
            "section": "Semantically Infused Temporal Trajectories ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Dedicated Time-series Model (DTM): In recent years, LSTM models have shown promise in many application areas including time-series and sequential data analysis. Inspired from these research outside of LBD, we employed a sequential LSTM model to analyse the derived temporal trajectories (see footnote 3). Similar to FTM model types, we analyse two types of DTM models namely; DTM-D and DTM-Ex. The estimated probability of DTM is considered to decide the potentiality of the knowledge linkage when it is in the testing slice.",
            "cite_spans": [],
            "section": "Semantically Infused Temporal Trajectories ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Table 2 reports the P@k for the golden datasets; (1) and (2) where k is progressively increased from 10 to 100. The P@k result tables for all the five golden test cases are reported in Section F of Supplementary material. While P@k indicates the coverage of correct recommendations, MAP@k (which is the arithmetic mean of Average Precision@k) measures the overall performance of the models considering their ranking order of correct recommendations. Table 3 reports the MAP@k of the five golden test cases. The results of both P@k and MAP@k indicate that all the variants of the proposed model consistently outperform the existing baselines which demonstrates the ability of detailed semantic shifts analysis in detecting meaningful novel knowledge linkages.\n\n",
            "cite_spans": [],
            "section": "Results and Discussion ::: Experiments",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 456,
                    "end": 457,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "We revealed the following trends through the analysis of the proposed variants of the our model. In terms of the coverage of correct recommendations (i.e. P@k), DTM-D reports the highest overall performance across datasets. This showcase the ability of LSTMs in detecting unforseen structures in the temporal trajectories that are useful in differentiating potential new knowledge from false connections. The MAP@k results indicate that FTM-Ex consistently outperform the remaining models by often front-loading the correct recommendations. i.e. this model tend to have a better ordering of the knowledge recommendations. This highlights that the LBD model is sensitive not only to the topic A and C alone, but also to their core meaning. In recommendation tasks such as LBD, it is unrealistic to expect that the user will examine and experiment the entire list of proposed new knowledge linkages. In other words, better ordering of the knowledge recommendations is crucial compared to coverage. Therefore, we believe that the slight P@k loss incurred using FTM-Ex in most of the test cases can be indemnified through its performance gain in MAP@k.",
            "cite_spans": [],
            "section": "Results and Discussion ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Overall, we consider the following reasons as the main strengths of the proposed model compared to the baselines; 1) Multiple characteristics: Even though most of the LBD studies strictly rely on one or two characteristics to elicit new knowledge, it is observed that new knowledge can be in multiple forms due to complexity of the knowledge structures in the scientific literature. For instance, Davies [9] has identified five forms of novel knowledge in FO-RD and MG-MIG test cases. Thus, our model take the advantage of detecting novel knowledge in different forms by capturing the semantic change at multiple levels; individual shifts, pairwise shifts and neighbourhood shifts, 2) Global analysis: Unlike most of the prior LBD research that merely focus on cues at local scale, we analysed the concepts\u2019 trajectories in a global context. This facilitates the analysis of concepts neighbourhood in a wider scope. For example, consider \u201cblood viscosity\u201d conceptual bridge of FO-RD test case. This conceptual bridge may also be associated with other chemical substances of FO (such as eicosapentaenoic acid). However, query specific local corpus often limits in accommodating such implicit interactions, 3) Detailed temporal analysis: While almost all the prior LBD research are based on the static literature, we considered the temporal behaviour of concepts to discover new knowledge. This allows the model to detect time-aware knowledge recommendations that have higher semantic meaning. Moreover, it is also evident that analysing the time-series in detail benefits in LBD workflow (in contrast to baselines such as DE [12]), and 4) Generalisability: The proposed temporal clues are free from knowledge inferences from domain-dependent resources (unlike baselines such as AR [11]). This meets our objective of generalisabe cues whose predictive effects do not rely on the specialised knowledgebases. Moreover, our initial preprocessing phase is also adaptable to multiple domains and languages due to strengths of DBpedia. Thus, our solution can be easily integrated to non-medical LBD settings.",
            "cite_spans": [
                {
                    "start": 405,
                    "end": 406,
                    "mention": "9",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1625,
                    "end": 1627,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1781,
                    "end": 1783,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Results and Discussion ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Further analysing the results, we observe that AR performs the best among the baselines. AR is arguably the most popular and well-maintained LBD system in the discipline that currently has nearly 1200 of monthly user-base [6]. We observe two main reasons for its performance gain compared to the remaining baselines. Firstly, it considers seven characteristics to determine the potentiality of the novel knowledge (i.e. the use of multiple characteristics). Secondly, three of their characteristics include global literature analysis which benefits in identifying the concept\u2019s global properties that are not visible to local corpus. However, three of its features require the analysis of UMLS and MeSH, which restricts the suitability of AR baseline only to the medical domain.",
            "cite_spans": [
                {
                    "start": 223,
                    "end": 224,
                    "mention": "6",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Results and Discussion ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In this study, we have described, evaluated and systematically compared our semantically infused temporal model in detecting novel knowledge linkages. The results indicate the challenge associated in detecting such novel linkages and emphasis the need of developing circumstantial solutions to handle the problem. Overall, the holistic integration of semantics and temporal information significantly outperformed all the existing baselines in the discipline. The supplementary material of this paper is also available at: https://tinyurl.com/lbd-supplementary.",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        },
        {
            "text": "In future research, we intend to take the advantage of the power of the proposed semantic shifts and the domain-independency of the model to contribute to LBD research in non-medical domains such as computer science (thus far, there exists only one LBD study in computer science [8]). Therefore, we believe that our model will be a successful first step towards promoting generalisable LBD systems.",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 281,
                    "mention": "8",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Mapping typical preprocessing steps used in LBD using DBpedia\nThe URI prefixes in DBpedia setting can be resolved at http://prefix.cc/",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Precision@k for FO-RD and MIG-MG golden test cases\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Mean Average Precision@k (MAP@k) for all golden test cases\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Schematic overview of hypotheses generation",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Overview of the proposed model",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "A quantitative model for linking two disparate sets of articles in medline",
            "authors": [
                {
                    "first": "VI",
                    "middle": [],
                    "last": "Torvik",
                    "suffix": ""
                },
                {
                    "first": "NR",
                    "middle": [],
                    "last": "Smalheiser",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Bioinformatics",
            "volume": "23",
            "issn": "13",
            "pages": "1658-1665",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btm161"
                ]
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "Learning the heterogeneous bibliographic information network for literature-based discovery",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sebastian",
                    "suffix": ""
                },
                {
                    "first": "EG",
                    "middle": [],
                    "last": "Siew",
                    "suffix": ""
                },
                {
                    "first": "SO",
                    "middle": [],
                    "last": "Orimaye",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Knowl.-Based Syst.",
            "volume": "115",
            "issn": "",
            "pages": "66-79",
            "other_ids": {
                "DOI": [
                    "10.1016/j.knosys.2016.10.015"
                ]
            }
        },
        "BIBREF10": {
            "title": "A systematic review on literature-based discovery: general overview, methodology, & statistical analysis",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Thilakaratne",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Falkner",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Atapattu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM Comput. Surv.",
            "volume": "52",
            "issn": "6",
            "pages": "1-34",
            "other_ids": {
                "DOI": [
                    "10.1145/3365756"
                ]
            }
        },
        "BIBREF11": {
            "title": "A systematic review on literature-based discovery workflow",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Thilakaratne",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Falkner",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Atapattu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "PeerJ Comput. Sci.",
            "volume": "5",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.7717/peerj-cs.235"
                ]
            }
        }
    }
}