{
    "paper_id": "22e76c1bc0633dc25d8fc4c0417f6a07f067fada",
    "metadata": {
        "title": "Text-Image-Video Summary Generation Using Joint Integer Linear Programming",
        "authors": [
            {
                "first": "Anubhav",
                "middle": [],
                "last": "Jangra",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Indian Institute of Technology Patna",
                    "location": {
                        "settlement": "Patna",
                        "country": "India"
                    }
                },
                "email": "anubhav0603@gmail.com"
            },
            {
                "first": "Adam",
                "middle": [],
                "last": "Jatowt",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Kyoto University",
                    "location": {
                        "settlement": "Kyoto",
                        "country": "Japan"
                    }
                },
                "email": "jatowt@gmail.com"
            },
            {
                "first": "Mohammad",
                "middle": [],
                "last": "Hasanuzzaman",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Cork Institute of Technology",
                    "location": {
                        "settlement": "Cork",
                        "country": "Ireland"
                    }
                },
                "email": "hasanuzzaman.im@gmail.com"
            },
            {
                "first": "Sriparna",
                "middle": [],
                "last": "Saha",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Indian Institute of Technology Patna",
                    "location": {
                        "settlement": "Patna",
                        "country": "India"
                    }
                },
                "email": "sriparna.saha@gmail.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "Automatically generating a summary for asynchronous data can help users to keep up with the rapid growth of multi-modal information on the Internet. However, the current multi-modal systems usually generate summaries composed of text and images. In this paper, we propose a novel research problem of text-image-video summary generation (TIVS). We first develop a multi-modal dataset containing text documents, images and videos. We then propose a novel joint integer linear programming multi-modal summarization (JILP-MMS) framework. We report the performance of our model on the developed dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Advancement in technology has led to rapid growth of multimedia data on the Internet, which prevent users from obtaining important information efficiently. Summarization can help tackle this problem by distilling the most significant information from the plethora of available content. Recent research in summarization [2, 11, 31] has proven that having multi-modal data can improve the quality of summary in comparison to uni-modal summaries. Multi-modal information can help users gain deeper insights. Including supportive representation of text can reach out to a larger set of people including those who have reading disabilities, users who have less proficiency in the language of text and skilled readers who are looking to skim the information quickly [26] . Although visual representation of information is more expressive and comprehensive in comparison to textual description of the same information, it is still not a thorough model of representation. Encoding abstract concepts like guilt or freedom [11] , geographical locations or environmental features like temperature, humidity etc. via images is impractical. Also images are a static medium and cannot represent dynamic and sequential information efficiently. Including videos could then help overcome these barriers since video contains both visual and verbal information. To the best of our knowledge, all the previous works have focused on creating text or text-image summaries, and the task of generating an extractive multimodal output containing text, images and videos from a multi-modal input has not been done before. We thus focus on a novel research problem of text-imagevideo summary generation (TIVS). To tackle the TIVS task, we design a novel Integer Linear Programming (ILP) framework that extracts the most relevant information from the multimodal input. We set up three objectives for this task, (1) salience within modality, (2) diversity within modality and (3) correspondence across modalities. For preprocessing the input, we convert the audio into text using an Automatic Speech Recognition (ASR) system, and we extract the key-frames from video. The most relevant images and videos are then selected in accordance with the output generated by our ILP model.",
            "cite_spans": [
                {
                    "start": 319,
                    "end": 322,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 323,
                    "end": 326,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 327,
                    "end": 330,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 760,
                    "end": 764,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To sum up, we make the following contributions: (1) We present a novel multimodal summarization task which takes news with images and videos as input, and outputs text, images and video as summary. (2) We create an extension of the multi-modal summarization dataset [12] by constructing multi-modal references containing text, images and video for each topic. (3) We design a joint ILP framework to address the proposed multi-modal summarization task.",
            "cite_spans": [
                {
                    "start": 266,
                    "end": 270,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Text summarization techniques are used to extract important information from textual data. A lot of research has been done in the area of extractive [10, 21] and abstractive [3, 4, 19, 23] summarization. Various techniques like graph-based methods [6, 15, 16] , artificial neural networks [22] and deep learning based approaches [18, 20, 29] have been developed for text summarization. Integer linear programming (ILP) has also shown promising results in extractive document summarization [1, 9] . Duan et al. [5] proposed a joint-ILP framework that produces summaries from temporally separate text documents.",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 153,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 154,
                    "end": 157,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 174,
                    "end": 177,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 178,
                    "end": 180,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 181,
                    "end": 184,
                    "text": "19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 185,
                    "end": 188,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 248,
                    "end": 251,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 252,
                    "end": 255,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 256,
                    "end": 259,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 289,
                    "end": 293,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 329,
                    "end": 333,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 334,
                    "end": 337,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 338,
                    "end": 341,
                    "text": "29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 489,
                    "end": 492,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 493,
                    "end": 495,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 510,
                    "end": 513,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Recent years have shown great promise in the emerging field of multi-modal summarization. Multi-modal summarization has various applications ranging from meeting recordings summarization [7] , sports video summarization [25] , movie summarization [8] to tutorial summarization [13] . Video summarization [17, 28, 30] is also a major sub-domain of multi-modal summarization. A few deep learning frameworks [2, 11, 31] show promising results, too. Li et al. [12] uses an asynchronous dataset containing text, images and videos to generate a textual summary. Although some work on document summarization has been done using ILP, to the best of our knowledge no one has ever used an ILP framework in the area of multi-modal summarization.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 190,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 220,
                    "end": 224,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 247,
                    "end": 250,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 277,
                    "end": 281,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "[17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 309,
                    "end": 312,
                    "text": "28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 313,
                    "end": 316,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 405,
                    "end": 408,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 409,
                    "end": 412,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 413,
                    "end": 416,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 456,
                    "end": 460,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Our objective is to generate a multimodal summary S = X sum I sum V sum such that the final summary S covers up all the important information in the original data while minimizing the length of summary, where ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "Each topic in our dataset comprises of text documents, images, audio and videos. As shown in Fig. 1 , we firstlextract key-frames from the videos [32] . These keyframes together with images from the original data form the image-set. The audio is transcribed into text (IBM Watson Speech-to-Text Service: www.ibm.com/ watson/developercloud/speech-to-text.html), which contributes to the text-set together with the sentences from text-documents. The images from then imageset are encoded by the VGG model [24] and the 4,096-dimensional vector from the pre-softmax layer is used as the image representation. Every sentence from the text-set is encoded using the Hybrid Gaussian-Laplacian Mixture Model (HGLMM) into a 6,000-dimensional vector. For text-image matching, these image and sentence vectors are fed into a two-branch neural network [27] to have a 512-dimensional vector for images and sentences in a shared space.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 150,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 839,
                    "end": 843,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [
                {
                    "start": 93,
                    "end": 99,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Pre-processing"
        },
        {
            "text": "ILP is a global optimization technique, used to maximize or minimize an objective function subject to some constraints. In this paper, we propose a joint-ILP technique to optimize the output to have high salience, diversity and crossmodal correlation. The idea of joint-ILP is similar to the one applied in the field of across-time comparative summarization [5] . However, to the best of our knowledge, an ILP framework was not used to solve multi-modal summarization (Gurobi optimizer is used for ILP optimization: https://www.gurobi.com/). Decision Variables. M txt is a n \u00d7 n binary matrix such that m txt i,i indicates whether sentence s i is selected as an exemplar or not and m txt i,j =i indicates whether sentence s i votes for s j as its representative. Similarly, M img is a p \u00d7 p binary matrix that indicates the exemplars chosen in the image set. M c is n \u00d7 p binary matrix that indicates the cross-modal correlation. m c i,j is true when there is some correlation between sentence s i and image I j .",
            "cite_spans": [
                {
                    "start": 358,
                    "end": 361,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Joint-ILP Framework"
        },
        {
            "text": "where mod, t, item \u2208 { text, n, s , img, p, I } is used to represent multiple modalities together in a simple way.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Objective Function"
        },
        {
            "text": "We need to maximize the objective function in Eq. 1, containing salience of text, images and cross-modal correlation. Similar to the joint-ILP formulation in [5] the diversity objective is implicit in this model. Equation 4 generates the set of entities that are a part of the cluster whose exemplar is item i . The salience is calculated by Eqs. 2 and 3 by taking cosine similarity over all the exemplars with the items belonging to their representative clusters separately for each modality. The cross-modal correlation score is calculated in Eq. 5. ",
            "cite_spans": [
                {
                    "start": 158,
                    "end": 161,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Objective Function"
        },
        {
            "text": "Equation 7 ensures that exactly k txt and k img clusters are formed in their respective uni-modal vector space. Equation 8 guarantees that an entity can either be an exemplar or be part of a single cluster. According to Eq. 9, a sentence or image must be exemplar in their respective vector space to be included in the sentence-image summary pairs. Values of m, k txt and k img are set to be 10, same as in [5] .",
            "cite_spans": [
                {
                    "start": 407,
                    "end": 410,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Objective Function"
        },
        {
            "text": "The Joint-ILP framework outputs the text summary (X sum ) and top-m images from the image-set. This output is used to prepare the image and video summary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Post-processing"
        },
        {
            "text": "Equation 11 selects all those images from top10 images that are not keyframes. Assuming that images which look similar would have similar annotation scores and would help users gain more insight, the images relevant to the images in I sum1 (at least with \u03b1 cosine similarity) but not too similar (at max with \u03b2 cosine similarity) to avoid redundancy are also selected to be a part of the final image summary I sum (Eq. 12). \u03b1 is set to 0.4 and \u03b2 is 0.8 in our experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extracting Images"
        },
        {
            "text": "Extracting Video. For each video, weighted sum of visual (Eq. 13) and verbal (Eq. 14) scores is computed. The video with the highest score is selected as our video summary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extracting Images"
        },
        {
            "text": "where KF is the set of all key-frames and ST is the set of speech transcriptions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extracting Images"
        },
        {
            "text": "There is no benchmark dataset for the TIVS task. Therefore, we created our own text-image-video dataset by extending and manually annotating the multi-modal summarization dataset introduced by Li et al. [12] . Their dataset comprised of 25 new topics. Each topic was composed of 20 text documents, 3 to 9 images, and 3 to 8 videos. The final summary however was unimodal, that is, in the form of only a textual summary containing around 300 words. We then extended it by selecting some images and a video for each topic that summarize the topic well. Three undergraduate students were employed to score the images and videos with respect to the benchmark text references. All annotators scored each image and video on a scale of 1 to 5, on the basis of similarity between the image/video and the text references (1 indicating no similarity and 5 denoting the highest level of similarity). Average annotation scores (AAS) were calculated for each image and video. The value of the minimum average annotation score for images was kept as a hyper-parameter to evaluate the performance of our model in various settings 2 . The video with the highest score is chosen to be the video component of the multi-modal summary 3 . ",
            "cite_spans": [
                {
                    "start": 203,
                    "end": 207,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Dataset Preparation"
        },
        {
            "text": "We evaluate the performance of our model using the dataset as described above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings and Results"
        },
        {
            "text": "We use the ROUGE scores [14] to evaluate the textual summary, and based on them we compare our results with the ones of three baselines. We use the multi-document summarization model proposed in [1] . For Baseline-1 we feed the model with embedded sentences from all the original documents together. The central vector is calculated as the average of all the sentence vectors. The model is given vectors for sentences from the text-set and images from the image-set in the joint space for other baselines. For Baseline-2, the average of all the vectors is taken as the central vector. For Baseline-3, the central vector is calculated as the weighted average of all the sentence and image vectors. We give equal weights to text, speech and images for simplicity.",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 28,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 195,
                    "end": 198,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings and Results"
        },
        {
            "text": "As shown in Table 1 , our model produces better results than the prepared baselines in terms of ROUGE-2 and ROUGE-l scores. Table 2 shows the average precision and recall scores as well as the variance. We set various threshold values for the annotation scores to generate multiple image test sets in order to evaluate the performance of our model. We get a higher precision score for low AAS value, because the number of images in the final solution increases on decreasing the threshold values. The proposed model gave 44% accuracy in extracting the most appropriate video (whereas random selection of images for 10 different iterations gives an average 16% accuracy).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 19,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 124,
                    "end": 131,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Experimental Settings and Results"
        },
        {
            "text": "Unlike other problems that focus on text-image summarization, we propose to generate a truly multi-modal summary comprising of text, images and video. We also develop a dataset for this task, and propose a novel joint ILP framework to tackle this problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Multi-document summarization model based on integer linear programming",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Alguliev",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Aliguliyev",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hajirahimova",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Intell. Control Autom",
            "volume": "1",
            "issn": "02",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Abstractive text-image summarization using multi-modal attentional hierarchical RNN",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhuge",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "4046--4056",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Fast abstractive summarization with reinforce-selected sentence rewriting",
            "authors": [
                {
                    "first": "Y",
                    "middle": [
                        "C"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bansal",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1805.11080"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Abstractive sentence summarization with attentive recurrent neural networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chopra",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Auli",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Rush",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "",
            "issn": "",
            "pages": "93--98",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Across-time comparative summarization of news articles",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jatowt",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "735--743",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "LexRank: graph-based lexical centrality as salience in text summarization",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Erkan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Radev",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "J. Artif. Intell. Res",
            "volume": "22",
            "issn": "",
            "pages": "457--479",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Multimodal summarization of meeting recordings",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Erol",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hull",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the 2003 International Conference on Multimedia and Expo",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Evangelopoulos",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Multimed",
            "volume": "15",
            "issn": "7",
            "pages": "1553--1568",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Extractive multi-document summarization with integer linear programming and support vector regression",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Galanis",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lampouras",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Androutsopoulos",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of COLING 2012",
            "volume": "",
            "issn": "",
            "pages": "911--926",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A trainable document summarizer",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kupiec",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pedersen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "68--73",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Multi-modal sentence summarization with modality attention and image filtering",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zong",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Multi-modal summarization for asynchronous collection of text, image",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zong",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Multimodal abstractive summarization for open-domain videos",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Libovick\u1ef3",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Palaskar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gella",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Metze",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Workshop on Visually Grounded Interaction and Language (ViGIL), NIPS",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "ROUGE: a package for automatic evaluation of summaries",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "74--81",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Graph-based ranking algorithms for sentence extraction, applied to text summarization",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mihalcea",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the ACL Interactive Poster and Demonstration Sessions",
            "volume": "",
            "issn": "",
            "pages": "170--173",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "TextRank: bringing order into text",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mihalcea",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tarau",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "404--411",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Streaming non-monotone submodular maximization: personalized video summarization on the fly",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mirzasoleiman",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jegelka",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krause",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Summarunner: a recurrent neural network based sequence model for extractive summarization of documents",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nallapati",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nallapati",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gulcehre",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1602.06023"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Classify or select: neural architectures for extractive document summarization",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nallapati",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.04244"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Constructing literature abstracts by computer: techniques and prospects",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Paice",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Inf. Process. Manag",
            "volume": "26",
            "issn": "1",
            "pages": "171--186",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Extractive single document summarization using multi-objective optimization: exploring self-organized differential evolution, grey wolf optimizer and water cycle algorithm. Knowl.-Based Syst",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Saini",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saha",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jangra",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bhattacharyya",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "164",
            "issn": "",
            "pages": "45--67",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Get to the point: summarization with pointergenerator networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "See",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1409.1556"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Multi-modal summarization of key events and top players in sports tournament videos",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tjondronegoro",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sasongko",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "H"
                    ],
                    "last": "Lau",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "2011 IEEE Workshop on Applications of Computer Vision (WACV)",
            "volume": "",
            "issn": "",
            "pages": "471--478",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Multimodal summarization of complex sentences",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Uzzaman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Bigham",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Allen",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 16th International Conference on Intelligent User Interfaces",
            "volume": "",
            "issn": "",
            "pages": "43--52",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Learning deep structure-preserving image-text embeddings",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lazebnik",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "5005--5013",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Video summarization via semantic attended networks",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ni",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Multiview convolutional neural networks for multidocument extractive summarization",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Er",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pratama",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Cybern",
            "volume": "47",
            "issn": "10",
            "pages": "3230--3242",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Deep reinforcement learning for unsupervised video summarization with diversity-representativeness reward",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qiao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "MSMO: multimodal summarization with multimodal output",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zong",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "4154--4164",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Adaptive key frame extraction using unsupervised clustering",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhuang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Rui",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "S"
                    ],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mehrotra",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proceedings 1998 International Conference on Image Processing, ICIP98 (Cat. No. 98CB36269)",
            "volume": "1",
            "issn": "",
            "pages": "866--870",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The framework of our proposed model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "i,j = 1; i \u2208 {1, 2, . . . , n} and p j=1 m img i,j = 1; i \u2208 {1, 2, . . . , p}",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Overall performance comparison for textual summary using ROUGE.Table 2. Overall performance of image summary using precision and recall. AAS denotes here the threshold value of image summary generation.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}