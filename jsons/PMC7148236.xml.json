{
    "paper_id": "PMC7148236",
    "metadata": {
        "title": "Seed-Guided Deep Document Clustering",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Mazar",
                "middle": [
                    "Moradi"
                ],
                "last": "Fard",
                "suffix": "",
                "email": "maziar.moradi-fard@univ-grenoble-alpes.fr",
                "affiliation": {}
            },
            {
                "first": "Thibaut",
                "middle": [],
                "last": "Thonet",
                "suffix": "",
                "email": "thibaut.thonet@naverlabs.com",
                "affiliation": {}
            },
            {
                "first": "Eric",
                "middle": [],
                "last": "Gaussier",
                "suffix": "",
                "email": "eric.gaussier@univ-grenoble-alpes.fr",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Clustering traditionally consists in partitioning data into subsets of similar instances with no prior knowledge on the clusters to be obtained. However, clustering is an ill-defined problem in the sense that the data partitions output by clustering algorithms have no guarantee to satisfy end users\u2019 needs. Indeed, different users may be interested in different views underlying the data [25]. For example, considering either the topics or the writing style in a collection of documents leads to different clustering results. In this study, we consider a setting where clustering is guided through user-defined constraints, which is known as constrained clustering [2]. Enabling users to provide clustering constraints in the context of an exploratory task can help obtain results better tailored to their needs. Typically, must-link and cannot-link constraints are considered (e.g., see [27, 29]), which state whether two data instances should be (respectively, should not be) in the same cluster. However, important manual annotation efforts may still be required to provide such constraints in sufficient number. In the specific case of document clustering, constraints can otherwise be provided in the form of seed words: each cluster that the user wishes to obtain is described by a small set of words (e.g., 3 words) which characterize the cluster. For example, a user who wants to explore a collection of news articles might provide the set of seed words {\u2018sport\u2019, \u2018competition\u2019, \u2018champion\u2019}, {\u2018finance\u2019, \u2018market\u2019, \u2018stock\u2019}, {\u2018technology\u2019, \u2018innovation\u2019, \u2018science\u2019} to guide the discovery of three clusters on sport, finance, and technology, respectively. Recent studies which include seed word constraints for document clustering are mostly focused on topic modeling approaches [8, 16, 17, 19], inspired by the Latent Dirichlet Allocation model [3]. Concurrently, important advances on clustering were recently enabled through its combination with deep representation learning (e.g., see [12, 23, 30, 31]), which is now known as deep clustering. A common approach to deep clustering is to jointly train an autoencoder and perform clustering on the learned representations [23, 30, 31]. One advantage of deep clustering approaches lies in their ability to leverage semantic representations based on word embeddings, enabling related documents to be close in the embedding space even when they use different (but related) words.",
            "cite_spans": [
                {
                    "start": 390,
                    "end": 392,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 667,
                    "end": 668,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 890,
                    "end": 892,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 894,
                    "end": 896,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1787,
                    "end": 1788,
                    "mention": "8",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1790,
                    "end": 1792,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1794,
                    "end": 1796,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1798,
                    "end": 1800,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1854,
                    "end": 1855,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1997,
                    "end": 1999,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 2001,
                    "end": 2003,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 2005,
                    "end": 2007,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 2009,
                    "end": 2011,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 2181,
                    "end": 2183,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 2185,
                    "end": 2187,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 2189,
                    "end": 2191,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The main contributions of this study can be summarized as follows: (a) We introduce the Seed-guided Deep Document Clustering (SD2C) framework,1 the first attempt, to the best of our knowledge, to constrain clustering with seed words based on a deep clustering approach; and (b) we validate this framework through experiments based on automatically selected seed words on five publicly available text datasets with various sizes and characteristics.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The remainder of the paper is organized as follows. In Sect. 2, we describe existing works on seed-guided constrained document clustering, also known as dataless text classification. Section 3 then introduces the seed-guided deep document clustering framework, which is then evaluated in Sect. 4. Section 5 concludes the paper and provides some perspectives on SD2C.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "One way to address the above problem is to try and identify multiple clustering views from the data in a purely unsupervised fashion [7, 24, 25]. While such an approach provides users with several possible clustering results to choose from, there is still no guarantee that the obtained clusters are those the users are interested in.",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 135,
                    "mention": "7",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 137,
                    "end": 139,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 141,
                    "end": 143,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The constrained clustering problem we are addressing in fact bears strong similarity with the one of seed-guided dataless text classification, which consist in categorizing documents based on a small set of seed words describing the classes/clusters. For a more general survey on constrained clustering, we invite the reader to refer to [2]. The task of dataless text classification was introduced independently by Liu et al. [20] and Ko et al. [14]. In [20], the seed words are provided by a user and exploited to automatically label a part of the unlabeled documents. On the other hand, in [14], seed words initially correspond to labels/titles for the classes of interest and are extended based on co-occurrence patterns. In both cases, a Naive Bayes classifier is applied to estimate the documents\u2019 class assignments. In the wake of these seminal works, several studies further investigated the exploitation of seed words for text classification [6, 9, 10]. Chang et al. [6] introduced both an \u2018on-the-fly\u2019 approach and a bootstrapping approach by projecting seed words and documents in the same space. The former approach simply consists in assigning each document to the nearest class in the space, whereas the latter learns a bootstrapping Naive Bayes classifier with the class-informed seed words as initial training set. Another bootstrapping approach is studied in [10], where two different methods are considered to build the initial training set from the seed words: Latent Semantic Indexing and Gaussian Mixture Models. The maximum entropy classifier proposed in [9] instead directly uses seed words\u2019 class information by assuming that documents containing seed words from a class are more likely to belong to this class.",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 339,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 427,
                    "end": 429,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 446,
                    "end": 448,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 455,
                    "end": 457,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 593,
                    "end": 595,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 951,
                    "end": 952,
                    "mention": "6",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 954,
                    "end": 955,
                    "mention": "9",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 957,
                    "end": 959,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 976,
                    "end": 977,
                    "mention": "6",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1376,
                    "end": 1378,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1577,
                    "end": 1578,
                    "mention": "9",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "More recently, the dataless text classification problem was addressed through topic modeling approaches [8, 16, 17, 19], extending the Latent Dirichlet Allocation model [3]. The topic model devised by Chen et al. [8] integrates the seed words as pseudo-documents, where each pseudo-document contains all the seed words given for a single class. The co-occurrence mechanism underlying topic models along with the known class membership of pseudo-documents help guide the actual documents to be classified towards their correct class. In [17], the Seed-guided Topic Model (STM) distinguishes between two types of topics: category topics and general topics. The former describe the class information and are associated with an informed prior based on the seed words, whereas the latter correspond to the general topics underlying the whole collection. The category topics assigned to a document are then used to estimate its class assignment. STM was extended in [16] to simultaneously perform classification and document filtering \u2013 which consists in identifying the documents related to a given set of categories while discarding irrelevant documents \u2013 by further dividing category topics into relevant and non-relevant topics. Similarly to STM, the Laplacian Seed Word Topic Model (LapSWTM) introduced by Li et al. [19] considers both category topics and general topics. It however differs from previous models in that it enforces a document manifold regularization to overcome the issue of documents containing no seed words. If these models outperform previously proposed models, they suffer from a lack of flexibility on the input representations they rely on. Indeed, topic models require documents to be organized as sets of discrete units \u2013 the word tokens. This prohibits the use of representation learning techniques such as word embeddings (e.g., word2vec [22] and GloVe [26]).",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 106,
                    "mention": "8",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 108,
                    "end": 110,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 112,
                    "end": 114,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 116,
                    "end": 118,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 170,
                    "end": 171,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 214,
                    "end": 215,
                    "mention": "8",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 537,
                    "end": 539,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 961,
                    "end": 963,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1316,
                    "end": 1318,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1866,
                    "end": 1868,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1881,
                    "end": 1883,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "To the best of our knowledge, only one deep learning-based approach was proposed to address a problem similar to dataless text classification [18]. In this recent work, Li et al. devised a deep relevance model for zero-shot document filtering \u2013 which consists at test time in predicting the relevance of documents with respect to a category unseen in the training set, where each category is characterized by a set of seed words. This problem is nonetheless different from dataless text classification as it focuses on estimating documents\u2019 relevance (or lack thereof) instead of class membership.",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 145,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "In practice, we use fully differentiable formulations of Problems 3 and 4. In the context of the k-Means algorithm, a popular clustering method, such differentiable formulations can be directly developed on top of the algorithms provided in [31] (called DCN) and [23] (called DKM), the latter proposing a truly joint formulation of the deep clustering problem. Other state-of-the-art deep clustering approaches, as IDEC [12], also based on cluster representatives, could naturally be adopted as well. The comparison between these approaches performed in [23] nevertheless suggests that DKM outperforms the other approaches. This difference was confirmed on the text collections retained in this study. We thus focus here on the DKM algorithm introduced in [23] with:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathcal {L}_{\\text {rec}}\\!\\left( \\mathcal {X}, g_{\\eta }\\!\\circ \\!f_{\\theta }(\\mathcal {X})\\right) = \\sum _{x \\in \\mathcal {X}} \\delta ^I\\!\\left( \\mathbf {x}, g_{\\eta }\\!\\circ \\!f_{\\theta }(\\mathbf {x})\\right) , \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta ^I$$\\end{document} denotes a dissimilarity in the input space, and:6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathcal {L}_{\\text {clust}}\\!\\left( f_{\\theta }(\\mathcal {X}), \\mathcal {R}\\right) = \\sum _{x \\in \\mathcal {X}} \\sum _{k=1}^K \\delta ^E\\!\\left( f_{\\theta }(\\mathbf {x}), \\mathbf {r}_k\\right) G_k(f_{\\theta }(\\mathbf {x}), \\alpha ; \\mathcal {R}) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha $$\\end{document} is an inverse temperature parameter and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_k(f_{\\theta }(\\mathbf {x}), \\alpha ; \\mathcal {R})$$\\end{document} is a softmax function parameterized by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha $$\\end{document} defined as follows:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} G_k(f_{\\theta }(\\mathbf {x}), \\alpha ; \\mathcal {R}) = \\frac{\\exp \\!\\left( -\\alpha \\cdot \\delta ^E\\!\\left( f_{\\theta }(\\mathbf {x}), \\mathbf {r}_k\\right) \\right) }{\\displaystyle \\sum _{k'=1}^K \\exp \\!\\left( -\\alpha \\cdot \\delta ^E\\!\\left( f_{\\theta }(\\mathbf {x}), \\mathbf {r}_{k'}\\right) \\right) }. \\end{aligned}$$\\end{document}The k-means solution is recovered when \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha $$\\end{document} tends to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$+\\infty $$\\end{document}.",
            "cite_spans": [
                {
                    "start": 242,
                    "end": 244,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 264,
                    "end": 266,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 421,
                    "end": 423,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 555,
                    "end": 557,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 757,
                    "end": 759,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Training ::: Seed-Guided Deep Document Clustering",
            "ref_spans": []
        },
        {
            "text": "Following prior deep clustering works [12, 23, 30, 31], we initialize the auto-encoder parameters through pretraining by first only optimizing the reconstruction loss of the auto-encoder. In the pretraining of SD2C-Doc, we also include the constraint-enforcing term (second term in Problem 3) so that learned representations are impacted by seed words early in the training. At the end of pretraining, the cluster centers are initialized by the seed words cluster embeddings \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{\\mathbf {s}_k\\}_{k=1}^K$$\\end{document}.2 Then, in the fine-tuning phase, the whole loss \u2013 including the clustering loss and the constraint-enforcing loss (for SD2C-Doc and SD2C-Rep) \u2013 is optimized.",
            "cite_spans": [
                {
                    "start": 39,
                    "end": 41,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 43,
                    "end": 45,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 47,
                    "end": 49,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 51,
                    "end": 53,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Training ::: Seed-Guided Deep Document Clustering",
            "ref_spans": []
        },
        {
            "text": "We measure the clustering performance in terms of clustering accuracy (ACC) and adjusted rand index (ARI), which are standard clustering metrics [5]. Table 1 first provides the macro-average (over the 5 datasets) of these measures for all methods, using the top 3 automatically selected seed words per cluster. As one can note, the use of seed words is beneficial to the clustering. Indeed, the approaches which use seed words (NN, STM, SD2C) have markedly higher ACC and higher ARI than those which do not (KM, AE-KM, DKM). Among these latter methods, DKM is the best ones (as a comparison, DCN and IDEC, mentioned in Sect. 3, respectively obtain 64.8 and 64.1 for ACC, and 49.3 and 47 for ARI). Among the methods exploiting seed words, SD2C methods are the best ones, outperforming the baseline NN and the STM method by up to 2.6 points for ACC and 3.5 points for ARI.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 147,
                    "mention": "5",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 156,
                    "end": 157,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We further provide in Table 2 a detailed account of the performance of the methods based on seed words. The results have been averaged over 10 runs and are reported with their standard deviation. We furthermore performed an unpaired Student t-test with a significance level of 0.01 to study whether differences are significant or not (all results in bold are not not statistically different from the best result). As one can note, the proposed SD2C models compare favorably against STM, the strongest baseline. Indeed, all SD2C approaches significantly outperform STM on 20NEWS, and SD2C-Doc-e/c as well as SD2C-Rep-e also significantly outperform STM on YAHOO and AGNEWS. On the other hand, STM obtained significantly better results in terms of both ACC and ARI on REUTERS and DBPEDIA, the difference on these collections (and especially on DBPEDIA) being nevertheless small. Among the SD2C methods, SD2C-Doc-c yields the best performance overall (as shown in Table 1).",
            "cite_spans": [],
            "section": "Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 28,
                    "end": 29,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 967,
                    "end": 968,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Runtime. We further compared, in Table 3, the efficiency of STM and the SD2C methods on a machine with eight i7-7700HQ CPUs at 2.80 GHz, 16 GB RAM, and an NVIDIA GeForce GTX 1070 (only used for the deep learning approaches). The runtime of the SD2C approaches is lower than that of STM on most datasets. STM was only faster on AGNEWS (between 2 and 3 times), yet far slower on 20NEWS (about 10 times). This discrepancy can be explained by the fact that STM\u2019s complexity is dominated by the total number of tokens (large for a small number of documents in 20NEWS), whereas SD2C models only depend on the number of documents (large with few tokens per document in AGNEWS). As to the SD2C models, SD2C-Rep runs faster than SD2C-Doc. This is due to the complexity of the constraint-enforcing loss term being lower for the former than for the latter.\n\n",
            "cite_spans": [],
            "section": "Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 39,
                    "end": 40,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Impact of the Number of Seed Words. In our general setting used to report the previous results, the number of seed words per class was arbitrarily set to 3. For comprehensiveness, we study the clustering results of the SD2C models when the number of (automatically selected) seed words per cluster is varied from 1 to 5. The evolution of the performance for the SD2C models in terms of accuracy is illustrated in Fig. 2. We observe that using more seed words leads to notable improvements in most cases \u2013 with the exception of SD2C-Doc-e, which seems to be less influenced by the number of seed words. This trend is particularly apparent when the number of seed words is increased from 1 to 2. Although slight performance gain is observed between 2 and 5 seed words, the results exhibit greater stability. This suggests that providing as few as 2 seed words per cluster \u2013 which constitutes a modest annotation effort for humans \u2013 can prove highly beneficial for the clustering results obtained by our SD2C approaches.",
            "cite_spans": [],
            "section": "Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 418,
                    "end": 419,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Comparing Automatic and Manual Seed Words. In order to check that the method we retained to automatically extract seed words is appropriate, we also computed the results obtained by STM and the SD2C methods using the manual seed words available for 20NEWS and REUTERS and presented in, e.g., [8, 17, 28] (denoted as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbb {S}^D$$\\end{document} in the latter). The corresponding list of seed words contain in average 5.1 words per category for 20NEWS and 6.8 words per category for REUTERS. The procedure to constitute these lists of descriptive seed words is detailed in [8]. Table 4 summarizes the results obtained with such seed words. These results first show that the scores obtained by the different methods using the manual seed words are close to the ones obtained with the automatically selected ones. For example, the difference in ACC for STM amounts to only 0.7 points on 20NEWS and 0.2 points on REUTERS. This shows that the automatic selected seed words are a reasonable substitute to manual seed words for evaluation purposes. In addition, SD2C methods still significantly outperform STM on 20NEWS. SD2C-Rep-c is here significantly better, even though the difference is not important, than STM on REUTERS \u2013 this is in line with our comment on Table 2 on the small differences between STM and SD2C on REUTERS.",
            "cite_spans": [
                {
                    "start": 293,
                    "end": 294,
                    "mention": "8",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 296,
                    "end": 298,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 300,
                    "end": 302,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 843,
                    "end": 844,
                    "mention": "8",
                    "ref_id": "BIBREF30"
                }
            ],
            "section": "Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 853,
                    "end": 854,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1534,
                    "end": 1535,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "We have introduced in this paper the SD2C framework, the first attempt, to the best of our knowledge, to constrain document clustering with seed words using a deep clustering approach. To do so, we have integrated constraints associated to seed words in the Deep k-Means optimization problem [23], modifying either the document embeddings, the cluster representatives or the input representations to make them closer to the seed words retained. The new methods thus derived have been evaluated on five text collections widely used for text classification purposes. For this evaluation, we have proposed a simple method to automatically select seed words that behaves comparably to manual seed words for evaluation purposes.",
            "cite_spans": [
                {
                    "start": 293,
                    "end": 295,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "Several perspectives for this work can be envisaged. First of all, it is possible to extend the current framework with a \u2018garbage\u2019 cluster to collect documents that do not fit well within the clusters defined by the seed words. This can be useful in particular for document filtering [16]. Other types of autoencoders and other attention mechanisms can also be designed to try and improve the results of the SD2C methods. Combinations of the different approaches can also be studied so as to benefit from their respective strengths. Lastly, if the SD2C-Doc-c method overall outperforms the other approaches in terms of accuracy and adjusted rand index, we want to better understand when it is beneficial to bias the document representations and when to bias the cluster representative ones.",
            "cite_spans": [
                {
                    "start": 285,
                    "end": 287,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Macro-average results in terms of accuracy (ACC) and adjusted rand index (ARI). The double vertical line separates approaches which leverage seed words (right) from approaches which do not (left). Bold values correspond to the best results.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Seed-guided constrained clustering results with 3 seed words per cluster. Bold results denote the best, as well as not significantly different from the best, results. Underlined SD2C results indicate a significant improvement over STM.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Execution time per run (in seconds) for each model on 20NEWS, REUTERS, YAHOO, DBPEDIA, and AGNEWS.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Seed-guided constrained clustering results with manual seed words.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Illustration of SD2C-Doc (left) and SD2C-Rep (right). Thick double arrows indicate the computation of a distance between two vectors.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Clustering results in terms of ACC for SD2C-Doc-e, SD2C-Doc-c, SD2C-Rep-e and SD2C-Rep-c with 1 to 5 seed words.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Improving text categorization bootstrapping via unsupervised learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gliozzo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Strapparava",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Dagan",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "ACM Trans. Speech Lang. Process.",
            "volume": "6",
            "issn": "1",
            "pages": "1-24",
            "other_ids": {
                "DOI": [
                    "10.1145/1596515.1596516"
                ]
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Basu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Davidson",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Wagstaff",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Constrained Clustering: Advances in Algorithms, Theory, and Applications",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Latent dirichlet allocation",
            "authors": [
                {
                    "first": "DM",
                    "middle": [],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "AY",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "MI",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res.",
            "volume": "3",
            "issn": "",
            "pages": "993-1022",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "Enriching word vectors with subword information",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Grave",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Trans. Assoc. Comput. Linguist.",
            "volume": "5",
            "issn": "",
            "pages": "135-146",
            "other_ids": {
                "DOI": [
                    "10.1162/tacl_a_00051"
                ]
            }
        },
        "BIBREF27": {
            "title": "Locally consistent concept factorization for document clustering",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Trans. Knowl. Data Eng.",
            "volume": "23",
            "issn": "6",
            "pages": "902-913",
            "other_ids": {
                "DOI": [
                    "10.1109/TKDE.2010.165"
                ]
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}