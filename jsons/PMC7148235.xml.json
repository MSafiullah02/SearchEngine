{
    "paper_id": "PMC7148235",
    "metadata": {
        "title": "Semantic Modelling of Citation Contexts for Context-Aware Citation Recommendation",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Tarek",
                "middle": [],
                "last": "Saier",
                "suffix": "",
                "email": "tarek.saier@kit.edu",
                "affiliation": {}
            },
            {
                "first": "Michael",
                "middle": [],
                "last": "F\u00e4rber",
                "suffix": "",
                "email": "michael.faerber@kit.edu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Citations are a central building block of scholarly discourse. They are the means by which scholars relate their research to existing work\u2014be it by backing up claims, criticising, naming examples, or engaging in any other form. Citing in a meaningful way requires an author to be aware of publications relevant to their work. Here, the ever increasing rate of new research being published poses a serious challenge. With the goal of supporting researchers in their choice of what to read and cite, approaches to paper recommendation and citation recommendation have been an active area of research for some time now [2].",
            "cite_spans": [
                {
                    "start": 617,
                    "end": 618,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we focus on the task of context-aware citation recommendation (see e.g. [7, 10, 11, 14]). That is, recommending publications for the use of citation within a specific, confined context (e.g. one sentence)\u2014as opposed to global citation recommendation and paper recommendation, where publications are recommended with respect to whole documents or user profiles. Within context-aware citation recommendation, we specifically investigate the explicit semantic modelling of citation contexts. While implicit semantic information (such as what is captured by word embeddings) greatly benefits scenarios like keyword search, we argue that the specificity of information needs in academia\u2014e.g. finding publications that use a certain data set or address a specific problem\u2014require a more rigidly modelled knowledge representations, such as those proposed in [21] or [8]. Regarding quality, such knowledge representations (e.g. machine readable annotations of scientific publications) would ideally be created manually by the researchers themselves (see [13]). However, neither have such ideals become the norm in academic writing so far, nor are large scale data sets with manually created annotations available. Thus, we create semantic models of citation contexts using NLP techniques to automatically derive such knowledge representations. Using our models we investigate if and when such novel representation formats are beneficial for context-aware citation recommendation.",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 89,
                    "mention": "7",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 91,
                    "end": 93,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 95,
                    "end": 97,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 99,
                    "end": 101,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 867,
                    "end": 869,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 875,
                    "end": 876,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1062,
                    "end": 1064,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Overall, we make the following contributions: We propose novel ways of deriving semantically-structured representations from citation contexts, based on entities and claims, intended for context-aware citation recommendation. To the best of our knowledge, this is the first approach of its kind, as previous uses of semantically-structured representations for citation recommendation were only ever applied to whole papers (i.e. in a setting where richer information including authors, list of references, venue, etc. is available).We perform a large-scale offline evaluation using four data sets, in which we test the effectiveness of our models.We also perform a user study to further evaluate the performance of our models and assess their conceptual soundness.We make the code for our models and details of our evaluation publicly available.1\n\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The rest of the paper is structured as follows: In Sect. 2 we outline existing works on citation recommendation. We then describe in Sect. 3 the novel semantic approaches to citation recommendation. Sect. 4 is dedicated for the evaluation of our approaches. We conclude in Sect. 5.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Citation recommendation can be classified into global citation recommendation and context-aware (sometimes also referred to as \u201clocal\u201d) citation recommendation [10]. Various approaches have been published in both areas, but there is, to the best of our knowledge, not one that both (a) is context-aware and (b) uses explicit semantic representations of citation contexts. We illustrate this in Table 1. In the following, we therfore outline the most related works on (semantic) global citation recommendation (upper right cell in Table 1), and (non-semantic) context-aware citation recommendation (lower left cell in Table 1).\n",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 163,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Related Work",
            "ref_spans": [
                {
                    "start": 400,
                    "end": 401,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 536,
                    "end": 537,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 623,
                    "end": 624,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Global Citation Recommendation. Global citation recommendation is characterized as a task for which the input of the recommendation engine is not a specific citation context but a whole paper. Various approaches have been published for global citation recommendation, some of which can also be used for paper recommendation, i.e., for recommending papers for the purpose of reading [1]. A few semantic approaches to global citation recommendation exist (see Table 2). They are based on a semantically-structured representation of papers\u2019 metadata (e.g., authors, title, abstract) [28, 29] and/or papers\u2019 contents [19]. Note that the approaches proposed in this paper are not using any of the papers\u2019 metadata or full text, as our goal is to provide fine-grained, semantically suitable recommendations for specific citation contexts.",
            "cite_spans": [
                {
                    "start": 383,
                    "end": 384,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 581,
                    "end": 583,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 585,
                    "end": 587,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 614,
                    "end": 616,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Related Work",
            "ref_spans": [
                {
                    "start": 464,
                    "end": 465,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Context-Aware Citation Recommendation. Context-aware citation recommendation approaches recommend publications for a specific citation context and are thus also called \u201clocal\u201d citation recommendation approaches. Existing context-aware citation recommendation approaches soly rely on lexical and syntactic features (n-grams, part-of-speech tags, word embeddings etc.) but do not attempt to model citation contexts in an explicit semantic fashion. Table 3 gives an overview of context-aware citation recommendation approaches. We can mention SemCir [29] as the only approach we are aware of that could be regarded as a semantic approach to context-aware citation recommendation. The explicit semantic representations are, however, not generated from citation contexts (not context-aware), but from papers (global), that are textually (not necessarily semantically) similar to the citation contexts. We therefore categorize it as a semantic global approach.",
            "cite_spans": [
                {
                    "start": 548,
                    "end": 550,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Related Work",
            "ref_spans": [
                {
                    "start": 452,
                    "end": 453,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "The intuition behind an entity-based approach is that there exists a reference publication for a named entity mentioned in the citation context. For instance, this can be a data set (\u201cCiteSeerx [37]\u201d), a tool (\u201cNeural ParsCit [23]\u201d), or a (scientific) concept (\u201cSemantic Web [37]\u201d). In a more loose sense this can also include publications being referred to as examples (\u201capproaches to context-aware citation recommendation [5\u20137, 10\u201312, 14, 16]\u201d). Because names of methods, data sets, tools, etc. in academia often are neologisms and only the most widely used ones of them are reflected in resources like DBpedia, we use a set of noun phrases found in academic publications as surrogates for named entities (instead of performing entity linking). For this, we extract noun phrases from the arXiv publications provided by [24] and filter out items that appear only once. In doing so we end up with a set of 2,835,929 noun phrases2 (NPs) that we use.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 229,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 425,
                    "end": 426,
                    "mention": "5",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 427,
                    "end": 428,
                    "mention": "7",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 430,
                    "end": 432,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 433,
                    "end": 435,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 437,
                    "end": 439,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 441,
                    "end": 443,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 822,
                    "end": 824,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Entity-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "In the following, we define two NP-based representations of citation contexts, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NP}}}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document}. For this, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {P}$$\\end{document} shall denote our set of NPs and c shall denote a citation context.",
            "cite_spans": [],
            "section": "Entity-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{R}}_{\\mathbf {NP}}$$\\end{document}. We define \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {NP}}(c)$$\\end{document} as the set of maximally long NPs contained in c. Formally, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {NP}}(c) = \\{t|t\\text { appears in }c \\wedge t\\in \\mathcal {P} \\wedge t^{+pre} \\notin \\mathcal {P}\\wedge t^{+suc} \\notin \\mathcal {P}\\}$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t^{+pre}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t^{+suc}$$\\end{document} denote an extension of t using its preceding or succeeding word respectively. A context \u201cThis has been done for language model training [37]\u201d, for example, would therefore have \u201clanguage model training\u201d in its representation, but not \u201clanguage model\u201d.",
            "cite_spans": [],
            "section": "Entity-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{R}}_{\\mathbf {NPmrk}}^{\\mathbf {2+}}$$\\end{document}. We define \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}(c)}$$\\end{document} as a subset of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NP}}(c)}$$\\end{document} containing, if present, the NP of minimum word length 2 directly preceding the citation marker which a recommendation is to be made for. Formally, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {NPmrk}}^{2+}(c) = \\{t|t\\in R_{\\text {NP}}(c)\\wedge len(t)\\ge 2 \\wedge t\\text { directly precedes } m\\}$$\\end{document} where m is the citation marker in c that a prediction is to be made for.",
            "cite_spans": [],
            "section": "Entity-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Recommendation. As is typical in context-aware citation recommendation [5, 6, 10] we aggregate citation contexts referencing a publication to describe it as a recommendation candidate. To that end, we define frequency vector representations for single citation contexts and documents as follows. A citation context vector is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$V(R(c)) = (t_{1}, t_{2}, ..., t_{|\\mathcal {P}|})$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_{i}$$\\end{document} denotes how often the ith term in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {P}$$\\end{document} appears in R(c). A document vector then is a sum of citation context vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sum \\limits _{c \\in \\varrho (d)} V(R(c))$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varrho (d)$$\\end{document} denotes the set of citation contexts referencing d. Similarities can then be calculated as the cosine of context and document vectors.",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 73,
                    "mention": "5",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 75,
                    "end": 76,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 78,
                    "end": 80,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Entity-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Our claim-based approach is motivated by the fact that citations are used to back up claims (see Table 4). These can, for example, be findings presented in a specific publication (\u201cIt has been shown, that ... [37].\u201d) or more general themes found across multiple works (\u201c... is still an unsolved task [37\u201339].\u201d).",
            "cite_spans": [],
            "section": "Claim-Based Recommendation ::: Approach",
            "ref_spans": [
                {
                    "start": 103,
                    "end": 104,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "For the extraction of claims, we considered a total of four state of the art [30] information extraction tools (PredPatt [27], Open IE 5.0 [17], ClausIE [4] and Ollie [18]) and found PredPatt to give the best quality results3. For the simple sentence \u201cThe paper shows that context-based methods can outperform global approaches.\u201d, Listing 1.1 shows the user interface output of PredPatt and Fig. 1 its internal representation using Universal Dependencies (UD) [20].\n\n",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 80,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 122,
                    "end": 124,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 140,
                    "end": 142,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 154,
                    "end": 155,
                    "mention": "4",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 168,
                    "end": 170,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 461,
                    "end": 463,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Claim-Based Recommendation ::: Approach",
            "ref_spans": [
                {
                    "start": 396,
                    "end": 397,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Because the predicates and especially arguments in the PredPatt user interface output can get very long\u2014e.g. \u201ccan outperform\u201d (including the auxiliary verb \u201ccan\u201d) and \u201ccontext-based methods can outperform global approaches\u201d (unlikely to appear in another citation context with the exact same wording)\u2014we build our claim-based representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim}}$$\\end{document} from UD trees, as explained in the following section.\n",
            "cite_spans": [],
            "section": "Claim-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{R}}_{\\mathbf {claim}}$$\\end{document}. For each claim that PredPatt detects, it internally builds one UD tree. To construct our claim-based representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim}}$$\\end{document}, we traverse each tree, identify the predicate and its arguments (subject and object) and save these in tuples. The exact procedure for this is given in Algorithm 1. If a sentence uses a copula (be, am, is, are, was), the actual predicate is a child node of the root with the relation type \u201ccop\u201d. This is resolved at marker (a). For the identification of useful arguments (markers (b\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathbf {1}$$\\end{document}) and (b\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathbf {2}$$\\end{document}) in Algorithm 1), we look at all nouns within the UD tree and resolve compounds (\u201ccompound\u201d, \u201cmwe\u201d, \u201cname\u201d relations), phrases split by formatting (\u201cgoeswith\u201d), conjunctions (\u201cconj\u201d) as well as adjectival and adverbial modifiers (\u201camod\u201d, \u201cadvmod\u201d). To give an example for this, the noun \u201cmethods\u201d in both trees in Fig. 1 has the adjectival modifier \u201ccontext-based\u201d. In such a case our model would not choose \u201cmethods\u201d as an argument to \u201coutperform\u201d but \u201ccontext-based methods\u201d. Listing 1.2 shows the complete representation generated for the example sentence.\n",
            "cite_spans": [],
            "section": "Claim-Based Recommendation ::: Approach",
            "ref_spans": [
                {
                    "start": 2041,
                    "end": 2042,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Recommendation. For a set of predicate-argument tuples \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {T}$$\\end{document}, we define frequency vector representations of citation contexts and documents as follows. A citation context vector is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$V(R(c)) = (t_{1}, t_{2}, ..., t_{|\\mathcal {T}|})$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_{i}$$\\end{document} denotes how often the ith tuple in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {T}$$\\end{document} appears in R(c). A document vector, again, is a sum of citation context vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sum \\limits _{c \\in \\varrho (d)} V(R(c))$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varrho (d)$$\\end{document} is the set of citation contexts referencing d. Similarities are then calculated as the cosine of TF-IDF weighted context and document vectors.",
            "cite_spans": [],
            "section": "Claim-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{R}}_{\\mathbf {claim+BoW}}$$\\end{document}. In addition to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim}}$$\\end{document}, we define a combined model \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} as a linear combination of similarity values given by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim}}$$\\end{document} and an bag-of-words model (BoW). Similarities in the combined model are calculated as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$sim(A,B)=\\sum \\limits _{m\\in \\mathcal {M}}\\alpha _msim_m(A,B)$$\\end{document} of the models \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {M}=\\{R_{\\text {claim}},\\text {BoW}\\}$$\\end{document} with the coefficients \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _{R_{\\text {claim}}}=1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _{\\text {BoW}}=2$$\\end{document}.",
            "cite_spans": [],
            "section": "Claim-Based Recommendation ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Our offline evaluation is performed in a citation re-prediction setting. That is, we take existing citation contexts from scientific publications and split them into training and test subsets. The training contexts are used to learn the representations of the cited documents. The test contexts are stripped of their citations, used as input to our recommender systems and the resulting recommendations checked against the original citations.",
            "cite_spans": [],
            "section": "Offline Evaluation ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "Table 5 shows the four data sources we use as well as applied filter criteria. RefSeer and ACL-ARC are often used in related work (e.g. [5, 7]), we therefore use both of them and two additional large data sets to ensure a thorough evaluation. Table 6 gives an overview of key properties of the training and test data for the evaluation. We split our data according to the citing paper\u2019s publication date and report #Candidate docs: the number of candidate documents to rank for a recommendation; #Test set items: the number of test set items (unit: citation contexts); Mean CC/RC: the mean number of citation contexts per recommendation candidate in the training set (i.e., a measure for how well the recommendation candidates are described, giving insight into how difficult the recommendation task for each of the data sets is).\n",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 138,
                    "mention": "5",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 140,
                    "end": 141,
                    "mention": "7",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "Offline Evaluation ::: Evaluation",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 249,
                    "end": 250,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Offline Evaluation ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "Figure 2 shows the results of our evaluation. We measure NDCG, MAP, MRR and Recall at cut-offs from 1 to 10. Note that the evaluation using the arXiv data differs from the other cases in two aspects. First, it is the only case where we can apply \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document}, because citation marker positions are given. Second, because for citation contexts with several citations (cf. Table 4, \u201cExemplification\u201d) the data set lists several cited documents (instead of just a single one), we are able to treat more than a single re-predicted citation as valid. We do this by counting re-predicted \u201cco-citations\u201d as relevant when calculating MAP scores and give them a relevance of 0.5 in the NDCG calculation. This also means that, looking at higher cut-offs, NDCG and MAP values can decrease because ideal recommendations require relevant re-predictions on all ranks above the cut-off.\n",
            "cite_spans": [],
            "section": "Offline Evaluation ::: Evaluation",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 674,
                    "end": 675,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "As for the performance of our models shown in Fig. 2, we see that for each of the data sets \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} outperforms the BoW baseline in each metric and for all cut-off values.4\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim}}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NP}}}$$\\end{document} do not compare in performance with the two aforementioned. This suggests that the claim structures we model with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim}}$$\\end{document} are not enough for well performing recommendations on their own, but do capture important information that non-semantic models (BoW) miss. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document}, only present in the arXiv evaluation, gives particularly good results for lower cut-offs and performs especially well in the MRR metric. It performs the worst at high cut-offs measured by NDCG. Note that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document} is only evaluated for test set items, where the model was applicable (i.e. where a noun phrase of minimum length 2 is directly preceding the citation marker; cf. Sect. 3.1). For our evaluation this was the case for 100,308 out of the 490,018 test set items (20.5%). The evaluation results for the citation marker-aware model \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document} indicate that it is comparatively well suited to recommend citations where there is one particularly fitting publication (e.g. a reference paper) and less suited for exemplifications (cf. Table 4).\n",
            "cite_spans": [],
            "section": "Offline Evaluation ::: Evaluation",
            "ref_spans": [
                {
                    "start": 51,
                    "end": 52,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 3290,
                    "end": 3291,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "To obtain more insights into the nature of our evaluation data, as well as a better understanding of our models, we perform a user study in which two human raters (the two authors) judge input-output pairs of our offline evaluation (i.e. citation contexts and the recommendations given for them). For this, we randomly choose 100 citation contexts from the arXiv evaluation, so that we can include \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document}. For each input context, we show raters the top 5 recommendations of the 3 best performing models of the offline evaluation, i.e., BoW, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document} models (resulting in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$100\\times 5\\times 3=1500$$\\end{document} items). Judgments are performed by looking at each citation context and the respective recommended paper. In addition, we let the raters judge the type of citation (Claim, NE, Exemplification, Other; cf. Table 4).",
            "cite_spans": [],
            "section": "User Study ::: Evaluation",
            "ref_spans": [
                {
                    "start": 2006,
                    "end": 2007,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "Table 7 shows the results based on the raters\u2019 relevance judgments. We present measurements for all contexts, as well as each of the citation classes on its own. We note that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} and BoW are close, but in contrast to the offline evaluation, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} only outperforms BoW in the Recall metric. In the case of NE type citations, the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document} model performs better than the other two models in all metrics. Furthermore, we can see that both \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document} achieve their best results for the type of citation they\u2019re designed for\u2014Claim and NE respectively. This indicates that both models actually capture the kind of information they\u2019re conceptualized for. Compared to the offline evaluation, we measure higher numbers overall. While the user study is of considerably smaller scale and a direct comparison therefore not necessarily possible, the notably higher numbers indicate, that a re-prediction setting involves a non-negligible number of false negatives (actually relevant recommendations counted as not relevant).",
            "cite_spans": [],
            "section": "User Study ::: Evaluation",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "7",
                    "ref_id": "TABREF6"
                }
            ]
        },
        {
            "text": "The entity-based model \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document}, which captures noun phrases preceding the citation marker, performs best at low cut-offs and in the MRR metric. Low cut-offs and measuring the MRR can be interpreted as emulating citations for reference publications. This interpretation is also backed by the results of the user study, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document} outperformed all other models when recommending for citation contexts that referenced a named entity or concept. We therefore conclude that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${R_{\\text {NPmrk}}^{2+}}$$\\end{document} is well suited for recommending such types of citations. Our claim-based model \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim}}$$\\end{document} does not compare in performance to a BoW baseline, but \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} outperforms aforementioned. We take this as an indication that the claim representation encodes important information which the non-semantic BoW model is not able to capture. In the user study \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\text {claim+BoW}}$$\\end{document} performs best for citation contexts, in which a claim is backed by the target citation. This suggests that the model indeed captures information related to claim structures.",
            "cite_spans": [],
            "section": "Main Findings ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "In the field of context-aware citation recommendation, the explicit semantic modeling of citation contexts is not well explored yet. In order to investigate the merit of such approaches, we developed semantic models of citation contexts based on entities as well as claim structures. We then evaluated our models on several data sets in a citation re-prediction setting and furthermore conducted a user study. In doing so, we could demonstrate their applicability and conceptual soundness. The next step from hereon is to move from semantically informed text-based models to explicit knowledge representations. Our research also shows, that differentiating between different semantic representations of citation contexts due to varying ways of citing information is reasonable. Developing different citation recommendation approaches, depending on the semantic citation types, might therefore be a promising next step in our research.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Overview of related work.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Semantic approaches to global citation recommendation.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Non-semantic approaches to context-aware citation recommendation.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Semantic structures identified in citation contexts from a range of citation functions used in the field of citation context analysis (NE = named entity).\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Citation context sources and filter criteria.\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6.: Key properties of data used for evaluation.\n",
            "type": "table"
        },
        "TABREF6": {
            "text": "Table 7.: User study evaluation scores at cut-off 5.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: UD trees as generated by PredPatt.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Evaluation using arXiv, MAG, RefSeer and ACL-ARC. Showing NDCG, MAP, MRR and Recall scores at cut of values from 1 to 10.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "NLP-driven citation analysis for scientometrics",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Jha",
                    "suffix": ""
                },
                {
                    "first": "AA",
                    "middle": [],
                    "last": "Jbara",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Qazvinian",
                    "suffix": ""
                },
                {
                    "first": "DR",
                    "middle": [],
                    "last": "Radev",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nat. Lang. Eng.",
            "volume": "23",
            "issn": "1",
            "pages": "93-130",
            "other_ids": {
                "DOI": [
                    "10.1017/S1351324915000443"
                ]
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Research-paper recommender systems: a literature survey",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Beel",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Gipp",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Langer",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Breitinger",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Digit. Libr.",
            "volume": "17",
            "issn": "4",
            "pages": "305-338",
            "other_ids": {
                "DOI": [
                    "10.1007/s00799-015-0156-0"
                ]
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "FaBiO and CiTO: ontologies for describing bibliographic resources and citations",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Peroni",
                    "suffix": ""
                },
                {
                    "first": "DM",
                    "middle": [],
                    "last": "Shotton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "J. Web Semant.",
            "volume": "17",
            "issn": "",
            "pages": "33-43",
            "other_ids": {
                "DOI": [
                    "10.1016/j.websem.2012.08.001"
                ]
            }
        },
        "BIBREF14": {
            "title": "Rhetorical functions of citations in high- and low-rated master\u2019s theses",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Petri\u0107",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J. Engl. Acad. Purp.",
            "volume": "6",
            "issn": "3",
            "pages": "238-253",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jeap.2007.09.002"
                ]
            }
        },
        "BIBREF15": {
            "title": "Neural ParsCit: a deep learning based reference string parser",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Prasad",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kaur",
                    "suffix": ""
                },
                {
                    "first": "MY",
                    "middle": [],
                    "last": "Kan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int. J. Digit. Libr.",
            "volume": "19",
            "issn": "",
            "pages": "323-337",
            "other_ids": {
                "DOI": [
                    "10.1007/s00799-018-0242-1"
                ]
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "SemCiR: a citation recommendation system based on a novel semantic distance measure",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zarrinkalam",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kahani",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Program: Electron. Libr. Inf. Syst.",
            "volume": "47",
            "issn": "",
            "pages": "92-112",
            "other_ids": {
                "DOI": [
                    "10.1108/00330331311296320"
                ]
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}