{
    "paper_id": "c171606123429039c3a9315e54a12d242d5d1b91",
    "metadata": {
        "title": "Cluster-based dual evolution for multivariate systems",
        "authors": [
            {
                "first": "Nick",
                "middle": [],
                "last": "James",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "NSW",
                    "location": {
                        "country": "Australia"
                    }
                },
                "email": ""
            },
            {
                "first": "Max",
                "middle": [],
                "last": "Menzies",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Tsinghua University",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "This paper proposes a cluster-based method to analyse multivariate systems that change over time. At each point in time, we partition data points into an appropriate number of clusters and thereby track both the total number of clusters and the individual constituents' cluster memberships changing over time. We can also compare and contrast several multivariate systems and determine patterns and anomalies between different data. We apply this method to study the evolution of COVID-19 cases and deaths over time. We detect significant similarities in the evolution of cases and deaths up to a suitable offset in time. We compute this offset and use it to identify anomalous countries in the progression of cases to deaths.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Scientists often seek to understand the evolutionary nature of multivariate systems over time. Such time-varying systems are ubiquitous in finance, physics, engineering, and other fields. Time series analysis is a wide field that studies such systems; researchers may either study each individual time series of a system or the properties of the system as a whole.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we seek to study both the overall properties of a multivariate system and the trends of individual elements through the lens of cluster analysis. Clustering algorithms seek to group elements of a data set according to their proximity. In statistics, the most common clustering algorithms are K-means and spectral clustering [1, 2] , which partition elements into discrete sets, and hierarchical clustering [3, 4] , which does not specify a precise number of clusters. The first two methods usually proceed with the number of clusters k chosen a priori. It is a subtle question of how to select this k -we draw upon several methods to do so.",
            "cite_spans": [
                {
                    "start": 339,
                    "end": 342,
                    "text": "[1,",
                    "ref_id": null
                },
                {
                    "start": 343,
                    "end": 345,
                    "text": "2]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 421,
                    "end": 424,
                    "text": "[3,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 425,
                    "end": 427,
                    "text": "4]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We apply our methodology to analyse the COVID-19 pandemic. First observed in late 2019, this disease has spread around the world, impacting each country differently. Studying the evolving numbers of cases and deaths by country gives two related multivariate systems that grow over time. We can track the progress of individual countries as they change cluster memberships, while the number of clusters tracks the spread of the system in its entirety. After smoothing out the number of clusters, we notice a considerable similarity between the evolution in the number of clusters relative to cases and deaths. Having shown broad similarity between the two systems, we then seek to identify countries that are anomalous in this correspondence. Such countries have anomalous relationships between their cases and deaths counts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "There is a wealth of epidemiological and sociological analysis to be done that may explain these anomalies, such as lack of testing in certain countries at certain times. Our focus is on the identification of trends and anomalies based purely on a mathematical analysis. A closer examination from experts in other fields could draw additional conclusions. Our methodology is flexible and can build off any desired clustering algorithm that may be appropriate for the particular context. This paper is structured as follows. Section 2 describes the existing theory and methods of cluster analysis that we draw upon in this paper. Section 3 describes our new methodology to analyse one or multiple time-varying multivariate systems. In Section 4, we present our results on analysing the properties of COVID-19 as an evolving system and notice considerable similarities in the cases and deaths count up to an offset, which we determine. In Section 5, we analyse individual countries that are anomalous relative to cases and deaths, taking this offset into consideration. We conclude in Section 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we describe the general clustering framework and the clustering techniques we use in our methodology and experiments. In our most general setup, x 1 , . . . , x n are elements of a normed space X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existing cluster theory"
        },
        {
            "text": "Hierarchical clustering is an iterative clustering technique that does not specify discrete groupings of elements. Rather, it seeks to build a hierarchy of similarity between elements. Hierarchical clustering is either agglomerative, where each element x i begins in its own cluster and branches between them are successively built, or divisive, where all elements begin in one cluster and are successively split. The results of hierarchical clustering are commonly displayed in dendrograms. For further details, see [3, 4] .",
            "cite_spans": [
                {
                    "start": 517,
                    "end": 520,
                    "text": "[3,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 521,
                    "end": 523,
                    "text": "4]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Hierarchical clustering"
        },
        {
            "text": "K-means clustering seeks to minimise an appropriate sum of square distances. With k chosen a priori, we investigate all possible partitions (disjoint unions) C 1 \u222a C 2 \u222a \u00b7 \u00b7 \u00b7 \u222a C k of {x 1 , . . . , x n }. Let z j be the centroid (average) of the subset C j . One seeks to minimise the sum of square distances within each cluster to its centroid:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "K-means clustering"
        },
        {
            "text": "For a normed space with dimension at least 2, it is NP-hard to find the global minimum of this problem. The K-means algorithm due to Lloyd [1] is an iterative algorithm that converges quickly and suitably to a locally optimal solution. It is usually sufficient for applications.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 142,
                    "text": "Lloyd [1]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "K-means clustering"
        },
        {
            "text": "On the other hand, the K-means optimisation problem is efficiently solvable in the one-dimensional case. That is, when x i are real numbers, they are equipped with an ordering, which considerably simplifies the problem. To cluster n elements of X = R into k clusters requires one to order the elements and then determine k \u2212 1 breaks in the ordering. This is far less computationally intensive than the higher-dimensional analogue. Wang et al. [5] implement a dynamic programming algorithm that guarantees optimal clustering in one dimension, choosing k a priori.",
            "cite_spans": [
                {
                    "start": 444,
                    "end": 447,
                    "text": "[5]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "K-means clustering"
        },
        {
            "text": "Spectral clustering is a technique that performs K-means clustering on the eigenvalue spectrum of a judiciously chosen matrix. Given elements x 1 , . . . , x n in a normed space (or more generally, a metric space), one forms the n \u00d7 n distance matrix D consisting of all pairwise distances between elements. With one of several transformations, one associates an affinity matrix A. The two most common transformations are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral clustering"
        },
        {
            "text": "We will refer to the first affinity matrix as standard and the second as Gaussian.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral clustering"
        },
        {
            "text": "They have the property that 0 \u2264 A ij \u2264 1 for all i, j and A ii = 1. Two points have affinity 1 if and only if they have distance 0, that is, they are equal. Next, let E be the diagonal degree matrix associated to A, that is, E ii = j A ij . Form the Laplacian matrix L = E \u2212 A and its normalisation L sym = E \u22121/2 AE \u22121/2 . Note L, L sym are n \u00d7 n symmetric matrices, and hence are diagonalisable with all real eigenvalues. By the definition of L and the normalisation L sym , all their eigenvalues are non-negative, 0 = \u03bb 1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bb n .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral clustering"
        },
        {
            "text": "Spectral clustering proceeds as follows. For a fixed choice of k, compute the normalised eigenvectors u 1 , . . . , u k \u2208 R n corresponding to the k smallest eigenvalues of L sym . Form the matrix U \u2208 R n\u00d7k whose columns are u 1 , . . . , u k . Let v i \u2208 R k be the rows of U , i = 1, . . . , n. Cluster these row vectors into clusters C 1 , . . . , C k according to K-means. Finally, output clusters A l = {i : v i \u2208 C l }, l = 1, . . . , k to assign the original n elements into the corresponding clusters. Our notation follows Luxburg [2] , who provides additional details.",
            "cite_spans": [
                {
                    "start": 538,
                    "end": 541,
                    "text": "[2]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Spectral clustering"
        },
        {
            "text": "In hierarchical clustering, choosing the number k of clusters is not required, or even applicable. In spectral clustering, there is a standard choice of k. One chooses k that maximises the eigengap \u03bb k+1 \u2212 \u03bb k , using the notation from the previous section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of number of clusters"
        },
        {
            "text": "On the other hand, how to best choose the number of clusters k for the K-means algorithm is a difficult problem. Different methods for estimating k may produce considerably differing results. In this paper, we draw upon six methods to determine the appropriate number of clusters before implementing K-means, in both the one and higher-dimensional cases. These methods are well-known: Ptbiserial index [6] , silhouette score [7] , KL index [8] , C index [9] , McClain-Rao index [10] and Dunn index [11] . We have chosen these methods based upon consultation with the literature and our own experiments. However, our methodology is flexible, and any combination of existing methods may be used. For one-dimensional data, it is often regarded as unsuitable to use higherdimensional K-means or spectral clustering, as optimal alternatives exist. Since we will study one-dimensional data in this paper, it is necessary to use these methods to choose the number k before implementation of the optimal K-means.",
            "cite_spans": [
                {
                    "start": 402,
                    "end": 405,
                    "text": "[6]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 425,
                    "end": 428,
                    "text": "[7]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 440,
                    "end": 443,
                    "text": "[8]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 454,
                    "end": 457,
                    "text": "[9]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 478,
                    "end": 482,
                    "text": "[10]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 498,
                    "end": 502,
                    "text": "[11]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Choice of number of clusters"
        },
        {
            "text": "In this section, we describe our methodology in detail. We report our results in Section 4. The methodology described in this section is not exhaustive. One may modify it by using different metrics, clustering methods, and choice of the number of clusters as appropriate for the data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "The most general setup of our methodology is as follows. Let X (t) i be a collection of n time series over a time interval of length T , with i = 1, . . . , n and t = 1, . . . , T . Assume each data point X (t) i is an element of a common normed space X. Slightly different procedures apply if X is one-dimensional, namely R, or higher-dimensional.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time-varying clustering of the time series"
        },
        {
            "text": "The goal is to cluster the data points X",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time-varying clustering of the time series"
        },
        {
            "text": "n at every time t by applying the procedures in Section 2 to the data points at that time. We wish to choose the number of clusters in such a way that provides us meaningful inference on how the system changes over time. In the higher-dimensional case, we may use either spectral clustering or K-means, but always choose the optimal implementation of K-means when X is one-dimensional.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time-varying clustering of the time series"
        },
        {
            "text": "When using spectral clustering, we simply choose the number of clusters k t according to the maximal eigengap method described in Section 2. By a perturbation-theoretic argument applied to the eigenvalue spectrum [2] , this choice of k is robust with respect to changes in the distance matrix over time, and so does not substantially vary between adjacent times. This is important, as a wildly varying value of the number of clusters can obscure inference on individual countries' cluster memberships changing with time.",
            "cite_spans": [
                {
                    "start": 213,
                    "end": 216,
                    "text": "[2]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Time-varying clustering of the time series"
        },
        {
            "text": "In the case of K-means, including the optimal implementation in one dimension, we combine several methods and exponential smoothing to yield a suitably changing number with time. We then feed back these smoothed values ofk t and cluster the data at each point into this number of clusters according to K-means. In our application of this paper, we combine six methods outlined in Section 2. This reduces the bias in our estimator. These six methods -Ptbisesial index, silhouette score, KL index, C index, McClain-Rao index and Dunn index -have been chosen after experimentation and consultation with the literature, but our method is flexible and could use any combination of methods. Given cluster numbers k (t) 1 , . . . , k (t) 6 offered by these methods, we compute the average k",
            "cite_spans": [
                {
                    "start": 731,
                    "end": 732,
                    "text": "6",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Time-varying clustering of the time series"
        },
        {
            "text": "j . Note this is not necessarily an integer; we do not compute clusters directly with this value. In our application, this average value k (t) av exhibits itself as approximately locally stationary. Thus, we apply exponential smoothing to k (t) av to produce a smoothed integer valuek t . Record the number of clustersk t if implementing K-means or k t if implementing spectral clustering as a function of t. Henceforth, we will refer to this ask t , the smoothed number of clusters, even in the case of spectral clustering. Whether using spectral clustering, higher-dimensional K-means or the optimal implementation of K-means in one dimension, we use this valuek t at each point in time to obtain a clustering at that point. In the case of one-dimensional data, the clusters are ordered and labelled according to the ordering on R.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time-varying clustering of the time series"
        },
        {
            "text": "Record the results of this analysis in several sequences of matrices. Let D (t) be the n \u00d7 n matrix of distances between elements X (t) i at times t. Following Section 2, form two different affinity matrices and adjacency matrices at every t. We depart from the standard notation of Section 2.3 for clarity. At each point, associate a standard and Gaussian affinity matrix:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matrix analysis of a time series system"
        },
        {
            "text": "The denominator in the Gaussian is chosen to appropriately normalise G. We will vary m = 1, 2, 3 in experiments so our affinity matrix elements mimic Gaussian spreads over 1, 2, 3 standard deviations. Associate an adjacency matrix defined by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matrix analysis of a time series system"
        },
        {
            "text": "are in the same cluster 0, else Finally, we can analyse the change in cluster memberships over time via the adjacency matrices. For a n \u00d7 n matrix A, define its Frobenius norm by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matrix analysis of a time series system"
        },
        {
            "text": "We can measure erratic periods in time with respect to changing cluster memberships by computing \u03b4(t) = ||Adj (t\u22121) \u2212 Adj (t) ||. This effectively calculates the number of edges that differ between the two cluster graphs. Even if the value ofk t changes, this difference between adjacency matrices provides an appropriate measure of how many elements have changed clusters relative to each other. We can also perform hierarchical clustering on the entire collection of adjacency matrices. Given two points in time s, t \u2208 [1, . . . , T ], we can consider the difference between the two cluster structures by defining d(s, t) = ||Adj (t) \u2212Adj (s) || and performing hierarchical clustering on the distances d(s, t). We term the resulting dendrogram a cluster evolution dendrogram.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matrix analysis of a time series system"
        },
        {
            "text": "Now suppose we have two related multivariate systems X",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Offset analysis"
        },
        {
            "text": "valued in a common normed space X. We wish to determine relations between the two, and individual constituents of the system that are anomalous in this comparison. In particular, with our primary application of coronavirus cases and deaths in mind, we are interested in whether two systems have any similarity up to an offset in time. So we perform three analyses to identify an optimal time offset to measure similarity; in the next section we can subsequently study anomalous individual countries.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Offset analysis"
        },
        {
            "text": "First, we define the system evolution offset with respect to the changing number of clusters. Let f (t) =k",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Offset analysis"
        },
        {
            "text": "Y be the smoothed number of clusters for each system. Given an offset \u03c4 , define the translated function f \u03c4 (t) = f (t + \u03c4 ). To determine the optimal system evolution offset, we seek to minimise the difference between g and a translated copy of f , namely minimising an L 1 norm ||f \u03c4 \u2212 g|| L 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Offset analysis"
        },
        {
            "text": "This will formalise the computation of an offset that will be clearly visible in our experiments in Section 4. Next, we compute the cluster consistency offset to determine under what offset in time is the cluster partition of the two systems most similar. We seek to minimise the discrepancy between adjacency matrices Adj X and Adj Y of the two systems. Thus, we choose an offset \u03c4 that minimises",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Offset analysis"
        },
        {
            "text": "Note that we normalise by the number of terms in this sum, which varies with \u03c4 , for an appropriate comparison. When \u03c4 > 0 we can rewrite this",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Offset analysis"
        },
        {
            "text": "We can also do the same for the offset in the standard or Gaussian affinity matrices Aff and G. Note all these matrices are normalised, so a comparison of their values is appropriate. We choose the normalisation parameter of the Gaussian affinity matrix in Equation (2) for this purpose.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "||"
        },
        {
            "text": "Having identified a suitable \u03c4 such that two multivariate systems exhibit similarity up to this offset, one can then compare affinity matrices to identify individual elements which are anomalous between the two systems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Anomaly offset analysis"
        },
        {
            "text": "To do so, we compute consistency matrices that measure the consistency between the two systems, up to an offset. Using the standardised affinity matrices Aff, the consistency matrices are defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Anomaly offset analysis"
        },
        {
            "text": "| where the absolute value of each matrix element is taken.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Anomaly offset analysis"
        },
        {
            "text": "We term a sequence of consistency matrices an anomalous matrix wall. There are several procedures we can apply to analyse such a matrix wall. At each slice in time, we can apply hierarchical clustering. The sequence of hierarchical clusters highlights the emergence and disappearance of specific anomalies and quantifies the total amount of anomalous behaviour across the system. We can also identify the most anomalous elements at any point in time. By computing c",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Anomaly offset analysis"
        },
        {
            "text": "ij we may assign an anomaly score to a particular element relative to its consistency between two multivariate systems. In Section 5, we can use this to rank the most anomalous countries and track the changes over time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Anomaly offset analysis"
        },
        {
            "text": "In this section, we apply our methodology to the cumulative counts of cases and deaths from COVID-19 on a country by country basis. Our data set is taken from the World Health Organisation on 30-4-2020 and records the cumulative number of cases and deaths in each country per day. We analyse data from 31-12-2019 to 30-4-2020, a period of 122 days. The number of countries is n = 208. We generate four multivariate systems from this data. Ordering the countries i = 1, . . . , 208 by alphabetical order, let x (t) i , y (t) i \u2208 R be the cumulative number of daily cases and deaths respectively at time t. We analyse these multivariate systems of length T = 122 with the one-dimensional optimal implementation of K-means. Also, letx",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "i \u2208 R 3 be rolling triples of counts of cases and deaths respectively. We analyse these higher-dimensional systems with K-means and spectral clustering. For these time series, T = 120.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Given the exponential nature of the data, we choose a logarithmic difference as our metric. First, we do some data pre-processing: any entry in the data that is empty -before any cases are detected -or 0, we replace with a 1, so that the log of that number is defined. Then we define a distance on cases and death counts by d(x, y) = | log(x) \u2212 log(y)|. Effectively, this pulls back the standard Euclidean distance on R under the homeomorphism log : R + \u2192 R and makes the positive real numbers a one-dimensional normed space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Givenx,\u1ef9 \u2208 R 3 we once again change all 0's or empty data to 1, and define an L 2 log norm d(x,\u1ef9) = 3 j=1 | log(x j ) \u2212 log(y j )| 2 1 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "In this section, we apply the optimal implementation of K-means to the one-dimensional daily counts of cases, and both K-means and spectral clustering to the 3-day rolling counts of cases, and record our results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis of the system of cases"
        },
        {
            "text": "We determine the number of clusters of the evolving system according to our methodology in Section 3.1. For the daily data, clusters range between {2, . . . , 17}, as depicted in Figure 1 . For the 3day windows, the number of clusters for K-means and spectral clustering ranges between {2, . . . , 15} and {2, . . . , 9} respectively. All trajectories in the number of clusters show similar patterns. First, the total number of clusters is two until late January or early February; following that, the number of clusters starts to increase rapidly towards a peak in early March. Subsequently, cluster numbers slowly decline until the end of our analysis window and appear to stabilise. This change in the number of clusters corresponds to the actual spread of COVID-19. Initially, China was the only country severely impacted by the virus. Subsequently, it spread around the world, with reported numbers changing from day to day. Later, the numbers have generally stabilised throughout the world from day to day, producing more consistent clustering results.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 179,
                    "end": 187,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Number of clusters:"
        },
        {
            "text": "We analyse the delta values between adjacent cluster times defined in Section 3.2 and depict our results in Figure 2 . This highlights periods in time where cluster membership is changing most significantly between adjacent days. Due to the high degree of overlap when considering rolling windows, the daily counts are the most important measurement here. We observe that March was the month that experienced the most significant changes in the cluster structure. Indeed, during this time, most countries were experiencing their highest growth rate in cases. The delta values begin to decrease in April, indicating increased consistency between cluster memberships. and associated adjacency matrix are all identical, with only China in its own cluster. This is displayed in Figure 3a .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 108,
                    "end": 116,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 774,
                    "end": 783,
                    "text": "Figure 3a",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Most erratic time windows:"
        },
        {
            "text": "In this section, we apply the optimal implementation of K-means to the onedimensional daily counts of deaths, and both K-means and spectral clustering to the 3-day rolling counts of deaths, and record our results. 2. Most erratic time windows: Again we analyse the delta values between different cluster times defined in Section 3.2 and depict our results in Figure  2 , focusing on the daily counts. The period with the most significant change in the graph is late March to early April -also approximately one month after the number of cases. Again we observe a similar trajectory in the behaviour of cases and deaths.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 359,
                    "end": 368,
                    "text": "Figure  2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Analysis of the system of deaths"
        },
        {
            "text": "3. Trends in severely impacted countries: Although we have highlighted a one-month offset in the general evolution of COVID-19 cases and deaths, there are striking dissimilarities regarding the membership of the highest cluster. In mid-March, China moves out of the worst affected cluster, into the second death cluster, demonstrating its relative success in responding to the pandemic. On the other hand, the United States, Spain, Italy, France and the United Kingdom have recently moved into this worst cluster. Upon examining the cluster constituencies after accounting for lag, we may yield insights into countries that have most and least effectively managed the progression from cases to deaths. Our method confirms that China has managed potential COVID-19 deaths most effectively, while Italy, Spain the United Kingdom and the United States have been least effective. In Figures 4 and 5 , we track the changing cluster membership of select and all countries respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 879,
                    "end": 894,
                    "text": "Figures 4 and 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Analysis of the system of deaths"
        },
        {
            "text": "4. Evolution of cluster structure: We compute the cluster evolution dendrogram defined in Section 3.2 for the daily death numbers. We exclude the first 66 points of time, in which the cluster structure and associated adjacency matrix are all identical. This is displayed in Figure 3b . Once again, adjusting for the offset, we see a remarkable similarity between cases and deaths in Figures 3a and 3b . These demonstrate a near-identical hierarchical clustering results for the two systems. Both systems identify two clusters via hierarchical clustering, and the visual depiction highlights two meaningful sub-clusters within the large cluster: one highly prominent cluster with a high degree of similarity, and a smaller cluster with less pronounced similarity.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 274,
                    "end": 283,
                    "text": "Figure 3b",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 383,
                    "end": 400,
                    "text": "Figures 3a and 3b",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Analysis of the system of deaths"
        },
        {
            "text": "In Sections 4.1 and 4.2, we noticed significant similarity in observed phenomena. This is clearly visible in Figure 1 . We can formalise this observation by computing the translation that minimises the L 1 norm described in Section 3.3. We determine this offset is \u03c4 = 32, confirming the one-month offset observations. In Section 3.3, we termed this the system evolution offset with respect to the number of clusters.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 109,
                    "end": 117,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Offset analysis"
        },
        {
            "text": "Next, we compute the cluster consistency offset also defined in Section 3.3. This minimises the total discrepancy between the adjacency matrices (or standard or Gaussian affinity matrices) between two systems. We perform this computation using three Gaussian affinity matrices, with m = 1, 2, 3 according to Equation (2) . Results are displayed in Table 1 . To illustrate the flexibility of the method, we choose different start dates for our offset analysis. After all, the first 30 days carry some triviality in the cluster structure, with very few cases observed outside China, so it may be desirable to exclude them from the analysis. Fortunately, the optimal offset differs only slightly with different start dates.",
            "cite_spans": [
                {
                    "start": 317,
                    "end": 320,
                    "text": "(2)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 348,
                    "end": 355,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Offset analysis"
        },
        {
            "text": "Note that the optimal cluster consistency offset is overwhelmingly around 16. This confirms known medical findings [12] indicating time from diagnosis to death has generally been around 17 days. This is quite different from the system evolution offset of 32 days, clearly visible in the changing cluster numbers in Figure 1 and points of discussion in Sections 4.1 and 4.2. While the cluster consistency offset seeks to align the similarity of cases and death counts among individual countries, the system evolution offset seeks to quantify the spread of the whole system.",
            "cite_spans": [
                {
                    "start": 115,
                    "end": 119,
                    "text": "[12]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 315,
                    "end": 323,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Offset analysis"
        },
        {
            "text": "One explanation for the system evolution offset being longer is that there is an additional delay between cluster membership changes with respect to cases and deaths that can be attributed to stresses on a country's healthcare system. First, the number of cases may increase significantly, placing a country into a different cluster relative to cases. This subsequently has an effect on the healthcare system, which subsequently leads to a greater impact in death counts. That is, the progression from elevation in cases cluster to deaths cluster is not necessarily just due to individual progressions from cases to deaths, but intermediate developments like stresses on hospitals. Perhaps the initial wave of patients can be treated with ventilators but these may quickly run out causing more deaths from later instances of cases. Regardless, it is an interesting observation that the clear offset of 32 days in the number of clusters does not minimise the offset in affinity or adjacency matrix norm differences. The minimal cluster consistency offset is determined in Figure 6 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1071,
                    "end": 1079,
                    "text": "Figure 6",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Offset analysis"
        },
        {
            "text": "Having learned a suitable offset between the evolution of cases and deaths, we can compare a country's number of cases and deaths up to the offset. We use the anomalous matrix walls of Section 3.4 to detect the 10 most anomalous countries from 17/3/2020 -30/4/2020 and display them in Table 2 . Anomalies may signify either disproportionately high or low number of deaths relative to the number of cases. Figure 7 displays the affinity matrices for cases and deaths, defined in Section 3.2, and the consistency matrix defined in Section 3.4 at various slices in time. We also compute a lag-adjusted death rate for each country, defined by",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 285,
                    "end": 292,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 405,
                    "end": 413,
                    "text": "Figure 7",
                    "ref_id": null
                }
            ],
            "section": "Anomaly analysis"
        },
        {
            "text": "where x (t) , y (t) are the case and death numbers at a particular time and \u03c4 is our chosen optimal offset. These ratios may be orders of magnitude higher than standard reported death rates, and are no longer bound between 0 and 1. This measure provides insight into the rate of spread, and how well a country has managed the total number of deaths, conditional on a given number of cases \u03c4 days prior.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Anomaly analysis"
        },
        {
            "text": "We use an offset value of \u03c4 = 16 from Table 1 and analyse both the lagadjusted death rate and the sequence of consistency matrices, which we have termed anomalous matrix walls. Several insights emerge:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 38,
                    "end": 45,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Anomaly analysis"
        },
        {
            "text": "1. Early in the analysis window, notable anomalous countries include the US, UK, Italy, Iran, Germany and France as seen in Table 2 . These countries started to experience a growth in their case numbers in late January and early February. However, these countries did not really start to experience exponential growth in COVID-19 deaths until early March. Interestingly, China is not identified as an anomalous country as it was the only country to have recorded deaths as a result of COVID-19. That is, China's behaviour was consistent across both phenomena.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 124,
                    "end": 131,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Anomaly analysis"
        },
        {
            "text": "2. Early in the history of COVID-19, Italy and Iran were internationally known as countries that were struggling to contain the number of deaths. Our consistency matrices identify both Iran and Italy as anomalous on 27/2/2020 and 8/3/2020. Both of these countries began to experience a sharp rise in the number of deaths well before other severely impacted countries. On the other hand, Singapore is identified as anomalous during this period due to its relatively small number of deaths. As at 7/3/2020, Singapore had 130 COVID-19 cases and had experienced 0 deaths.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Anomaly analysis"
        },
        {
            "text": "A similar trend continues until late March, during which Spain and Italy are identified as the most consistently anomalous countries with high death rates. The respective lag-adjusted death rates for Spain and Italy are 227% and 73.3% respectively. Indeed, the number of deaths in Spain on 28/3/2020, was more than 2 times greater than the number of cases 16 days earlier. This confirms the severity of the COVID-19 pandemic: Spain and Italy suffered a massive number of deaths within a short window. As of late March, Singapore was still identified as anomalous due to the relatively small number of deaths.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3."
        },
        {
            "text": "4. Towards the end of our analysis window, Qatar and Australia are also identified as anomalous with low death rates, with the UK and Bangladesh identified as anomalous with high death rates. The lag-adjusted death rates for Qatar, and Australia as at 27/4/2020 are 0.42%, and 1.33% respectively. The velocity-adjusted death rates for the UK and Bangladesh are 34.2%. and 34.1% respectively. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3."
        },
        {
            "text": "Our methodology has identified a significant similarity in the spread of cases and deaths due to COVID-19 across various countries. We have noticed that the number of clusters pertaining to these two phenomena exhibit very similar behaviour up to an offset. Our clustering and analysis of affinity and adjacency matrices has also given us an alternative way to compute a suitable offset between these multivariate systems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "With such an offset under consideration, our anomalous matrix walls are able to identify both positive and negative anomalies over time. This provides a framework for sequential anomaly analysis, where two phenomena are evolving over time and anomalies may emerge and disappear sequentially. Our methodology is flexible: different metrics between data, clustering methods and means of learning offset in data could all be used to study related time-varying multivariate systems and identify similarity and anomalies that evolve with time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "/2/2020 : cases",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "/4/2020 : cases (h) 27/4/2020 : deaths (i) 27/4/2020 : consistency Figure 7: Cases distance matrix, deaths distance matrix and consistency matrix with \u03c4 = 16",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Least squares quantization in PCM",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lloyd",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "28",
            "issn": "",
            "pages": "129--137",
            "other_ids": {
                "DOI": [
                    "10.1109/tit.1982.1056489"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A tutorial on spectral clustering",
            "authors": [
                {
                    "first": "U",
                    "middle": [],
                    "last": "Luxburg",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Statistics and Computing",
            "volume": "17",
            "issn": "",
            "pages": "395--416",
            "other_ids": {
                "DOI": [
                    "10.1007/s11222-007-9033-z"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Hierarchical grouping to optimize an objective function",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Ward",
                    "suffix": ""
                }
            ],
            "year": 1963,
            "venue": "Journal of the American Statistical Association",
            "volume": "58",
            "issn": "",
            "pages": "236--244",
            "other_ids": {
                "DOI": [
                    "10.1080/01621459.1963.10500845"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Hierarchical clustering via joint betweenwithin distances: Extending ward's minimum variance method",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Szekely",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Rizzo",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Journal of Classification",
            "volume": "22",
            "issn": "",
            "pages": "151--183",
            "other_ids": {
                "DOI": [
                    "10.1007/s00357-005-0012-9"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "dp: Optimal k-means clustering in one dimension by dynamic programming",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Ckmeans.1d",
            "volume": "3",
            "issn": "",
            "pages": "29--33",
            "other_ids": {
                "DOI": [
                    "10.32614/rj-2011-015"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "An examination of the effect of six types of error perturbation on fifteen clustering algorithms",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "W"
                    ],
                    "last": "Milligan",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "Psychometrika",
            "volume": "45",
            "issn": "",
            "pages": "325--342",
            "other_ids": {
                "DOI": [
                    "10.1007/bf02293907"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Rousseeuw",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "Journal of Computational and Applied Mathematics",
            "volume": "20",
            "issn": "",
            "pages": "90125--90132",
            "other_ids": {
                "DOI": [
                    "10.1016/0377-0427(87)90125-7"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A criterion for determining the number of groups in a data set using sum-of-squares clustering",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Krzanowski",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "T"
                    ],
                    "last": "Lai",
                    "suffix": ""
                }
            ],
            "year": 1988,
            "venue": "Biometrics",
            "volume": "44",
            "issn": "",
            "pages": "23--34",
            "other_ids": {
                "DOI": [
                    "10.2307/2531893"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A general statistical framework for assessing categorical clustering in free recall",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Hubert",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Levin",
                    "suffix": ""
                }
            ],
            "year": 1976,
            "venue": "Psychological Bulletin",
            "volume": "83",
            "issn": "",
            "pages": "1072--1080",
            "other_ids": {
                "DOI": [
                    "10.1037/0033-2909.83.6.1072"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "CLUSTISZ: A program to test for the quality of clustering of a set of objects",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "O"
                    ],
                    "last": "Mcclain",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "R"
                    ],
                    "last": "Rao",
                    "suffix": ""
                }
            ],
            "year": 1975,
            "venue": "Journal of Marketing Research",
            "volume": "12",
            "issn": "",
            "pages": "456--460",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Well-separated clusters and optimal fuzzy partitions",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Dunn",
                    "suffix": ""
                }
            ],
            "year": 1974,
            "venue": "Journal of Cybernetics",
            "volume": "4",
            "issn": "",
            "pages": "95--104",
            "other_ids": {
                "DOI": [
                    "10.1080/01969727408546059"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Guan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "China: a retrospective cohort study",
            "volume": "395",
            "issn": "",
            "pages": "30566--30569",
            "other_ids": {
                "DOI": [
                    "10.1016/s0140-6736(20)30566-3"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Number of clusters: Again we apply the methodology of Section 3.1. For the daily data, clusters range between {1, . . . , 17}, as depicted in Figure 1. For the 3-day windows, the number of clusters for K-means and spectral clustering ranges between {1, . . . , 17} and {1, . . . , 8} respectively. Trajectories for the number of clusters display largely similar patterns. The total (smoothed) number of clusters is 1 until early February, where the estimated number of clusters increases quickly until late March. Beyond this point, all clustering methods suggest that the number of clusters starts to stabilise. Figure 1 displays a remarkable similarity in the trajectories of number of clusters for daily cases and deaths, with a lag of about one month.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "of daily cases and deaths (b) Cluster numbers after translation by \u03c4 Evolution of the number of cases and deaths over time. The trajectory is almost identical after accounting for the estimated lag effect. The system evolution offset, defined in Section 3.3, is calculated to be \u03c4 = 32.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Cluster structure changes\u03b4(t) defined in Section 3.2.(a) Cases cluster evolution dendrogram (b) Deaths cluster evolution dendrogram",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Cluster evolution dendrograms defined in Section 3.2. These exclude first 50 observations for cases and first 66 observations for deaths due to triviality.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Heat map representing changing cluster membership over time for 15 worst impacted countries. Darker colours indicate larger cluster rankings.(c) Rolling cases: K-means (d) Rolling deaths: K-means (e) Rolling cases: spectral clustering (f) Rolling deaths: spectral clustering",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Heat map representing changing cluster membership over time for all countries.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Optimal cluster consistency offset for adjacency matrix",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "QA SG KR UK CN UA NO ZA AU TR 17/4/2020 BD QA SG UK AU KR BE ZA AT FR 27/4/2020 QA SG BD ME AU UK SW BE DE IL",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "10 most anomalous countries at various times, as defined in Section 3.4. AE: United Arab Emirates, AT: Austria, AU: Australia, BD: Bangladesh, BY: Belarus, CA: Canada, CN: China, DE: Germany, DO: Dominican Republic, ES: Spain, FR: France, UK: United Kingdom, ID: Indonesia, IE: Ireland, IL: Israel, IN: India, IR: Iran, IT: Italy, JP: Japan, KR: South Korea, MY: Malaysia, NL: Netherlands, NO: Norway, QA: Qatar, SG: Singapore, SW: Sweden, TR: Turkey, UA: Ukraine, US: United States of America, ZA: South Africa",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}