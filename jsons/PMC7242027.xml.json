{
    "paper_id": "PMC7242027",
    "metadata": {
        "title": "Towards a Shared Specification Repository",
        "authors": [
            {
                "first": "Alexander",
                "middle": [],
                "last": "Raschke",
                "suffix": "",
                "email": "alexander.raschke@uni-ulm.de",
                "affiliation": {}
            },
            {
                "first": "Dominique",
                "middle": [],
                "last": "M\u00e9ry",
                "suffix": "",
                "email": "dominique.mery@loria.fr",
                "affiliation": {}
            },
            {
                "first": "Frank",
                "middle": [],
                "last": "Houdek",
                "suffix": "",
                "email": "frank.houdek@daimler.com",
                "affiliation": {}
            },
            {
                "first": "Philipp",
                "middle": [],
                "last": "K\u00f6rner",
                "suffix": "",
                "email": "p.koerner@uni-duesseldorf.de",
                "affiliation": {}
            },
            {
                "first": "Michael",
                "middle": [],
                "last": "Leuschel",
                "suffix": "",
                "email": "michael.leuschel@uni-duesseldorf.de",
                "affiliation": {}
            },
            {
                "first": "Jannik",
                "middle": [],
                "last": "Dunkelau",
                "suffix": "",
                "email": "jannik.dunkelau@uni-duesseldorf.de",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Our group in D\u00fcsseldorf has collected since 2003 thousands of B and Event-B machines: our ProB repository contains around 13 000 machines, of which more than 3500 are publicly available. The examples are used for ProB\u2019s regression, performance and feature tests. Those public examples contain some duplicates, as they are compiled from different sources: e.g., from tickets in our bug tracker, teaching, literature, case studies, or student projects.",
            "cite_spans": [],
            "section": "Introduction and Motivation",
            "ref_spans": []
        },
        {
            "text": "Naturally, not all machines are relevant to all research questions: infinite state spaces might be interesting in order to evaluate symbolic model checking techniques [11], whereas large yet finite state spaces are the important class for distributed model checking [10]. Other use cases, such as data validation [7] work by executing a model along one particular, linear path, while others, like constraint solving problems, sometimes work on machines without variables, consisting of a single state. Most recently, machine learning (ML) techniques are applied to model checking or synthesis as well, and require a large number of specifications, e.g., in order to extract and re-combine predicates [6]. Even with access to numerous machines, it is time-consuming and cumbersome to identify machines to use for benchmarking, especially since only a small amount of data can be presented in a typical research article. Without any doubt, other research groups have their individual set of B machines they use for testing and evaluation. Thus, we propose that individual sets of benchmarks from different parts of the community are combined into a global, shared repository. With this paper, we start this endeavour, and create an index of our specifications as described in Sect. 2. Benefits include:Benchmarks are publicly available and experiments can be replicated easily.Performance comparisons of several tools in different versions can be drawn.Suitable benchmarks can be quickly identified.Examples for translations between formalisms or ML are available.Particularly successful examples can be shared for teaching.\n",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 170,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 267,
                    "end": 269,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 314,
                    "end": 315,
                    "mention": "7",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 701,
                    "end": 702,
                    "mention": "6",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction and Motivation",
            "ref_spans": []
        },
        {
            "text": "While we are most involved in the B and Event-B community, we think that similar issues are present in other communities which make up the ABZ conference. Thus, we explicitly want to invite everyone to contribute specifications written in other formalisms as well. The repository is located at:",
            "cite_spans": [],
            "section": "Introduction and Motivation",
            "ref_spans": []
        },
        {
            "text": "\nhttps://github.com/hhu-stups/specifications\n",
            "cite_spans": [],
            "section": "Introduction and Motivation",
            "ref_spans": []
        },
        {
            "text": "Since our initial set of models is rather large, it is vital that a sufficient amount of meta-information is attached to the models. For this, we suggest usage of edn1, a serialisation format with parsers available in most mainstream programming languages. For each specification, some basic information should be offered:Which formalism is this specification written in?A SHA-256 hash code to identify duplicates, and to ensure reproducibility of experiments regarding the specification.Number of deferred sets, enumerated sets, constants, state variables and operations/events, number of included machines, etc.Number of states and state transitions in the machine (if known).2\nPresence of invariant violations, deadlocks, etc. (if the property is known).Optional link to another (previous) model (e.g., a correction or evolution).\n",
            "cite_spans": [],
            "section": "Proposed Index",
            "ref_spans": []
        },
        {
            "text": "The information above is known to never change, but can be extended once further properties are considered. Additional information depending on the tool, its configuration or the use case altogether can be included as well, such as temporal properties (e.g., expressed in LTL or CTL) which are expected to hold or to be violated, tool name and version/revision which is able to parse or execute the specification, or settings, walltime and memory usage required for application of a technique such as model checking.",
            "cite_spans": [],
            "section": "Proposed Index",
            "ref_spans": []
        },
        {
            "text": "Optional Fields. Naturally, this data must also be extensible via optional fields. For instance, additional information due to a new use case can be gathered, e.g., the amount of states when using state space reduction techniques. As runtime might depend on the hardware it was ran on, relevant data should be included as well. They also allow extension of the information, e.g., for further tools such as Atelier-B [4] or handling of entirely different file formats, e.g., Rodin [1] archives. In order to select suitable set of specifications, one can simply apply a filter predicate testing the formalism or dialect of it. Furthermore, optinal fields enable links between different machines (e.g., due to refinement or different parameter instantiation) and to external information, such as references to articles describing the model, descriptions of the models as well as the author(s) and their contact information. Finally, certain metrics do not make sense for specific use cases of a formalism, or cannot be applied to other formalisms at all. Thus, such data must not be a mandatory field (but may be mandatory for a given formalism)3.",
            "cite_spans": [
                {
                    "start": 417,
                    "end": 418,
                    "mention": "4",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 481,
                    "end": 482,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Proposed Index",
            "ref_spans": []
        },
        {
            "text": "Filtering Specification. As previously mentioned, we use edn for the meta information because this format can easily be processed. A short example written in Clojure is given in Listing 1.1. There, all files containing meta-information in the directory are located (ll. 1\u20135). Then, they are read in and filtered (ll. 7\u201315). The expression starting in l. 9 returns a list of all file names of specifications written in the B formalism that are known to have a state space of at least 100 000 states. At the time of writing, there are 45 such machines. This example shows that finding specifications based on certain criteria is fairly easy and necessary for verification tool maintainers.",
            "cite_spans": [],
            "section": "Proposed Index",
            "ref_spans": []
        },
        {
            "text": "Table 1 provides an overview of the information of B machines currently present in the repository, compiled after running each machine with a timeout of 30 min in the ProB model checker.",
            "cite_spans": [],
            "section": "Proposed Index",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "On Updating Versions. We strongly argue that the published version of a specification must not be replaced. Once they are online, they may be used by any researcher. Even though git clearly documents the history of a file, it would be unclear which version was used as a benchmark or presented in an article. If mistakes were spotted, new versions can be submitted as a modified copy.\n",
            "cite_spans": [],
            "section": "Proposed Index",
            "ref_spans": []
        },
        {
            "text": "We firmly believe that a shared repository of specifications will benefit all communities coming together at ABZ. Aside from making benchmarks available for replication, it can assist courses teaching the formal methods. Furthermore, it builds the foundation for exciting new research that relies on such a dataset.",
            "cite_spans": [],
            "section": "Conclusions, Related and Future Work",
            "ref_spans": []
        },
        {
            "text": "Similar issues have been found in other communities. This led to the creation of central benchmarking sets, e.g., BEEM for models written in DVE [13], or the PRISM benchmark suite [12] for models written in PRISM. Yet, to our knowledge, it is not possible to contribute to these databases. This has led to criticism that, e.g., not many models that are large enough are featured. Also, a fixed set of benchmarks is not a viable approach in the B community, that creatively uses the B language in order to solve very different types of problems.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 148,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 181,
                    "end": 183,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Conclusions, Related and Future Work",
            "ref_spans": []
        },
        {
            "text": "In other communities, such as SMT and SAT solving, shared benchmark sets are established for many years [3, 8]. They both grow via community contributions and are the foundation for solver competitions [2, 9]. SMT-LIB in particular is a success story, containing more than 100000 benchmarks. There are many other examples for competitions and problem collections, e.g., SV-COMP4, TPLP5 [15], which we cannot exhaustively list here due to page limitations.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 106,
                    "mention": "3",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 108,
                    "end": 109,
                    "mention": "8",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 203,
                    "end": 204,
                    "mention": "2",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 206,
                    "end": 207,
                    "mention": "9",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 387,
                    "end": 389,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Conclusions, Related and Future Work",
            "ref_spans": []
        },
        {
            "text": "An interesting question we could not answer in this paper is to what extent our examples match the reality of (confidential) industrial specifications. An answer requires to take a closer look at the data that is available to us. When considering state space size, number of variables and operations as well as idioms used, e.g., usage of program counters or certain data structures, it might be possible to label some public machines accordingly.",
            "cite_spans": [],
            "section": "Conclusions, Related and Future Work",
            "ref_spans": []
        },
        {
            "text": "Furthermore, research papers often contain links to download pages not only for benchmarks, but also tools themselves. Some tools presented years ago are hard or near impossible to find now. Some conferences, e.g., POPL, established artifact evaluation committees, yet making artifacts permanently available often is optional. ACM conferences offer different badges6 depending on availability, replicability, etc. A similar, mandatory repository containing at least one binary version or even the source code of tools presented at conferences might prove useful to the research community as well. Worth mentioning here is the StarExec platform [14], that allows storage and execution of tools and benchmark problems, which may serve this effort to a satisfactory extent already.",
            "cite_spans": [
                {
                    "start": 645,
                    "end": 647,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Conclusions, Related and Future Work",
            "ref_spans": []
        },
        {
            "text": "In order for the presented endeavour to be successful, the effort of the entire community is required and their contributions to this repository will be appreciated.",
            "cite_spans": [],
            "section": "Conclusions, Related and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Overview of available machine meta data with a timeout of 30 min.\n",
            "type": "table"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "An open extensible tool environment for Event-B",
            "authors": [
                {
                    "first": "J-R",
                    "middle": [],
                    "last": "Abrial",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Butler",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hallerstede",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Voisin",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Formal Methods and Software Engineering",
            "volume": "",
            "issn": "",
            "pages": "588-605",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Distributed model checking using ProB",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "K\u00f6rner",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bendisposto",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "NASA Formal Methods",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "BEEM: benchmarks for explicit model checkers",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pel\u00e1nek",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Model Checking Software",
            "volume": "",
            "issn": "",
            "pages": "263-267",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "StarExec: a cross-community infrastructure for logic solving",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Stump",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Sutcliffe",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Tinelli",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Automated Reasoning",
            "volume": "",
            "issn": "",
            "pages": "367-373",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "The TPTP problem library and associated infrastructure. From CNF to TH0, TPTP v6.4.0",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Sutcliffe",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J. Autom. Reason.",
            "volume": "59",
            "issn": "4",
            "pages": "483-502",
            "other_ids": {
                "DOI": [
                    "10.1007/s10817-017-9407-7"
                ]
            }
        },
        "BIBREF7": {
            "title": "SMT-COMP: satisfiability modulo theories competition",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Barrett",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "de Moura",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Stump",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Computer Aided Verification",
            "volume": "",
            "issn": "",
            "pages": "20-23",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Automated backend selection for ProB using deep learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dunkelau",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Krings",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidt",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NASA Formal Methods",
            "volume": "",
            "issn": "",
            "pages": "130-147",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Using B and ProB for data validation projects",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hansen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Schneider",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Leuschel",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Abstract State Machines, Alloy, B, TLA, VDM, and Z",
            "volume": "",
            "issn": "",
            "pages": "167-182",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "The international SAT solver competitions",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "J\u00e4rvisalo",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Le Berre",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Roussel",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Simon",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Ai Mag.",
            "volume": "33",
            "issn": "1",
            "pages": "89-92",
            "other_ids": {
                "DOI": [
                    "10.1609/aimag.v33i1.2395"
                ]
            }
        }
    }
}