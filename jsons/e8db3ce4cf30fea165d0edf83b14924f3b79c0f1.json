{
    "paper_id": "e8db3ce4cf30fea165d0edf83b14924f3b79c0f1",
    "metadata": {
        "title": "\"VIRUS HUNTING\" USING RADIAL DISTANCE WEIGHTED DISCRIMINATION 1",
        "authors": [
            {
                "first": "Jie",
                "middle": [],
                "last": "Xiong",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of North Carolina at Chapel Hill",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "D",
                "middle": [
                    "P"
                ],
                "last": "Dittmer",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of North Carolina at Chapel Hill",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "J",
                "middle": [
                    "S"
                ],
                "last": "Marron",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of North Carolina at Chapel Hill",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Motivated by the challenge of using DNA-seq data to identify viruses in human blood samples, we propose a novel classification algorithm called \"Radial Distance Weighted Discrimination\" (or Radial DWD). This classifier is designed for binary classification, assuming one class is surrounded by the other class in very diverse radial directions, which is seen to be typical for our virus detection data. This separation of the 2 classes in multiple radial directions naturally motivates the development of Radial DWD. While classical machine learning methods such as the Support Vector Machine and linear Distance Weighted Discrimination can sometimes give reasonable answers for a given data set, their generalizability is severely compromised because of the linear separating boundary. Radial DWD addresses this challenge by using a more appropriate (in this particular case) spherical separating boundary. Simulations show that for appropriate radial contexts, this gives much better generalizability than linear methods, and also much better than conventional kernel based (nonlinear) Support Vector Machines, because the latter methods essentially use much of the information in the data for determining the shape of the separating boundary. The effectiveness of Radial DWD is demonstrated for real virus detection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "1. Introduction. A current major scientific challenge is the detection of viruses in human blood samples. Cogent examples include HIV, the cause of AIDS; poliovirus, which was considered eradicated, but has now emerged in Syria and the Middle East; or middle east respiratory syndrome (MERS), which entered the United States in May 2014 via a 44-year-old male who traveled from Jeddah, South Africa, to Orlando, Florida, via London. At home he developed fever, chills and a slight cough. He was admitted to the hospital and later diagnosed with the MERS coronavirus. Since May 9, 2014, A consequence of this L1 normalization is that the normalized data vectors can be geometrically represented as points on the standard unit simplex. Data points with more nonzero entries lie more toward the interior of the unit simplex. When all entries are approximately the same, the data point is near the center. On the contrary, the more zeros in a vector, the closer this data point is to one of the vertices of the unit simplex. In the extreme case with only one nonzero entry \"1\" in the vector, the data point is at a vertex. Figure 1 shows how different the virus positive and virus negative samples are, and motivates exploiting simplex geometry, by showing an overlaid plot of normalized data vectors from an HSV-1 (a human herpesvirus) detection problem. HSV-1, or human herpesvirus-1, is the leading cause of nontraumatic blindness and can cause fatal encephalitic disease in children. The virus can be treated with acyclovir, if and only if diagnosed rapidly and accurately. Both serum and cerebral spinal fluid are used for diagnosis and can be readily obtained for sequencing. In Figure 1 , we overlaid 2 (out of 8) data vectors for the HSV-1 positive (the +1) class (top panel) and 3 (out of 24) data vectors from the HSV-1 negative (the \u22121) class (lower panel). The overall entries of the positive data vectors are relatively small and have relatively comparable amplitudes (top panel). The nonzero entries of the negative data vectors are very sparse and have much larger amplitudes (about 200 times larger than that of the positive samples, lower panel of Figure 1 ). This is a property of all virus detection problems, since the negative sequences are chosen to be genetically very different from the virus. The aligned reads (from the negatives to the virus sequence), on the other hand, are often short stretches of sequence which are of reduced complexity, that is, repeats or single nucleotide (either A, C, T or G) runs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1119,
                    "end": 1127,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1681,
                    "end": 1689,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 2161,
                    "end": 2169,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": ""
        },
        {
            "text": "If we keep the same range of y-axis in plotting the positive data vectors as in plotting the negative data vectors, one can see almost nothing since the amplitudes of the former ones are much smaller than the latter ones. Equivalently speaking, the positives are close to the center of the simplex, while the negatives lie near to a diverse set of vertices of the unit simplex, because the differently colored spikes of the negative data vectors are located at quite divergent positions. A simple model for data on the unit simplex is given in Figure 2 , where the 3-d unit simplex is shown as a gray triangle while some +1 (\u22121) class data are shown as red plus signs (or blue circles, resp.). It is not hard to see that linear methods will struggle to capture the differences between classes in this case.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 544,
                    "end": 552,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": ""
        },
        {
            "text": "In our work, the DNA alignment data vectors are often of dimension 100,000 to 200,000, while the sample size is usually much smaller. Equivalently, data can be seen as points on the high-dimensional unit simplex. A simplified example of normalized data vectors of the positive (+1 class) data (red plus signs) and the negative (\u22121 class) data points (blue circles). The unit simplex is shown as the gray triangle. Because there are many zeros in the \u22121 data vectors, they typically locate at the vertices of the unit simplex while the positives are closer to the center. Figures 1 and 2 suggest that the \u22121 class departs from the center of the simplex (where the +1 class lies) in many diverse directions so that the theoretical Bayes classification boundary (assuming a probability distribution for each class) is highly nonlinear. Note that the discrimination in radial directions appears to be attractive. This motivates the development of Radial DWD in order to incorporate such a nonlinear pattern. As detailed in Section 4, by optimizing a hypersphere over its center and radius, Radial DWD separates the 2 classes, favoring putting the +1 (\u22121) class inside (outside) the hypersphere. The computation of Radial DWD through solving a sequence of Second Order Cone Programs Alizadeh and Goldfarb (2003) is carried out by an interior point optimization package called SDPT3, developed by Tutuncu, Toh and Todd (2001) . A future sample will be classified as +1 (\u22121) when it is located inside (outside) the hypersphere.",
            "cite_spans": [
                {
                    "start": 1278,
                    "end": 1306,
                    "text": "Alizadeh and Goldfarb (2003)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1391,
                    "end": 1419,
                    "text": "Tutuncu, Toh and Todd (2001)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 571,
                    "end": 586,
                    "text": "Figures 1 and 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": ""
        },
        {
            "text": "A standard approach to HDLSS classification problems is linear methods, such as Mean Difference [MD, Sch\u00f6lkopf and Smola (2002) ], penalized logistic regression with LASSO penalty [LASSO, Tibshirani (1996) ], Support Vector Machine [SVM, Vapnik (1995) , Shawe-Taylor and Cristianini (2004) ] and Distance Weighted Discrimination [DWD, Marron, Todd and Ahn (2007) ]. Figure 2 suggests that, as the dimension and diversity of the \u22121 class grow, such methods will be severely inefficient. This issue is carefully studied for actual \"virus hunting\" in Section 2 and by simulation in Section 3. It is natural to wonder if a more serious competitor to Radial DWD is a nonlinear kernel Support Vector Machine classification [Burges (1998) , Hastie, Tibshirani and Friedman (2009) ]. The most popular of these is the Radial Basis Function (RBF) kernel. The virus detection capability of these methods are compared in Figure 3 , where RBF kernel SVM and Radial DWD classification are illustrated. Note that in the machine learning literature, RBF is a synonym for \"Gaussian kernel.\"",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 127,
                    "text": "Sch\u00f6lkopf and Smola (2002)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 188,
                    "end": 205,
                    "text": "Tibshirani (1996)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 232,
                    "end": 251,
                    "text": "[SVM, Vapnik (1995)",
                    "ref_id": null
                },
                {
                    "start": 254,
                    "end": 289,
                    "text": "Shawe-Taylor and Cristianini (2004)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 335,
                    "end": 362,
                    "text": "Marron, Todd and Ahn (2007)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 717,
                    "end": 731,
                    "text": "[Burges (1998)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 734,
                    "end": 772,
                    "text": "Hastie, Tibshirani and Friedman (2009)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 366,
                    "end": 374,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 909,
                    "end": 917,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": ""
        },
        {
            "text": "In Figure 3 , RBF kernel SVM and Radial DWD are trained using 8 HSV-1 positive (red plus signs) and 24 HSV-1 negative (blue circles) data vectors, which are partially shown as an overlaid plot in Figure 1 . Signed distances to the corresponding separating boundary (the black vertical dashed line) are depicted along the x-axis as a jitter plot. Random heights are assigned in order to visually separate the points. Additionally, 127 new samples are used as a test set and kernel density estimates are given for each group. While the majority of test samples are shown as gray x-symbols, 4 are highlighted in magenta since they are HSV-1 positive human samples; 14 are highlighted in green since they are highly related herpesviruses (with nonhuman hosts). The related viruses share significant sequence identity (traditionally larger than 35%) with the reference virus, but may infect animals rather than humans. Domestic cats and cattle, for instance, can be infected with a herpesvirus homologous to HSV-1. The performance of RBF kernel SVM is far from satisfactory: although positive samples (magenta and green asterisks) are very close to the true positives, many (69) grays (unrelated samples) are also classified as HSV-1 positive. This is expected since kernel methods require a type of \"data richness,\" that is not present in the virus hunting problem. In particular, they work well in situations where training data can be found in all of the various regions where the test data will appear. But in virus hunting data analysis, that completely breaks down.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 196,
                    "end": 204,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": ""
        },
        {
            "text": "Radial DWD shows a superior classification result not only because it correctly classified all HSV-1 positive samples but also because the positive samples are grouped reasonably well: HSV-1 positive human samples (magenta asterisks) are tightly clustered with the positive training data (red plus signs); related herpesviruses (green asterisks) are clustered according to the host species that they infect-from the right to the left-monkey, pig and cattle. The grouping property of Radial DWD can be exploited to classify new viruses, for example, in different animal hosts, as they would be related, but not identical to the known ones.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "For some data sets, it will be sensible to use a given point, for example, the center of the simplex or the sample mean, as the centerpoint of the sep-7 arating sphere. Therefore, solving the associated optimization problem will be generally easier. However, the center of the simplex seems inappropriate for virus hunting, as due to various biological effects, even in the limit as the number of reads goes to infinity, the read depth vector is not flat. The sample mean can be appropriate in many situations, but as the centroid classifier is often a lot less efficient in many high-dimensional biological settings, we expect Radial DWD to often be worth the overhead of the more complex optimization. Furthermore, we also have our eye on generalizing to other data types, where we believe the property of Radial DWD having conventional DWD as a limit (as the center goes to infinity in a particular direction, with the radius also growing) will become very important.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "A full description of this HSV-1 classification is given in Section 2, where we carefully compare Radial DWD with some linear and nonlinear competitors and the superiority of Radial DWD under this radial context is discussed in detail. A similar conclusion can be drawn from the simulation study in Section 3. Radial DWD optimization and an iterative algorithm to solve it can be found in Section 4. An introduction to virus detection, insights about the Dirichlet distribution and the high-dimensional unit simplex, along with more details of our data sets and some proofs, can be found in the supplementary materials in Xiong, Dittmer and Marron (2015) .",
            "cite_spans": [
                {
                    "start": 622,
                    "end": 654,
                    "text": "Xiong, Dittmer and Marron (2015)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "2. Virus detection data analysis. As briefly described in Section 1, Radial DWD presents an appealing virus detection capability. A broader comparison between Radial DWD and its linear and nonlinear competitors is given in this section through analyzing a real data example of detecting the \u03b1-Human Simplexvirus 1 (\u03b1-HSV-1 or HSV-1). This virus is a subfamily of Human Herpesvirus (HHV). The data set consists of the following 2 subsets:",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "\u2022 The training data are n + = 8, n \u2212 = 24 vectors of dimension 152,261, which is the DNA length of HSV-1. Entries of each data vector correspond to the nucleotide positions in the virus DNA sequence. The training data of the +1 (HSV-1 positive) and \u22121 (HSV-1 negative) classes are normalized to the unit simplex (of dimension 152,261). The +1 class tends to locate near the center while the \u22121 class tends to locate near a diverse set of vertices of the simplex. Classifiers are trained using the +1 versus the \u22121 classes. \u2022 The test set consists of the DNA alignment vectors from the following samples: 4 HSV-1 positive human samples (not appearing in the training), 14 nonhuman \u03b1 Simplexvirus-1 (including 5 monkey Simplexvirus-1, 8 pig Simplexvirus-1 and 1 cow Simplexvirus-1) and 109 much more distantly related viruses.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Nonzero data vectors are normalized to the unit simplex and can be viewed as points on it. Samples whose DNA alignment vectors are zero vectors are put at \u2212\u221e. This is reasonable since zero vectors only exist in the \u22121 class training set or the test set: (a) if the sample comes from the \u22121 class training set, it has no effect on the calculation of the separating sphere (interpreting the reciprocal of \u2212\u221e to be zero); (b) if the sample comes from the test set, it should surely be classified as \u22121 and \u2212\u221e is viewed as outside the separating hypersphere.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The classification performance of Radial DWD is compared with a number of popular classification methods including MD, LASSO, linear DWD, SVM and RBF Kernel SVM in Figure 4 . Quadratic SVM gave results that were quite similar to RBF SVM, so it is not shown here. For methods including a separating plane, relative performance comes from the projection onto the normal vector, shown as the horizontal axes in Figure 4 . Radial DWD is similarly interpreted as the signed distance to the separating sphere. The +1 training data are shown as red plus signs, \u22121 training data as blue circles, HSV-1 positive human samples (real human DNA samples that are infected by HSV-1) as magenta asterisks, related \u03b1 simplex herpesviruses as green asterisks and other samples as gray x-symbols (known to be HSV-1 negative). The position of the separation boundary is shown by the black vertical dashed line. Signed distances to the separating boundaries are . Real data example of an HSV-1 classification problem. We show 6 panels of 1-dimensional \"signed distance to separating boundary\" plots to compare Radial DWD [panel ( f)] with MD, LASSO, linear DWD, SVM and RBF SVM. Red plus signs are +1, blue circles are \u22121, magenta asterisks are HSV-1 positive humans, green asterisks are related nonhuman herpesviruses, gray x-symbols are nonpositive samples. Figure 4 shows the superior performance of Radial DWD.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 164,
                    "end": 172,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 408,
                    "end": 416,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 1340,
                    "end": 1348,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": ""
        },
        {
            "text": "picted along the horizontal axis, while the vertical perturbation is used for visual separation of the points. Kernel density plots are provided as well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "9"
        },
        {
            "text": "Panels (e) and (f) are the same as in Figure 3 (except that the lengthwidth ratio of the figures are different) and the superior performance of Radial DWD is explained there. While the training data is well separated in all cases in Figure 4 , the good classification property may not carry over to the test samples. The performance of MD, SVM, DWD and RBF SVM tend to be similar in this example where the false positive rates are very high (larger than 50%), that is, most of the negative gray x-symbols are to the left of the dashed line. This performance contrasts sharply with panel (f) where all gray x-symbols are to the right. Meanwhile, LASSO presents a unique behavior with zero false positive. However, it fails to correctly classify 8 (out of 14) HSV-1 related viruses (green asterisks) since they fall on the left-hand side of the LASSO-separating hyperplane. The other 6 HSV-1 related viruses are much further from the positive training data (red plus signs) to the right. Our simulations show that LASSO tends to pick out a small subset of nucleotide positions and classify data merely based on very limited information gained on those positions, which results in poor classification.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 38,
                    "end": 46,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 233,
                    "end": 241,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "9"
        },
        {
            "text": "An additional insightful comparison of methods using simulated data sets appears in Section 3 and Radial DWD will be shown to have a much better classification accuracy in terms of both lower false positive and lower false negative error rates, while all the other competitors considered here perform poorly. Note that real data examples of \u03b2-HHVs and \u03b3-HHVs (the other 2 subfamilies of HHV) classification were also analyzed and examples can be found in the supplementary materials in Xiong, Dittmer and Marron (2015) .",
            "cite_spans": [
                {
                    "start": 486,
                    "end": 518,
                    "text": "Xiong, Dittmer and Marron (2015)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "9"
        },
        {
            "text": "In addition to giving outstanding classification results when one class is widely distributed around the other, the computation of Radial DWD is fast enough to be useful for modern scale bioinformatics data sets. The computational speed is nearly independent of the dimension of the data vectors because the method is based on a QR decomposition (see Section 4 for detail). In particular, the full set of simulations shown in Section 3, involving many replications, was done in a few hours.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "9"
        },
        {
            "text": "3. Simulation study. Section 2 showed that Radial DWD outperforms its linear and nonlinear competing classifiers for real virus detection data, and this idea is further emphasized in this section by a simulation study. Our simulations are based on Dirichlet distributions which are a popular and broad family of distributions on the unit simplex. Figure 5 shows the classification results, aimed at modeling the behavior observed in real data in Figure 4 , detailed in Section 3.1. Broader simulation results are discussed in Section 3.2. 3.1. Simulation 1. The simulated data in Figure 5 have dimension d = 50, with n + = 20 class +1 samples represented as red plus signs and n \u2212 = 20 class \u22121 samples represented as blue circles. Data are simulated using the Dirichlet distribution Dirichlet(\u03b1), supported on the unit simplex.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 347,
                    "end": 355,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 446,
                    "end": 454,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 580,
                    "end": 588,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "The parameter \u03b1 \u2208 R d+ determines the mode and dispersion of the Dirichlet distribution. If all the entries in \u03b1 are the same, the distribution is centered on the unit simplex. Suppose the common entries are larger (less) than 1, increasing (decreasing) the entries makes the distribution more concentrated to the center (vertices) of the unit simplex; suppose the common entries are exactly 1s, the corresponding distribution is the uniform on the simplex. Examples in 3 dimensions can be found in the supplementary materials in Xiong, Dittmer and Marron (2015) .",
            "cite_spans": [
                {
                    "start": 530,
                    "end": 562,
                    "text": "Xiong, Dittmer and Marron (2015)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "9"
        },
        {
            "text": "The +1 class data in Figure 5 are drawn from Dirichlet(\u03b1 + ) with \u03b1 + = (5, . . . , 5) and the \u22121 class data are generated from Dirichlet(\u03b1 \u2212 ) with \u03b1 \u2212 = (0.5, . . . , 0.5).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 21,
                    "end": 29,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "Classifiers, including MD, LASSO, linear SVM, RBF Kernel SVM, DWD and Radial DWD, are trained on the red pluses and blue circles. We assess the performance by classifying 200 new test samples drawn from the \u22121 class population. The test samples are shown in Figure 5 as gray x symbols. Note that the Quadratic Kernel SVM (QSVM) was also considered. It performed very similarly with RBF SVM in this particular example, and hence is not shown here.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 258,
                    "end": 266,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "In each panel of Figure 5 , the signed distances of the data points to the optimal separating hyperplane are shown on the horizontal axes. The position of each separating hyperplane is shown as a dashed line. Data points that fall on the same side of the hyperplane as the +1 (\u22121) class will have positive (negative) distances. Kernel density plots (e.g., smooth histograms) are provided as another way of viewing the population of each class. As shown in Figure 5 (a), MD performs poorly (with many gray test points to the right of the boundary) since the separation of classes in this example is not a shift of means. In particular, 152 out of 200 samples are misclassified as +1. Figure 5 (b) shows that the 2 training classes are linearly separable by using SVM, but the training data from both classes pile up at the margin. Moreover, 129 out of 200 test samples are misclassified as +1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 17,
                    "end": 25,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 456,
                    "end": 464,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 683,
                    "end": 691,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "Data piling is a sign of overfitting and is very undesirable since the corresponding separating hyperplane is driven heavily by the particular realization of the data at hand [see Marron, Todd and Ahn (2007) ]. DWD was developed to address this ubiquitous problem with SVM, yet Figure 5 (c) is similar to (b). The phenomenon of data piling is diminished as expected from the ideas of Marron, Todd and Ahn (2007) . However, the performance of DWD for this test set is far from satisfactory because radial separation is the key: again, many (142 out of 200) test samples are misclassified as +1.",
            "cite_spans": [
                {
                    "start": 180,
                    "end": 207,
                    "text": "Marron, Todd and Ahn (2007)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 384,
                    "end": 411,
                    "text": "Marron, Todd and Ahn (2007)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 278,
                    "end": 286,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "Classification using LASSO is illustrated in Figure 5 (d). The training data are well separated, but 117 of the 200 test samples are misclassified. Figure 5 (e) shows the classification using the RBF (nonlinear) Kernel SVM. When the training set is linearly separable, kernel SVM behaves like the linear counterpart but may overfit the training data more severely under HDLSS assumptions. Although the dimension is fairly moderate, data piling still exists. The expected improvement over the linear counterpart is present in the sense that only 120 out of 200 test samples are misclassified, although this is still unacceptably poor.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 45,
                    "end": 53,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 148,
                    "end": 156,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "A much improved performance and classification accuracy can be observed in Figure 5 (f) where Radial DWD is applied. Training data are well separated with no signs of data piling and, except for one test data point, all the other test samples are correctly classified, showing that Radial DWD solves the overfitting problem one may intuitively expect from the RBF kernel SVM in HDLSS radial contexts.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 75,
                    "end": 83,
                    "text": "Figure 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "It is not surprising that, despite the underlying nonlinear pattern, SVM and DWD successfully separate the 2 training classes due to the large size of the data space. However, the good classification performance does not carry over to the test samples, which may differ from the +1 class in directions that do not appear in the \u22121 class training data. This highlights the limitation of linear methods in this type of context. Figures 4 and 5 together make it clear that the intuitive ideas in Section 1 are indeed the drivers of the observed superior performance of Radial DWD. Thus, simulating data from the Dirichlet distribution is useful and insightful to understand the data structure of the virus discovery.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 426,
                    "end": 441,
                    "text": "Figures 4 and 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "9"
        },
        {
            "text": "3.2. Simulation 2. Next a broader simulation study is conducted. The training data and the test data are simulated on the unit simplex using Dirichlet(\u03b1) with \u03b1 summarized in Table 1 . In each example, n + = 20 +1 class and n \u2212 = 50 \u22121 class data of dimension d = 10, 50, 100, 500, 1000, 5000, 10,000, 50,000, 100,000 are generated in order to cover a range from non-HDLSS to extreme HDLSS cases. Additionally, in panels (a1) and (a2), 5000 test samples are drawn from the \u22121 class in order to assess the false positive rate; in panels (b1) and (b2), 5000 test samples are drawn from the +1 class in order to assess the false negative rate. Thirty repetitions are done for each case and each dimension.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 175,
                    "end": 182,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "9"
        },
        {
            "text": "The tuning parameters in LASSO, (linear/Quadratic/RBF) SVM, DWD are determined by 5-fold cross-validation. Classifiers are trained using the +1 versus the \u22121 class. Classification error (false positive and false negative) is calculated for classifying the 5000 test samples and is illustrated in Figure 6 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 296,
                    "end": 304,
                    "text": "Figure 6",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "9"
        },
        {
            "text": "In the first simulation in panels (a1), (b1) and (c1), the +1 class is simulated uniformly on the simplex using Dirichlet(1 \u00b7 \u00b7 \u00b7 1), while the \u22121 class is simulated near the vertices of the simplex, as given in Table 1 . The class separation is hard in low dimensions, but, as dimension grows, the relatively low sample size of the training data makes the separation easier. It can be seen in panel (a1) that when dimension is low (around 10), RBF kernel SVMs and Radial DWD perform similarly well with false positive error rates below 10%, LASSO and Quadratic kernel SVM follows and all the other linear methods perform poorly. As dimension goes to \u221e, the false positive error of Radial DWD shrinks to zero quickly, while that of the MD/SVM/DWD/RBF kernel SVM/Quadratic kernel SVM goes to 1; that of LASSO converges to around 50%. A quite different tendency can be observed in panel (b1) when the false negative rate is being examined. When dimension is low, LASSO tends to have a very large false negative error, but the error shrinks to zero quickly as dimension grows, as do the false negative error rates for the other methods. The average of the 2 types of errors is summarized in (c1). It is not hard to see that the kernel SVMs and Radial DWD are comparably good in low dimensions; the error rate of the former one converges to around 50%, while that of the latter one converges to zero quickly as dimension grows. Additionally, the average error rate of A color key is also given. Error bars are obtained by repeating the simulation 30 times for each dimension d. Figure 6 shows the outstanding performance of Radial DWD relative to typical methods in these radial settings.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 212,
                    "end": 219,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 1574,
                    "end": 1582,
                    "text": "Figure 6",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "9"
        },
        {
            "text": "MD/SVM/DWD is relatively stable (around 50%); the average error rate of LASSO is around 35% for large dimensions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "9"
        },
        {
            "text": "The second simulation in panels (a2), (b2) and (c2) is similar to the first except that the \u22121 class is closer to the center. This is even a harder classification problem when dimension is low. An almost opposite tendency could be observed in (a2) and (b2), compared to (a1) and (b1). Except LASSO, the false positive rate [in (a2)] of all methods shrinks to zero, while that of Radial DWD decreases much faster; the false positive error rate of LASSO is around 35% for large dimensions. Shown in panel (b2), the false negative Table 1 Parameter \u03b1 used in simulation Case # +1 class \u22121 class Corresponding panels in Figure 6 1 (1 \u00b7 \u00b7 \u00b7 1) (0.1 \u00b7 \u00b7 \u00b7 0.1) (a.1) (b.1) and (c.1) 2",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 528,
                    "end": 535,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 616,
                    "end": 624,
                    "text": "Figure 6",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "9"
        },
        {
            "text": "(1 \u00b7 \u00b7 \u00b7 1) (0.5 \u00b7 \u00b7 \u00b7 0.5) (a.2) (b.2) and (c.2) error rate of Radial DWD still decreases to zero as dimension grows, however, the error rates of its competitors goes to 1 (or above 60% for LASSO). The average of the false positive and the false negative rate is illustrated in panel (c2) where a similar pattern as (c1) can be observed, except that even in low dimensions, kernel SVMs did not work as well as Radial DWD. When dimension is high, all Radial DWD's competing classifiers have error rates around about 50% (i.e., essentially random choice). We also studied several other examples [see Supplement in Xiong, Dittmer and Marron (2015) ]. They show fairly similar results. As suggested by our current simulations, Radial DWD outperforms MD, LASSO (linear, Quadratic, RBF) SVM and linear DWD when the radial separation is the key player to discriminate classes. As noted before, the full set of simulations shown in Section 3, involving many replications, was done in a few hours.",
            "cite_spans": [
                {
                    "start": 613,
                    "end": 645,
                    "text": "Xiong, Dittmer and Marron (2015)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "9"
        },
        {
            "text": "Radial DWD performs well with this type of data because of the particular geometry. In Figure 1 , we show that there are scaling issues with these coverage vectors as data objects, which are handled by dividing by the sum of the entries. This transformation means the data live on the unit simplex, hence, we study its geometry. Furthermore, because the dominant spikes in Figure 1 are in different locations, the data negative samples are widely distributed around the simplex, in many different directions. We tried to illustrate this phenomenon with a grossly simplified (because human perception tends to fail beyond 3 dimensions) toy example in Figure 2 . But it is the major exaggeration of this effect, that naturally occurs in this HDLSS context, that drives the major breakthrough of Radial DWD relative to the existing competitors (which were not designed for this setting).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 87,
                    "end": 95,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 373,
                    "end": 381,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 650,
                    "end": 658,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "9"
        },
        {
            "text": "4.1. Formulate the optimization problem. To set notation, let n denote the number of training d-vectors x i with corresponding class labels y i \u2208 {\u22121, +1}, i = 1 \u00b7 \u00b7 \u00b7 n. We let X denote the d \u00d7 n matrix whose columns are x i . Let e denote an n-vector of 1s. Let n + = n i=1 I {y i =+1} and n \u2212 = n i=1 I {y i =\u22121} = n \u2212 n + be the sample size of the +1 class and the \u22121 class, respectively. Denote O \u2208 R d as the center of a candidate separating sphere, and let R \u2208 R + be the radius, and define the signed residual of the ith data point asr i = y i (R \u2212 x i \u2212 O 2 ), where \u00b7 2 represents the Euclidean norm. We would like to search for O and R such thatr i are positive and large, which requires the +1 class to lie inside and the \u22121 class to lie outside the hypersphere. However, in order to incorporate the case when the 2 classes are not separable by a hypersphere, we allow classification error by adding nonnegative \"slack variable\" \u03b5 i , as in Burges (1998) and Marron, Todd and Ahn (2007) , and define perturbed residuals as",
            "cite_spans": [
                {
                    "start": 953,
                    "end": 966,
                    "text": "Burges (1998)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 971,
                    "end": 998,
                    "text": "Marron, Todd and Ahn (2007)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": "We now define the optimization problem for Radial DWD as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": "where r is a vector of r i , \u03b5 is a vector of \u03b5 i and r \u2265 0 and \u03b5 \u2265 0 are in the component-wise sense, and C is the penalty parameter of misclassification, as appears in SVM and DWD. It can be seen that the influence of the \u22121 class data decreases as they get further away from the separating hypersphere. The influence shrinks to zero for the \u22121 class data located at infinity. However, this is not true for the +1 class (because of the penalty term). Following Marron, Todd and Ahn (2007) , we linearize the objective function by defining \u03c1 i = (r i + 1 r i )/2 and \u03c3 i = ( 1 r i \u2212 r i )/2, so that 1",
            "cite_spans": [
                {
                    "start": 463,
                    "end": 490,
                    "text": "Marron, Todd and Ahn (2007)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": ". . , n} to the second order cone constraint",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": "where the Second Order Cone of dimension k is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": "One can show that when the 2 classes are separable by using a hypersphere, this relaxation will not change the optimal solution. By the transformation of 1 r i and the substitution with Second Order Cone constraints, the optimization problem becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": "This problem is almost a Second Order Cone Program except that the equality constraints",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": "are nonlinear (which also makes the problem nonconvex). We use the first order Taylor expansion iteratively to approximate the nonlinear equalities by linear ones, which is detailed in the following algorithm in Section 4.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Radial DWD optimization."
        },
        {
            "text": "An iterative algorithm to numerically solve radial DWD. We consider applying the first order Taylor expansion iteratively to bypass the nonlinearity of the equality constraints and numerically solve Radial DWD:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "Initialization (Step 0): Choose an initial center of the separating hypersphere and denote it as O 0 (e.g., the mean or the coordinate-wise median of the +1 class training data), let the initial objective value be Obj 0 = \u22121 (an arbitrary negative number).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "The iteration at",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "Step k: k \u2265 1. Apply the first order Taylor expansion on d i around O k\u22121 , that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "Notice that d \u2032 i is a linear function of O. By substituting d i with d \u2032 i , the optimization becomes a valid Second Order Cone Program and could be solved for O k and R k using SDPT3. Let Obj k be the current objective value at step k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "Stop: if |Obj k \u2212 Obj k\u22121 | < \u01eb, where \u01eb is a predetermined precision parameter.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "At each step k, to approximate well the nonlinear terms by using the first order Taylor expansion, we further confine O k in a neighborhood of O k\u22121 (the solution computed from the previous step) by adding one more constraint: O k \u2212 O k\u22121 2 \u2264 \u03b4 k , where \u03b4 k \u2208 R + is called the step length parameter. A small \u03b4 k guarantees the precision of the Taylor expansion but may slow down the computation. This additional constraint is a Second Order Cone constraint (\u03b4 k , O k \u2212 O k\u22121 ) \u2208 S d+1 so that we still end up with a valid Second Order Cone Program at step k. In our current data analysis, we choose \u01eb = 10 \u22124 and \u03b4 k = 10 \u22123 . The choice of penalty C will be revisited after the discussion of Radial DWD optimality conditions in Section 4.3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "4.3. The dual problem of radial DWD. To gain more insights about Radial DWD optimization, it is useful to give the dual formulation of the Second Order Cone Program (at the kth step). Let w k\u22121",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "2 and they are functions of x i (since O k\u22121 is computed from the previous step). After some algebra, we could formulate the dual program at step k as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "The primal and dual problems can be expressed more compactly in matrixvector form. Keep all the defined notation unchanged and denote y as an n-vector of y i , z an n-vector of z i , \u03c1 and \u03c3 the n-vectors of \u03c1 i and \u03c3 i , respectively, Y an n-by-n matrix with y i on the diagonal. Additionally, let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "Then, the primal-dual pair becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "where \u221a z is a n-vector with \u221a z i as entries.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "One can show the existence of strict feasible solutions to both the primal and dual problems. Since the primal and the dual are convex, it follows that the solution of the following optimality conditions are guaranteed to be optimal or, equivalently, the following equations are sufficient and necessary optimality conditions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "It is important to note that the optimal radius is strictly positive in case the 2 training classes are separable and the penalty term C is large enough, which is shown in Theorem 1 [see the supplementary materials in Xiong, Dittmer and Marron (2015) ]. If this is true, we could replace the condition {R \u2265 0, y T z \u2264 0, R(y T z) = 0} by {R > 0, y T z = 0}. As one will see in Section 4.4, this condition gives an insight to the Radial DWD optimization. Besides, Theorem 1 also implies that the choice of the penalty parameter C should satisfy the following: C(d k\u22121 i ) 2 > 1 for all d i , i \u2208 {i : y i = \u22121}. Solving the primal/dual problem in an ultra high dimension may be inefficient. To deal with this issue, we first factor the data matrix X using a QR decomposition, for example, X = QU where Q \u2208 R d\u00d7n has orthonormal columns and U \u2208 R n\u00d7n is an upper triangular matrix. Then we solve the optimization problem by replacing X by U and call it a reduced problem. The reduced problem could be solved more efficiently because it shrinks the intrinsic dimension of the problem from d to the sample size n. Note that it is fairly easy to recover X from U once we solve the reduced problem. The reduced problem does not change the optimal solution or the optimal value, which is shown in Theorem 2 [see the supplementary materials in Xiong, Dittmer and Marron (2015) ].",
            "cite_spans": [
                {
                    "start": 218,
                    "end": 250,
                    "text": "Xiong, Dittmer and Marron (2015)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1336,
                    "end": 1368,
                    "text": "Xiong, Dittmer and Marron (2015)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "MD, LASSO logistic regression, SVM and DWD, perform poorly with high classification error. Meanwhile, kernel SVM shows a very limited improvement over its linear counterpart in high dimensions. Since standard nonlinear methods, including kernel methods, require a type of \"data richness,\" that is not present in the virus hunting problem. In particular, they work well in situations (such as all the usual machine learning examples) where training data can be found in all of the various regions where the test data will appear. But in our particular data context, that completely breaks down, so all the classical nonlinear methods fare just as poorly as the linear ones. By using a much more appropriate spherical separating boundary, Radial DWD shows both low false positive and low false negative classification error. These are shown by real data analysis and simulation studies. Its computation through solving a sequence of Second Order Cone Programs is efficient, even with high-dimensional data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        },
        {
            "text": "We believe Radial DWD will be applicable in some settings beyond virus hunting. This will happen in classification contexts where there is one class with relatively small variation, and the other with much larger variation tending toward a number of quite divergent directions. For example, cancer is a disease of sometimes massive disruption of the genome, and these disruptions can go in many diverse directions, while the normal genome is far more stable. Another potential for this methodology comes in imaging bones and cartilage, where the normal population is relatively homogeneous, but severe wear and other types of abnormalities can go in many directions in the image space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4.2."
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Second-order cone programming. Math. Program",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Alizadeh",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Goldfarb",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "95",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "A tutorial on support vector machines for pattern recognition",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "J C"
                    ],
                    "last": "Burges",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Data Min. Knowl. Discov",
            "volume": "2",
            "issn": "",
            "pages": "955--974",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A selective overview of variable selection in high dimensional feature space",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Statist. Sinica",
            "volume": "20",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Sequencing studies in human genetics: Design and interpretation",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Goldstein",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Allen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Keebler",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "H"
                    ],
                    "last": "Margulies",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Petrou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Petrovski",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sunyaev",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Nat. Rev. Genet",
            "volume": "14",
            "issn": "",
            "pages": "460--470",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Next-generation sequencing: Methodology and application",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Grada",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Weinbrecht",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J. Invest. Dermatol",
            "volume": "133",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Geometric representation of high dimension, low sample size data",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hall",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Neeman",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "J. R. Stat. Soc. Ser. B Stat. Methodol",
            "volume": "67",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Robust centroid based classification with minimum error rates for high dimension, low sample size data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "J. Statist. Plann. Inference",
            "volume": "139",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "PCA consistency in high dimension, low sample size context",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jung",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Ann. Statist",
            "volume": "37",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Statistical significance of clustering for high-dimension, low-sample size data",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "N"
                    ],
                    "last": "Hayes",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nobel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "J. Amer. Statist. Assoc",
            "volume": "103",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Distance-weighted discrimination",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Todd",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ahn",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J. Amer. Statist. Assoc",
            "volume": "102",
            "issn": "",
            "pages": "1267--1271",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Sequencing technologies-the next generation",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Metzker",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Nat. Rev. Genet",
            "volume": "11",
            "issn": "",
            "pages": "31--46",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Cancer genome-sequencing study design",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Mwenifumbo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Marra",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Nat. Rev. Genet",
            "volume": "14",
            "issn": "",
            "pages": "321--332",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Weighted distance weighted discrimination and its asymptotic properties",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Qiao",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "H"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Todd",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J. Amer. Statist. Assoc",
            "volume": "105",
            "issn": "",
            "pages": "401--414",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Disease-targeted sequencing: A cornerstone in the clinic",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "L"
                    ],
                    "last": "Rehm",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Nat. Rev. Genet",
            "volume": "14",
            "issn": "",
            "pages": "295--300",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Learning with Kernels",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Kernel Methods for Pattern Analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shawe-Taylor",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Cristianini",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Consistency of sparse PCA in high dimension, low sample size contexts",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J. Multivariate Anal",
            "volume": "115",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Regression shrinkage and selection via the lasso",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "J. Roy. Statist. Soc. Ser. B",
            "volume": "58",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "SDPT3-a MATLAB software package for semidefinite-quadratic-linear programming",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "H"
                    ],
                    "last": "Tutuncu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "C"
                    ],
                    "last": "Toh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Todd",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "The Nature of Statistical Learning Theory",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "N"
                    ],
                    "last": "Vapnik",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Middle East respiratory syndrome coronavirus (MERS-CoV) summary and literature update-as of 9",
            "authors": [],
            "year": 2014,
            "venue": "World Health Organization WHO",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Supplement to: \"Virus hunting\" using Radial Distance Weighted Discrimination",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Dittmer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Marron",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1214/15-AOAS869SUPP"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "PCA consistency for the power spiked model in high-dimensional settings",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yata",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aoshima",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J. Multivariate Anal",
            "volume": "122",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Carolina at Chapel Hill North Carolina 27599-3260 USA E-mail: xiongj@unc.edu D. P. Dittmer Lineberger Comprehensive Cancer Center University of North Carolina at Chapel Hill 450 West Drive, CB# 7295 Chapel Hill, North Carolina 27599-7295 USA E-mail: dirk dittmer@med.unc",
            "authors": [],
            "year": null,
            "venue": "J. S",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Overlaid plot of 2 normalized data vectors from the HSV-1 positive (the +1) class in the top panel and 3 data vectors from the HSV-1 negative (the \u22121) class in the lower panel, all with different colors. The overall entries of the +1 data vectors are relatively small and have quite comparable amplitudes, while the entries of the \u22121 data vectors have \"spikes\" (which are located at quite divergent positions).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2. A simplified example of normalized data vectors of the positive (+1 class) data (red plus signs) and the negative (\u22121 class) data points (blue circles). The unit simplex is shown as the gray triangle. Because there are many zeros in the \u22121 data vectors, they typically locate at the vertices of the unit simplex while the positives are closer to the center.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "An HSV-1 classification example to compare the performance of RBF kernel SVM and Radial DWD. Trained on red plus signs versus blue circles, the former method endures a high false positive error since many negative test samples (gray x-symbols) are on the same side of the separating boundary as the positive class, while Radial DWD successfully classified all positive HSV-1 samples (magenta asterisks) and related viruses (green asterisks) with no false positive.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "A simulated example illustrating the potential for improved performance of Radial DWD, in the spirit ofFigure 4. Class +1 is shown as red pluses, \u22121 as blue circles, test samples as gray x symbols. The vertical axis shows random heights to visually separate the points, along with kernel density estimates (i.e., smooth histograms). The separating boundaries are calculated using the following:(a) MD, (b) Linear SVM, (c) Linear DWD, (d) LASSO, (e) RBF SVM and (f) Radial DWD. Except Radial DWD, all the other methods have poor classification performance for the test samples, which should be mostly to the left of the dashed line in each case.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "A simulation study illustrating the potential for improved performance of Radial DWD. The false positive rate is depicted in panels (a1) and (a2) under each parameter setting, with the corresponding false negative rate (under the same training setup) in (b1) and (b2). The average of the false positive and false negative rate is shown in panels (c1) and (c2), respectively. Classification error is calculated for the following: \u2212\u00b7 MD \u00b7\u00b7 LASSO, \u2212\u2212 Linear/Quadratic/RBF SVM, \u2212 Linear DWD and \u2212 Radial DWD (RDWD).",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgments. The authors would like to thank the Editor, Associate Editor and referees for their insightful, constructive comments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        },
        {
            "text": "Supplement to: \"Virus hunting\" using Radial Distance Weighted Discrimination. (DOI: 10.1214/15-AOAS869SUPP; .pdf). In the supplementary materials, we first introduce some useful biology background for virus detection in Section 1, DNA alignment process in Section 2, and then discuss the insights of the Dirichlet distribution in Section 3. Real data examples and simulation studies are included in Sections 4 and 5, respectively. Theorems and proofs are in Section 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUPPLEMENTARY MATERIAL"
        },
        {
            "text": "Interpretation of the radial DWD dual problem. Assume that the two classes are separable with a \"proper\" hypersphere (a hypersphere with nonzero radius R > 0) so that y T z = 0 is obtained at optima. Notice that y T z = 0 implies e T + z + = e T \u2212 z \u2212 , where z + (z \u2212 ) is the subvector of z corresponding to the +1 class (\u22121 class) and e + (e \u2212 ) the corresponding vector of ones. It makes sense to scale z such that e T + z + = e T \u2212 z \u2212 = 1. We can write z as \u03b7z * , where \u03b7 is a positive scalar and z * satisfies the additional scaling condition. By maximizing the dual objective function with respect to \u03b7 for a fixed z,.Equivalently, the dual objective function becomeswhere P is the index set of the +1 class, and N the index set of the \u22121 class., i \u2208 P , and it can be interpreted as an average distance from the current center of the separating sphere to the +1 class data points. A similar RADIAL DWD 19 interpretation applies for i\u2208N d k\u22121 i z * i . When the two classes are separable (and R > 0), the positive (negative) data points will be located inside (outside) the separating sphere so that the average distances of negative points are larger than that of the positive ones, which implies> 0 is true when two classes are separable. Note that \u2212d T k\u22121 Y z * is a measure of separability of the 2 classes and the bigger the absolute value, the bigger the separability. Meanwhile, w k\u22121is a vector of unit Euclidean norm, pointing from the current center to each data point. Define the centroid of the +1 (\u22121) class as the convex combination of w k\u22121 i , i \u2208 P (or i \u2208 N , resp.) under weights z * i . Therefore, i\u2208Ni is the vector pointing from the centroid of the +1 class to the centroid of the \u22121 class, and its Euclidean norm scaled by \u03b4 k is also a measure of separability. As a consequence, the whole denominator of (4.6) is positive and is a measure of separability of the 2 classes. To ensure optima, the dual problem minimizes the separability between classes divided by the square of the sum of the square roots of the convex weights.Note that in some situations, the proportions of the 2 classes in the data set may not reflect the real proportions in a target population due to sampling bias, or the 2 classes are extremely unbalanced. The separating boundary tends to be closer to the class with smaller training sample size. In the case of biased sampling or unbalanced data, a weighted version of Radial DWD is more appropriate. Qiao et al. (2010) developed a weighting scheme to improve linear DWD and we follow the same line to set up weighted Radial DWD, by optimizing the following objective function:subject to the same set of constraints defined before. Note that w(y i ) is the weight associated with the ith training data point and it only depends on the class label y i . In our data analysis we use w(+1) = n \u2212 n + +n \u2212 ; w(\u22121) = n + n + +n \u2212 as default. The above discussion about the Radial DWD optimization could be easily generalized to the case when weights are applied.",
            "cite_spans": [
                {
                    "start": 2460,
                    "end": 2478,
                    "text": "Qiao et al. (2010)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "4.4."
        }
    ]
}