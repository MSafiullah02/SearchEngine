{
    "paper_id": "a8d72ea1d99c07cd1a99cebe3340ac433ca67fce",
    "metadata": {
        "title": "Dual-Component Deep Domain Adaptation: A New Approach for Cross Project Software Vulnerability Detection",
        "authors": [
            {
                "first": "Van",
                "middle": [],
                "last": "Nguyen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Clayton",
                        "country": "Australia"
                    }
                },
                "email": "van.nk@monash.edu"
            },
            {
                "first": "Trung",
                "middle": [],
                "last": "Le",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Clayton",
                        "country": "Australia"
                    }
                },
                "email": "trunglm@monash.edu"
            },
            {
                "first": "Olivier",
                "middle": [],
                "last": "De Vel",
                "suffix": "",
                "affiliation": {},
                "email": "olivier.devel@dst.defence.gov.au"
            },
            {
                "first": "Paul",
                "middle": [],
                "last": "Montague",
                "suffix": "",
                "affiliation": {},
                "email": "paul.montague@dst.defence.gov.au"
            },
            {
                "first": "John",
                "middle": [],
                "last": "Grundy",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Clayton",
                        "country": "Australia"
                    }
                },
                "email": "john.grundy@monash.edu"
            },
            {
                "first": "Dinh",
                "middle": [],
                "last": "Phung",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Clayton",
                        "country": "Australia"
                    }
                },
                "email": "dinh.phung@monash.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Owing to the ubiquity of computer software, software vulnerability detection (SVD) has become an important problem in the software industry and computer security. One of the most crucial issues in SVD is coping with the scarcity of labeled vulnerabilities in projects that require the laborious manual labeling of code by software security experts. One possible solution is to employ deep domain adaptation (DA) which has recently witnessed enormous success in transferring learning from structural labeled to unlabeled data sources. Generative adversarial network (GAN) is a technique that attempts to bridge the gap between source and target data in the joint space and emerges as a building block to develop deep DA approaches with state-of-the-art performance. However, deep DA approaches using the GAN principle to close the gap are subject to the mode collapsing problem that negatively impacts the predictive performance. Our aim in this paper is to propose Dual Generator-Discriminator Deep Code Domain Adaptation Network (Dual-GD-DDAN) for tackling the problem of transfer learning from labeled to unlabeled software projects in SVD to resolve the mode collapsing problem faced in previous approaches. The experimental results on real-world software projects show that our method outperforms stateof-the-art baselines by a wide margin.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In the software industry, software vulnerabilities relate to specific flaws or oversights in software programs which allow attackers to expose or alter sensitive information, disrupt or destroy a system, or take control of a program or computer system. The software vulnerability detection problem has become an important issue in the software industry and in the field of computer security. Computer software development employs of a vast variety of technologies and different software development methodologies, and much computer software contains vulnerabilities. This has necessitated the development of automated advanced techniques and tools that can efficiently and effectively detect software vulnerabilities with a minimal level of human intervention. To respond to this demand, many vulnerability detection systems and methods, ranging from open source to commercial tools, and from manual to automatic methods have been proposed and implemented. Most of the previous works in software vulnerability detection (SVD) [1, 8] have been developed based on handcrafted features which are manually chosen by knowledgeable domain experts who may have outdated experience and underlying biases. In many situations, handcrafted features normally do not generalize well. For example, features that work well in a certain software project may not perform well in other projects. To alleviate the dependency on handcrafted features, the use of automatic features in SVD has been studied recently [11] [12] [13] . These works have shown the advantages of automatic features over handcrafted features in the context of software vulnerability detection.",
            "cite_spans": [
                {
                    "start": 1026,
                    "end": 1029,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1030,
                    "end": 1032,
                    "text": "8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1494,
                    "end": 1498,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1499,
                    "end": 1503,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1504,
                    "end": 1508,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, most of these approaches lead to another crucial issue in SVD research, namely the scarcity of labeled projects. Labelled vulnerable code is needed to train these models, and the process of labeling vulnerable source code is very tedious, timeconsuming, error-prone, and challenging even for domain experts. This has led to few labeled projects compared with the vast volume of unlabeled ones. A viable solution is to apply transfer learning or domain adaptation which aims to devise automated methods that make it possible to transfer a learned model from the source domain with labels to the target domains without labels. Studies in domain adaptation can be broadly categorized into two themes: shallow [6] and deep domain adaptations [3, 14, 18] . These recent studies have shown the advantages of deep over shallow domain adaptation (i.e., higher predictive performance and capacity to tackle structural data). Deep domain adaptation encourages the learning of new representations for both source and target data in order to minimize the divergence between them [3, 14, 18] . The general idea is to map source and target data to a joint feature space via a generator, where the discrepancy between the source and target distributions is reduced. Notably, the work of [3, 18] employed generative adversarial networks (GANs) [4] to close the gap between source and target data in the joint space. However, most of aforementioned works mainly focus on transfer learning in the computer vision domain. The work of [16] is the first work which applies deep domain adaptation to SVD with promising predictive performance on real-world source code projects. The underlying idea is to employ the GAN to close the gap between the source and target domains in the joint space and enforce the clustering assumption [2] to utilize the information carried in the unlabeled target samples in a semi-supervised context.",
            "cite_spans": [
                {
                    "start": 715,
                    "end": 718,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 747,
                    "end": 750,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 751,
                    "end": 754,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 755,
                    "end": 758,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1076,
                    "end": 1079,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1084,
                    "end": 1087,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1281,
                    "end": 1284,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1285,
                    "end": 1288,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1337,
                    "end": 1340,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1524,
                    "end": 1528,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1818,
                    "end": 1821,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "GANs are known to be affected by the mode collapsing problem [5, 7, 10, 17] . In particular, the study in [17] recently studied the mode collapsing problem and further classified this into the missing mode problem i.e., the generated samples miss some modes in the true data, and the boundary distortion problem i.e., the generated samples can only partly recover some modes in the true data. It is certain that deep domain adaptation approaches that use the GAN principle will inherently encounter both the missing mode and boundary distortion problems. Last but not least, deep domain adaptation approaches using the GAN principle also face the data distortion problem. The representations of source and target examples in the joint feature space degenerate to very small regions that cannot preserve the manifold/clustering structure in the original space.",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 64,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 65,
                    "end": 67,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 68,
                    "end": 71,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 72,
                    "end": 75,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 106,
                    "end": 110,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our aim in this paper is to address not only deep domain adaptation mode collapsing problems but also boundary distortion problems when employing the GAN as a principle in order to close the gap between source and target data in the joint feature space. Our two approaches are: i) apply manifold regularization for enabling the preservation of manifold/clustering structures in the joint feature space, hence avoiding the degeneration of source and target data in this space; and ii) invoke dual discriminators in an elegant way to reduce the negative impacts of the missing mode and boundary distortion problems in deep domain adaptation using the GAN principle as mentioned before. We name our mechanism when applied to SVD as Dual Generator-Discriminator Deep Code Domain Adaptation Network (Dual-GD-DDAN). We empirically demonstrate that our Dual-GD-DDAN can overcome the missing mode and boundary distortion problems which is likely to happen as in Deep Code Domain Adaptation (DDAN) [16] in which the GAN was solely applied to close the gap between the source and target domains in the joint space (see the discussion in Sects. 2.3 and 3.3, and the visualization in Fig. 3 ). In addition, we incorporate the relevant approaches -minimizing the conditional entropy and manifold regularization with spectral graph -proposed in [16] to enforce the clustering assumption [2] and arrive at a new model named Dual Generator-Discriminator Semi-supervised Deep Code Domain Adaptation Network (Dual-GD-SDDAN). We further demonstrate that our Dual-GD-SDDAN can overcome the mode collapsing problem better than SCDAN in [16] , hence obtaining better predictive performance.",
            "cite_spans": [
                {
                    "start": 989,
                    "end": 993,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1331,
                    "end": 1335,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1373,
                    "end": 1376,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1615,
                    "end": 1619,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 1172,
                    "end": 1178,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "We conducted experiments using the data sets collected by [13] , that consist of five real-world software projects: FFmpeg, LibTIFF, LibPNG, VLC and Pidgin to compare our proposed Dual-GD-DDAN and Dual-GD-SDDAN with the baselines. The baselines consider to include VULD (i.e., the model proposed in [12] without domain adaptation), MMD, DIRT-T, DDAN and SCDAN as mentioned [16] and D2GAN [15] (a variant of the GAN using dual-discriminator to reduce the mode collapse for which we apply this mechanism in the joint feature space). Our experimental results show that our proposed methods are able to overcome the negative impact of the missing mode and boundary distortion problems inherent in deep domain adaptation approaches when solely using the GAN principle as in DDAN and SCDAN [16] . In addition, our method outperforms the rival baselines in terms of predictive performance by a wide margin. ",
            "cite_spans": [
                {
                    "start": 58,
                    "end": 62,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 299,
                    "end": 303,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 373,
                    "end": 377,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 388,
                    "end": 392,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 784,
                    "end": 788,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "is also a sequence of L embedding vectors. We wish to bridge the gap between the source and target domains in the joint feature space. This allows us to transfer a classifier trained on the source domain to predict well on the target domain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "To handle sequential data in the context of domain adaptation of software vulnerability detection, the work of [16] proposed an architecture referred to as the Code Domain Adaptation Network (CDAN). This network architecture recruits a Bidirectional RNN to process the sequential input from both source and target domains (i.e.,",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 115,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Deep Code Domain Adaptation with a Bidirectional RNN"
        },
        {
            "text": ". A fully connected layer is then employed to connect the output layer of the Bidirectional RNN with the joint feature layer while bridging the gap between the source and target domains. Furthermore, inspired by the Deep Domain Adaptation approach [3] , the authors employ the source classifier C to classify the source samples, the domain discriminator D to distinguish the source and target samples and propose Deep Code Domain Adaptation (DDAN) whose objective function is as follows:",
            "cite_spans": [
                {
                    "start": 248,
                    "end": 251,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Deep Code Domain Adaptation with a Bidirectional RNN"
        },
        {
            "text": "We observe that DDAN suffers from several shortcomings. First, the data distortion problem (i.e., the source and target data in the joint space might collapse into small regions) may occur since there is no mechanism in DDAN to circumvent this. Second, since DDAN is based on the GAN approach, DDAN might suffer from the mode collapsing problem [5, 17] . In particular, [17] has recently studied the mode collapsing problem of GANs and discovered that they are also subject to i) the missing mode problem (i.e., in the joint space, either the target data misses some modes in the source data or vice versa) and ii) the boundary distortion problem (i.e., in the joint space either the target data partly covers the source data or vice versa), which makes the target distribution significantly diverge from the source distribution. As shown in Fig. 1 , both the missing mode and boundary distortion problems simultaneously happen since the target distribution misses source mode 2, while the source distribution can only partly cover the target mode 2 in the target distribution and the target distribution can only partly cover the source mode 1 in the source distribution. ",
            "cite_spans": [
                {
                    "start": 345,
                    "end": 348,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 349,
                    "end": 352,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 370,
                    "end": 374,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 842,
                    "end": 848,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "The Shortcomings of DDAN"
        },
        {
            "text": "We employ two discriminators (namely, D and D T (G T (x T )) are possibly high. This helps to mitigate the issues of missing mode and boundary distortion since as in Fig. 1 , if the target mode 1 can only partly cover the source mode 1, then D T cannot receive large values from source mode 1. Another important aspect of our approach is to maintain the cluster/manifold structure of source and target data in the joint space via the manifold regularization to avoid the data distortion problem.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 172,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Key Idea of Our Approach"
        },
        {
            "text": "To address the two inherent problems in the DDAN mentioned in Sect. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "where \u03b8 > 0. Note that a high value of \u03b8 encourages D S and D T place higher values on G S x S and G T x T respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "Updating the Source Classifier. The source classifier is employed to classify the source examples with labels as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "where specifies the crossentropy loss function for the binary classification (e.g., using cross-entropy).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "Updating the Generators. The two generators G S and G T are trained to i) maintain the manifold/cluster structures of source and target data in their original spaces to avoid the data distortion problem and ii) move the target samples toward the source samples in the joint space and resolve the missing mode and boundary distortion problems in the joint space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "To maintain the manifold/cluster structures of source and target data in their original spaces, we propose minimizing the manifold regularization term as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "in which the weights are defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "are the last hidden states of the bidirectional RNN with input x.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "To move the target samples toward the source samples and resolve the missing mode and boundary distortion problems in the joint space, we propose minimizing the following objective function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "Moreover, the source generator G S has to work out the representation that is suitable for the source classifier, hence we need to minimize the following objective function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "Finally, to update G S and G T , we need to minimize the following objective function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "where \u03b1, \u03b2 > 0 are two non-negative parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Deep Code Domain Adaptation Network"
        },
        {
            "text": "Below we explain why our proposed Dual-GD-DDAN is able to resolve the two critical problems that occur with the DDAN approach. First, if x S i and x S j are proximal to each other and are located in the same cluster, then their representations h(x S i ) and h(x S j ) are close and hence, the weight \u00b5 i j is large. This implies G S (x S i ) and G S (x S j ) are encouraged to be close in the joint space because we are minimizing \u00b5 i j ||G S (x S i ) \u2212 G S (x S j )|| 2 . This increases the chance of the two representations residing in the same cluster in the joint space. Therefore, Dual-GD-DDAN is able to preserve the clustering structure of the source data in the joint space. By using the same argument, we reach the same conclusion for the target domain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Rationale for Our Dual Generator-Discriminator Deep Code Domain Adaptation Network Approach"
        },
        {
            "text": "Second, following Eqs. (1, 2) , the discriminator D S is trained to encourage large values for the source modes (i.e., G S (x S )), while the discriminator D T is trained to produce large values for the target modes (i.e., G T (x T )). Moreover, as in Eq. (3), G s is trained to move the source domain examples x S to the high-valued region of D T (i.e., the target modes or G T (x T )) and G T is trained to move the target examples x T to the highvalued region of D S (i.e., the source modes or G S (x S )). As a consequence, eventually, the source modes (i.e., G S (x S )) and target modes (i.e., G T (x T )) overlap, while D S and D T place large values on both source (i.e., G S (x S )) and target (i.e., G T (x T )) modes. The mode missing problem is less likely to happen since, as shown in Fig. 1 , if the target data misses source mode 2, then D T cannot receive large values from source mode 2. Similarly, the boundary distortion problem is also less likely to happen since as in Fig. 1 , if the target mode 1 can only partly cover the source mode 1, then D T cannot receive large values from source mode 1. Therefore, Dual-GD-DDAN allows us to reduce the impact of the missing mode and boundary distortion problems, hence making the target distribution more identical to the source distribution in the joint space. Fig. 2 . The architecture of our Dual-GD-DDAN. The generators G S and G T take the sequential code tokens of the source domain and target domain in vectorial form respectively and map this sequence to the joint layer (i.e., the joint space). The vector representation of each statement x in source code is denoted by i. The discriminators D S and D T are invoked to discriminate the source and target data. The source classifier C is trained on the source domain with labels. We note that the source and target networks do not share parameters and are not identical.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 23,
                    "end": 29,
                    "text": "(1, 2)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 798,
                    "end": 804,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 990,
                    "end": 996,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1326,
                    "end": 1332,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "The Rationale for Our Dual Generator-Discriminator Deep Code Domain Adaptation Network Approach"
        },
        {
            "text": "Our proposed model can be incorporated with minimizing the conditional entropy and using the spectral graph to inspire the smoothness to enforce the clustering assumption [2] proposed in [16] ",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 174,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 187,
                    "end": 191,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Dual Generator-Discriminator Semi-supervised Deep Code Domain Adaptation Network"
        },
        {
            "text": "In this section, firstly, we compare our proposed Dual-GD-DDAN with VulDeePecker without domain adaptation, MMD, D2GAN, DIRT-T and DDAN using the architecture CDAN proposed in [16] . Secondly, we do Boundary Distortion Analysis to further demonstrate the efficiency of our proposed Dual-GD-DDAN in alleviating the boundary distortion problem caused by using the GAN principle. Finally, we compare our Dual-GD-SDDAN and SCDAN introduced in [16] .",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 180,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 439,
                    "end": 443,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Experimental Data Set. We use the real-world data sets collected by [13] , which contain the source code of vulnerable and non-vulnerable functions obtained from five real-world software projects, namely FFmpeg (#vul-funcs: 187, #non-vul-funcs: 5,427), LibTIFF (#vul-funcs: 81, #non-vul-funcs: 695), LibPNG (#vul-funcs: 43, #non-vulfuncs: 551), VLC (#vul-funcs: 25, #non-vul-funcs: 5,548) and Pidgin (#vul-funcs: 42, #non-vul-funcs: 8,268) where #vul-funcs and #non-vul-funcs is the number of vulnerable and non-vulnerable functions respectively. The data sets contain both multimedia (FFmpeg, VLC, Pidgin) and image (LibPNG, LibTIFF) application categories. In our experiment, data sets from the multimedia category were used as the source domain whilst data sets from the image category were used as the target domain (see Table 1 ). We split the data of the source domain into two random partitions containing 80% for training and 20% for validation. We also split the data of the target domain into two random partitions. The first partition contains 80% for training the models of VulDeePecker, MMD, D2GAN, DIRT-T, DDAN, Dual-GD-DDAN, SCDAN and Dual-GD-SDDAN without using any label information while the second partition contains 20% for testing the models. We additionally apply gradient clipping regularization to prevent over-fitting in the training process of each model. We implement eight mentioned methods in Python using Tensorflow which is an open-source software library for Machine Intelligence developed by the Google Brain Team. ",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 72,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 825,
                    "end": 832,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Experimental Setup"
        },
        {
            "text": "Code Domain Adaptation for a Fully Non-labeled Target Project. We investigate the performance of our proposed Dual-GD-DDAN compared with other methods including VulDeePecker (VULD) without domain adaptation [12] , DDAN [16] , MMD [14] , D2GAN [15] and DIRT-T [18] with VAP applied in the joint feature layer using the architecture CDAN introduced in [16] . The VulDeePecker method is only trained on the source data and then tested on the target data, while the MMD, D2GAN, DIRT-T, DDAN and Dual-GD-DDAN methods employ the target data without using any label information for domain adaptation.",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 219,
                    "end": 223,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 230,
                    "end": 234,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 243,
                    "end": 247,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 259,
                    "end": 263,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 350,
                    "end": 354,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "In Table 1 , the experimental results show that our proposed Dual-GD-DDAN achieves a higher performance for detecting vulnerable and non-vulnerable functions for most performance measures, including FNR, FPR, Recall, Precision and F1measure in almost cases of the source and target domains, especially for F1-measure. Particularly, our Dual-GD-DDAN always obtains the highest F1-measure in all cases. For example, for the case of the source domain (FFmpeg) and target domain (LibPNG), Dual-GD-DDAN achieves an F1-measure of 88.89% compared with an F1-measure of 84.21%, 84.21%, 80%, 77.78% and 75% obtained with DDAN, DIRT-T, D2GAN, MMD and VulDeePecker respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Quantitative Results. To quantitatively demonstrate the efficiency of our proposed Dual-GD-DDAN in alleviating the boundary distortion problem caused by using the GAN principle, we reuse the experimental setting in Sect. 5.2 [17] . The basic idea is, given two data sets S 1 and S 2 , to quantify the degree of cover of these two data sets. We train a classifier C 1 on S 1 , then test on S 2 and another classifier C 2 on S 2 , then test on S 1 . If these two data sets cover each other well with reduced boundary distortion, we expect that if C 1 predicts well on S 1 , then it should predict well on S 2 and vice versa if C 2 predicts well on S 2 , then it should predict well on S 1 . This would seem reasonable since if boundary distortion occurs (i.e., assume that S 2 partly covers S 1 ), then C 2 trained on S 2 would struggle to predict S 1 well which is much larger and possibly more complex. Therefore, we can utilize the magnitude of the accuracies and the accuracy gap of C 1 and C 2 when predicting their training and testing sets to assess the severity of the boundary distortion problem. Inspired by this observation, we compare our Dual-GD-DDAN with DDAN using the representations of the source and target samples in the joint feature space corresponding to their best models. In particular, for a given pair of source and target data sets and for comparing each method, we train a neural network classifier on the best representations of the source data set in the joint space, then predict on the source and target data set and do the same but swap the role of the source and target data sets. We then measure the difference of the corresponding accuracies as a means of measuring the severity of the boundary distortion. We choose to conduct such a boundary distortion analysis for two pairs of the source (FFmpeg and Pidgin) and target (LibPNG) domains. As shown in Table 2 , all gaps obtained by our Dual-GD-DDAN are always smaller than those obtained by DDAN, while the accuracies obtained by our proposed method are always larger. We can therefore conclude that our Dual-GD-DDAN method produces a better representation for source and target samples in the joint space and is less susceptible to boundary distortion compared with the DDAN method.",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 229,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 1887,
                    "end": 1894,
                    "text": "Table 2",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Boundary Distortion Analysis"
        },
        {
            "text": "Visualization. We further demonstrate the efficiency of our proposed Dual-GD-DDAN in alleviating the boundary distortion problem caused by using the GAN principle. Using a t-SNE [9] projection, with perplexity equal to 30, we visualize the feature distributions of the source and target domains in the joint space. Specifically, we project the source and target data in the joint space (i.e., G (x)) into a 2D space with domain adaptation (DDAN) and with dual-domain adaptation (Dual-GD-DDAN). In Fig. 3 , we observe these cases when performing domain adaptation from a software project (FFmpeg) to another (LibPNG). As shown in Fig. 3 , with undertaking domain adaptation (DDAN, the left figure) and dual-domain adaptation (Dual-GD-DDAN, the right figure) , the source and target data sampled are intermingled especially for Dual-GD-DDAN. However, it can be observed that DDAN when solely applying the GAN is seriously vulnerable to the boundary distortion issue. In particular, in the clusters/data modes 2, 3 and 4 (the left figure) , the boundary distortion issue occurs since the blue data only partly cover the corresponding red ones (i.e., the source and target data do not totally mix up). Meanwhile, for our Dual-GD-DDAN, the boundary distortion issue is much less vulnerable, and the mixing-up level of source and target data is significantly higher in each cluster/data mode. ",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 181,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 497,
                    "end": 503,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 629,
                    "end": 635,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 749,
                    "end": 756,
                    "text": "figure)",
                    "ref_id": null
                },
                {
                    "start": 1028,
                    "end": 1035,
                    "text": "figure)",
                    "ref_id": null
                }
            ],
            "section": "Boundary Distortion Analysis"
        },
        {
            "text": "Code Domain Adaptation. In this section, we compare the performance of our Dual-GD-SDDAN with Semi-supervised Deep Code Domain Adaptation (SCDAN) [16] on four pairs of the source and target domains. In Table 3 , the experimental results show that our Dual-GD-SDDAN achieves a higher performance than SCDAN for detecting vulnerable and non-vulnerable functions in terms of FPR, Precision and F1-measure in almost cases of the source and target domains, especially for F1-measure. For example, to the case of the source domain (VLC) and target domain (LibPNG), our Dual-GD-SDDAN achieves an F1-measure of 76.19% compared with an F1-measure of 72.73% obtained with SCDAN. These results further demonstrate the ability of our Dual-GD-SDDAN for dealing with the mode collapsing problem better than SCDAN [16] , hence obtaining better predictive performance in the context of software domain adaptation. ",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 150,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 799,
                    "end": 803,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 202,
                    "end": 209,
                    "text": "Table 3",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "Quantitative Results of Dual Generator-Discriminator Semi-supervised Deep"
        },
        {
            "text": "Software vulnerability detection (SVD) is an important problem in the software industry and in the field of computer security. One of the most crucial issues in SVD is to cope with the scarcity of labeled vulnerabilities in projects that require the laborious labeling of code by software security experts. In this paper, we propose the Dual Generator-Discriminator Deep Code Domain Adaptation Network (Dual-GD-DDAN) method to deal with the missing mode and boundary distortion problems which arise from the use of the GAN principle when reducing the discrepancy between source and target data in the joint space. We conducted experiments to compare our Dual-GD-DDAN method with the state-of-the-art baselines. The experimental results show that our proposed method outperforms these rival baselines by a wide margin in term of predictive performances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Supporting automated vulnerability analysis using formalized vulnerability signatures",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Almorsy",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Grundy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ibrahim",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering, ASE",
            "volume": "",
            "issn": "",
            "pages": "100--109",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Semi-supervised classification by low density separation",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chapelle",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zien",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "57--64",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Unsupervised domain adaptation by backpropagation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ganin",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lempitsky",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 32nd International Conference on Machine Learning",
            "volume": "37",
            "issn": "",
            "pages": "1180--1189",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Generative adversarial nets",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2672--2680",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Nips 2016 tutorial: Generative adversarial networks",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1701.00160"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Domain adaptation for object recognition: an unsupervised approach",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gopalan",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ruonan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 2011 International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "999--1006",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "MGAN: training generative adversarial nets with multiple generators",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hoang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Phung",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Learning Representation",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "VUDDY: a scalable approach for vulnerable code clone discovery",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Woo",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Oh",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Symposium on Security and Privacy",
            "volume": "",
            "issn": "",
            "pages": "595--614",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Visualizing data using t-SNE",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "M"
                    ],
                    "last": "Laurens",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Geoffrey",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "J. Mach. Learn. Res",
            "volume": "9",
            "issn": "",
            "pages": "2579--2605",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Learning generative adversarial networks from multiple data sources",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hoang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Vu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Bui",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Phung",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Maximal divergence sequential autoencoder for binary software vulnerability detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "VulDeePecker: a deep learning-based system for vulnerability detection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Cross-project transfer representation learning for vulnerable function discovery",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Industr. Inform",
            "volume": "14",
            "issn": "",
            "pages": "3289--3297",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Learning transferable features with deep adaptation networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 32nd International Conference on Machine Learning. Proceedings of Machine Learning Research",
            "volume": "37",
            "issn": "",
            "pages": "97--105",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Dual discriminator generative adversarial nets",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Vu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Phung",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Deep domain adaptation for vulnerable code function identification",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The International Joint Conference on Neural Networks (IJCNN)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A classification-based study of covariate shift in GAN distributions",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Santurkar",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Schmidt",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Madry",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research",
            "volume": "80",
            "issn": "",
            "pages": "4480--4489",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A DIRT-t approach to unsupervised domain adaptation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Shu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Bui",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Narui",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ermon",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An illustration of the missing mode and boundary distortion problems of DDAN. In the joint space, the target distribution misses source mode 2, while the source distribution can only partly cover the target mode 2 in the target distribution and the target distribution can only partly cover the source mode 1 in the source distribution.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "S and D T ) to classify the source and target examples and vice versa and two separate generators (namely, G S and G T ) to map the source and target examples to the joint space respectively. In particular, D S produces high values on the source examples in the joint space (i.e., G S (x S )) and low values on the target examples in the joint space (i.e., G T (x T )), while D T produces high values on the target examples in the joint space (i.e., G T (x T )) and low values on the source examples (i.e., G S (x S )). The generator G S is trained to push G S x S to the high value region of D T and the generator G T is trained to push G T (x T ) to the high value region of D S . Eventually, both D S (G S (x S )) and D S (G T (x T )) are possibly high and both D T (G S (x S ))",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "2.3, we employ two different generators G S and G T to map source and target domain examples to the joint space and two discriminators D S and D T to distinguish source examples against target examples and vice versa together with the source classifier C which is used to classify the source examples with labels as shown in Fig. 2. We name our proposed model as Dual Generator-Discriminator Deep Code Domain Adaptation Network (Dual-GD-DDAN). Updating the Discriminators. The two discriminators D S and D T are trained to distinguish the source examples against the target examples and vice versa as follows:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Configuration. For training the eight methods -VulDeePecker, MMD, D2GAN, DIRT-T, DDAN, Dual-GD-DDAN, SCDAN and Dual-GD-SDDAN -we use one-layer bidirectional recurrent neural networks with LSTM cells where the size of hidden states is in {128, 256} for the generators. For the source classifier and discriminators, we use deep feed-forward neural networks with two hidden layers in which the size of each hidden layer is in {200, 300}. We embed the opcode and statement information in the {150, 150} dimensional embedding spaces respectively (see our Supplementary Material for Data Processing and Embedding, available at https://app.box.com/s/aijcavbcp). We employ the Adam optimizer with an initial learning rate in {10 \u22123 , 10 \u22124 }. The mini-batch size is 64. The trade-off parameters \u03b1, \u03b2, \u03b3, \u03bb are in {10 \u22121 , 10 \u22122 , 10 \u22123 }, \u03b8 is in {0, 1} and 1/(2\u03c3 2 ) is in {2 \u221210 , 2 \u22129 }.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Performance results in terms of false negative rate (FNR), false positive rate (FPR), Recall, Precision and F1-measure of VulDeePecker (VULD), MMD, D2GAN, DIRT-T, DDAN and Dual-GD-DDAN for predicting vulnerable and non-vulnerable code functions on the testing set of the target domain (Best performance in bold).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "A 2D t-SNE projection for the case of the FFmpeg \u2192 LibPNG domain adaptation. The blue and red points represent the source and target domains in the joint space respectively. In both cases of the source and target domains, data points labeled 0 stand for non-vulnerable samples and data points labeled 1 stand for vulnerable samples.(Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "to form Dual Generator-Discriminator Semi-supervised Deep Code Domain Adaptation Network (Dual-GD-SDDAN). Please read our Supplementary Material for more technical details, available at https://app.box.com/s/aijcavbcp.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Accuracies obtained by the DDAN and Dual-GD-DDAN methods when predicting vulnerable and non-vulnerable code functions on the source and target domains. Note that tr src, ts tar, tr tar, ts src, and acc gap are the shorthands of train source, test target, train target, test source, and accuracy gap respectively. For the accuracy gap, a smaller value is better.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Performance results in terms of false negative rate (FNR), false positive rate (FPR), Recall, Precision and F1-measure of SCDAN and Dual-GD-SDDAN for predicting vulnerable/non-vulnerable code functions on the testing set of the target domain (Best performance in bold).FFmpeg \u2192 LibTIFF SCDAN 5.38% 14.29% 85.71% 57.14% 68.57% Dual-GD-SDDAN 3.01% 35.29% 64.71% 73.33% 68.75% Dual-GD-SDDAN 4.39% 11.11% 88.89% 66.67% 76.19%",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Group's Next Generation Technologies Program.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement. This research was supported under the Defence Science and Technology"
        }
    ]
}