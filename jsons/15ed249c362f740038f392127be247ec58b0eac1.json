{
    "paper_id": "15ed249c362f740038f392127be247ec58b0eac1",
    "metadata": {
        "title": "A Framework for Feature Selection to Exploit Feature Group Structures",
        "authors": [
            {
                "first": "Kushani",
                "middle": [],
                "last": "Perera",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Melbourne",
                    "location": {
                        "postCode": "3010",
                        "settlement": "Melbourne",
                        "region": "VIC",
                        "country": "Australia"
                    }
                },
                "email": "bperera@student.unimelb.edu.au"
            },
            {
                "first": "Jeffrey",
                "middle": [],
                "last": "Chan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "RMIT University",
                    "location": {
                        "postCode": "3000",
                        "settlement": "Melbourne",
                        "region": "VIC",
                        "country": "Australia"
                    }
                },
                "email": "jeffrey.chan@rmit.edu.au"
            },
            {
                "first": "Shanika",
                "middle": [],
                "last": "Karunasekera",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Melbourne",
                    "location": {
                        "postCode": "3010",
                        "settlement": "Melbourne",
                        "region": "VIC",
                        "country": "Australia"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Filter feature selection methods play an important role in machine learning tasks when low computational costs, classifier independence or simplicity is important. Existing filter methods predominantly focus only on the input data and do not take advantage of the external sources of correlations within feature groups to improve the classification accuracy. We propose a framework which facilitates supervised filter feature selection methods to exploit feature group information from external sources of knowledge and use this framework to incorporate feature group information into minimum Redundancy Maximum Relevance (mRMR) algorithm, resulting in GroupMRMR algorithm. We show that GroupMRMR achieves high accuracy gains over mRMR (up to \u223c35%) and other popular filter methods (up to \u223c50%). GroupMRMR has same computational complexity as that of mRMR, therefore, does not incur additional computational costs. Proposed method has many real world applications, particularly the ones that use genomic, text and image data whose features demonstrate strong group structures.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Feature selection is proven to be an effective method in preparing high dimensional data for machine learning tasks such as classification. The benefits of feature selection include increasing the prediction accuracy, reducing the computational costs and producing more comprehensible data and models. Among the three main feature selection methods, filter methods are preferred to wrapper and embedded methods in applications where the computational efficiency, classifier independence, simplicity, ease of use and the stability of the results are required. Therefore, filter feature selection remains an interesting topic in many recent research areas such as biomarker identification for cancer prediction and drugs discovery, text classification and predicting defective software [3] [4] [5] 10, 11, 16, 18] and has growing interest in big data applications [19] ; according to the Google Scholar search results, the number of research papers published related to filter methods in year 2018 is \u223c1,800 of which \u223c170 are in gene selection area.",
            "cite_spans": [
                {
                    "start": 784,
                    "end": 787,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 788,
                    "end": 791,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 792,
                    "end": 795,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 796,
                    "end": 799,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 800,
                    "end": 803,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 804,
                    "end": 807,
                    "text": "16,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 808,
                    "end": 811,
                    "text": "18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 862,
                    "end": 866,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Most of the existing filter methods perform feature selection based on the instance-feature data alone [7] . However, in real world datasets, there are external sources of correlations within feature groups which can improve the usefulness of feature selection. For example, the genes in genomic data can be grouped based on the Gene Ontology terms they are annotated with [2] to improve bio-marker identification for the tasks such as disease prediction and drugs discovery. The words in documents can be grouped according to their semantics to select more significant words which are useful in document analysis [14] . The nearby pixels in images can be grouped together based on their spatial locality to improve selection of pixels for image classification. In software data, software metrics can be grouped according to their granularity in the code to improve the prediction of defective software [11, 18] . In Sect. 4, using a text dataset as a concrete example, we demonstrate the importance of feature group information for filter feature selection to achieve good classification accuracy.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 106,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 373,
                    "end": 376,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 614,
                    "end": 618,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 903,
                    "end": 907,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 908,
                    "end": 911,
                    "text": "18]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Although feature group information have been used to improve feature selection in wrapper and embedded approaches [8, 12] , group information is only rarely used to improve the feature selection accuracy in filter methods. Yu et al. [19] proposes a group based filter method, GroupSAOLA (GSAOLA), yet being an online method, it achieves poor accuracy, which we show experimentally. The common method used by embedded methods to exploit feature group information is minimising the L 1 and L 2 norms of the feature weight matrix, while minimising the classification error. Depending on whether the features are encouraged from the same group [8] or different groups [12] , L 1 norm is used to cause inter group or intra group sparsity. Selecting features from different groups is shown to be more effective than selecting features from the same group [12] .",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 117,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 118,
                    "end": 121,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 233,
                    "end": 237,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 640,
                    "end": 643,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 664,
                    "end": 668,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 849,
                    "end": 853,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Motivated by these approaches, we show that squared L 0,2 norm minimization of the feature weight matrix can be used to encourage features from different feature groups in filter feature selection. We propose a generic framework which combines existing filter feature ranking methods with feature weight matrix norm minimisation and use this framework to incorporate feature group information in to mRMR objective [7] because mRMR algorithm achieves high accuracy and efficiency at the same time, compared to other filter methods [3, 4] . However, the proposed framework can be used to improve any other filter method, such as information gain based methods. As L 0 norm minimization is an NP-hard problem, we propose a greedy feature selection algorithm, GroupMRMR, to achieve the feature selection objective, which has the same computational complexity as the mRMR algorithm. We experimentally show that for the datasets with feature group structures, GroupMRMR obtains significantly higher classification accuracy than the existing filter methods. Our main contributions are as follows.",
            "cite_spans": [
                {
                    "start": 414,
                    "end": 417,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 530,
                    "end": 533,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 534,
                    "end": 536,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We propose a framework which supports the filter feature selection methods to utilise feature group information to improve their classification accuracy. -Using the proposed framework, we integrate feature group information into mRMR algorithm and propose a novel feature selection algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-Through extensive experiments we show that our algorithm obtains significantly higher classification accuracy than the mRMR and existing filter feature selection algorithms for no additional computational costs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Utilization of feature group information to improve prediction accuracy has been popular in embedded feature selection [8, 12, 17] . Among them, algorithms such as GroupLasso [8] encourage features from the same group while algorithms such as Uncorrelated GroupLasso [12] encourage features from different groups. We select the second approach as it is proven to be more effective for real data [12] . Filter feature selection is preferred over wrapper and embedded methods due to their classifier independence, computational efficiency and simplicity, yet have comparatively low prediction accuracy. However, most filter methods select the features based on the instance-feature data alone, which are coded in the data matrix, using information theoretic measures [7, 13, 15] . Some methods [20] use the feature group concept, yet the groups are also formed using instance-feature data to reduce feature redundancy. None of these methods take advantage of the external sources of knowledge about feature group structures. GSAOLA [19] is an online filter method which exploits feature groups, however we experimentally show that our method significantly outperforms it in terms of accuracy.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 122,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 123,
                    "end": 126,
                    "text": "12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 127,
                    "end": 130,
                    "text": "17]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 175,
                    "end": 178,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 267,
                    "end": 271,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 395,
                    "end": 399,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 765,
                    "end": 768,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 769,
                    "end": 772,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 773,
                    "end": 776,
                    "text": "15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 792,
                    "end": 796,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1030,
                    "end": 1034,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this section and Table 1 , we introduce the terms used later in the paper. Let C be the class variable of a dataset, D, and f i , f j any two feature variables.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 20,
                    "end": 27,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Preliminaries"
        },
        {
            "text": "Given that X and Y are two feature variables in D, with feature values x and y respectively, mutual information between X and Y , is given by ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1."
        },
        {
            "text": "f i and f j = Red(f i , f j ) = I(f i ; f j ). Given that W \u2208 R M \u00d7N , W i is the i th row of W , W ij is the j th element in W i , the squared L 0,2 norm of W is defined as W 2 0,2 = M i=1 ( W i 0 ) 2 = M i=1 N 2 i where N i = W i 0 = # (j|W ij = 0). For the scenarios in which the rows of W have different importance levels, we define W 2 0,2 = M i=1 i ( W i 0 ) 2 = M i=1 N 2 i i . i is the weight of W i . k is the required number of features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 3. The redundancy between"
        },
        {
            "text": "Ignoring the external sources of correlations within feature groups may result in poor classification accuracy for the datasets whose features show a group behaviour. We demonstrate this using mRMR algorithm as a concrete example, a filter method which otherwise achieves good accuracy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Background"
        },
        {
            "text": "mRMR Algorithm: mRMR objective for selecting a feature subset S \u2286 F of size k is as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Background"
        },
        {
            "text": "To achieve the above objective, mRMR selects one feature at a time to maximise the relevancy of the new feature x with the class variable and to minimise its redundancy with the already selected feature set, as shown in Eq. (2) .",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 227,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Motivation and Background"
        },
        {
            "text": "Example 1: Consider selecting two features from the dataset in Fig. 1 . In this dataset, each document is classified into one of the four types: Botany, Zoology, Physics or Agriculture. The rows represent the feature vector, the words which have occurred in the documents. 1 means the word has occurred within the document (or has occurred with high frequency) and 0 means otherwise. The relevancies of the features, Apple, Rice, Cow and Sheep are 0.549, 0.443, 0.311 and 0.311, respectively. mRMR first selects Apple, which has the highest relevancy. The redundancies of Rice, Cow and Sheep with respect to Apple are 0.07, 0.017 and 0.016, respectively. Therefore, mRMR next selects Rice, the feature with the highest relevancy redundancy difference, 0.373 (0.443 -0.07). Global mRMR optimisation approaches [15] A Class (a=1, r=0) 25% 0% 0% 50% A (a=1, s=0) 75% 0% 0% 25% B (a=0, r=1) 25% 0% 0% 25% A, B (a=0, s=1) 0% 50% 0% 0% Z (a=0, r=0) 0% 100% 100% 0% P, Z (a=0, s=0) 25% 50% 100% 25% P (a=1, r=1) 50% 0% 0% 25% B (a=1, s=1) 0% 0% 0% 50% A Exploiting Feature Group Semantics: Figure 2 shows the value pattern distribution of {Apple, Sheep} and {Apple, Rice} pairs within each class. In {Apple, Sheep}, the highest probability value pattern in each class is different from one another. Therefore, each value pattern is associated with a different class, which helps distinguishing all the document types from one another. In {Apple, Rice}, there is no such distinctive relationship between the value patterns and classes. Using the value pattern distribution, the classification algorithm cannot distinguish between the Zoology and Physics documents and between Agriculture and Botany documents. This shows that features from different groups have achieved better class discrimination.",
            "cite_spans": [
                {
                    "start": 809,
                    "end": 813,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 63,
                    "end": 69,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 1083,
                    "end": 1091,
                    "text": "Figure 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Motivation and Background"
        },
        {
            "text": "The reason behind the suboptimal result of the mRMR algorithm is its ignorance about the high level feature group structures. The words Apple and Rice form a group as they are plant names. Cow and Sheep form another group as they are animal names. The documents are classified according to whether they contain plant names or/and animal names, regardless of the exact plant or animal name they contain. Botany documents (d 1 -d 4 ) contain plant names (Apple or Rice) and no animal names. Zoology documents (d 5 -d 8 ) contain animal names (Cow or Sheep) and no plant names. This high level insight is not captured by the instance-feature data alone. Using feature group information as an external source of knowledge and encouraging features from different feature groups help solving this problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Background"
        },
        {
            "text": "We propose a framework which facilitates filter feature selection methods to exploit feature group information to achieve better classification accuracy. Using this framework, we extend mRMR algorithm into GroupMRMR algorithm, which encourages features from different groups to bring in different semantics which help selecting a more balanced set of features. We select mRMR algorithm for extension because it has proven good classification accuracy with low computation costs, compared to other filter feature selection methods. The feature groups are assigned weights (\u03b1 i ) to represent their importance levels, and GroupMRMR selects more features from the groups with higher importance. Group weights may be decided according to factors such as group size and group quality. For this paper, we assume that the feature groups do not overlap but plan to investigate overlapping groups in the future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Method: GroupMRMR"
        },
        {
            "text": "Our feature selection objective includes both the filter feature selection objective and encouraging features from different feature groups. To encourage features from different groups, we minimise W 2 0,2 of the feature weight matrix, W . Using L 0 norm at intra group level enforces intra group sparsity, discouraging features to be selected from the same group. Using L 2 norm at inter group level encourages features from different feature groups [12] .",
            "cite_spans": [
                {
                    "start": 451,
                    "end": 455,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Feature Selection Objective"
        },
        {
            "text": "Let W \u2208 R |G|\u00d7|F | be a feature weight matrix such that W ij = 1 if f j \u2208 S and f j \u2208 G i . Otherwise, W ij = 0. Given that g(W ) is any maximisation quantity used in an existing filter feature selection objective which can be expressed a function of W and \u03bb is a user defined parameter, our objective is to select S \u2286 F to maximise the following subject to |S| = k, k \u2208 Z + :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Selection Objective"
        },
        {
            "text": "That is, the maximisation quantity in mRMR objective in Eq. (1) is a function of W . Consequently, g(W ) in Eq. (3) can be replaced with the mRMR objective as shown in Eq. (4).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Selection Objective"
        },
        {
            "text": "Definition 4. Given that S and G i are as defined in Table 1 ,",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 53,
                    "end": 60,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Feature Selection Objective"
        },
        {
            "text": "Given n i is as defined in Definition 4, according to Sect. 3, W 2 0,2 = |G| i=1 n 2 i . When the feature groups have different weights, the rows of W also have different importance levels. In such scenarios,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Selection Objective"
        },
        {
            "text": "Consequently, we can rewrite the objective in Eq. (4) as in Eq. (5) subject to |S| = k, k \u2208 Z + . As the feature groups do not overlap, |G| i=1 n i = |S|. Using Eq. (5), we present Theorem 1 that shows minimising W 2 0,2 is equivalent to encouraging features from different groups in to S.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Selection Objective"
        },
        {
            "text": "input : Dataset (D), Required feature count (r), Group weights (\u03b11 \u00b7 \u00b7 \u00b7 \u03b1 |G| ) output: Selected feature subset (S) Proof. Using Lagrange multipliers method, we show minimum",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. GroupMRMR algorithm"
        },
        {
            "text": "Please refer to this link 1 for the detailed proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. GroupMRMR algorithm"
        },
        {
            "text": "As L 2 0,2 minimisation is NP-hard, we propose a heuristic algorithm to achieve the objective in Eq. (4). The algorithm selects a feature, f t , at each iteration t to maximise the difference between h(S t ) and h(S t\u22121 ), where S t and S t\u22121 are the feature subsets selected after Iteration t and t \u2212 1 respectively and h(.) is as defined in Eq. (5) . As there are datasets with millions of features we seek an algorithm to select f t with linear complexity. Theorem 2 shows that h(S t )h(S t\u22121 ) can be maximised by adding the term, \u03bb 2np+1 \u03b1p to the mRMR algorithm in Eq. (2) . p is the feature group of the evaluated feature (f x ), n p is the number of features already selected from p before Iteration t and \u03b1 p is the weight of p.",
            "cite_spans": [
                {
                    "start": 347,
                    "end": 350,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 575,
                    "end": 578,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Iterative Feature Selection"
        },
        {
            "text": "Proof. To prove this, we use the fact that |S t | and |S t\u22121 | are constants at a given iteration. Please refer to this link (see footnote 1) for the detailed proof. Based on Theorem 2, we propose GroupMRMR algorithm. At each iteration, the feature score of each feature in U is computed as shown in Line 5 of Algorithm 1. The feature with the highest score is removed from U and added to S (Line 7-10 in Algorithm 1). The algorithm can be modified to encourage the features from the same group as well by setting \u03bb < 0. Example 1 Revisited: Next, we apply GroupMRMR for Example 1. We assume \u03bb = 1 and \u03b1 i = \u03b1 j = 1, \u2200 i, j \u2208 I. GroupMRMR first selects Apple, the feature with highest relevancy (0.549). In Iteration 2, n p value for Rice, Cow, and Sheep are 1, 0 and 0, respectively and ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Iterative Feature Selection"
        },
        {
            "text": "The computational complexity of GroupMRMR is the same as that of mRMR, which is O(|S||F |). |S| and |F | are the cardinalities of the selected feature subset and the complete feature set, respectively. As |S| << |F |, GroupMRMR is effectively linear with |F |.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Computation Complexity:"
        },
        {
            "text": "This section discusses the experimental results for GroupMRMR for real datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We evaluate GroupMRMR, using real datasets, which are benchmark datasets used to test group based feature selection. Table 2 shows a summary of them. Images in Yale have a 32 \u00d7 32 pixel map. GRV is a JIRA software defect dataset whose features are code quality metrics.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 117,
                    "end": 124,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Datasets:"
        },
        {
            "text": "The pixel map of the images are partitioned into m \u00d7 m non overlapping squares such that each square is a feature group. This introduces spatial locality information, not available from just the data (instance-feature) itself. The genes in genomic data are clustered based on the Gene Ontology term annotations as described in [2] . The number of groups is set to 0.04 of the original feature set, based on the previous findings for MT dataset [2] . Words in BBC dataset are clustered using k-means algorithm, based on the semantics available from Word2Vec [14] . We use only 2,411 features, only the words available in the Brown's corpus. Number of word groups is 50, which is selected by cross validation results on the training data. The code metrics in software defect data are grouped into five groups based on their granularity in the code [18] .",
            "cite_spans": [
                {
                    "start": 327,
                    "end": 330,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 444,
                    "end": 447,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 557,
                    "end": 561,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 846,
                    "end": 850,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Grouping Features:"
        },
        {
            "text": "We compare GroupMRMR with existing filter methods which have proven high accuracy. mRMR algorithm, of which the GroupMRMR is an extension, is a greedy approach to achieve mRMR objective while SPECCMI [15] is a global optimisation algorithm to achieve the same. Conditional Mutual Information (CMIM) [15] is a mutual information based filter method not belonging to the mRMR family. ReliefF [13] is a distance based filter method. GSAOLA [19] is an online filter method which utilises feature group information. Experimental Results: Table 3 shows that GroupMRMR achieves the highest AVGF in all datasets over baselines. In LK dataset, the 100% accuracy is achieved with a lower number of features than baselines. GroupMRMR achieves higher or same average accuracy compared to baselines in 32 out of 35 cases. Figure 3 shows that, despite the slightly low average accuracy compared to Reli-efF, GroupMRMR maintains a higher accuracy than baselines in Multi-A for most of the selected feature numbers. Other datasets also show similar results, yet we show only three graphs due to the space limitations. Please refer to this link (see footnote 1) to see all the results graphs. The maximum accuracy gain of GroupMRMR over the accuracy gained by the complete feature set is 2%, 10%, 2%, 2%, 1% and 6% for MT, CNS, Multi-A, Yale, BBC and GRV datasets, respectively. The maximum accuracy gain of GroupMRMR is 50% over SPECCMI in Yale dataset at 50 selected features. The highest accuracy gain of GroupMRMR over mRMR is 35% in CNS dataset at 70 selected features. Figure 4a shows that the classification accuracy of GroupMRMR for 8 \u00d7 8 image partitions is less than for 4 \u00d7 4 and 2 \u00d7 2 partitions. Figure 4b shows that the classification accuracy is not much sensitive to \u03bb in the [10 \u22123 , 1] range, yet degrades to a large extent when \u03bb < 0. Figure 4c shows that the runtime of GroupMRMR is almost the same as the run time of mRMR algorithm and lower than most of the other baseline methods (\u223c10 times lower than SPECCMI and CMIM for BBC dataset).",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 204,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 299,
                    "end": 303,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 390,
                    "end": 394,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 437,
                    "end": 441,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 533,
                    "end": 540,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 809,
                    "end": 817,
                    "text": "Figure 3",
                    "ref_id": null
                },
                {
                    "start": 1558,
                    "end": 1567,
                    "text": "Figure 4a",
                    "ref_id": null
                },
                {
                    "start": 1692,
                    "end": 1701,
                    "text": "Figure 4b",
                    "ref_id": null
                },
                {
                    "start": 1837,
                    "end": 1846,
                    "text": "Figure 4c",
                    "ref_id": null
                }
            ],
            "section": "Baselines:"
        },
        {
            "text": "Evaluation Insights: GroupMRMR consistently shows good classification accuracy compared to baselines for all the datasets (highest average accuracy and highest maximum accuracy in almost all datasets). The equal run times of GroupMRMR and mRMR show that the accuracy gain is obtained for no additional costs and supports the time complexity analysis in Sect. 5. Better prediction accuracy is obtained for small groups because large feature groups resemble the original feature set with no groupings. This shows the importance of feature group information to gain high feature selection accuracy. The accuracy is lower when the features are encouraged from the same group (\u03bb < 0) instead from different groups (\u03bb > 0), which supports our hypothesis. The classification accuracy is less sensitive to \u03bb \u2265 10 \u22123 , therefore parameter tuning is less required.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baselines:"
        },
        {
            "text": "We propose a framework which facilitates filter feature selection methods to exploit feature group information as an external source of information. Using this framework, we incorporate feature group information into mRMR algorithm, resulting in GroupMRMR algorithm. We show that compared to baselines, GroupMRMR achieves high classification accuracy for the datasets with feature group structures. The run time of GroupMRMR is same as the run time of mRMR, which is lower than many existing feature selection algorithms. Our future work include experimenting the proposed framework for other filter methods and detecting whether a dataset contains feature group structures.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Cancer program datasets",
            "authors": [],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Unsupervised gene selection using biological knowledge: application in sample clustering",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Acharya",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saha",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Nikhil",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "BMC Bioinform",
            "volume": "18",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Heuristic filter feature selection methods for medical datasets",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alirezanejad",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Enayatifar",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Motameni",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Genomics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1016/j.ygeno.2019.07.002"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "On the scalability of feature selection methods on high-dimensional data",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Bol\u00f3n-Canedo",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rego-Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Peteiro-Barral",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alonso-Betanzos",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Guijarro-Berdi\u00f1as",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "S\u00e1nchez-Maro\u00f1o",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Knowl. Inf. Syst",
            "volume": "56",
            "issn": "2",
            "pages": "395--442",
            "other_ids": {
                "DOI": [
                    "10.1007/s10115-017-1140-3"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Benchmark for filter methods for feature selection in high-dimensional classification data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bommert",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Bischl",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "CSDA",
            "volume": "143",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Learning a spatially smooth subspace for face recognition",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of IEEE CVPR",
            "volume": "",
            "issn": "",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Minimum redundancy feature selection from microarray gene expression data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "JBCB",
            "volume": "3",
            "issn": "02",
            "pages": "185--205",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "A note on the group lasso and a sparse group lasso",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1001.0736"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Practical solutions to the problem of diagonal dominance in kernel document clustering",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Greene",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cunningham",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the 23rd ICML",
            "volume": "",
            "issn": "",
            "pages": "377--384",
            "other_ids": {
                "DOI": [
                    "10.1145/1143844.1143892"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Differential evolution for filter feature selection based on information theory and feature ranking. Knowl.-Based Syst",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Hancer",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Xue",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "140",
            "issn": "",
            "pages": "103--119",
            "other_ids": {
                "DOI": [
                    "10.1016/j.knosys.2017.10.028"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Autospearman: Automatically mitigating correlated metrics for interpreting defect models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiarpakdee",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Tantithamthavorn",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Treude",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1806.09791"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Uncorrelated group lasso",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1765--1771",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Estimating attributes: analysis and extensions of RELIEF",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kononenko",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "ECML 1994",
            "volume": "784",
            "issn": "",
            "pages": "171--182",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Support vector machines and word2vec for text classification with semantic features",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lilleberg",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 14th IEEE ICCI* CC",
            "volume": "",
            "issn": "",
            "pages": "136--140",
            "other_ids": {
                "DOI": [
                    "10.1109/ICCI-CC.2015.7259377"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Effective global approaches for mutual information based feature selection",
            "authors": [
                {
                    "first": "X",
                    "middle": [
                        "V"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Romano",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 20th ACM SIGKDD",
            "volume": "",
            "issn": "",
            "pages": "512--521",
            "other_ids": {
                "DOI": [
                    "10.1145/2623330.2623611"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A novel probabilistic feature selection method for text classification. Knowl.-Based Syst",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Uysal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gunal",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "36",
            "issn": "",
            "pages": "226--235",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Online feature selection with group structure analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE TKDE",
            "volume": "27",
            "issn": "11",
            "pages": "3029--3041",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Mining software defects: should we consider affected releases? In: Proceedings of the 41st International Conference on Software Engineering",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yatish",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiarpakdee",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Thongtanunam",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "654--665",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Scalable and accurate online feature selection for big data",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ACM TKDD",
            "volume": "11",
            "issn": "2",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1145/2976744"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Stable feature selection via dense feature groups",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Loscalzo",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 14th ACM SIGKDD",
            "volume": "",
            "issn": "",
            "pages": "803--811",
            "other_ids": {
                "DOI": [
                    "10.1145/1401890.1401986"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Set of all features I Set of all feature group indices S Selected feature subset, S \u2286 F Gi Set of features in i th feature group G Set of all feature groups \u03b1i The weight of the i th feature group",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Value pattern probabilities created by different feature subsets in each class, A: Agriculture, B: Botany, P: Physics, Z: Zoology, Class: The class assigned to the value pattern, %: #(x,y)value patterns in class c #instances in class c \u00d7 100; x, y \u2208 {0,1}, a: Apple, r: Rice, s: Sheep",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "\u2190 argmax x\u2208U scorex; 8 S \u2190 S + fmax; U \u2190 Ufmax; 9 j \u2190 Group index of Gj where fmax \u2208 Gj;10 nj++; f eaCount++; 11 end 12 return S;",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "of each feature with Apple are same as computed in Sect. 4. The feature scores for Rice, Cow and Sheep are \u22122.627 (0.443-0.07-3), 0.294 (0.311-0.017-0) and 0.295 (0.311-0.016-0), respectively and GroupMRMR selects Sheep, the feature with the highest feature score. Therefore, GroupMRMR selects {Apple, Sheep}, the optimal feature subset, as discussed in Sect. 4.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Classification accuracy variation with the number of selected features (a) Group size (b) \u03bb parameter (c) Run time Accuracy and runtime variations for Yale and BBC datasets (a) Accuracy variation with the group size (Yale) (b) Accuracy variation with \u03bb (Yale) (c) Average run time variation (in log scale) of the algorithms (BBC). 95% confidence interval error bars are too small to be visible due to the high precision (standard deviations \u223c2 s) Evaluation Method: The classifier's prediction accuracy on the test dataset with selected features is considered as the prediction accuracy of the feature selection algorithm. It is measured in terms of the Macro-F1, the average of the F1-scores for each class (AVGF). Average accuracy is the average of AVGFs for all the selected feature numbers up to the point algorithm accuracies converge. The log value of the average run time (measured in seconds) is reported.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Frequently used definitions",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Fig. 1. Example text document dataset. Column (di): a document/instance, Row: a word/feature, Class: document type, 1/0: Occurrence of a word, B: Botany, Z: Zoology, P: Physics, A: Agriculture",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Dataset description. m: # features, n: # instances, c: # classes",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Comparison of accuracies achieved by different algorithms. Row 1: The maximum accuracy (in AVGF) gained by each algorithm in each dataset. The highest maximum AVGF for each dataset is in bold letters. Row 2 (x): the number of features at which the highest AVGF is achieved. Row 3 (%): The average accuracy gain of GroupMRMR over the baseline. +: GroupMRMR wins, \u2212: GroupMRMR losses",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Experimental Setup:We split each dataset, 60% instances for training set and 40% for test set, using stratified random sampling method. Feature selection is performed on the training set and the classifier is trained on the training set with the selected features. The classifier is then used to predict the labels of the test set. Due to the small sample size of the datasets we do not use a separate validation set for tuning \u03bb. Instead, we select \u03bb \u2208 [0, 2], which gives the highest classification accuracy on the training set. The classifier used is the Support Vector Machine. For image data, default m = 4. For genomic data, \u03b1 i = 1, \u2200 i. For other datasets, \u03b1 i = |Gi| |F | (G i ,F are defined inTable 1).Experiment 1: Measures the classification accuracy obtained for the datasets with selected features. Experiment 2: Performs feature selection for image datasets with different feature group sizes: m \u00d7 m (m = 2,4,8). This tests the effect of the group size on the classification accuracy. Experiment 3: Runs GroupMRMR for different \u03bb \u2208 [\u22121, 1]. This tests the effect of \u03bb on the classification accuracy. Experiment 4: Executes each feature selection algorithm 20 times and compute the average run time to evaluate algorithm efficiency.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This work is supported by the Australian Government.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}