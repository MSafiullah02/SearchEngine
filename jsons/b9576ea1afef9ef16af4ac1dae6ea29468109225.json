{
    "paper_id": "b9576ea1afef9ef16af4ac1dae6ea29468109225",
    "metadata": {
        "title": "Deep-COVID: Predicting COVID-19 From Chest X-Ray Images Using Deep Transfer Learning",
        "authors": [
            {
                "first": "Shervin",
                "middle": [],
                "last": "Minaee",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Snap Inc",
                    "location": {
                        "settlement": "Seattle",
                        "region": "WA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Rahele",
                "middle": [],
                "last": "Kafieh",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Isfahan University of Medical Sciences",
                    "location": {
                        "country": "Iran"
                    }
                },
                "email": ""
            },
            {
                "first": "Milan",
                "middle": [],
                "last": "Sonka",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The University of Iowa",
                    "location": {
                        "settlement": "Iowa City",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Shakib",
                "middle": [],
                "last": "Yazdani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Isfahan University of Technology",
                    "location": {
                        "country": "Iran"
                    }
                },
                "email": ""
            },
            {
                "first": "Ghazaleh",
                "middle": [
                    "Jamalipour"
                ],
                "last": "Soufi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Isfahan University of Medical Sciences",
                    "location": {
                        "settlement": "Isfahan",
                        "country": "Iran"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Since December 2019, a novel corona-virus (SARS-CoV-2) has spread from Wuhan to the whole China, and many other countries. By April 18, more than 2 million confirmed cases, and more than 150,000 deaths cases were reported in the world [1] . Due to unavailability of therapeutic treatment or vaccine for novel COVID-19 disease, early diagnosis is of real importance to provide the opportunity of immediate isolation of the suspected person and to decrease the chance of infection to healthy population. Reverse transcription polymerase chain reaction (RT-PCR) or gene sequencing for respiratory or blood specimens are introduced as main screening methods for COVID-19 [2] . However, total positive rate of RT-PCR for throat swab samples is reported to be 30 to 60 percent, which accordingly yields to un-diagnosed patients, which may contagiously infect a huge population of healthy people [3] . Chest radiography imaging (e.g., X-ray or computed tomography (CT) imaging) as a routine tool for pneumonia diagnosis is easy to perform with fast diagnosis. Chest CT has a high sensitivity for diagnosis of COVID-19 [4] and X-ray images show visual indexes correlated with COVID-19 [5] . The reports of chest imaging demonstrated multilobar involvement and peripheral airspace opacities. The opacities most frequently reported are ground-glass (57%) and mixed attenuation (29%) [6] . During the early course of COVID-19, ground glass pattern is seen in areas that edges the pulmonary vessels and may be difficult to appreciate visually [7] . Asymmetric patchy or diffuse airspace opacities are also reported for COVID-19 [8] . Such subtle abnormalities can only be interpreted by expert radiologists. Considering huge rate of suspected people and limited number of trained radiologists, automatic methods for identification of such subtle abnormalities can assist the diagnosis procedure and increase the rate of early diagnosis with high accuracy. Artificial intelligence (AI)/machine learning solutions are potentially powerful tools for solving such problems. So far, due to the lack of availability of public images of COVID-19 patients, there has not been any detailed study looking at the potential of AI/machine learning solutions for automatic detection of COVID-19 from X-ray (or Chest CT) images. Recently a small dataset of COVID-19 X-ray images have been collected by some researchers, which made it possible for AI researchers to train machine learning models to perform automatic COVID-19 diagnostics from X-ray images [10] . These images were extracted from academic publications reporting the results on COVID-19 X-ray and CT images. With the help of a board-certified radiologist, we re-labeled those images, and only kept the ones which were detected to have a clear sign of COVID-19 by our radiologist. Three sample images with their corresponding marked areas by our radiologist are shown in Figure 1 . We then used a subset of images from ChexPert [11] dataset, as the negative samples for COVID-19 detection. The combined dataset has around 5,000 Chest X-ray images (called COVID-Xray-5k), which is dividing into 2,000 training, and 3,000 testing samples. It is worth mentioning that some of the earlier works in the past few weeks used the images from pediatric patients of one to five years old (from a Kaggle competition) as the negative class, which may not be the best idea, as there is a big difference among the age range of the positive and negative class in that case.",
            "cite_spans": [
                {
                    "start": 235,
                    "end": 238,
                    "text": "[1]",
                    "ref_id": null
                },
                {
                    "start": 667,
                    "end": 670,
                    "text": "[2]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 889,
                    "end": 892,
                    "text": "[3]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "text": "[4]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1177,
                    "end": 1180,
                    "text": "[5]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1373,
                    "end": 1376,
                    "text": "[6]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1531,
                    "end": 1534,
                    "text": "[7]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1616,
                    "end": 1619,
                    "text": "[8]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 2528,
                    "end": 2532,
                    "text": "[10]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 2964,
                    "end": 2968,
                    "text": "[11]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 2907,
                    "end": 2915,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "We then use a machine a learning framework to predict the COVID-19, from the Chest X-ray images. Unlike the classical approaches for medical image classification which follow a two-step procedure (hand-crafted feature extrac-tion+recognition), we use an end-to-end deep learning framework which directly predicts the COVID-19 from raw images without any need of feature extraction. Deep learning based models (and more specifically convolutional neural networks (CNN)) have been shown to outperform the classical AI approaches in most of computer vision and and medical image analysis tasks in recent years, and have been used in a wide range of problems from classification, segmentation, face recognition, to super-resolution and image enhancement [12] , [18] - [21] .",
            "cite_spans": [
                {
                    "start": 750,
                    "end": 754,
                    "text": "[12]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 757,
                    "end": 761,
                    "text": "[18]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 764,
                    "end": 768,
                    "text": "[21]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Here, we train 4 popular convolutional networks which have achieved promising results in several tasks during recent years (including ResNet18, ResNet50, SqueezeNet, and DesneNet-161) on COVID-Xray-5k dataset, and analyze their performance for COVID-19 detection. Since so far there are only a few X-ray images publicly available for COVID-19 class, we cannot simply train these models from scratch. We used two strategies to address the COVID-19 image scarcity issue in this work:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 We use data augmentation to create transformed version of COVID-19 images (such as flipping, small rotation, adding small amount of distortions), to increase the number of samples by a factor of 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 Instead of training these models from scratch, we finetune the last layer of the pre-trained version of these models on ImageNet. In this way, the model can be trained with less labeled samples from each class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The above two strategies help us to train these networks with the available images, and achieve reasonable performance on the test set of 3,000 images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The best performing model out of the above four networks, achieves a sensitivity of 97.5%, and specificity of around 95%. Since the number of samples for COVID-19 class is limited, we also calculate the confidence interval of the performance metrics. To report a summarizing performance of these models, we provide the Receiver operating characteristic (ROC) curve, and area under the curve (AUC) for each of these models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Here are the contribution of this paper:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 We prepared a dataset of 5,000 images with binary labels, for COVID-19 detection from Chest X-ray images. This dataset can serve as a benchmark for the research community. The images in COVID-19 class, are labeled by a board-certified radiologist, and only those with a clear sign are used for testing purpose.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 We trained 4 promising deep learning models on this dataset, and evaluated their performance on a test set of 3,000 images. Our best performing model achieved a sensitivity rate of 97.5%, while having a specificity of 95%.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 We also provided the ROC curve, AUC, and the histogram of the predicted scores by these models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 We provided a detailed experimental analysis on the performance of these models, in terms of sensitivity, specificity, ROC curve, area under the curve, and confusion matrix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 We make the dataset, the trained models, and the implementation publicly available.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "It is worth to mention that, although the result of this work is very encouraging, given the amount of the labeled data the result of this work is still preliminary, and more concrete conclusion requires further experiments on a larger dataset of COVID-19 labeled X-ray images. We believe this work can serve as a benchmark for future works and comparisons.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The structure of the rest of this paper is as follows. Section II provides a summary of the prepared COVID-Xray-5k Dataset. Section III presents the description of the overall proposed framework. Section IV provides the experimental studies and comparison with previous works. And finally the paper is concluded in Section V.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "We have used the X-ray images from two datasets, to create the COVID-Xray-5k dataset. The COVID-Xray-5k dataset contains 2,031 training images, and 3,040 test images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. COVID-XRAY-5K DATASET"
        },
        {
            "text": "One of the used datasets is the recently published Covid-Chestxray-Dataset, which contains a set of images from publications on COVID-19 topics, collected by Joseph Paul Cohen [9], [10] . This dataset contains a mix of chest Xray and CT images. As of March, 23, 2020, it contained 23 CT images (22 COVID-19, and 1 Non-Covid), and 126 Xray images (102 COVID-19, and 24 Non-COVID images). It is mentioned that this dataset is continuously updated. It also contains some meta-data about each patients, such as sex and age. Our COVID-19 images are all coming from this dataset. The provided 102 COVID-19 images were examined by our board-certified radiologist which led to elimination of all lateral images and some less-reliable anterior-posterior images, yielding 71 X-ray images with COVID-19. Therefore, we have chosen 40 COVID-19 images to include in the test set (to meet some maximum confidence interval value), and 31 COVID-19 images for the training set. Data augmentation is applied to the training set to increase the number of COVID-19 samples to 496 (by a combination of flipping, rotation, small distortion, and over-sampling). We made sure all images for each patient go only to one of the training or test sets. It is worth mentioning that our radiologist marked some of the likely regions, which can have some sign of Covid-19 too.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 185,
                    "text": "[10]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "II. COVID-XRAY-5K DATASET"
        },
        {
            "text": "Since the number of Non-Covid images was very small in the [9] dataset, additional images were employed from the ChexPert dataset [11] , a large public dataset for chest radiograph interpretation consisting of 224,316 chest radiographs of 65,240 patients, labeled for the presence of 14 sub-categories (no-finding, Edema, Pneumonia, etc.). For the non-COVID samples in the training set, we only used images belonging to a single sub-category, composed of 700 images form nofinding class and 100 images from each remaining 13 subclasses, resulting in 2,000 non-COVID images. As for the Non-COVID samples in the test dataset, we selected 1,700 images from no-finding category and around 100 images from each remaining 13 sub-classes in distinct sub-folders, resulting into 3000 images in total. The exact number of images of each class for both training and testing is given in Table I . ",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 134,
                    "text": "[11]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 876,
                    "end": 883,
                    "text": "Table I",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "II. COVID-XRAY-5K DATASET"
        },
        {
            "text": "Since so far, the number of publicly available images, which are labeled as COVID-19 are very limited, it may not be possible to train a deep convolutional neural network from scratch to detect COVID-19 from X-ray images. To overcome this issue, we use a well-known strategy in machine learning, called \"transfer learning\", and fine-tune four popular pre-trained deep neural networks on the training images of COVID-Xray-5k dataset. We will first provide a quick introduction of transfer learning, and then discuss the proposed framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE PROPOSED FRAMEWORK"
        },
        {
            "text": "In transfer learning, a model trained on one task is repurposed on another related task, usually by some adaptation toward the new task. For example, one can imagine using an image classification model trained on ImageNet (which contains millions of labeled images) to initiate task-specific learning for COVID-19 detection on a smaller dataset. Transfer learning is mainly useful for tasks where enough training samples are not available to train a model from scratch, such as medical image classification for rare or emerging diseases, in which sufficiently large numbers of labeled samples may not be available. This is especially the case for models based on deep neural networks, which have a large number of parameters to train. By using transfer learning, the model parameters start with a;ready-good initial values that only need some small modifications to be better curated toward the new task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Transfer Learning Approach"
        },
        {
            "text": "There are two main ways in which the pre-trained model is used for a different task. In one approach, the pre-trained model is treated as a feature extractor (i.e., the internal weights of the pre-trained model are not adapted to the new task), and a classifier is trained on top of it to perform classification. In another approach, the whole network, or a subset thereof, is fine-tuned on the new task. Therefore the pre-trained model weights are treated as the initial values for the new task, and are updated during the training stage.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Transfer Learning Approach"
        },
        {
            "text": "In our case, since the number of images in COVID-19 category is very limited, we only fine-tune the last layer of the convolutional neural networks, and essentially use the pre-trained models as a feature extractor. We evaluate the performance of four popular pre-trained models, ResNet18 [14] , ResNet50 [14] , SqueezeNet [15] , and DenseNet-121 [16] . In the next section we provide a quick overview of the architecture of these models, and how they are used for COVID-19 recognition.",
            "cite_spans": [
                {
                    "start": 289,
                    "end": 293,
                    "text": "[14]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 305,
                    "end": 309,
                    "text": "[14]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 323,
                    "end": 327,
                    "text": "[15]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 347,
                    "end": 351,
                    "text": "[16]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "A. Transfer Learning Approach"
        },
        {
            "text": "One of the models used in this work, is the pre-trained ResNet18, trained on ImageNet dataset. ResNet is one of the most popular CNN architecture, which provides easier gradient flow for more efficient training, and was the winner of the 2015 ImageNet competition. The core idea of ResNet is introducing a so-called identity shortcut connection that skips one or more layers. This would help the network to provide a direct path to the very early layers in the network, making the gradient updates for those layers much easier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. COVID-19 Detection Using Residual ConvNet -ResNet18 and ResNet50"
        },
        {
            "text": "The overall block diagram of ResNet18 model, and how it is used for COVID-19 detection is illustrated in Figure 3 . ResNet50 architecture is pretty similar to ResNet18, the main difference being having more layers.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 105,
                    "end": 113,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "B. COVID-19 Detection Using Residual ConvNet -ResNet18 and ResNet50"
        },
        {
            "text": "SqueezeNet [15] proposed by Iandola et al., is a small CNN architecture, which achieves AlexNet-level [13] accuracy on ImageNet with 50x fewer parameters. Using model Fig. 3 . The architecture of ResNet18 model [14] .",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[15]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 102,
                    "end": 106,
                    "text": "[13]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 211,
                    "end": 215,
                    "text": "[14]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 167,
                    "end": 173,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "C. COVID-19 Detection Using SqueezeNet"
        },
        {
            "text": "compression techniques, the authors were able to compress SqueezeNet to less than 0.5MB, which made it very popular for applications that require light-weight models. They alternate a 1x1 layer that \"squeezes\" the incoming data in the vertical dimension followed by two parallel 1x1 and 3x3 convolutional layers that \"expand\" the depth of the data again. Three main strategies used in SqueezeNet includes: replace 3x3 filters with 1x1 filters, decrease the number of input channels to 3x3 filters, Down-sample late in the network so that convolution layers have large activation maps. Figure 4 shows the architecture of a simple SqueezeNet. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 585,
                    "end": 593,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "C. COVID-19 Detection Using SqueezeNet"
        },
        {
            "text": "Dense Convolutional Network (DenseNet) is another popular architecture [16] , which was the winner of the 2017 ImageNet competition. In DenseNet, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Each layer is receiving a collective knowledge from all preceding layers. Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e., number of channels can be fewer (so, it have higher computational efficiency and memory efficiency). The architecture of sample DenseNet is shown in Figure 5 .",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[16]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 601,
                    "end": 609,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "D. COVID-19 Detection Using DenseNet"
        },
        {
            "text": "All employed models are trained with a cross-entropy loss function, which tries to minimize the distance between the predicted probability scores, and the ground truth probabilities (derived from labels), and is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Model Training"
        },
        {
            "text": "where p i and q i denote the ground-truth, and predicted probabilities for each image, respectively. We can then minimize this loss function using stochastic gradient descent algorithm (and its variations). We also tried to add regularization to the loss function, but the resulting model was not better than the case without regularization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Model Training"
        },
        {
            "text": "In this section we provide the experimental results of the four neural networks trained for COVID-19 detection, the histogram of their predicted scores on the test images, and quantitative performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. EXPERIMENTAL RESULTS"
        },
        {
            "text": "We fine-tuned each model for 100 epochs. The batch size is set to 20, and ADAM optimizer is used to optimize the loss function, with a learning rate of 0.0001. All images are down-sampled to 224x224 before being fed to the neural network (as these pre-trained models are usually trained with a specific image resolution). All our implementations are done in PyTorch [22] , and are publicly available here: https://github.com/shervinmin/DeepCovid.git",
            "cite_spans": [
                {
                    "start": 366,
                    "end": 370,
                    "text": "[22]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A. Model Hyper-parameters"
        },
        {
            "text": "There are different metrics which can be used for evaluating the performance of classification models, such as classification accuracy, sensitivity, specificity, precision, and F1-score. Since the current test dataset is highly imbalanced (as there are 40 images with COVID-19, and 3000 images that are Non-COVID), sensitivity and specificity are two propoer metrics which can be used for reporting the model performance. These metrics are also widely used in medical domain, and can be defined as Eq 2: Sensitivity = #Images correctly predicted as COVID-19 #Total COVID-19 Images , Specificity = #Images correctly predicted as Non-COVID #Total Non-COVID Images .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Evaluation Metrics"
        },
        {
            "text": "(2)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Evaluation Metrics"
        },
        {
            "text": "As mentioned earlier, we focused on four popular convolutional networks, ResNet18, ResNet50, SqueezeNet, DenseNet121. These models predict a probability score for each image, which shows the likelihood of the image being detected as COVID-19. By comparing this probability with a cut-off threshold, we can derive a binary label showing if the image is COVID-19 or not. An ideal model should predict the probability of all COVID-19 samples close to 1, and non-COVID samples close to 0. Figures 6, 7, 8, and 9 show the distribution of predicted probability scores for the images in the test set, by ResNet18, ResNet50, SqueezeNet, and DenseNet-161 respectively. Since Non-COVID class in our study contains both normal cases, as well as other types of diseases, we provide the distribution of predicted scores for three classes: COVID-19, Non-COVID normal, and Non-COVID other diseases. As we can see the Non-Covid images with other types disease have slightly larger scores than the Non-COVID normal cases. This makes sense, since those images are more difficult to distinguish from COVID-19, than normal samples.",
            "cite_spans": [
                {
                    "start": 485,
                    "end": 495,
                    "text": "Figures 6,",
                    "ref_id": null
                },
                {
                    "start": 496,
                    "end": 498,
                    "text": "7,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 499,
                    "end": 501,
                    "text": "8,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 502,
                    "end": 507,
                    "text": "and 9",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C. Model Predicted Scores"
        },
        {
            "text": "Based on these figures, the images for COVID-19 patients, are predicted to have much higher probabilities than the Non-COVID images, which is really encouraging, as it shows the model is learning to discriminate COVID-19 from non-COVID images. Among different models, it can be observed that SqueezeNet does a much better job in pushing the predicted scores for COVID-19 and Non-COVID images far apart from each other. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Model Predicted Scores"
        },
        {
            "text": "As we can see from previous part, each model predicts a probability score showing the chance of the image being COVID-19. We can then compare these scores with a threshold to infer if the image is COVID-19 or not (if the score is bigger than the threshold it will be predicted as COVID -19) . The predicted labels are then used to estimate the sensitivity and specificity of each model. Depending on the value of cut-off threshold, we can get different sensitivity and specificity rates for each model. Tables II, III , IV, and V show the sensitivity and specificity rates for different thresholds, using ResNet18, ResNet50, SqueezeNet, and DenseNet-121 models, respectively. As we can see, all these models achieve very promising results, in which for a sensitivity rate of around 97%, their specificity rate is in the range of 84-97%. SqueezeNet and ResNet50 achieve slightly better performance than the other models. ",
            "cite_spans": [
                {
                    "start": 286,
                    "end": 290,
                    "text": "-19)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 503,
                    "end": 517,
                    "text": "Tables II, III",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "D. Model Sensitivity and Specificity"
        },
        {
            "text": "It is worth mentioning that since so far the number of reliably labeled COVID-19 X-ray images is very limited, and we only have 40 test images in COVID-19 class, it is hard to believe that all the sensitivity and specificity rates reported ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Small Number of COVID-19 Cases and Model Reliability"
        },
        {
            "text": "where z denotes the significance level of the confidence interval (the number of standard deviation of the Gaussian distribution), and accuracy is the estimated accuracy (in our cases sensitivity and specificity), and N denotes the number of samples for that class. Here we use 95% confidence interval, for which the corresponding value of z is 1.96.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Small Number of COVID-19 Cases and Model Reliability"
        },
        {
            "text": "As for COVID-19 diagnostic, having a sensitive model is crucial, we choose the cut-off threshold corresponding to a sensitivity rate of 97.5% for each model, and compare their specificity rates. Table VI provides a comparison of the performance of these four models on the test set. As we can see the confidence interval of specificity rates are small (around 1%), since we have around 3000 samples for this class, whereas for the sensitivity rate we get slightly higher confidence interval (around 4.8%) because of the limited number of samples.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 195,
                    "end": 203,
                    "text": "Table VI",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "E. Small Number of COVID-19 Cases and Model Reliability"
        },
        {
            "text": "As we can, it is hard to compare different models only based on their sensitivity and specificity rates, since these rates change by varying the cut-off thresholds. To see the overall comparison between these models, we need to look at the comparison for all possible threshold values. One way to do this, is through the Receiver Operating Characteristic (ROC) curve, which provides the true positive rate as a function of false positive rate. The ROC curve of these four models is shown in Figure 10 . As we can see all models have relatively similar area under the curve, but SqueezeNet achieve slightly higher AUC than other models. To see the exact number of correctly samples as COVID-19 and Non-COVID, we also provide the confusion matrix the two top-performing models. The confusion matrix of the finetuned ResNet50, and SqueezeNet models on the set of 3040 test images are shown in Figure 11 , and 12. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 491,
                    "end": 500,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 890,
                    "end": 899,
                    "text": "Figure 11",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "F. The ROC Curve of Each Model and Confusion Matrix"
        },
        {
            "text": "In this work we propose a deep learning framework for COVID-19 detection from Chest X-ray images, by fine-tuning four pre-trained convolutional models (ResNet18, ResNet50, SqueezeNet, and DenseNet-121) on our training set. We prepared a dataset of around 5k images, called COVID-Xray-5k (using images from two datasets), with the help of a boardcertified radiologist to confirm the COVID-19 labels. We make this dataset publicly available for the research community to use as a benchmark for training and evaluating future machine learning models for COVID-19 binary classification task. We performed a detail experimental analysis evaluating the performance of each of these 4 models on the test set of of COVID-Xray-5k Dataset, in terms of sensitivity, specificity, ROC, and AUC. For a sensitivity rate of 97.5%, these models achieved a specificity rate of around 90% on average. This is really encouraging, as it shows the promise of using X-ray images for COVID-19 diagnostics. This study is conducted on a set of publicly available images, which contains less than 100 COVID-19 images, and more than 5,000 non-COVID images. Due to the limited number of COVID-19 images publicly available so far, further experiments are needed on a larger set of cleanly labeled COVID-19 images for a more reliable estimation of the the sensitivity rates.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Detection of SARS-CoV-2 in Different Types of Clinical Specimens",
            "authors": [
                {
                    "first": "Wenling",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Yanli",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Ruqin",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Roujian",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Guizhen",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Wenjie",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Jama",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Laboratory diagnosis and monitoring the viral shedding of 2019-nCoV infections",
            "authors": [
                {
                    "first": "Yang",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Minghui",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Chenguang",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Fuxiang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Jing",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "Jinxiu",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Mingxia",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases",
            "authors": [
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Ai",
                    "suffix": ""
                },
                {
                    "first": "Zhenlu",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Hongyan",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "Chenao",
                    "middle": [],
                    "last": "Zhan",
                    "suffix": ""
                },
                {
                    "first": "Chong",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Wenzhi",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                },
                {
                    "first": "Qian",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "Ziyong",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Liming",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Essentials for radiologists on COVID-19: an updateradiology scientific expert panel",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [
                        "P"
                    ],
                    "last": "Kanne",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Brent",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [
                        "H"
                    ],
                    "last": "Little",
                    "suffix": ""
                },
                {
                    "first": "Brett",
                    "middle": [
                        "M"
                    ],
                    "last": "Chung",
                    "suffix": ""
                },
                {
                    "first": "Loren",
                    "middle": [
                        "H"
                    ],
                    "last": "Elicker",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ketai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Chest imaging appearance of COVID-19 infection",
            "authors": [
                {
                    "first": "Weifang",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "Prachi",
                    "middle": [
                        "P"
                    ],
                    "last": "Agarwal",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology: Cardiothoracic Imaging",
            "volume": "2",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Fleischner Society: glossary of terms for thoracic imaging",
            "authors": [
                {
                    "first": "David",
                    "middle": [
                        "M"
                    ],
                    "last": "Hansell",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [
                        "A"
                    ],
                    "last": "Bankier",
                    "suffix": ""
                },
                {
                    "first": "Heber",
                    "middle": [],
                    "last": "Macmahon",
                    "suffix": ""
                },
                {
                    "first": "Theresa",
                    "middle": [
                        "C"
                    ],
                    "last": "Mcloud",
                    "suffix": ""
                },
                {
                    "first": "Nestor",
                    "middle": [
                        "L"
                    ],
                    "last": "Muller",
                    "suffix": ""
                },
                {
                    "first": "Jacques",
                    "middle": [],
                    "last": "Remy",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Radiology",
            "volume": "246",
            "issn": "3",
            "pages": "697--722",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "An update on COVID-19 for the radiologist -A British society of Thoracic Imaging statement",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "C L"
                    ],
                    "last": "Rodrigues",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Clinical Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "COVID-19 image data collection",
            "authors": [
                {
                    "first": "Joseph",
                    "middle": [],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Paul",
                    "suffix": ""
                },
                {
                    "first": "Lan",
                    "middle": [],
                    "last": "Morrison",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.11597"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison",
            "authors": [
                {
                    "first": "Jeremy",
                    "middle": [],
                    "last": "Irvin",
                    "suffix": ""
                },
                {
                    "first": "Pranav",
                    "middle": [],
                    "last": "Rajpurkar",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Ko",
                    "suffix": ""
                },
                {
                    "first": "Yifan",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Silviana",
                    "middle": [],
                    "last": "Ciurea-Ilcus",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Chute",
                    "suffix": ""
                },
                {
                    "first": "Henrik",
                    "middle": [],
                    "last": "Marklund",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "33",
            "issn": "",
            "pages": "590--597",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Gradient-based learning applied to document recognition",
            "authors": [
                {
                    "first": "Yann",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proceedings of the IEEE",
            "volume": "",
            "issn": "",
            "pages": "2278--2324",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Imagenet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and\u00a1 0.5 MB model size",
            "authors": [
                {
                    "first": "Forrest",
                    "middle": [
                        "N"
                    ],
                    "last": "Iandola",
                    "suffix": ""
                },
                {
                    "first": "Song",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [
                        "W"
                    ],
                    "last": "Moskewicz",
                    "suffix": ""
                },
                {
                    "first": "Khalid",
                    "middle": [],
                    "last": "Ashraf",
                    "suffix": ""
                },
                {
                    "first": "William",
                    "middle": [
                        "J"
                    ],
                    "last": "Dally",
                    "suffix": ""
                },
                {
                    "first": "Kurt",
                    "middle": [],
                    "last": "Keutzer",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1602.07360"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Densely connected convolutional networks",
            "authors": [
                {
                    "first": "Gao",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Zhuang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Laurens",
                    "middle": [],
                    "last": "Van Der Maaten",
                    "suffix": ""
                },
                {
                    "first": "Kilian",
                    "middle": [
                        "Q"
                    ],
                    "last": "Weinberger",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "4700--4708",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation",
            "authors": [
                {
                    "first": "Vijay",
                    "middle": [],
                    "last": "Badrinarayanan",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Kendall",
                    "suffix": ""
                },
                {
                    "first": "Roberto",
                    "middle": [],
                    "last": "Cipolla",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE transactions",
            "volume": "39",
            "issn": "",
            "pages": "2481--2495",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Learning a deep convolutional network for image super-resolution",
            "authors": [
                {
                    "first": "Chao",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Biometric Recognition Using Deep Learning: A Survey",
            "authors": [
                {
                    "first": "Shervin",
                    "middle": [],
                    "last": "Minaee",
                    "suffix": ""
                },
                {
                    "first": "Amirali",
                    "middle": [],
                    "last": "Abdolrashidi",
                    "suffix": ""
                },
                {
                    "first": "Hang",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Mohammed",
                    "middle": [],
                    "last": "Bennamoun",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1912.00271"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Three sample COVID-19 images, and the corresponding marked areas by our radiologist.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "shows 16 sample images from COVID-Xray-5k dataset, including 4 COVID-19 images (the first row), 4 normal images from ChexPert (the second row), and 8 images with one of the 13 diseases in ChexPert (third and fourth rows).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Sample images from COVID-Xray-5k dataset. The images in the first row show 4 COVID-19 images. The images in the second row are 4 sample images of no-finding category in Non-COVID images from ChexPert. The images in the third and fourth rows give 8 sample images from other subcategotries in ChexPert.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The architecture of SqueezeNet based on \"fire modules\". Courtesy of Google[17].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The architecture of a DenseNet with 5 layers, with expansion of 4. Courtesy of model[16].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "The predicted probability scores by ResNet18 on the test set.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "The predicted probability scores by ResNet50 on the test set.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "The predicted probability scores by SqueezeNet on the test set.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "The predicted probability scores by DesneNet-121 on the test set.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "The ROC curve of four CNN architectures on COVID-19 test set.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "The confusion matrix of the proposed ResNet50 model.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "NUMBER OF IMAGES PER CATEGORY IN COVID-XRAY-5K",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "95% confidence interval of the reported sensitivity and specificity rates here, to see what is the possible range of these values for the current number of test samples in each class. The confidence interval of the accuracy rates can be calculated as Eq 3:",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "SENSITIVITY AND SPECIFICITY RATES OF SQUEEZENET MODEL, FOR DIFFERENT THRESHOLD VALUES. SENSITIVITY AND SPECIFICITY RATES OF DENSENET-121 MODEL, FOR DIFFERENT THRESHOLD VALUES.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "COMPARISON OF SENSITIVITY AND SPECIFICITY OF FOUR STATE-OF-THE-ART DEEP NEURAL NETWORKS.Fig. 12. The confusion matrix of the proposed SqueezeNet framework.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors would like to thank Joseph Paul Cohen for collecting the COVID-Chestxray-dataset. We would also like to thank the providers of ChexPert dataset, which are used as the negative samples in our case.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "ACKNOWLEDGMENT"
        }
    ]
}