{
    "paper_id": "2186bf359e5b204ba5595893f939645bccb97f6e",
    "metadata": {
        "title": "Human Activity Recognition Using Semi-supervised Multi-modal DEC for Instagram Data",
        "authors": [
            {
                "first": "Dongmin",
                "middle": [],
                "last": "Kim",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Korea Advanced Institute of Science and Technology",
                    "location": {
                        "settlement": "Daejeon",
                        "country": "Republic of Korea"
                    }
                },
                "email": ""
            },
            {
                "first": "Sumin",
                "middle": [],
                "last": "Han",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Korea Advanced Institute of Science and Technology",
                    "location": {
                        "settlement": "Daejeon",
                        "country": "Republic of Korea"
                    }
                },
                "email": "suminhan@kaist.ac.kr"
            },
            {
                "first": "Heesuk",
                "middle": [],
                "last": "Son",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Korea Advanced Institute of Science and Technology",
                    "location": {
                        "settlement": "Daejeon",
                        "country": "Republic of Korea"
                    }
                },
                "email": "heesuk.son@kaist.ac.kr"
            },
            {
                "first": "Dongman",
                "middle": [],
                "last": "Lee",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Korea Advanced Institute of Science and Technology",
                    "location": {
                        "settlement": "Daejeon",
                        "country": "Republic of Korea"
                    }
                },
                "email": "dlee@kaist.ac.kr"
            }
        ]
    },
    "abstract": [
        {
            "text": "Human Activity Recognition (HAR) using social media provides a solid basis for a variety of context-aware applications. Existing HAR approaches have adopted supervised machine learning algorithms using texts and their meta-data such as time, venue, and keywords. However, their recognition accuracy may decrease when applied to imagesharing social media where users mostly describe their daily activities and thoughts using both texts and images. In this paper, we propose a semi-supervised multi-modal deep embedding clustering method to recognize human activities on Instagram. Our proposed method learns multi-modal feature representations by alternating a supervised learning phase and an unsupervised learning phase. By utilizing a large number of unlabeled data, it learns a more generalized feature distribution for each HAR class and avoids overfitting to limited labeled data. Evaluation results show that leveraging multi-modality and unlabeled data is effective for HAR and our method outperforms existing approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Social media has been an ambient data platform on which people share their activities of daily living and memorable experience. Using the embedded behavioral patterns in the shared data, Human Activity Recognition (HAR) provides a solid basis for a variety of applications such as context-aware recommendation systems and health-care services [9] [10] [11] . To achieve better HAR performance using popular image-sharing social media such as Instagram and Yelp, various machine learning models have been presented [1] [2] [3] . Since these social media allow users to put their geolocation features when uploading the posts, human activities extracted from them have high potential to enhance awareness of real world dynamics.",
            "cite_spans": [
                {
                    "start": 343,
                    "end": 346,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 347,
                    "end": 351,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 352,
                    "end": 356,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 514,
                    "end": 517,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 518,
                    "end": 521,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 522,
                    "end": 525,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Existing HAR approaches [1] [2] [3] leverage supervised machine learning models (e.g., SVM, LSTM) which take texts and meta-data of social media posts for learning important patterns with respect to human activities. However, its use of uni-modal textual features cannot capture enough patterns of human activities shared on the social media because users mostly describe their daily activities and thoughts using both texts and images. Such a limitation can be relieved by incorporating the inherent multi-modality of social media into the learning process as in [4, 5] . These multi-modal approaches adopt an early fusion technique which leverages concatenated features of text and image to their proposed classifiers. However, none of them has investigated applicability to HAR and it is not trivial to construct a labeled dataset which is large enough to train their multi-modal supervised methods; to the best of our knowledge, no such dataset has been published yet for multi-modal HAR using social media.",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 27,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 28,
                    "end": 31,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 32,
                    "end": 35,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 564,
                    "end": 567,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 568,
                    "end": 570,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we present a semi-supervised method for HAR using multimodal Instagram data, that is, both image and text, which achieves a high recognition accuracy while using only a limited amount of labeled data. On social media, the number of unlabeled data exponentially increases every day while only a few labeled data for a specific task exists. In such a domain, semisupervised learning methods which leverage a small amount of labeled data and a much larger set of unlabeled data together are effective alternatives to improve learning performance [13] . To devise a semi-supervised learning method for HAR, we adopt the state-of-the-art clustering method, MultiDEC [7] , which can learn deep embedded feature representations of social media image and text, respectively and extend it into a semi-supervised model which incorporates a small portion of labeled data into its training procedure. The proposed method minimizes both cross-entropy loss and Kullback-Leibler divergence loss. This enables the proposed model to learn more generalized feature distribution by leveraging the feature distribution of a large unlabeled dataset while optimizing the learning results to the labeled features. Evaluation results show that our method achieves 71.58% of recognition accuracy which outperforms the best accuracy of the existing HAR methods (maximum 64.15%).",
            "cite_spans": [
                {
                    "start": 558,
                    "end": 562,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 676,
                    "end": 679,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In general, previous HAR methods using social media incorporate text features with metadata such as timestamps, venues, and keywords into their supervised machine learning models. Zack et al. [1, 2] leverage linear SVM for their human activity classifier and train the model using the text features from Twitter and Instagram data which they have collected and labeled using crowd-sourcing. Besides, Gong et al. [3] leverage Yelp data for HAR: They split each caption of a Yelp post into word tokens and create a sequence of embedded Word2Vec features. In addition to the extracted features, they use keyword dictionary knowledge embedding and temporal information encoding (e.g., date and week) for training a Long Short-Term Memory (LSTM) model to classify Yelp posts to human activity classes based on Yelp taxonomy 1 .",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 195,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 196,
                    "end": 198,
                    "text": "2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 412,
                    "end": 415,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Human Activity Recognition Using Social Media"
        },
        {
            "text": "To improve the recognition performance of the existing methods, multi-modal features (i.e., image and caption) of social media posts can be leveraged; there have been several approaches published already. Roy et al. [4] concatenate image features from a CNN model and text features from a Doc2Vec model and use them together to train a fully connected neural networks to identify social media posts related to illicit drugs. Schifanella et al. [5] use visual semantics from a CNN model and text features from an NLP network model together to train traditional models, SVM and DNN, respectively. After that, they leverage the trained models to detect sarcastic social media posts.",
            "cite_spans": [
                {
                    "start": 216,
                    "end": 219,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 444,
                    "end": 447,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Human Activity Recognition Using Social Media"
        },
        {
            "text": "Deep embedded clustering (DEC) [6] is an unsupervised method which simultaneously learns optimal feature representations and cluster assignments of unlabeled data by iteratively optimizing clustering objective. During the training process, DEC minimizes the KL divergence between the cluster soft assignment probability and the proposed target distribution so that its embedding network can learn feature representation. MultiDEC [7] is an extended version of DEC to deal with multi-modal data such as image-caption pairs. While DEC has a single embedding network, MultiDEC is composed of two embedding networks which are jointly trained to simultaneously learn image and text representations having similar distributions.",
            "cite_spans": [
                {
                    "start": 31,
                    "end": 34,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 430,
                    "end": 433,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Deep Embedding Clustering and Semi-supervised Learning"
        },
        {
            "text": "While DEC and MultiDEC are powerful methods, they cannot be directly used for HAR because they are clustering algorithms. To leverage their proven effectiveness for dealing with classification tasks, a semi-supervised DEC, SSLDEC [8] , has been recently presented. SSLDEC learns the target distribution of labeled data, iteratively estimates the class probability distribution of unlabeled data by measuring its feature distances from the clusters of labeled data and optimizes them during training. Thus, SSLDEC is a transductive learning method that retains recognition performance even with a relatively small amount of labeled dataset. However, the supervision mechanism of SSLDEC may fail to maximize the HAR performance when the labeled data cannot represent the target distribution correctly or its distribution cannot accommodate that of unlabeled data. Especially, when applied to social media data where such characteristics are evident, the applicability of SSLDEC may drastically decrease.",
            "cite_spans": [
                {
                    "start": 230,
                    "end": 233,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Deep Embedding Clustering and Semi-supervised Learning"
        },
        {
            "text": "The proposed method aims to predict human activity classes of multi-modal Instagram posts while training the optimal embedding network. More specifically, models in the proposed method should be well trained by means of a large number of unlabeled data where a limited number of labeled data is available. Figure 1 illustrates an overview of our proposed semi-supervised multi-modal DEC method to fulfill the key requirement. When multi-modal Instagram posts are given as a training dataset, our method preprocesses them to extract image and text features. Then, two embedding networks with parameters \u03b8 and \u03b8 are initialized for learning deep representations of the image and text features, respectively. These networks are intended to embed image data, X, and text data, X , into the corresponding latent spaces, Z and Z . Our method trains the embedding networks by alternating a supervised learning phase and an unsupervised learning phase. In the supervised learning phase, labeled multi-modal data is utilized for learning a class assignment. In the unsupervised learning phase, we leverage rich unlabeled data for computing cluster assignment probabilities for both features. After that, we compare them to the target joint probability distribution for adjusting cluster centroids, \u03bc and \u03bc , and eventually improving the cluster purity. This semi-supervised method helps us to learn the optimal representations of image and text features and apply Multi-modal DEC to HAR.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 306,
                    "end": 314,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Overview"
        },
        {
            "text": "Instagram data consists of image-text pairs where text is given as a mixture of a caption and multiple hashtags. For image preprocessing, we extract the 2048-dimensional feature representations of ResNet-50 [17] pretrained on Ima-geNet dataset [18] . We use this embedding method for image data because these features are known to be effective in image clustering as well as classification [19] . We compress the extracted 2048-dimensional features once again into 300dimensions using Principal Component Analysis (PCA) [20] . For text preprocessing, we split a text into separate words using a Korean tokenizer [14] , and embed them onto 300-dimensions of Doc2Vec [15] Skip-gram feature space. It is verified that Doc2vec trained in a large corpora can produce robust vector representations for long text paragraphs [16] . ",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 244,
                    "end": 248,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 390,
                    "end": 394,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 520,
                    "end": 524,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 612,
                    "end": 616,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 665,
                    "end": 669,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 817,
                    "end": 821,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Multi-modal Data Pre-processing"
        },
        {
            "text": "Once the preprocessed image and text data points, X and X , are given, two embedding networks with initial parameters, \u03b8 and \u03b8 , are created as in the original MultiDEC [7] . For the embedding networks, we train two symmetric stacked autoencoders which contain encoding and decoding layers; since the networks are symmetric, we describe only one embedding network for image data hereafter, supposing that text data embedding is gone through the same procedure, simultaneously. The stacked autoencoder with parameter \u03b8 compresses the input data X into a latent space Z in the encoder of stacked DNN layers and regenerates X * from Z in the decoder with the minimized mean square error loss between X and X * . To leverage the embedded features for HAR with j human activity classes, we apply the K-means algorithm to Z where k equals to j and generate j initial clusters with centroids \u03bc. Then, to associate the generated j clusters with the most relevant human activity classes, we generate a j \u00d7 j confusion matrix. The (m, n) element of this matrix indicates how many data with the mth class label is contained in the nth cluster. We create a cost matrix by subtracting the maximum value of the confusion matrix from the value of each cell and find the class assignment that minimizes the cost by applying the Hungarian algorithm. Finally, we rearrange the centroids \u03bc to follow the assignment we find (Fig. 2) .",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 172,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 1404,
                    "end": 1412,
                    "text": "(Fig. 2)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Embedding Network Initialization"
        },
        {
            "text": "When the latent features are ready to be associated with the label-oriented supervision, we initiate the supervised learning phase. In this phase, we optimize the model by minimizing the cross-entropy between the soft assignment probabilities, q ij and r ij , of the image and text samples, x i and x i , from the labeled training set L and the given class label indicators, y ij , respectively. y ij is a binary indicator and it is assigned by 1 if a data point x i is assigned to the cluster of its correct class label j and closely located to its centroid, otherwise 0. Soft Assignment. We calculate the image soft assignment probability, q ij , which is the similarity between the image embedding, z i , and the image cluster centroid, \u03bc j , by making use of the Student's t-distribution [22] on 1 degree of freedom (Eq. 1). Similarly, we calculate the text soft assignment probability, r ij , using the text embedding, z i , and the text cluster centroid, \u03bc j , (Eq. 2).",
            "cite_spans": [
                {
                    "start": 792,
                    "end": 796,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Supervised Learning"
        },
        {
            "text": "Cross Entropy Minimization. The supervised loss functions for image and text models are defined by a sum of cross-entropy values between the calculated soft assignment probability and the given class label indicator of each sample x i \u2208 L as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Learning"
        },
        {
            "text": "During the supervised learning phase, our model learns to locate the embedded feature z i of the labeled data points x i \u2208 L as close as possible to the centroid \u03bc j of each labeled class j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Learning"
        },
        {
            "text": "In the unsupervised learning phase, our model is trained by using deep embedded clustering both on the unlabeled and labeled datasets, U \u222a L. This learning proceeds by minimizing the KL-divergence of q and r against the generated target probability distribution p.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised Learning"
        },
        {
            "text": "Joint Target Distribution. The target distribution p ij is computed using q ij and r ij jointly. We apply the second power distribution to q ij and r ij , respectively, in order to improve cluster purity and give more emphasis on data points assigned with high confidence as proposed in the DEC [6] . We take the mean distribution of them to calculate both q ij and r ij evenly (i.e., late fusion), following the MultiDEC [7] .",
            "cite_spans": [
                {
                    "start": 295,
                    "end": 298,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 422,
                    "end": 425,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Unsupervised Learning"
        },
        {
            "text": "KL Divergence Minimization. Once the joint target distribution is computed, we train our model by minimizing KL divergence with the unsupervised loss functions defined in Eq. 6 and 7. In addition to KL divergence minimization among p, q and r, DEC models can be trained by introducing extra losses between the mean h of the target probability distribution p and the prior knowledge w of the class distribution [7] . Here, our prior knowledge is obtained from the class distribution of L.",
            "cite_spans": [
                {
                    "start": 410,
                    "end": 413,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Unsupervised Learning"
        },
        {
            "text": "where h j = i p ij /N and w j = i y ij /N .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised Learning"
        },
        {
            "text": "Require: models M = (Mimg, Mtxt), labeled set L = (Limg, Ltxt), unlabeled set U = (Uimg, Utxt), supervised learning rate \u03b7s, unsupervised learning rate \u03b7u. Initialization:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. Semi-Supervised Multi-Modal DEC"
        },
        {
            "text": "based on loss defined in (4) Unsupervised Learning :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. Semi-Supervised Multi-Modal DEC"
        },
        {
            "text": "based on loss defined in (6) Mtxt \u2190 train model (P , R2) based on loss defined in (7) until end condition is met;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. Semi-Supervised Multi-Modal DEC"
        },
        {
            "text": "In this phase, we set a learning rate, \u03b7 u , smaller than that in the supervised learning phase, \u03b7 s , so that Z L is not affected too much by U ; in this work, we use \u03b7 u = \u03ba \u00d7 \u03b7 s , where \u03ba is an input parameter. Our learning method leverages the feature distributions of U and L together to make our model learn more generalized parameters, \u03b8 and \u03bc, and prevent the trained model from being overfitted to L. Algorithm 1 presents how the presented algorithms are used together through the entire procedure.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. Semi-Supervised Multi-Modal DEC"
        },
        {
            "text": "For dataset construction, we have collected geo-tagged Instagram posts containing various human activities in urban places nearby 25 stations on subway line 2 in Seoul from January 2015 to December 2018. Our collection is limited to Korean Instagram posts with non-empty captions. We have refined the captions by removing URLs, numbers, email addresses, or emoticons. We filtered out spam posts created multiple times by the same author, with the same caption, or at the same location. If a single post has more than one image, we take only the first image. Eventually, we constructed a dataset of 967,598 image-text pairs. For data labeling, we use major human activity classes in American Time Use Survey (ATUS) taxonomy [12] which has been widely-used for HAR. To establish a qualified dataset with a consensus mechanism, we first gathered 27 participants and divided them into three groups. Then, each group is given the same set of Instagram posts and asked to annotate the most likely human activity that each post represents. We use only posts that more than two participants agreed on the same human activity class label. Table 1 shows 23 human activity class labels and their corresponding counts. Out of 17 ATUS classes, Socializing, Relaxing, and Leisure class appears too frequently in social media. We divide its Instagram posts into their sub-classes, Socializing & Communicating, Attending or Hosting Social Events, Relaxing & Leisure, and Arts & Entertainment (other than sports), defined in ATUS taxonomy. Additionally, we add Advertisement and Unknown classes for filtering out spam and ambiguous posts. Eventually, we establish an HAR dataset of 16,894 labeled posts. From the dataset, we use 16,248 posts of 12 most frequently appearing classes for this evaluation, where they are marked as asterisks.",
            "cite_spans": [
                {
                    "start": 723,
                    "end": 727,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 1130,
                    "end": 1137,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Dataset Construction"
        },
        {
            "text": "For evaluation metric, we adopt accuracy score, macro f1 score, and Normalized Mutual Information (NMI). Accuracy score indicates the straightforward recognition performance, macro f1 score is a normalized HAR performance metric, and NMI indicates similarity between the probability distributions of the actual classes of the test set and the probability distributions of the predicted classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Setup"
        },
        {
            "text": "With these three standard metrics, we perform five-fold cross-testing and measure the average of the results. For training our model, we use unlabeled data with the training set of labeled data. In addition, we use the stochastic gradient descent (SGD) optimizer with a batch size of 256 and a learning rate of 0.01 (\u03b7 s ). For training the stacked autoencoders in our model, we use the same configurations (i.e., layer structure and hyper-parameters) as in the original DEC model [6] .",
            "cite_spans": [
                {
                    "start": 481,
                    "end": 484,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Setup"
        },
        {
            "text": "For a comparative evaluation, we implement baseline models with different data modalities. For text-only baseline models, we implement a linear SVM with a TF-IDF text input vector (TF-IDF+SVM) [1] , an LSTM with a text Word2Vec input (Word2Vec+LSTM) [3] , and an LSTM with dictionary embedding and a Word2Vec input (Word2Vec+LSTM+DE). For image-only baseline models, we implement a ResNet50 model which is pre-trained on ImageNet. This model is known to be robust to image classification [21] but no empirical results has been presented on HAR using social media data yet. To evaluate whether our proposed model utilizes the multi-modality of image-sharing social media data effectively and enhances the HAR performance, we also implement semi-supervised DEC models using uni-modality (i.e., text or image only) and compare their performance with that of the multi-modal one.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 196,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 250,
                    "end": 253,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 488,
                    "end": 492,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Setup"
        },
        {
            "text": "Evaluation results are summarized in Table 2 . Above all, we find that our proposed model (Semi-supervised multi DEC) performs the best for HAR in all evaluation metrics. For example, in terms of accuracy score, its performance is 7.43% and 11.89% higher than the best performance of the text-only and image-only models, respectively. In terms of a normalized metric, macro f1 score, our proposed model performs 5.69% and 21.41% better than the text-only and image-only models. The higher NMI of our proposed model indicates that the semi-supervised multi-modal method is more effective for achieving an optimal clustering to the correct distribution than the baseline approaches. Considering that the semi-supervised uni-modal DEC performs worse than the baseline models in text-only case, we can deduce that our proposed model's achievement of the high accuracy is not attributed to the DEC component. In addition, by comparing the semi-supervised models with multi-modality and uni-modality, we can clearly find that incorporating multi-modal data helps to improve the HAR performance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 37,
                    "end": 44,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Evaluation Results"
        },
        {
            "text": "In order to verify the effect of semi-supervised learning using unlabeled samples, we measure the accuracy improvement as raising the number of unlabeled data while fixing the number of labeled data. Since our model generalizes itself by learning the feature distribution of the unlabeled data and avoids overfitting issue, its recognition accuracy is supposed to increase as the number of unlabeled data does. We fix the number of labeled data to 12,998 (i.e., 5-fold training data) and raise the number of unlabeled data n(U ) from 0 to 951,350. From the results in Fig. 3 (a), we observe that the accuracy increases logarithmically as the number of unlabeled data does in general. Compared to the accuracy with no unlabeled data given, we achieve about 8.9% of the performance gain when we use all unlabeled data. The accuracy increases very rapidly up to 100K unlabeled data, but the improvement speed slows down when the number of unlabeled data is from 100K to 200K. This means that at least 100K to 200K unlabeled data is required to generalize the feature distribution of 12K labeled data based on our data set. Hence, we conclude that incorporation of unlabeled data into the model training is helpful to improve HAR performance and our proposed model is capable of taking the advantage effectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 568,
                    "end": 574,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Evaluation Results"
        },
        {
            "text": "When we set the supervised learning rate and the unsupervised learning rate to the same value (\u03ba = 1), the performance is 66.56%, which is 2.41% higher than that of the baseline model. We conduct a further experiment here by putting \u03ba, an input parameter that adjusts the unsupervised learning rate, so that supervised learning is not overly influenced by unsupervised learning. As a result, our model shows the highest performance of 71.58% when \u03ba is 0.2 on our dataset when the number of unlabeled data is one million. This result assures that we can control the generalization effect of unsupervised learning with \u03ba. We note that the value of \u03ba varies according to the characteristics of the data such as the number of unlabeled data, the type of dataset, ambiguity among classes, etc. and we leave it for our future research.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Results"
        },
        {
            "text": "In this paper, we present a semi-supervised multi-modal deep embedded clustering method for human activity recognition using social media. We adopt Mul-tiDEC to leverage both image and text modalities and extend it into a semisupervised learning method. By leveraging both labeled and unlabeled data, our model learns generalized feature representations and avoids being overfitted to the labeled features. Our proposed model achieves an improved HAR accuracy, compared to those of existing uni-modal approaches. In addition, we find that the incorporation of unlabeled data into the training procedure is helpful to improve HAR performance and our proposed model is capable of taking the advantage effectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Human activity recognition using social media data",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Blanke",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Calatroni",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tr\u00f6ster",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Recognizing composite daily activities from crowdlabelled social media data",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Blanke",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tr\u00f6ster",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Pervasive Mob. Comput",
            "volume": "26",
            "issn": "",
            "pages": "103--120",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Recognizing human daily activity using social media sensors and deep learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. J. Environ. Res. Public Health",
            "volume": "16",
            "issn": "20",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Automated detection of substance userelated social media posts based on image and text analysis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Paul",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Pirsiavash",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)",
            "volume": "",
            "issn": "",
            "pages": "772--779",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Detecting sarcasm in multimodal social platforms",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schifanella",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "De Juan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tetreault",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 24th ACM International Conference on Multimedia",
            "volume": "",
            "issn": "",
            "pages": "1136--1145",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Unsupervised deep embedding for clustering analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "478--487",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "MultiDEC: multi-modal clustering of imagecaption pairs",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "H"
                    ],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Howe",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1901.01860"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Semi-supervised learning with deep embedded clustering for image classification and segmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Enguehard",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "O&apos;halloran",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gholipour",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Access",
            "volume": "7",
            "issn": "",
            "pages": "11093--11104",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Social media in health care: the case for organizational policy and employee education",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cain",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Am. J. Health-Syst. Pharm",
            "volume": "68",
            "issn": "11",
            "pages": "1036--1040",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A personalized time-bound activity recommendation system",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Sinha",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)",
            "volume": "",
            "issn": "",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Instilling social to physical: co-regularized heterogeneous transfer learning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "W K"
                    ],
                    "last": "Leung",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Thirtieth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Developing the American time use survey activity classification system",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Shelley",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Monthly Lab. Rev",
            "volume": "128",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Introduction to Semi-Supervised Learning",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Goldberg",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
            "volume": "3",
            "issn": "1",
            "pages": "1--130",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "KoNLPy: Korean natural language processing in Python",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 26th Annual Conference on Human & Cognitive Language Technology",
            "volume": "6",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Distributed representations of sentences and documents",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1188--1196",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "An empirical evaluation of doc2vec with practical insights into document embedding generation",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lau",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Baldwin",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1607.05368"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "ImageNet: a large-scale hierarchical image database",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "248--255",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "CNN features are also great at unsupervised classification",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gu\u00e9rin",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Gibaru",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Thiery",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Nyiri",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1707.01700"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Principal component analysis: a review and recent developments",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "T"
                    ],
                    "last": "Jolliffe",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cadima",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Philos. Trans. R. Soc. A Math. Phys. Eng. Sci",
            "volume": "374",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Is robustness the cost of accuracy?-A comprehensive study on the robustness of 18 deep image classification models",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "Y"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "volume": "",
            "issn": "",
            "pages": "631--648",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Visualizing data using t-SNE",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Der Maaten",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "J. Mach. Learn. Res",
            "volume": "9",
            "issn": "",
            "pages": "2579--2605",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An overview of our proposed semi-supervised multi-modal DEC",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Core components of semi-supervised multi-modal DEC",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "a) different number of unlabeled posts (b) different value of \u03ba (n(U ) = 1M) The result of semi-supervised multi-DEC accuracy",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The number of labeled Instagram posts for each human activity classes",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The result across different modals",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}