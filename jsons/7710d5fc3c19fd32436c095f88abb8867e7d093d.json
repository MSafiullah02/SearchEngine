{
    "paper_id": "7710d5fc3c19fd32436c095f88abb8867e7d093d",
    "metadata": {
        "title": "RETINAMASK: A FACE MASK DETECTOR",
        "authors": [
            {
                "first": "Mingjie",
                "middle": [],
                "last": "Jiang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Hong",
                    "location": {
                        "settlement": "Kong Hong Kong"
                    }
                },
                "email": ""
            },
            {
                "first": "Xinqi",
                "middle": [],
                "last": "Fan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Hong",
                    "location": {
                        "settlement": "Kong Hong Kong"
                    }
                },
                "email": "xinqifan2-c@my.cityu.edu.hk"
            }
        ]
    },
    "abstract": [
        {
            "text": "Coronavirus disease 2019 has affected the world seriously, because people cannot work as usual in case of infection. One of the effective protection methods for human beings is to wear masks in public areas. Furthermore, many public service providers require customers to use the service only if they wear masks correctly. However, there are only a few research studies about face mask detection. To contribute to public healthcare for human beings, we propose RetinaMask, which is a high-accuracy and efficient face mask detector. The proposed RetinaMask is a one-stage detector, which consists of a feature pyramid network to fuse high-level semantic information with multiple feature maps, and a novel context attention module to focus on detecting face masks. In addition, we also propose a novel cross-class object removal algorithm to reject predictions with low confidences and the high intersection of union. Experimental results show that RetinaMask achieves state-of-the-art results on a public face mask dataset with 2.3% and 1.5% higher than the baseline result in the face and mask detection precision, respectively, and 11.0% and 5.9% higher than baseline for recall. Besides, we also explore the possibility of implementing RetinaMask with a light-weighted neural network MobileNet for embedded or mobile devices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The situation report 96 of world health organization (WHO) [1] presented that coronavirus disease 2019 (COVID- 19) has globally infected over 2.7 million people and caused over 180,000 deaths. In addition, there are several similar large scale serious respiratory diseases, such as severe acute respiratory syndrome (SARS) and the Middle East respiratory syndrome (MERS), which occurred in the past few years [2, 3] . Liu et al. [4] reported that the reproductive number of COVID-19 is higher compared to the SARS. Therefore, more and more people are concerned about their health, and public health is considered as the top priority for governments [5] . Fortunately, Leung et al. [6] showed that the surgical face masks could cut the spread of coronavirus. At the moment, WHO recommends that people should wear face masks if they have respiratory symptoms, or they are taking care of the people with symptoms [7] . Furthermore, many public service providers require customers to use the service only if they wear masks [5] . Therefore, face mask detection has become a crucial computer vision task to help the global society, but research related to face mask detection is limited.",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 62,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 111,
                    "end": 114,
                    "text": "19)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 409,
                    "end": 412,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 413,
                    "end": 415,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 429,
                    "end": 432,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 649,
                    "end": 652,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 681,
                    "end": 684,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 910,
                    "end": 913,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Face mask detection refers to detect whether a person wearing a mask or not and what is the location of the face [9] . The problem is closely related to general object detection to detect the classes of objects and face detection is to detect a particular class of objects, i.e. face [10, 11] . Applications of object and face detection can be found in many areas, such as autonomous driving [12] , education [13] , surveillance and so on [10] . Traditional object detectors are usually based on handcrafted feature extractors. Viola Jones detector uses Haar feature with integral image method [14] , while other works adopt different feature extractors, such as histogram of oriented gradients (HOG), scale-invariant feature transform (SIFT) and so on [15] . Recently, deep learning based object detectors demonstrated excellent performance and dominate the development of modern object detectors. Without using prior knowledge for forming feature extractors, Figure 1 : Examples of Face Mask Dataset [8] deep learning allows neural networks to learn features with an end-to-end manner [16] . There are one-stage and two-stage deep learning based object detectors. One-stage detectors use a single neural network to detect objects, such as single shot detector (SSD) [17] and you only look once (YOLO) [18] . In contrast, two-stage detectors utilize two networks to perform a coarse-to-fine detection, such as region-based convolutional neural network (R-CNN) [19] and faster R-CNN [20] . Similarly, face detection adopts similar architecture as general object detector, but adds more face related features, such as facial landmarks in RetinaFace [21] , to improve face detection accuracy. However, there is rare research focusing on face mask detection.",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 116,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 284,
                    "end": 288,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 289,
                    "end": 292,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 392,
                    "end": 396,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 409,
                    "end": 413,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 439,
                    "end": 443,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 594,
                    "end": 598,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 753,
                    "end": 757,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1087,
                    "end": 1091,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1268,
                    "end": 1272,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1483,
                    "end": 1487,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1648,
                    "end": 1652,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 961,
                    "end": 969,
                    "text": "Figure 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we proposed a novel face mask detector, RetinaMask, which is able to detect face masks and contribute to public healthcare. To the best of our knowledge, RetinaMask is one of the first dedicated face mask detectors. In terms of the network architecture, RetinaMask uses multiple feature maps and then utilizes feature pyramid network (FPN) to fuse the high-level semantic information. To achieve better detection, we propose a context attention detection head and a cross-class object removal algorithm to enhance the detection ability. Furthermore, since the face mask dataset is a relatively small dataset where features may be hard to extract, we use transfer learning to transfer the learned kernels from networks trained for a similar face detection task on an extensive dataset. The proposed method is tested on a face mask dataset [8] , whose examples can be found in Fig. 1 . The dataset covers a various masked or unmasked faces images, including faces with masks, faces without masks, faces with and without masks in one image and confusing images without masks The experimental results shows that RetinaMask achieves state-of-the-art results, which is 2.3% and 1.5% higher than the baseline result in face and mask detection precision respectively, and 11.0% and 5.9% higher than baseline for recall.",
            "cite_spans": [
                {
                    "start": 853,
                    "end": 856,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 890,
                    "end": 896,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "The rest of this paper is organized as follows. In Section II, we review related works on object detection and neural networks. The proposed methodology is presented in Section III. Section IV describes datasets, experiment settings, evaluation metrics, results, and ablation study. Finally, Section V concludes the paper and discusses the future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Traditional object detection uses a multi-step process [22] . The most famous detector is Viola Joins detector, which is able to achieve real-time detection [14] . The algorithm extracts feature by Haar feature descriptor with an integral image method, selects useful features, and detects objects through a cascaded detector. Although it utilizes integral image to facilitate the algorithm, it is still very computationally expensive. In [23] for human detection, an effective feature extractor called HOG is proposed, which computes the directions and magnitudes of oriented gradients over image cells. Later on, deformable part-based model (DPM) detects objects parts and then connects them to judge classes that objects belong to [15] .",
            "cite_spans": [
                {
                    "start": 55,
                    "end": 59,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 157,
                    "end": 161,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 439,
                    "end": 443,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 734,
                    "end": 738,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Object Detection"
        },
        {
            "text": "Rather than using handcrafted features, deep learning based detector demonstrated excellent performance in these years, due to its robustness and high feature extraction capability [22] . There are two popular categories, one-stage object detectors and two-stage object detectors. Two-stage detector generates region proposals in the first stage and then fine-tune these proposals in the second stage. The two-stage detector can provide high detection performance but with low speed. The seminal work R-CNN is proposed by R. Girshick et al. [19] . R-CNN uses selective search to propose some candidate regions which may contain objects. After that, the proposals are fed into a CNN model to extract features, and a support vector machine (SVM) is used to recognize classes of objects. However, the second-stage of R-CNN is computationally expensive, since the network has to detect proposals on a one-by-one manner and uses pooling layer to input all proposal regions at once [24] . Finally, a region proposal network (RPN) is proposed in faster R-CNN to take the place of selective search, which limits the speed of such detectors [20] . Faster R-CNN integrates each individual detection components, such as region proposal, feature extractor, detector into an end-to-end neural network architecture. One-stage detector utilizes only a single neural network to detect objects. In order to achieve this, some anchor boxes which specifies the ratio of width and heights of objects should be predefined [22] . Rather than the two-stage detector, one-stage detectors scarify a little performance for improving the speed of detection. In order to achieve the goal, YOLO divided the image into several cells and then tried to match the anchor boxes to objects for each cell, but this approach is not good for small objects [18] . The researchers found that one-stage detector performs not well by using the last feature output only, because the last feature map has fixed receptive fields, which can only observe certain areas on original images. Therefore, multi-scale detection has been introduced in SSD, which conducts detection on several feature maps to allow to detect faces in different sizes [17] . Later on, in order to improve detection accuracy, Lin et. al [25] proposes Retina Network (RetinaNet) by combining an SSD and FPN architecture, which also include a novel focal loss function to mitigate class imbalance problem.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 185,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 541,
                    "end": 545,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 976,
                    "end": 980,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1132,
                    "end": 1136,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1818,
                    "end": 1822,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 2196,
                    "end": 2200,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 2264,
                    "end": 2268,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Object Detection"
        },
        {
            "text": "CNN plays an important role in computer vision related pattern recognition tasks, because of its superior spatial feature extraction capability and fewer computation cost [26] . CNN uses convolution kernels to convolve with the original images or feature maps to extract higher-level features. However, how to design better convolutional neural network architectures still remains as an opening question. Inception network proposed in [27] allows the network to learn the best combination of kernels. In order to train much deeper neural networks, K. He et al. propose Residual Network (ResNet) [28] , which can learn an identity mapping from the previous layer. As object detectors are usually deployed on mobile or embedded devices, where the computation resources are very limited, Mobile Network (MobileNet) [29] is proposed. It uses depth-wise convolution to extract features and channel wised convolutions to adjust channel numbers, so the computational cost of MobileNet is much lower than networks using standard convolutions.",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 175,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 435,
                    "end": 439,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 595,
                    "end": 599,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 812,
                    "end": 816,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "Attention mechanism is used to mimic human attention, which can focus on important information. Attention is first used in recurrent neural network (RNN) with a decoder-encoder mechanism is introduced in [30] . In the convolutional block attention module (CBAM), a simpler but effective convolutional attention mechanism is proposed, which contains spatial and channel attention [31] . Spatial attention uses max and average pooling to learn a spatial attention map. Channel attention aims to learn a set of channel attention maps by training the max and average pooling output into a ",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 208,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 379,
                    "end": 383,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Attention Mechanism"
        },
        {
            "text": "The architecture of proposed RetinaMask is shown in Fig. 2 . In order to design an effective network for face mask detection, we adopt the object detector framework proposed in [32] , which suggests a detection network with a backbone, a neck and heads. The backbone refers to a general feature extractor made up of convolutional neural networks to extract information in images to feature maps. In RetinaMask, we adopt ResNet as a standard backbone, but also include MobileNet as a backbone for comparison and for reducing computation and model size in deployment scenarios with limited computing resources. In terms of the neck, it is an intermediate component between a backbone and heads, and it can enhance or refine original feature maps. In RetinaMask, FPN is applied as a neck, which can extract high-level semantic information and then fuse this information into previous layers' feature maps by adding operation with a coefficient. Finally, heads stand for classifiers, predictors, estimators, etc., which are able to achieve the final objectives of the network. In RetinaMask, we adopt a similar multi-scale detection strategy as SSD to make a prediction with multiple FPN feature maps, because it can have different receptive fields to detect various sizes of objects. Particularly, RetinaMask utilizes three feature maps, and each of them is fed into a detection head. Inside each detection head, we further add a context attention module to adjust the size of receptive fields and focus on specific areas, which is similar to Single Stage Headless (SSH) [33] but with an attention mechanism. The output of the detection head is through a fully convolutional network rather than a fully connected network to further reduce the number of parameters in the network. We name the detector as RetinaMask, since it follows the architecture of RetinaNet, which consists of a SSD and a FPN, and able to detect small face masks as well.",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1568,
                    "end": 1572,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [
                {
                    "start": 52,
                    "end": 58,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Network Architecture"
        },
        {
            "text": "Due to the limited size of the face mask dataset, it is difficult for learning algorithms to learn better features. As deep learning based methods often require larger dataset, transfer learning is proposed to transfer learned knowledge from a source task to a related target task. According to [34] , transfer learning has helped with the learning in a significant way as long as it has a close relationship. In our work, we use parts of the network pretrained on a large scale face detection dataset -widerface, which consists of 32,203 images and 393,703 annotated faces [35] . In RetinaMask, only the parameters of the backbone and neck are transferred from widerface, the heads are initialized as Xaiver. In addition, pretrained imagenet [26] weights are considered as a standard initialization of our backbones in basic cases.",
            "cite_spans": [
                {
                    "start": 295,
                    "end": 299,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 574,
                    "end": 578,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 743,
                    "end": 747,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Transfer learning"
        },
        {
            "text": "To improve the detection performance for face masks, RetinaMask proposes a novel context attention module as its detection heads (Fig. 3) . Similar to the context module in SSH, we utilize different sizes of kernels to form an Inception-like block. It is able to obtain different receptive fields from the same feature map, so it would be able to incorporate more different sizes of objects through concatenation operations. However, the original context module does not take into faces or masks into account, so we simply cascade an attention module CBAM after the original context module to allow RetinaMask to focus on face and masks features. The context-aware part has three subbranches, including one 3 \u00d7 3, two 3 \u00d7 3, and three 3 \u00d7 3 kernels in each branch individually. Then, the concatenated feature maps are fed into CBAM through a channel attention to select useful channels by a multi-layer perceptron and then a spatial attention to focus on important regions. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 129,
                    "end": 137,
                    "text": "(Fig. 3)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Context Attention Module"
        },
        {
            "text": "RetinaMask yields two outputs for each input image, localization offset prediction, Y loc \u2208 R p\u00d74 and classification prediction, Y c \u2208 R p\u00d7c , where p and c denote the number of generated anchors and the number of classes. We also have the default anchors P \u2208 R p\u00d74 , the ground truth boxes, Y loc \u2208 R o\u00d74 and the classification label Y c \u2208 R o\u00d71 , where o refers to the number of object.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Loss Function"
        },
        {
            "text": "First, we matched the default anchors P with the ground truth boxes Y loc and the classification label Y c to obtain P ml \u2208 R p\u00d74 and P mc \u2208 R p\u00d71 , where each row in P ml and P mc denote the offsets and top classification label for each default anchor, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Loss Function"
        },
        {
            "text": "Here we defined positive localization prediction and positive matched default anchors, Y + loc \u2208 R p+\u00d74 and P + ml \u2208 R p+\u00d71 , where p + denotes the number of default anchors whose top classification label is not zero. Then we computed the smooth loss between Y + loc and P + ml , L loc ( Y + loc , P + ml ). Then we perform hard negative mining [36] , and the sampling negative default and predictive anchors, P \u2212 mc \u2208 R p\u2212\u00d71 and Y \u2212 c \u2208 R p\u2212\u00d71 , where p \u2212 is the number of sampling negative anchors. We computed the confidence loss by",
            "cite_spans": [
                {
                    "start": 345,
                    "end": 349,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Loss Function"
        },
        {
            "text": ". Therefore, the entire loss function is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Loss Function"
        },
        {
            "text": "where N is the number of matched default anchors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Loss Function"
        },
        {
            "text": "In inference, the model produced the object localization D \u2208 R p\u00d74 and object confidence C \u2208 R p\u00d73 , where the second column of C , C f \u2208 R p\u00d71 , was the confidence of face; the third column of C, C m \u2208 R p\u00d71 , was the confidence of mask. We removed the object with the confidence being lower than t c and then performed the non maximum suppression (NMS) to produce D f \u2208 R f \u00d74 , C f \u2208 R f \u00d71 and D m \u2208 R m\u00d74 , C m \u2208 R m\u00d71 , where f and m denote the number of selected faces and masks, respectively. Here, we proposed a novel method, object removal cross classes (ORCC), to remove the object which had high intersect-over-union (IoU) with other objects and lower confidence. The concrete algorithm is given in Algorithm 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inference"
        },
        {
            "text": "Face Mask Dataset [8] contains 7959 images, and its faces are annotated with either with a mask or without a mask. However, Face Mask Dataset a combined dataset made up of widerface [35] and MAsked FAces (MAFA) dataset [37] . widerface contains 32,203 images with 393,703 normal faces with various illumination, pose, occlusion etc. MAFA contains 30,811 images and 34,806 masked faces, but some faces are masked by hands or other objects rather than physical masks, which brings advantages to the dataset to improves variants of Face Mask Dataset. Some samples are shown in Fig. 1 , including faces with masks, faces without masks, faces with and without masks in one image and confusing images without masks. ",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 21,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 182,
                    "end": 186,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 219,
                    "end": 223,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [
                {
                    "start": 574,
                    "end": 580,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "In the experiments, we employed stochastic gradient descent (SGD) with learning rate \u03b1 = 10 \u22123 , momentum \u03b2 = 0.9 and 250 epochs as optimization algorithm. We trained the models on a NVIDIA GeForce RTX 2080 Ti. The dataset has been split into a train, validation and test set with 4905, 1226 and 3678 images individually. The algorithm is developed with PyTorch [38] deep learning framework and the implementation is also based on RetinaFace [21] . Each experiment operates 250 epochs. For ResNet backbone, the input image size is 840 \u00d7 840 with batch size 2; for MoileNet backbone, the input image size is 640 \u00d7 640 with batch size 32.",
            "cite_spans": [
                {
                    "start": 362,
                    "end": 366,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 442,
                    "end": 446,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "We employed precision and recall as metrics, they are defined as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "where T P , F P and F N denoted the true positive, false positive and false negative, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "The performance of RetinaMask is compared with a public baseline result published by the creator of the dataset [8] .",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 115,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Result and Analysis"
        },
        {
            "text": "Due to the limited methods proposed for this dataset, we also used ResNet and MobileNet as different backbones for comparison. The accuracy is given in Table 1 . Table 1 showed that RetinaMask with ResNet-backbone achieved higher accuracy than with MobileNet-backbone by around 10% in precision and recall respectively. In addition, RetinaMask with ResNet-backbone achieves the state-of-the-art result compared to the baseline model. Particularly, RetinaMask is 2.3% and 1.5% higher than the baseline result in face and mask detection precision respectively, and 11.0% and 5.9% higher than baseline for recall. Fig. 4 , where the red and green boxes refer to the face and mask predictions, respectively. Figure 4 indicates that RetinaMask can recognize the confusing face without mask.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 152,
                    "end": 159,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 162,
                    "end": 169,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 611,
                    "end": 617,
                    "text": "Fig. 4",
                    "ref_id": null
                },
                {
                    "start": 704,
                    "end": 712,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "Result and Analysis"
        },
        {
            "text": "We perform ablation study on transfer learning, attention mechanism and backbones. The experimental results are summarized in Table 2 and details are given as follows.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 126,
                    "end": 133,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Ablation Study"
        },
        {
            "text": "Transfer Learning. All experiments adopt pretained weights from the imagenet dataset [26] for backbones. This setup is considered as a baseline result in our comparison. The ablation study of transfer learning means that the network uses pretrained weights from a similar task -face detection, and trained on a specific dataset widerface. The results using MobileNet backbone show that transfer learning can dramatically increase the detection performance by 3 \u2212 4% in both face and mask detection results. One possible reason is that the enhanced feature extraction ability is enhanced by utilizing pretrained weights from a closely related task.",
            "cite_spans": [
                {
                    "start": 85,
                    "end": 89,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Ablation Study"
        },
        {
            "text": "Attention Mechanism. We perform experiments to evaluate the performance of the proposed context attention detection head. It can be shown that, by using attention mechanism, there is a 1.5% increase in face detection precision and 3.9% increase in mask detection precision with MoileNet backbone. These results demonstrate that the attention modules are able to focus on the desired face and mask features to improve the final detection performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ablation Study"
        },
        {
            "text": "Backbone. Although different network components are able to increase the detection performance, the biggest accuracy improvement is achieved by ResNet backbone. From Table 2 , it can be seen that the precision of ResNet with imagenet pretrain is more than 10% compared to that with MobileNet backbone in both face and mask detection precision. However, other components do not work well on ResNet, but this may be due to that the network is achieving the limitation of the dataset performance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 173,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Ablation Study"
        },
        {
            "text": "In this paper, we have proposed a novel face mask detector, namely RetinaMask, which can possibly contribute to public healthcare. The architecture of RetinaMask consists of ResNet or MobileNet as the backbone, FPN as the neck, and context attention modules as the heads. The strong backbone ResNet and light backbone MobileNet can be used for high and low computation scenarios, respectively. In order to extract more robust features, we utilize transfer learning to adopt weights from a similar task face detection, which is trained on a very large dataset. Furthermore, we have proposed a novel context attention head module to focus on the face and mask features, and a novel algorithm object removal cross class, i.e. ORCC, to remove objects with lower confidence and higher IoU. The proposed method achieves state-of-the-art results on a public face mask dataset, where we are 2.3% and 1.5% higher than the baseline result in the face and mask detection precision respectively, and 11.0% and 5.9% higher than baseline for recall.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "World Health Organization et al. Coronavirus disease 2019 (covid-19): situation report",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Characterization of a novel coronavirus associated with severe acute respiratory syndrome. science",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Paul A Rota",
                    "suffix": ""
                },
                {
                    "first": "Stephan",
                    "middle": [
                        "S"
                    ],
                    "last": "Steven Oberste",
                    "suffix": ""
                },
                {
                    "first": "Allan",
                    "middle": [],
                    "last": "Monroe",
                    "suffix": ""
                },
                {
                    "first": "Ray",
                    "middle": [],
                    "last": "Nix",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Campagnoli",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Joseph",
                    "suffix": ""
                },
                {
                    "first": "Silvia",
                    "middle": [],
                    "last": "Icenogle",
                    "suffix": ""
                },
                {
                    "first": "Bettina",
                    "middle": [],
                    "last": "Penaranda",
                    "suffix": ""
                },
                {
                    "first": "Kaija",
                    "middle": [],
                    "last": "Bankamp",
                    "suffix": ""
                },
                {
                    "first": "Min-Hsin",
                    "middle": [],
                    "last": "Maher",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "300",
            "issn": "",
            "pages": "1394--1399",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Family cluster of middle east respiratory syndrome coronavirus infections",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ziad",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Memish",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Alimuddin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zumla",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Rafat",
                    "suffix": ""
                },
                {
                    "first": "Abdullah",
                    "middle": [
                        "A"
                    ],
                    "last": "Al-Hakeem",
                    "suffix": ""
                },
                {
                    "first": "Gwen",
                    "middle": [
                        "M"
                    ],
                    "last": "Al-Rabeeah",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Stephens",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "New England Journal of Medicine",
            "volume": "368",
            "issn": "26",
            "pages": "2487--2494",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "The reproductive number of covid-19 is higher compared to sars coronavirus",
            "authors": [
                {
                    "first": "Ying",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Albert",
                    "middle": [
                        "A"
                    ],
                    "last": "Gayle",
                    "suffix": ""
                },
                {
                    "first": "Annelies",
                    "middle": [],
                    "last": "Wilder-Smith",
                    "suffix": ""
                },
                {
                    "first": "Joacim",
                    "middle": [],
                    "last": "Rockl\u00f6v",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of travel medicine",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Transmission dynamics of the covid-19 outbreak and effectiveness of government interventions: A data-driven analysis",
            "authors": [
                {
                    "first": "Yaqing",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Yiting",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "Marshare",
                    "middle": [],
                    "last": "Penny",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of medical virology",
            "volume": "92",
            "issn": "6",
            "pages": "645--659",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Respiratory virus shedding in exhaled breath and efficacy of face masks",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "L"
                    ],
                    "last": "Nancy",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Leung",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "W"
                    ],
                    "last": "Daniel",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "C"
                    ],
                    "last": "Eunice",
                    "suffix": ""
                },
                {
                    "first": "Kwok-Hung",
                    "middle": [],
                    "last": "Shiu",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [
                        "J"
                    ],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mcdevitt",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Benien",
                    "suffix": ""
                },
                {
                    "first": "Hui-Ling",
                    "middle": [],
                    "last": "Hau",
                    "suffix": ""
                },
                {
                    "first": "Yuguo",
                    "middle": [],
                    "last": "Yen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Dennis",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ip",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Malik Peiris",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Medicine",
            "volume": "",
            "issn": "",
            "pages": "1--5",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Rational use of face masks in the covid-19 pandemic. The Lancet Respiratory Medicine",
            "authors": [
                {
                    "first": "Shuo",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "Chen",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Nan",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Mengzhen",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Benjamin J",
                    "middle": [],
                    "last": "Cowling",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Detect faces and determine whether people are wearing mask",
            "authors": [
                {
                    "first": "Daniell",
                    "middle": [],
                    "last": "Chiang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Masked face recognition dataset and application",
            "authors": [
                {
                    "first": "Zhongyuan",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Guangcheng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Baojin",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Zhangyang",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "Qi",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "Hao",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Peng",
                    "middle": [],
                    "last": "Yi",
                    "suffix": ""
                },
                {
                    "first": "Kui",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Nanxi",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Yingjiao",
                    "middle": [],
                    "last": "Pei",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.09093"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Object detection with deep learning: A review",
            "authors": [
                {
                    "first": "Zhong-Qiu",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Peng",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Shou-Tao",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Xindong",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE transactions on neural networks and learning systems",
            "volume": "30",
            "issn": "",
            "pages": "3212--3232",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Face detection techniques: a review",
            "authors": [
                {
                    "first": "Ashu",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "Amandeep",
                    "middle": [],
                    "last": "Kaur",
                    "suffix": ""
                },
                {
                    "first": "Munish",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Artificial Intelligence Review",
            "volume": "52",
            "issn": "2",
            "pages": "927--948",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Deep learning and control algorithms of direct perception for autonomous driving",
            "authors": [
                {
                    "first": "Der-Hau",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kuan-Lin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Kuan-Han",
                    "middle": [],
                    "last": "Liou",
                    "suffix": ""
                },
                {
                    "first": "Chang-Lun",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jinn-Liang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1910.12031"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "How's the turnout to the class? a face detection system for universities",
            "authors": [
                {
                    "first": "Aini",
                    "middle": [],
                    "last": "Ks Savita",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hasbullah",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Shakirah Mohd Taib",
                    "suffix": ""
                },
                {
                    "first": "Manoranjitham",
                    "middle": [],
                    "last": "Izuddin Zainal Abidin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Muniandy",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 IEEE Conference on e-Learning, e-Management and e-Services (IC3e)",
            "volume": "",
            "issn": "",
            "pages": "179--184",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Rapid object detection using a boosted cascade of simple features",
            "authors": [
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Viola",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition",
            "volume": "1",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A discriminatively trained, multiscale, deformable part model",
            "authors": [
                {
                    "first": "Pedro",
                    "middle": [],
                    "last": "Felzenszwalb",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Mcallester",
                    "suffix": ""
                },
                {
                    "first": "Deva",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Deep learning for generic object detection: A survey",
            "authors": [
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Wanli",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                },
                {
                    "first": "Xiaogang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Fieguth",
                    "suffix": ""
                },
                {
                    "first": "Jie",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Xinwang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Matti",
                    "middle": [],
                    "last": "Pietik\u00e4inen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "International journal of computer vision",
            "volume": "128",
            "issn": "2",
            "pages": "261--318",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Ssd: Single shot multibox detector",
            "authors": [
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Dragomir",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "Dumitru",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "Christian",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "Scott",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "Cheng-Yang",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "Alexander C",
                    "middle": [],
                    "last": "Berg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "European conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "21--37",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "You only look once: Unified, real-time object detection",
            "authors": [
                {
                    "first": "Joseph",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "Santosh",
                    "middle": [],
                    "last": "Divvala",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "Ali",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "779--788",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "authors": [
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Donahue",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                },
                {
                    "first": "Jitendra",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "580--587",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "Shaoqing Ren",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "91--99",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Retinaface: Single-stage dense face localisation in the wild",
            "authors": [
                {
                    "first": "Jiankang",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "Jia",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Yuxiang",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Jinke",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Irene",
                    "middle": [],
                    "last": "Kotsia",
                    "suffix": ""
                },
                {
                    "first": "Stefanos",
                    "middle": [],
                    "last": "Zafeiriou",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.00641"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Object detection in 20 years: A survey",
            "authors": [
                {
                    "first": "Zhengxia",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                },
                {
                    "first": "Zhenwei",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Yuhong",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Jieping",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.05055"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Histograms of oriented gradients for human detection",
            "authors": [
                {
                    "first": "Navneet",
                    "middle": [],
                    "last": "Dalal",
                    "suffix": ""
                },
                {
                    "first": "Bill",
                    "middle": [],
                    "last": "Triggs",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "IEEE computer society conference on computer vision and pattern recognition (CVPR'05)",
            "volume": "1",
            "issn": "",
            "pages": "886--893",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Fast r-cnn",
            "authors": [
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE international conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "1440--1448",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection",
            "authors": [
                {
                    "first": "Tsung-Yi",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Priya",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Imagenet: A large-scale hierarchical image database",
            "authors": [
                {
                    "first": "Jia",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Li-Jia",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "2009 IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "248--255",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Going deeper with convolutions",
            "authors": [
                {
                    "first": "Christian",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Yangqing",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "Pierre",
                    "middle": [],
                    "last": "Sermanet",
                    "suffix": ""
                },
                {
                    "first": "Scott",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "Dragomir",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "Dumitru",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "Vincent",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Rabinovich",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Xiangyu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shaoqing",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Andrew",
                    "suffix": ""
                },
                {
                    "first": "Menglong",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Dmitry",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Weijun",
                    "middle": [],
                    "last": "Kalenichenko",
                    "suffix": ""
                },
                {
                    "first": "Tobias",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Marco",
                    "middle": [],
                    "last": "Weyand",
                    "suffix": ""
                },
                {
                    "first": "Hartwig",
                    "middle": [],
                    "last": "Andreetto",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1704.04861"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Ashish",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "Llion",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "Aidan",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "\u0141ukasz",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module",
            "authors": [
                {
                    "first": "Sanghyun",
                    "middle": [],
                    "last": "Woo",
                    "suffix": ""
                },
                {
                    "first": "Jongchan",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Open mmlab detection toolbox and benchmark",
            "authors": [
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Jiaqi",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Jiangmiao",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "Yuhang",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "Xiaoxiao",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Shuyang",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Wansen",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "Ziwei",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jiarui",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.07155"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Ssh: Single stage headless face detector",
            "authors": [
                {
                    "first": "Mahyar",
                    "middle": [],
                    "last": "Najibi",
                    "suffix": ""
                },
                {
                    "first": "Pouya",
                    "middle": [],
                    "last": "Samangouei",
                    "suffix": ""
                },
                {
                    "first": "Rama",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                },
                {
                    "first": "Larry S",
                    "middle": [],
                    "last": "Davis",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "4875--4884",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Taskonomy: Disentangling task transfer learning",
            "authors": [
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Amir R Zamir",
                    "suffix": ""
                },
                {
                    "first": "William",
                    "middle": [],
                    "last": "Sax",
                    "suffix": ""
                },
                {
                    "first": "Leonidas",
                    "middle": [
                        "J"
                    ],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Jitendra",
                    "middle": [],
                    "last": "Guibas",
                    "suffix": ""
                },
                {
                    "first": "Silvio",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Savarese",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3712--3722",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Wider face: A face detection benchmark",
            "authors": [
                {
                    "first": "Shuo",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Ping",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "Chen-Change",
                    "middle": [],
                    "last": "Loy",
                    "suffix": ""
                },
                {
                    "first": "Xiaoou",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "5525--5533",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Training region-based object detectors with online hard example mining",
            "authors": [
                {
                    "first": "Abhinav",
                    "middle": [],
                    "last": "Shrivastava",
                    "suffix": ""
                },
                {
                    "first": "Abhinav",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "761--769",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Detecting masked faces in the wild with lle-cnns",
            "authors": [
                {
                    "first": "Shiming",
                    "middle": [],
                    "last": "Ge",
                    "suffix": ""
                },
                {
                    "first": "Jia",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Qiting",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "Zhao",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2682--2690",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Paszke",
                    "suffix": ""
                },
                {
                    "first": "Sam",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                },
                {
                    "first": "Francisco",
                    "middle": [],
                    "last": "Massa",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Lerer",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Bradbury",
                    "suffix": ""
                },
                {
                    "first": "Gregory",
                    "middle": [],
                    "last": "Chanan",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Killeen",
                    "suffix": ""
                },
                {
                    "first": "Zeming",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Natalia",
                    "middle": [],
                    "last": "Gimelshein",
                    "suffix": ""
                },
                {
                    "first": "Luca",
                    "middle": [],
                    "last": "Antiga",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "8024--8035",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Architecture of RetinaMask a separate SVM for final classification. Fast R-CNN solves this problem by introducing a region of interest (ROI)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Context Attention Detection Head multi-layer perceptron. Then, the spacial attention map and channel attention map can multiply with the original feature maps with the element-wise operation to produce attention feature maps.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Object removal cross classes Require: selected face: D f , C f ; selected mask D m , C m for p f in predictive face detection D f do for p m in predictive mask detection D m do if IoU (p f , p m ) > thresh then remove the object of lower confidence end if end for end for",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "Ablation study of RetinaMask Some experimental results are shown in",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}