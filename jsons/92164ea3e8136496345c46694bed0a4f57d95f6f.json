{
    "paper_id": "92164ea3e8136496345c46694bed0a4f57d95f6f",
    "metadata": {
        "title": "Mask-Guided Region Attention Network for Person Re-Identification",
        "authors": [
            {
                "first": "Cong",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Telecommunications School of Computer Science and Technology",
                    "location": {
                        "postCode": "210023",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Han",
                "middle": [],
                "last": "Yu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Telecommunications School of Computer Science and Technology",
                    "location": {
                        "postCode": "210023",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": "han.yu@njupt.edu.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "Person re-identification (ReID) is an important and practical task which identifies pedestrians across non-overlapping surveillance cameras based on their visual features. In general, ReID is an extremely challenging task due to complex background clutters, large pose variations and severe occlusions. To improve its performance, a robust and discriminative feature extraction methodology is particularly crucial. Recently, the feature alignment technique driven by human pose estimation, that is, matching two person images with their corresponding parts, increases the effectiveness of ReID to a certain extent. However, we argue that there are still a few problems among these methods such as imprecise handcrafted segmentation of body parts, and some improvements can be further achieved. In this paper, we present a novel framework called Mask-Guided Region Attention Network (MGRAN) for person ReID. MGRAN consists of two major components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA). MRA aims to generate spatial attention masks and meanwhile mask out the background clutters and occlusions. Moreover, the generated masks are utilized for region-level feature alignment in the MA module. We then evaluate the proposed method on three public datasets, including Market-1501, DukeMTMC-reID and CUHK03. Extensive experiments with ablation analysis show the effectiveness of this method. estimation [5] [6] [7] , some researches [8-10] utilize the estimation results as spatial attention maps to learn features from pedestrian body parts and then align them. These methods achieve great success and prove that extracting features exactly from body regions rather than background regions is helpful for ReID.",
            "cite_spans": [
                {
                    "start": 1427,
                    "end": 1430,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1431,
                    "end": 1434,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1435,
                    "end": 1438,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "However, there are still notable problems in these methods concluded as follows. 1) As shown in Fig. 1 , these methods tend to extract features from imprecise part shapes set by handcraft, such as patches [1, 11] and rectangular regions of interest (RoIs) [9, 12] , which can introduce noise. 2) Part-level feature alignment which means matching two pedestrians with their heads, arms, legs, and other body parts is improper for ReID. 3) Feature representation is not accurate and comprehensive enough.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 208,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 209,
                    "end": 212,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 256,
                    "end": 259,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 260,
                    "end": 263,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 96,
                    "end": 102,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        },
        {
            "text": "In the first problem, the main reason that the handcrafted shapes cannot precisely describe the silhouettes of body parts is that the shapes of body parts are irregular. Feature alignment based on these shapes can introduce noise from background clutters, occlusions and even adjacent parts as in Fig. 1 , leading to inaccurate matching. To deal with this problem, we propose to use pedestrian masks as the spatial attention maps for masking out clutters and meanwhile obtaining the finer silhouettes of body parts both in pixel-level, as shown in Fig. 2(a) . These silhouettes obtained by pedestrian masks should be more precise and closer to the reality of body shapes. For the second problem, the works mentioned above generally align features based on part-level and this is inappropriate for ReID. As walking is a dynamic process, and in this process, the moving arms and legs have huge morphological changes and often cause heavy selfocclusion, which implies that a body part will inevitably be occluded by other parts. For example, left legs are often occluded by right legs. Due to self-occlusion, it is difficult to align features based on part-level. Furthermore, each pedestrian has his own walking postures that are different from others', which means his head, upper body and lower body have their own morphological characters when walking. But the part-level alignment may discard these characters, as shown in Fig. 2(b) . Meanwhile, the head, upper body and lower body are generally separate from each other in a walking pedestrian, which indicates there is no self-occlusion among these three parts as demonstrated in Fig. 2(b) .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 297,
                    "end": 303,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 548,
                    "end": 557,
                    "text": "Fig. 2(a)",
                    "ref_id": null
                },
                {
                    "start": 1425,
                    "end": 1434,
                    "text": "Fig. 2(b)",
                    "ref_id": null
                },
                {
                    "start": 1634,
                    "end": 1643,
                    "text": "Fig. 2(b)",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        },
        {
            "text": "Based on the above analysis, it is concluded that region-level feature alignment based on head, upper body and lower body is more reasonable for ReID. Furthermore, apart from self-occlusion, pedestrians may have some carry-on items, such as Patch Rectangular RoI Fig. 1 . Imprecise shapes of body parts set by handcraft, such as patches [1, 11] and rectangular RoIs [9, 12], include extensive noise.",
            "cite_spans": [
                {
                    "start": 337,
                    "end": 340,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 341,
                    "end": 344,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 263,
                    "end": 269,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Person re-identification (ReID) aims to identify the same individual across multiple cameras. In general, it is considered as a sub-problem of image retrieval. Given a query image containing a target pedestrian, ReID is to rank the gallery images and search for the same pedestrian. It plays an important role in various surveillance applications, such as intelligent security and pedestrian tracking.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the past years, many methods [1] [2] [3] [4] have been proposed to address the ReID problem. However, it still remains as an incomplete task due to large pose variations, complex background clutters, various camera views, severe occlusions and uncontrollable illumination conditions. Recently, with the improvement of human pose backpacks, handbags and caps. These items are definitely helpful for ReID and we can treat them as special parts of pedestrians, which should be included in the corresponding local region like in Fig. 2 (b). In the third problem, these methods like [1, 9, 12] only align the part features, considered as local features, and the global feature of the whole pedestrian region is not considered. However, each pedestrian is intuitively associated with a global feature including body shape, walking posture and so on, which cannot be replaced by local features. Due to the neglect of global features, the final feature representation will not be comprehensive and robust enough. Meanwhile, previous works [13, 14] extract the global feature from the entire pedestrian image including background clutters and occlusions, which will introduce noise and lead to the inaccuracy of feature representation. Here, we utilize pedestrian masks to redesign the global features, removing clutters with masks firstly and then extracting the global features of pedestrians. After these operations, multi-feature fusion can be used to align features.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 35,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 36,
                    "end": 39,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 40,
                    "end": 43,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 44,
                    "end": 47,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 581,
                    "end": 584,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 585,
                    "end": 587,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 588,
                    "end": 591,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1039,
                    "end": 1042,
                    "text": "14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 528,
                    "end": 534,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Based on above motivations, we propose a new Mask-Guided Region Attention Network for person re-identification. The contributions of our work can be summarized as follows: \u2022 To make the better use of feature alignment technique for person re-identification, a unified framework called Mask-Guided Region Attention Network (MGRAN) is proposed. \u2022 To further reduce the noise from background clutters and occlusions, we explore to utilize masks to separate pedestrians from them and obtain the finer silhouettes of pedestrian bodies. \u2022 Region-level feature alignment, based on head, upper body and lower body, is introduced as a more appropriate method for ReID. \u2022 We redesign the global feature and utilize multi-feature fusion to improve the accuracy and the completeness of feature representation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "2 Related Work",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, person re-identification methods based on deep learning achieved great success [13, 15, 16] . In general, these methods can be classified into two categories, namely feature representation and distance metric learning. The first category [1, 3, 17, 18] often treats ReID as a classification problem. These methods dedicate to design view-invariant representations for pedestrians. The second category [19] [20] [21] mainly aims at measuring the similarity between pedestrian images by learning a robust distance metric. Among these methods, many of them [9, 12] achieved the success by feature alignment. Numerous studies proved the importance of feature alignment for ReID. For example, Su et al. [5] proposed a Pose-driven Deep Convolutional model (PDC) that used Spatial Transformer Network (STN) to crop body regions based on pre-defined centers. Xu et al. [9] achieved the more precise feature alignment based on their proposed network called Attention-Aware Compositional Network (AACN) and further improved the performance of identification. However, these methods align the part features based on the body shapes set by handcraft, which is usually imprecise. In our model, we utilize pedestrian masks in pixel-level to align features, intending to obtain more precise information of body parts.",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 93,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 94,
                    "end": 97,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 98,
                    "end": 101,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 248,
                    "end": 251,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 252,
                    "end": 254,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 255,
                    "end": 258,
                    "text": "17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 259,
                    "end": 262,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 411,
                    "end": 415,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 416,
                    "end": 420,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 421,
                    "end": 425,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 564,
                    "end": 567,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 568,
                    "end": 571,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 708,
                    "end": 711,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 871,
                    "end": 874,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Person Re-Identification"
        },
        {
            "text": "With the rapid development of instance segmentation based on deep learning methods such as Mask R-CNN [22] and the Fully Convolutional Networks (FCN) [23] , now we can easily obtain high-quality pedestrian masks which can be used in person reidentification. Furthermore, these instance segmentation methods can be naturally extended to human pose estimation by modeling keypoint locations as one-hot masks. We can further improve the performance of person re-identification by integrating the results of instance segmentation and human pose estimation.",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 106,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 150,
                    "end": 154,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Instance Segmentation and Human Pose Estimation"
        },
        {
            "text": "Spatial attention mechanism has achieved great success in understanding images and it has been widely used in various tasks, such as semantic segmentation [24] , object detection [25] and person re-identification [26] . For example, Chu et al. [6] proposed a multi-context attention model for pose estimation. Inspired by these methods, we use spatial attention maps to remove the undesirable clutters in pedestrian images. However, different from them, we use binary pedestrian masks as spatial attention maps to obtain more precise information of pedestrian bodies.",
            "cite_spans": [
                {
                    "start": 155,
                    "end": 159,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 179,
                    "end": 183,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 213,
                    "end": 217,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 244,
                    "end": 247,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Spatial Attention Mechanism"
        },
        {
            "text": "The overall framework of our Mask-Guided Region Attention Network (MGRAN) is illustrated in Fig. 3 . MGRAN consists of two main components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA). The MRA module aims to generate two kinds of attention maps: pedestrian masks and human body keypoints. It is constructed by a two-branch neural network, which predicts the attention maps of the pedestrians and their keypoints, respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 92,
                    "end": 98,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Overall Architecture"
        },
        {
            "text": "The MA module is constructed by a four-branch neural network. It utilizes the estimated attention maps to extract global features and local features. A series of extracted features are then fused for multi-feature alignment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-feature Alignment"
        },
        {
            "text": "Different from other works, we use binary masks as attention maps for highlighting specific regions of human body in the image. With the rapid development of instance segmentation, there are many alternative methods to generate pedestrian masks. In this paper, we choose Mask R-CNN [22] to predict the masks due to its high accuracy and flexibility. As shown in Fig. 4 , there are two types of masks: pedestrian masks and keypoint masks. They are simultaneously learned in a unified form through our proposed Mask-guided Region Attention module.",
            "cite_spans": [
                {
                    "start": 282,
                    "end": 286,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [
                {
                    "start": 362,
                    "end": 368,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Mask-Guided Region Attention"
        },
        {
            "text": "Pedestrian Masks P. A pedestrian mask P is the encoding of an input image's spatial layout. It is a binary encoding which means that the pixels of pedestrian region are encoded as number 1 and the others are encoded as number 0. Following the original article of Mask R-CNN, we set hyper-parameters as suggested by existing Faster R-CNN work [27] and define the loss L mask P \u00f0 \u00de on each sampled RoI in Mask R-CNN as the average binary cross-entropy loss,",
            "cite_spans": [
                {
                    "start": 342,
                    "end": 346,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Mask-Guided Region Attention"
        },
        {
            "text": "where N is the number of pixels in a predicted mask, r denotes the sigmoid function, P i is a single pixel in the mask, and P \u00c3 i is the corresponding ground truth pixel. Furthermore, the classification loss L cls and the bounding-box loss L box of each sampled RoI are set as indicated in [21] . Fig. 4 . Two types of masks: pedestrian masks and keypoint masks. In this paper, we define four keypoints (Blue Dots). By connecting two adjacent keypoints, we can divide the pedestrian region into three local regions: the head, the upper body and the lower body. (Color figure online) masks, one for each of the four keypoints as shown in Fig. 4 . Following the original article of Mask R-CNN, during training, we minimize the cross-entropy loss over an m 2 -way softmax output for each visible ground-truth keypoint, which encourages a single point to be detected.",
            "cite_spans": [
                {
                    "start": 290,
                    "end": 294,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 297,
                    "end": 303,
                    "text": "Fig. 4",
                    "ref_id": null
                },
                {
                    "start": 561,
                    "end": 582,
                    "text": "(Color figure online)",
                    "ref_id": null
                },
                {
                    "start": 637,
                    "end": 643,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Mask-Guided Region Attention"
        },
        {
            "text": "Based on the attention masks generated by Mask-guided Region Attention module, we propose a Multi-feature Alignment (MA) module to align the global feature and local features. MA consists of two main stages called Space Alignment (SA) and Multifeature Fusion (MF). The complete structure of MA is shown in Fig. 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 306,
                    "end": 312,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Multi-feature Alignment"
        },
        {
            "text": "Space Alignment (SA). Space Alignment aims to obtain the pedestrian region and the three local regions. Based on the attention masks generated by MRA module, we propose a simple and effective approach to obtain them. Specifically, we firstly apply Hadamard Product between the original image M and the corresponding pedestrian mask P to obtain the pedestrian region, as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-feature Alignment"
        },
        {
            "text": "where, denotes the Hadamard Product operator which performs element-wise product on two matrices or tensors and M \u00c3 denotes the pedestrian region. It is worth noting that we use Hadamard Product on the original image to guarantee the accuracy of features. Some works [9, 12] use spatial attention maps on processed data such as data processed by convolution, which will introduce noise into the attention region from other regions in the image. Secondly, based on the obtained pedestrian body region, we utilize the four keypoint masks to obtain the three local regions by connecting two adjacent keypoints and segmenting the pedestrian region, as shown in Fig. 4 .",
            "cite_spans": [
                {
                    "start": 267,
                    "end": 270,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 271,
                    "end": 274,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 657,
                    "end": 663,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Multi-feature Alignment"
        },
        {
            "text": "In this module, we use four ResNet-50 networks [28] to extract the features of the four regions generated by SA module, respectively. Then feature fusion is used to align features, as follows:",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 51,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Multi-feature Fusion (MF)."
        },
        {
            "text": "where Concat \u00c1 \u00f0 \u00de denotes the concatenation operation on feature vectors, f g represents the global feature of the whole pedestrian body region, f 1 l , f 2 l and f 3 l denote the features of the three local regions respectively, and F is the final feature vector for the input pedestrian image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-feature Fusion (MF)."
        },
        {
            "text": "Overall, our framework integrated the MRA and MA to extract features for input pedestrian images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-feature Fusion (MF)."
        },
        {
            "text": "We construct the Mask R-CNN model with a ResNet-50-FPN backbone and use the annotated person images in the COCO dataset [29] to train it. Furthermore, the floatingnumber mask output is binarized at a threshold of 0.5. In MF, the four ResNet-50 networks share the same parameters and we use the Margin Sample Mining Loss (MSML) [30] to conduct distance metric learning based on the four features extracted by ResNet-50. We scale the all images input into Mask R-CNN and ResNet-50 with a factor of 1/256. Finally, MRA and MA are trained independently.",
            "cite_spans": [
                {
                    "start": 120,
                    "end": 124,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 327,
                    "end": 331,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Implementation Details"
        },
        {
            "text": "In this section, the performance of Mask-Guided Region Attention Network (MGRAN) is compared with several state-of-the-art methods on three public datasets. Furthermore, detailed ablation analysis is conducted to validate the effectiveness of MGRAN components.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We evaluate our method on three large-scale public person ReID datasets, including Market-1501 [31] , DukeMTMC-reID [32] and CUHK03 [1] , details of them are shown in Table 1 . For fair comparison, we follow the official evaluation protocols of each dataset. For Market-1501 and DukeMTMC-reID, rank-1 identification rate (%) and mean Average Precision (mAP) (%) are used. For CUHK03, Cumulated Matching Characteristics (CMC) at rank-1 (%) and rank-5 (%) are adopted.",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 99,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 116,
                    "end": 120,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 132,
                    "end": 135,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 167,
                    "end": 174,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Datasets and Protocols"
        },
        {
            "text": "We choose 13 methods in total with state-of-the-art performance for comparisons with our proposed framework MGRAN. These methods can be categorized into two classes according to whether human pose information is used. The Spindle-Net (Spindle) [12] , Deeply-Learned Part-Aligned Representations (DLPAR) [10] , MSCAN [33] , and the Attention-Aware Compositional Network (AACN) [9] are pose-relevant. The Online Instance Matching (OIM) [14] , Re-ranking [34] , the deep transfer learning method (Transfer) [35] , the SVDNet [15] , the pedestrian alignment network (PAN) [36] , the Part-Aligned Representation (PAR) [10] , the Deep Pyramid Feature Learning (DPFL) [13] , DaF [37] and the null space semi-supervised learning method (NFST) [38] are pose-irrelevant. The experimental results are presented in Table 2 , 3 and 4.",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 248,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 303,
                    "end": 307,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 316,
                    "end": 320,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 376,
                    "end": 379,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 434,
                    "end": 438,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 452,
                    "end": 456,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 504,
                    "end": 508,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 522,
                    "end": 526,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 568,
                    "end": 572,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 613,
                    "end": 617,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 661,
                    "end": 665,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 672,
                    "end": 676,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 735,
                    "end": 739,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [
                {
                    "start": 803,
                    "end": 810,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Comparison with the State-of-the-Art Methods"
        },
        {
            "text": "Based on the experimental results, it is obvious that our MGRAN framework outperforms the compared methods, showing the advantages of our approach. To be specific, compared with the second best method on each dataset, our framework achieves 6.10%, 1.89%, 1.28%, 7.62% and 6.57% rank-1 accuracy improvement on Table 3 . Comparison results on DukeMTMC-reID dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 309,
                    "end": 316,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Comparison with the State-of-the-Art Methods"
        },
        {
            "text": "DukeMTMC-reID Rank-1 mAP SVDNet [15] 76.70 56.80 OIM [14] 68.10 -PAN [36] 71.59 51.51 AACN [9] 76.84 59.25 MGRAN (Ours) 78.12 63.57 ",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 36,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 53,
                    "end": 57,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 69,
                    "end": 73,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 91,
                    "end": 94,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Comparison with the State-of-the-Art Methods"
        },
        {
            "text": "In this section, we evaluate the effect of our proposed multi-feature fusion and regionlevel feature alignment by ablation analysis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ablation Analysis"
        },
        {
            "text": "Multi-feature Fusion (MF). We verify the effectiveness of MF on Market-1501 and DukeMTMC-reID dataset by removing global features in final feature vectors. As shown in Table 5 , MF increases the rank-1 accuracy by 2.61%, 2.25% and 0.81% on Market-1501 (Single Query), Market-1501 (Multiple Query) and DukeMTMC-reID. Furthermore, 3.77%, 0.77% and 3.47% mAP improvement on Market-1501 (Single Query), Market-1501 (Multiple Query) and DukeMTMC-reID are achieved based on MF.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 168,
                    "end": 175,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Ablation Analysis"
        },
        {
            "text": "Region-Level Feature Alignment (RFA). We align features based on part-level and region-level respectively to verify the effectiveness of our proposed region-level feature alignment. Specifically, we replace region-level feature alignment in MGRAN with part-level feature alignment and keep the other parts unchanged. As shown in Table 6 , RFA increases the rank-1 accuracy by 1.19% and 1.32% on CUHK03 (Labeled) and CUHK03 (Detected). Meanwhile, RFA increases the rank-5 accuracy by 1.53% and 1.46% on CUHK03 (Labeled) and CUHK03 (Detected). The experimental results show the usefulness of our proposed RFA. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 329,
                    "end": 336,
                    "text": "Table 6",
                    "ref_id": null
                }
            ],
            "section": "Ablation Analysis"
        },
        {
            "text": "In this paper, we propose a novel Mask-Guided Region Attention Network (MGRAN) for person re-identification to deal with the clutter and misalignment problem. MGRAN consists of two main components: Mask-guided Region Attention (MRA) and Multifeature Alignment (MA). MRA generates spatial attention maps to mask out undesirable clutters and obtain finer silhouettes of pedestrian bodies. MA aims to align features based on region-level which is more appropriate for ReID. Our method has achieved some success, but with the rapid development of science, a great number of excellent technologies have been created, such as GAN, and in the future work, we propose to use these technologies to further improve the performance of ReID.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "DeepReID: deep filter pairing neural network for person re-identification",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "152--159",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Deep ranking for person re-identification via joint representation learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "Z"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lai",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Image Process",
            "volume": "25",
            "issn": "5",
            "pages": "2353--2367",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Person re-identification by multichannel parts-based cnn with improved triplet loss function",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "1335--1344",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Deep attributes driven multi-camera person re-identification",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECCV 2016",
            "volume": "9906",
            "issn": "",
            "pages": "475--491",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-46475-6_30"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Pose-driven deep convolutional model for person re-identification",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "3960--3969",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Multi-context attention for human pose estimation",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "1831--1840",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Stacked hourglass networks for human pose estimation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Newell",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECCV 2016",
            "volume": "9912",
            "issn": "",
            "pages": "483--499",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-46484-8_29"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Pose-aware person recognition",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Namboodiri",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Paluri",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "V"
                    ],
                    "last": "Jawahar",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "6223--6232",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Attention-aware compositional network for person re-identification",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "2119--2128",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Deeply-learned part-aligned representations for person re-identification",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhuang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "3219--3228",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Person re-identification by salience matching",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "2528--2535",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Spindle Net: person re-identification with human body region guided feature decomposition and fusion",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "1077--1085",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Person re-identification by deep learning multi-scale representations",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "2590--2600",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Joint detection and identification feature learning for person search",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "3415--3424",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "SVDNet for pedestrian retrieval",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "3800--3808",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Cross-view asymmetric metric learning for unsupervised person re-identification",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "X"
                    ],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Zheng",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "994--1002",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "PersonNet: person re-identification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "V D"
                    ],
                    "last": "Hengel",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1601.07255"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Embedding deep metric for person re-identification: a study against large variations",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECCV 2016",
            "volume": "9905",
            "issn": "",
            "pages": "732--748",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-46448-0_44"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "defense of the triplet loss for person re-identification",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hermans",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Beyer",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Leibe",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1703.07737"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Deep feature learning with relative distance comparison for person re-identification",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chao",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Pattern Recognit",
            "volume": "48",
            "issn": "10",
            "pages": "2993--3003",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Fast R-CNN. In: ICCV",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1440--1448",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Mask R-CNN. In: ICCV",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gkioxari",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2961--2969",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Fully convolutional networks for semantic segmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "3431--3440",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Attention to scale: scale-aware semantic image segmentation",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "C"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "3640--3649",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "SCA-CNN: spatial and channel-wise attention in convolutional networks for image captioning",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "5659--5667",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "End-to-end comparative attention networks for person re-identification",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Image Process",
            "volume": "26",
            "issn": "7",
            "pages": "3492--3506",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "NIPS",
            "volume": "",
            "issn": "",
            "pages": "91--99",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Microsoft COCO: Common Objects in Context. In: Fleet",
            "authors": [
                {
                    "first": "T.-Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "8693",
            "issn": "",
            "pages": "740--755",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-10602-1_48"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Margin sample mining loss: a deep learning based method for person re-identification",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1710.00478"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Scalable person re-identification: a benchmark",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "1116--1124",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Unlabeled samples generated by GAN improve the person re-identification baseline in vitro",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "3754--3762",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Learning deep context-aware features over body and latent parts for person re-identification",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "384--393",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Re-ranking person re-identification with k-reciprocal encoding",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "1318--1327",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Deep transfer learning for person re-identification",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Geng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.05244"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Pedestrian alignment network for large-scale person reidentification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Circ. Syst. Video Technol",
            "volume": "29",
            "issn": "10",
            "pages": "3037--3045",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Divide and fuse: a re-ranking approach for person reidentification",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1708.04169"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Learning a discriminative null space for person reidentification",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "1239--1248",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "(a): Pedestrian masks can be used to mask out clutters and obtain the finer silhouettes of pedestrian body parts. (b): Pedestrians' heads, upper bodies and lower bodies have their own morphological characters which can not be presented by a single body part. For example, the morphological characters of upper bodies are presented by arms and upper torsos, such as the amplitude of arm swing (Yellow Arrow). (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Mask-Guided Region Attention Network (MGRAN). Our proposed MGRAN consists of two main components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA). MRA aims to generate two types of attention maps: pedestrian masks and human body keypoint masks. MA utilizes the attention maps generated by MRA to obtain the pedestrian region and the three associated local regions. Then the global feature and local features are extracted and multi-feature fusion is used to align them.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "K. Mask R-CNN can easily be extended to keypoints detection. We model a keypoint's location as a one-hot mask and use Mask R-CNN to predict four",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The details of three public datasets used in experiments.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Comparison results on Market-1501 dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Comparison results on CUHK03 dataset. MGRAN (Ours) 93.02 98.94 90.67 98.21",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Effectiveness of MF. MGRAN \u00c0 GF means removing global features in final feature vectors.Table 6. Effectiveness of RFA. MGRAN-PL means aligning features based on part-level. MGRAN-RL means aligning features based on region-level.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}