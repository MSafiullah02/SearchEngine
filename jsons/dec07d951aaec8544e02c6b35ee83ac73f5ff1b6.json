{
    "paper_id": "dec07d951aaec8544e02c6b35ee83ac73f5ff1b6",
    "metadata": {
        "title": "TemporalGAT: Attention-Based Dynamic Graph Representation Learning",
        "authors": [
            {
                "first": "Ahmed",
                "middle": [],
                "last": "Fathy",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Beijing Institute of Technology",
                    "location": {
                        "postCode": "10081",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "ahmedfathy@bit.edu.cn"
            },
            {
                "first": "Kan",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Beijing Institute of Technology",
                    "location": {
                        "postCode": "10081",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "likan@bit.edu.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "Learning representations for dynamic graphs is fundamental as it supports numerous graph analytic tasks such as dynamic link prediction, node classification, and visualization. Real-world dynamic graphs are continuously evolved where new nodes and edges are introduced or removed during graph evolution. Most existing dynamic graph representation learning methods focus on modeling dynamic graphs with fixed nodes due to the complexity of modeling dynamic graphs, and therefore, cannot efficiently learn the evolutionary patterns of real-world evolving graphs. Moreover, existing methods generally model the structural information of evolving graphs separately from temporal information. This leads to the loss of important structural and temporal information that could cause the degradation of predictive performance of the model. By employing an innovative neural network architecture based on graph attention networks and temporal convolutions, our framework jointly learns graph representations contemplating evolving graph structure and temporal patterns. We propose a deep attention model to learn low-dimensional feature representations which preserves the graph structure and features among series of graph snapshots over time. Experimental results on multiple real-world dynamic graph datasets show that, our proposed method is competitive against various state-of-the-art methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Many appealing real-world applications involve data streams that cannot be well represented in a planar structure, but exist in irregular domain. This case applies to knowledge bases [35] , 3D models [18] , social media [22] , and biological networks [7] which are usually represented by graphs.",
            "cite_spans": [
                {
                    "start": 183,
                    "end": 187,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 200,
                    "end": 204,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 220,
                    "end": 224,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 251,
                    "end": 254,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In graph representation learning, the key challenge is to learn a lowdimensional representation of the data that is most informative to preserve the structural information among the nodes in graphs. Through graph embedding, we can represent the nodes in a low-dimensional vector form. This paves the way to apply machine learning in graph analysis and data mining tasks easily and efficiently such as node classification [11, 22] , link prediction [7] , clustering [4] , and visualization [30] .",
            "cite_spans": [
                {
                    "start": 421,
                    "end": 425,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 426,
                    "end": 429,
                    "text": "22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 448,
                    "end": 451,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 465,
                    "end": 468,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 489,
                    "end": 493,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, there has been significant interest in graph representation learning mainly focuses on static graphs [5, 7, 8, 11, 22, 29] which attracted the attention of researchers due to its extensive usage in numerous real-world applications. However, a wide range of real-world applications are intrinsically dynamic and the underlying graph structure evolves over time and are usually represented as a sequence of graph snapshots over time [14] .",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 114,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 115,
                    "end": 117,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 118,
                    "end": 120,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 121,
                    "end": 124,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 125,
                    "end": 128,
                    "text": "22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 129,
                    "end": 132,
                    "text": "29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 441,
                    "end": 445,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Learning dynamic graph representations is challenging due to the timevarying nature of graph structures, where the graph nodes and edges are in continues evolution. New nodes and edges can be introduced or removed in each time step. Consequently, this requires the learned representations not only to preserve structural information of the graphs, but also to efficiently capture the temporal variations over time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, novel methods for learning dynamic graph representations have been proposed in literature. Some recent work attempts to learn dynamic graph representation such as [10, 15, 36, 37] , where they mainly apply a temporally regularized weights to enforce the smoothness of node representations from different adjacent time steps. However, these methods generally fail to learn effective representations when graph nodes exhibit substantially distinct evolutionary behaviors over time [24] .",
            "cite_spans": [
                {
                    "start": 173,
                    "end": 177,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 178,
                    "end": 181,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 182,
                    "end": 185,
                    "text": "36,",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 186,
                    "end": 189,
                    "text": "37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 489,
                    "end": 493,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Trivedi et al. [27] handle temporal reasoning problem in multi-relational knowledge graphs through employing a recurrent neural network. However, their learned temporal representations are limited to modeling first-order proximity between nodes, while ignoring the higher-order proximities among neighborhoods which are essential for preventing the graph structure as explained in [25, 34] .",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 381,
                    "end": 385,
                    "text": "[25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 386,
                    "end": 389,
                    "text": "34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, the authors in [24] propose dynamic graph embedding approach that leverage self-attention networks to learn node representations. This method focus on learning representations that capture structural properties and temporal evolutionary patterns over time. However, this method cannot effectively capture the structural evolution information over time, since it employs structure attention layers to each time step separately and generate node representations, which is followed by temporal attention layers to capture the variations in generated representations.",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 29,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, attention mechanisms have achieved great success in NLP and sequential learning tasks [1, 31] . Attention mechanisms learn a function that aggregates a variable-sized inputs while focusing on the most relevant sequences of the input to make decisions, which makes them unique. An attention mechanism is commonly referred to as self-attention, when it computes the representation of a single sequence.",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 99,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 100,
                    "end": 103,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Veli\u010dkovi\u0107 et al. [29] extend the self-attention mechanism and apply it on static graphs by enabling each node to attend over its neighbors. In this paper, we specifically focus on applying graph attention networks (GATs) [29] because of its effectiveness in addressing the shortcomings of prior methods based on graph convolutions such as [8, 11] . GATs allow for assigning different weights to nodes of the same neighborhood by applying multi-head self-attention layers, which enables a leap in model capacity. Additionally, the self-attention mechanism is applied to all graph edges, and thus, it does not depend on direct access to the graph structure or its nodes, which was a limitation of many prior dynamic graph representation learning techniques.",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 22,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 222,
                    "end": 226,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 340,
                    "end": 343,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 344,
                    "end": 347,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Inspired by this recent work, we present a temporal self-attention neural network architecture to learn node representations on dynamic graphs. Specifically, we apply self-attention along structural neighborhoods over temporal dynamics through leveraging temporal convolutional network (TCN) [2, 20] . We learn dynamic node representation by considering the neighborhood in each time step during graph evolution by applying a self-attention strategy without violating the ordering of the graph snapshots.",
            "cite_spans": [
                {
                    "start": 292,
                    "end": 295,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 296,
                    "end": 299,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Overall our paper makes the following contributions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We present a novel neural architecture named (TemporalGAT) to learn representations on dynamic graphs through integrating GAT, TCN, and a statistical loss function. -We conduct extensive experiments on real-world dynamic graph datasets and compare with state-of-the-art approaches which validate our method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we aim to solve the problem of dynamic graph representation learning. We represent dynamic graph G as a sequence of graph snapshots,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "where V t , E t and F t represent the nodes, edges and features of the graph respectively. The goal of dynamic graph representation learning is to learn effective latent representations for each node in the graph v \u2208 V at each time step t = 1, 2, . . . , T . The learned node representations should efficiently preserve the graph structure for all node v \u2208 V at any time step t.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "In this section, we present our proposed TemporalGAT framework, as illustrated in Fig. 1 . We propose a novel model architecture to learn representations for dynamic graphs through utilizing GATs and TCNs networks to promote the model ability in capturing temporal evolutionary patterns in a dynamic graph. We employ multi-head graph attentions and TCNs as a special recurrent structure to improve model efficiency. TCNs has proven to be stable and powerful for modeling long-range dependencies as discussed in previous studies [2, 20] . In addition, this architecture can take a sequence of any length and map it to an output sequence of specific length which can be very effective in dynamic graphs due to varying size of adjacency and feature matrices. The input graph snapshot is applied to GAT layer which has dilated causal convolutions to ensure no information leakage from future to past graph snapshots. Formally, for an input vector x \u2208 R n and a filter f : {0, . . . , k \u2212 1} \u2192 R, the dilated convolution operation C d on element u of the vector x is defined as:",
            "cite_spans": [
                {
                    "start": 528,
                    "end": 531,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 532,
                    "end": 535,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 82,
                    "end": 88,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "TemporalGAT Framework"
        },
        {
            "text": "where d is the dilation factor, k is the filter size, and u \u2212 d \u00b7 i makes up for the direction of the past information. When using a large dilation factors, the output at the highest level can represent a wider range of inputs, thus effectively expanding the receptive field [32] of convolution networks. For instance, through applying dilated convolution operations, it is possible to aggregate the input features from previous snapshots towards final snapshot. The inputs to a single GAT layer are graph snapshots (adjacency matrix) and graph feature or 1-hot encoded vectors for each node. The output is node representations across time that capture both local structural and temporal properties. The self-attention layer in GAT attends over the immediate neighbors of each node by employing self-attention over the node features. The proposed GAT layer is a variant of GAT [29] , with dilated convolutions applied on each graph snapshot:",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 877,
                    "end": 881,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "TemporalGAT Framework"
        },
        {
            "text": "where h u is the learned hidden representations of node u, \u03c3 is a non-linear activation function, N u represents the immediate neighbors of u, W d is the shared transformation weight of dilated convolutions, x v is the input representation vector of node v, and \u03b1 vu is the coefficient learned by the attention mechanism defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "TemporalGAT Framework"
        },
        {
            "text": "where A vu is the edge weight of the adjacency matrix between u and v, a T is a weight vector parameter of the attention function implemented as feed-forward layer and is the concatenation operator. \u03b1 vu is based on softmax function over the neighborhood of each node. This is to indicate the importance of node v to node v at the current snapshot. We use residual connections between GAT layers to avoid vanishing gradients and ensure smooth learning of the deep architecture. Following, we adopt binary cross-entropy loss function to predict the existence of an edge between a pair of nodes using the learned node representations similar to [24] . The binary cross-entropy loss function for certain node v can be defined as:",
            "cite_spans": [
                {
                    "start": 643,
                    "end": 647,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "TemporalGAT Framework"
        },
        {
            "text": "where T is the number of training snapshots, pos t is the set of nodes connected with edges to v at snapshot t, neg t is the negative sampling distribution for snapshot t, W neg is the negative sampling parameter, \u03c3 is the sigmoid function and the dot operator represents the inner product operation between the representations of node pair.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "TemporalGAT Framework"
        },
        {
            "text": "In this section, we conduct extensive experiments to evaluate the performance of our method via link prediction task. We present experiential results of our proposed method against several baselines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We use real-world dynamic graph datasets for analysis and performance evaluation. An outline of the datasets we use in our experiments is given in Table 1 . The detailed dataset descriptions are listed as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 147,
                    "end": 154,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "-Enron [12] and UCI [21] are online communication network datasets. Enron dataset is constructed by email interactions between employees where the employees represent the nodes and the email communications represent the edges. UCI dataset is an online social network where the messages sent between users represent the edges.",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 11,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 20,
                    "end": 24,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "-Yelp 1 is a rating network (Round 11 of the Yelp Dataset Challenge) where the ratings of users and businesses are collected over specific time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "The datasets have multiple graph time steps and were created based on specific interactions in fixed time windows. For more details on the dataset collection and statistics see [24] .",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "We evaluate the performance of different baselines by conducting link prediction experiment. We learn dynamic graph representations on snapshots S = {1, 2, . . . , t \u2212 1} and use the links of t \u2212 1 to predict the links at t graph snapshot. We follow the experiment design by [24] and classify each node pair into linked and non-linked nodes, and use sampling approach to achieve positive and negative node pairs where we randomly sample 25% of each snapshot nodes for training and use the remaining 75% for testing.",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "For our method, we train the model using Adam optimizer and adopt dropout regularization to avoid model over-fitting. We trained the model for a maximum of 300 epochs and the best performing model on the validation set, is chosen for link perdition evaluation. For the datasets, we use a 4 TCN blocks, with each GAT layer comprising attention heads computing 32 features, and we concatenate the output features. The output low-dimensional embedding size of the last fully-connected layer is set to 128.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter Settings"
        },
        {
            "text": "We evaluate our method against the several baseline algorithms including static graph representation approaches such as: GAT [29] , Node2Vec [7] , GraphSAGE [8] , graph autoencoders [9] , GCN-AE and GAT-AE as autoencoders for link prediction [38] . Dynamic graph representation learning including Know-Evolve [27] , DynamicTriad [36] , DynGEM [10] and DySAT [24] .",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 129,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 141,
                    "end": 144,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 157,
                    "end": 160,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 182,
                    "end": 185,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 242,
                    "end": 246,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 309,
                    "end": 313,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 329,
                    "end": 333,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 343,
                    "end": 347,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 358,
                    "end": 362,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Baseline Algorithms"
        },
        {
            "text": "The task of link prediction is to leverage structural and temporal information up to time step t and predict the existence of an edge between a pair of vertices (u, v) at time t + 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Link Prediction"
        },
        {
            "text": "To evaluate the link prediction performance of each baseline model, we train a logistic regression classifier similar to [36] . We use Hadmard operator to compute element-wise product of feature representation for an edge using the connected pair of nodes as suggested by [7] . We repeat the experiment for 10 times and report the average of Area Under the ROC Curve (AUC) score. We evaluate each baseline at each time step t separately, by training the models up to snapshot t and evaluate the performance at t + 1 for each snapshot up to T snapshots. We report the averaged micro and macro AUC scores over all time steps for the methods in Table 2 (given in paper [24] ).",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 125,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 272,
                    "end": 275,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 666,
                    "end": 670,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [
                {
                    "start": 642,
                    "end": 649,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Link Prediction"
        },
        {
            "text": "From the results, we observe that TemporalGAT outperforms state-of-theart methods in micro and macro AUC scores. Moreover, the results suggest that GAT using TCN architecture with minimal tuning outperforms graph representation methods, which validates the efficient of TCN in capturing the temporal and structural properties of dynamic graph snapshots.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Link Prediction"
        },
        {
            "text": "Static graph embedding can be observed as dimensionality reduction approach that maps each node into a low dimensional vector space which preserves the vertex neighborhood proximities. Earlier research work for linear (e.g., PCA) and non-linear (e.g., IsoMap) dimensionality reduction methods have been studied extensively in the literature [3, 23, 26] .",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 344,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 345,
                    "end": 348,
                    "text": "23,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 349,
                    "end": 352,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Static Graph Representation Learning"
        },
        {
            "text": "To improve large-scale graph embedding scalability, several approaches have been proposed such as [6, 7, 22] , which adopt random walks and skip-gram procedure to learn network representations. Tang et al. [25] designed two loss functions to capture the local and global graph structure.",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 101,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 102,
                    "end": 104,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 105,
                    "end": 108,
                    "text": "22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 206,
                    "end": 210,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Static Graph Representation Learning"
        },
        {
            "text": "More recently, network embedding approaches design models that rely on convolutions to achieve good generalizations such as [8, 11, 19, 29] . These methods usually provide performance gains on network analytic tasks such as node classification and link prediction. However, these approaches are unable to efficiency learn representations for dynamic graphs due to evolving nature.",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 127,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 128,
                    "end": 131,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 132,
                    "end": 135,
                    "text": "19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 136,
                    "end": 139,
                    "text": "29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Static Graph Representation Learning"
        },
        {
            "text": "Methods for dynamic graphs representation learning are often an extension of static methods with an additional component to model the temporal variation. For instance, in matrix factorization approaches such as [3, 26] the purpose is to learn node representations that come from eigenvectors of the graph Laplacian matrix decomposition. DANE [16] is based on this idea to update the eigenvectors of graph Laplacian matrix over time series.",
            "cite_spans": [
                {
                    "start": 211,
                    "end": 214,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 215,
                    "end": 218,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 342,
                    "end": 346,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Graph Representation Learning"
        },
        {
            "text": "For the methods based on random walk such as [7, 22] , the aim is to model the node transition probabilities of random walks as the normalized inner products of the corresponding node representations. In [33] , the authors learn representations through observing the graph changes and incrementally re-sample a few walks in the successive time step.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 48,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 49,
                    "end": 52,
                    "text": "22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 204,
                    "end": 208,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Graph Representation Learning"
        },
        {
            "text": "Another line of works for dynamic graph representation employ temporal regularization that acts as smoothness factor to enforce embedding stability across time steps [36, 37] . Recent works learn incremental node representations across time steps [10] , where the authors apply an autoencoder approach that minimizes the reconstruction loss with a distance metric between connected nodes in the embedding space. However, this may not guarantee the ability of model to capture long-term proximities.",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 170,
                    "text": "[36,",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 171,
                    "end": 174,
                    "text": "37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 247,
                    "end": 251,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Graph Representation Learning"
        },
        {
            "text": "Another category of dynamic graph representation learning is point processes that are continuous in time [13, 17, 28] . These approaches model the edge occurrence as a point process and parameterize the intensity function by applying the learned node representations as an input to a neural network.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 109,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 110,
                    "end": 113,
                    "text": "17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 114,
                    "end": 117,
                    "text": "28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Graph Representation Learning"
        },
        {
            "text": "More recently, [24] proposed an approach that leverage the most relevant historical contexts through self-attention layers to preserve graph structure and temporal evolution patterns. Unlike this approach, our framework captures the most relevant historical information through applying a temporal self-attention architecture using TCN and GAT layers to learn dynamic representations for real-world data.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Dynamic Graph Representation Learning"
        },
        {
            "text": "In this paper, we introduce a novel end-to-end dynamic graph representation learning framework named TemporalGAT. Our framework architecture is based on graph attention networks and temporal convolutional network and operates on dynamic graph-structured data through leveraging self-attention layers over time. Our experiments on various real-world dynamic graph datasets show that the proposed framework is superior to existing graph embedding methods as it achieves significant performance gains over several state-of-the-art static and dynamic graph embedding baselines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "There are several challenges for future work. For instance, learning representations for multi-layer dynamic graphs while incorporating structural and feature information is a promising direction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Neural machine translation by jointly learning to align and translate",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bahdanau",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Conference on Learning Representations (ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Z"
                    ],
                    "last": "Kolter",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Koltun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1803.01271"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Laplacian Eigenmaps for dimensionality reduction and data representation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Belkin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Niyogi",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Neural Comput",
            "volume": "15",
            "issn": "6",
            "pages": "1373--1396",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Deep neural networks for learning graph representations",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1145--1152",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "FastGCN: fast learning with graph convolutional networks via importance sampling",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1801.10247"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "ComNE: reinforcing network embedding with community learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fathy",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ICONIP 2019. CCIS",
            "volume": "1142",
            "issn": "",
            "pages": "397--405",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-36808-1_43"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "node2vec: scalable feature learning for networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Grover",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "855--864",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Inductive representation learning on large graphs",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hamilton",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1024--1034",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Representation learning on graphs: methods and applications",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "L"
                    ],
                    "last": "Hamilton",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1709.05584"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "DynGEM: deep embedding method for dynamic graphs",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kamra",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IJCAI International Workshop on Representation Learning for Graphs (ReLiG",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Semi-supervised classification with graph convolutional networks",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "N"
                    ],
                    "last": "Kipf",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1609.02907"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Introducing the Enron corpus",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Klimt",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Learning dynamic embeddings from temporal interactions",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Graph evolution: densification and shrinking diameters",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Faloutsos",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "ACM Trans. Knowl. Discov. Data",
            "volume": "1",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Attributed network embedding for learning in a dynamic environment",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dani",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management",
            "volume": "",
            "issn": "",
            "pages": "387--396",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/3132847.3132919"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Attributed network embedding for learning in a dynamic environment",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dani",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Continuous-time dynamic network embeddings",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "H"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Rossi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "K"
                    ],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Companion of the The Web Conference",
            "volume": "",
            "issn": "",
            "pages": "969--976",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Spatio-semantic comparison of large 3D city models in CityGML using a graph database",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "H"
                    ],
                    "last": "Kolbe",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Learning convolutional neural networks for graphs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Niepert",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kutzkov",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "2014--2023",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "WaveNet: a generative model for raw audio",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "V D"
                    ],
                    "last": "Oord",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1609.03499"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Patterns and dynamics of users' behavior and interaction: network analysis of an online community",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Panzarasa",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Opsahl",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Carley",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "J. Am. Soc. Inform. Sci. Technol",
            "volume": "60",
            "issn": "5",
            "pages": "911--932",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "DeepWalk: online learning of social representations",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Perozzi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Al-Rfou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Skiena",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "701--710",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Nonlinear dimensionality reduction by locally linear embedding",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Roweis",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "K"
                    ],
                    "last": "Saul",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Science",
            "volume": "290",
            "issn": "5500",
            "pages": "2323--2326",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Dynamic graph representation learning via self-attention networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sankar",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Gou",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Line: large-scale information network embedding",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Mei",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th International Conference on World Wide Web",
            "volume": "",
            "issn": "",
            "pages": "1067--1077",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "A global geometric framework for nonlinear dimensionality reduction",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "De Silva",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Langford",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Science",
            "volume": "290",
            "issn": "5500",
            "pages": "2319--2323",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Know-evolve: deep temporal reasoning for dynamic knowledge graphs",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Trivedi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 34th International Conference on Machine Learning",
            "volume": "70",
            "issn": "",
            "pages": "3462--3471",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Representation learning over dynamic graphs",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Trivedi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Farajtbar",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Biswal",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zha",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Graph attention networks",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Veli\u010dkovi\u0107",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cucurull",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Casanova",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Romero",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Li\u00f2",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Structural deep network embedding",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cui",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "1225--1234",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "QANet: combining local convolution with global self-attention for reading comprehension",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dohan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Luong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Multi-scale context aggregation by dilated convolutions",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Koltun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1511.07122"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Net-Walk: a flexible deep embedding approach for anomaly detection in dynamic networks",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Aggarwal",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "2672--2681",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/3219819.3220024"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Network representation learning: a survey",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1801.05852"
                ]
            }
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Knowledge graph embedding by translating on hyperplanes",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Dynamic network embedding by modeling triadic closure process",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhuang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Scalable temporal latent space inference for link prediction in dynamic social networks",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "V"
                    ],
                    "last": "Steeg",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Galstyan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "28",
            "issn": "10",
            "pages": "2765--2777",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Modeling polypharmacy side effects with graph convolutional networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zitnik",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Bioinformatics",
            "volume": "34",
            "issn": "13",
            "pages": "457--466",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The framework of TemporalGAT.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": ".6 \u00b1 0.2 81.0 \u00b1 0.2 85.8 \u00b1 0.1 70.2 \u00b1 0.1 69.9 \u00b1 0.1 TemporalGAT 86.4 \u00b1 0.4 86.8 \u00b1 0.3 82.7 \u00b1 0.2 85.2 \u00b1 0.2 71.9 \u00b1 0.3 70.3 \u00b1 0.2",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Dynamic graph datasets used for performance evaluation.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Link prediction results on Enron, UCI and Yelp datasets.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}