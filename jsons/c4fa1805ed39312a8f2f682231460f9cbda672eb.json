{
    "paper_id": "c4fa1805ed39312a8f2f682231460f9cbda672eb",
    "metadata": {
        "title": "CRISISBERT: A ROBUST TRANSFORMER FOR CRISIS CLASSIFICATION AND CONTEXTUAL CRISIS EMBEDDING",
        "authors": [
            {
                "first": "Junhua",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Singapore University of Technology and Design",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Trisha",
                "middle": [],
                "last": "Singhal",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Singapore University of Technology and Design",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Lucienne",
                "middle": [
                    "T M"
                ],
                "last": "Blessing",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Singapore University of Technology and Design",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Kristin",
                "middle": [
                    "L"
                ],
                "last": "Wood",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Singapore University of Technology and Design",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Kwan",
                "middle": [
                    "Hui"
                ],
                "last": "Lim",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Singapore University of Technology and Design",
                    "location": {}
                },
                "email": "kwanhui_lim@sutd.edu.sg"
            }
        ]
    },
    "abstract": [
        {
            "text": "Classification of crisis events, such as natural disasters, terrorist attacks and pandemics, is a crucial task to create early signals and inform relevant parties for spontaneous actions to reduce overall damage. Despite the crises, such as natural disaster, that can be predicted by professional institutions, certain events are first signaled by everyday citizens, i.e., civilians, such as the recent COVID-19 pandemics. Social media platforms such as Twitter often expose firsthand signals on such crises through high volume information exchange. In the case of Twitter, this corresponds to on average over half a billion tweets posted daily. Prior works proposed various crisis embeddings and classification using conventional Machine Learning and Neural Network models. However, none of the works perform crisis embedding and classification using state of the art attention-based deep neural networks models, such as Transformers and document-level contextual embeddings. This work proposes CrisisBERT, an end-to-end transformer-based model for two crisis classification tasks, namely crisis detection and crisis recognition, which shows promising results across accuracy and f1 scores. The proposed model demonstrates superior robustness over various benchmarks, as it shows marginal performance compromise while extending from 6 to 36 events with only 51.4% additional data points. We also propose Crisis2Vec, an attention-based, document-level contextual embedding architecture for crisis embedding, which achieves better performance than conventional crisis embedding methods such as Word2Vec and GloVe. To the best of our knowledge, our works are first to propose using transformer-based crisis classification and document-level contextual crisis embedding in the literature.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Crisis-related events, such as earthquakes, hurricanes and train or airliner accidents, often stimulate a sudden surge of attention and actions from both media and the general public. Despite the fact that crises, such as natural disasters, can be predicted by professional institutions, certain events are first signaled by everyday citizens, i.e., civilians. For instance, the recent COVID-19 pandemics was first informed by general public in China via Weibo, a popular social media site, before pronouncements by government officials.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Social media sites have become centralized hubs that facilitate timely information exchange across government agencies, enterprises, working professionals and the general public. As one of the most popular social media sites, Twitter enables users to asynchronously communicate and exchange information with tweets, which are mini-blog posts limited to 280 characters. There are on average over half a billion tweets posted daily [1] . Therefore, one can leverage on such high volume and frequent information exchange to expose firsthand signals on crisis-related events for early detection and warning systems to reduce overall damage and negative impacts.",
            "cite_spans": [
                {
                    "start": 430,
                    "end": 433,
                    "text": "[1]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Event detection from tweets has received significant attention in research in order to analyze crisis-related messages for better disaster management and increasing situational awareness. Several recent works studied various natural crisis events, such as hurricanes and earthquakes, and artificial disasters, such as terrorist attacks and explosions [2, 3, 4, 5] .",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 354,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 355,
                    "end": 357,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 358,
                    "end": 360,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 361,
                    "end": 363,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "These works focus on binary classifications for various attributes of crisis, such as classifying source type, predicting relatedness between tweets and the crises, and assessing informativeness and applicability [6, 7, 8] . On the other hand, several works proposed multi-label classifiers on affected individuals, infrastructure, casualties, donations, caution, advice, etc. [9, 10] . Crisis recognition tasks are likewise conducted such as identifying crisis types, i.e. hurricanes, floods and fires [11, 12] .",
            "cite_spans": [
                {
                    "start": 213,
                    "end": 216,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 217,
                    "end": 219,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 220,
                    "end": 222,
                    "text": "8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 377,
                    "end": 380,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 381,
                    "end": 384,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 508,
                    "end": 511,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Machine Learning-based models are commonly introduced in performing the above mentioned tasks. Conventional linear models such as Logistic Regression, Naive Bayes and Support Vector Machine (SVM) are reported for automatic binary classification on informativeness [13] and relevancy [8] , among others. These models were implemented with pre-trained word2vec embeddings [14] . Several unsupervised approaches are also proposed for classifying crisis-related events, such as the CLUSTOP algorithm utilizing Community Detection for automatic topic modelling [15] . A transfer-learning approach is also proposed [16] , though its classification is only limited to two classes. The ability for cross-crisis evaluation remains questionable.",
            "cite_spans": [
                {
                    "start": 264,
                    "end": 268,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 283,
                    "end": 286,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 370,
                    "end": 374,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 556,
                    "end": 560,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 609,
                    "end": 613,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "More recently, numerous works proposed Neural Networks (NN) models for crisis-related data detection and classification. For instance, ALRashdi and O'Keefe investigated two deep learning architectures, namely Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural Networks (CNN) using domain-specific and GloVe embeddings [17] . Nguyen et al. propose a CNN-based classifier with Word2Vec embedding pretrained on Google News [14] and domain-specific embeddings [18] . Lastly, parallel CNN architecture was proposed to detect disaster-related events using tweets [19, 20] .",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 342,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 440,
                    "end": 444,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 476,
                    "end": 480,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 577,
                    "end": 581,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 582,
                    "end": 585,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "While prior works report remarkable performance on various crisis classification tasks using NN models and word embeddings, no studies are found to leverage the most recent Natural Language Understanding (NLU) techniques, such as attention-based deep classification models [21] and document-level contextual embeddings [22] , which reportedly improve state-of-the-art performance for many challenging natural language problems from upstream tasks such as Named Entity Recognition and Part of Speech Tagging, to downstream tasks such as Machine Translation and Neural Conversation.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 277,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 319,
                    "end": 323,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This work focuses on deep attention-based classification models and document-level contextual representation models to address two important crisis classification tasks. We study recent NLU models and techniques that reportedly demonstrated drastic improvement on state-of-the-art and localize for domain-specific crisis related tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Contributions"
        },
        {
            "text": "Overall, our main contribution of this work includes:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Contributions"
        },
        {
            "text": "\u2022 proposing CrisisBERT, an attention-based classifier that improves state-of-the-art performance for both crisis detection and recognition tasks;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Contributions"
        },
        {
            "text": "\u2022 demonstrating superior robustness over various benchmarks, where extending CrisisBERT from 6 to 36 events with 51.4% of additional data points only results in marginal performance decline, while increasing crisis case classification by 500%;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Contributions"
        },
        {
            "text": "\u2022 proposing Crisis2Vec, a document-level contextual embedding approach for crisis representation, and showing substantial improvement over conventional crisis embedding methods such as Word2vec and GloVe . . .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Contributions"
        },
        {
            "text": "To the best of our knowledge, this work is the first to propose a transformer-based classifier for crisis classification tasks. We are also first to propose a document-level contextual crisis embedding approach.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main Contributions"
        },
        {
            "text": "In this section, we discuss the recent works that propose various machine learning approaches for crisis classification tasks. While these works report substantial improvement in performance over prior works, none of the works uses state of the art attentionbased models, i.e., Transformers [21] , to perform crisis classification tasks. We propose CrisisBERT, a transformer-based architecture that builds upon a Distilled BERT model, fine-tuned by large-scale hyper-parameter search.",
            "cite_spans": [
                {
                    "start": 291,
                    "end": 295,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Attention-based Neural Crisis Classifier"
        },
        {
            "text": "Various works propose linear classifiers for crisis-related events. For instance, Parilla-Ferrer et al. proposed an automatic binary classification, based on informative and uninformative tweets using Naive Bayes and Support Vector Machine (SVM) [13] . A SVM with pretrained word2vec embeddings approach was also proposed [14] .",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 250,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 322,
                    "end": 326,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Conventional Crisis classifiers"
        },
        {
            "text": "Besides linear models, recent works also propose deep learning based methods with different neural network architectures. For instance, ALRashdi and O'Keefe investigated Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural figure) and passed into a DistrilBERT model. Since we are performing classification task, the CLS token vector, i.e. the first output vector, is then passed into a linear classifier for detection or recognition task, whereas the remainder of the output vectors are average-pooled to create Crisis2Vec embeddings.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 241,
                    "end": 248,
                    "text": "figure)",
                    "ref_id": null
                }
            ],
            "section": "Conventional Crisis classifiers"
        },
        {
            "text": "Network (CNN) models using domain-specific and GloVe embeddings [23] . Nguyen et al. proposed a CNN model to classify tweets to get information types using Google News and domain-specific embeddings [18] .",
            "cite_spans": [
                {
                    "start": 64,
                    "end": 68,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 199,
                    "end": 203,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Conventional Crisis classifiers"
        },
        {
            "text": "In 2017, Vaswani et al. from Google introduced Transformer [21], a new category of deep learning models which are solely attention-based and without convolution and recurrent mechanisms. Later, Google proposed the Bidirectional Encoder Representations from Transformers (BERT) model [24] which drastically improved state-of-the-art performance for multiple challenging Natural Language Processing (NLP) tasks. Since then, multiple transformer-based models have been introduced, such as GPT [25] and XLNet [26] , among others. Transformer-based models were also deployed to solve domain specific tasks, such as medical text inferencing [27] and occupational title embedding [28] , and demonstrated remarkable performance.",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 287,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 490,
                    "end": 494,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 505,
                    "end": 509,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 635,
                    "end": 639,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 673,
                    "end": 677,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Transformer"
        },
        {
            "text": "The Bidirectional Encoder Representation of Transformer (BERT), for instance, is a multi-layer bidirectional Transformer en-coder with attention mechanism [24] . The proposed BERT model has two variants, namely (a) BERT Base, which has 12 transformer layers, a hidden size of 768, 12 attention heads, and 110M total parameters; and (b) BERT Large, which has 24 transformer layers, a hidden size of 1024, 16 attention heads, and 340M total parameters. BERT is pre-trained with self-supervised approaches, i.e., Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).",
            "cite_spans": [
                {
                    "start": 155,
                    "end": 159,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Transformer"
        },
        {
            "text": "While Transformers such as BERT are reported to perform well in natural language processing, understanding and inference tasks, to the best of our knowledge, no prior works propose and examine the performance of transformer-based models for crisis classification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Transformer"
        },
        {
            "text": "In this work, we investigate the transformer approach for crisis classification tasks and propose CrisisBERT, a transformer-based classification model that surpasses conventional linear and deep learning models in performance and robustness. [29] , optimizers, learning rates, and batch sizes. Table 1 shows the breakdown of the search space and the final hyper-parameters for CrisisBERT. Each set of parameters is randomly chosen and ran with 3 epochs and two trials. In total, we evaluate over 300 hyper-parameters sets using a Nvidia Titan-X (Pascal) for over 1,000 GPU hours.",
            "cite_spans": [
                {
                    "start": 242,
                    "end": 246,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [
                {
                    "start": 294,
                    "end": 301,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "CrisisBERT"
        },
        {
            "text": "Taking into consideration of performance and efficiency trade-off, we select the DistilBERT model for our Transformer LM layer. DistilBERT is a compressed version of BERT Base through Knowledge Distillation. With utilization of only 50% of the layers of BERT, DistilBERT performs 60% faster while preserving 97% of the capabilities in language understanding tasks. The optimal set of hyper-parameters for DistilBERT includes an AdamW [30] optimizer, and initial learning rate of 5e-5, and a batch size of 32.",
            "cite_spans": [
                {
                    "start": 434,
                    "end": 438,
                    "text": "[30]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "CrisisBERT"
        },
        {
            "text": "Output Layer. The output layer of DistilBERT LM is a set of 768-d vectors led by the class header vector. Since we are conducting classification tasks, only the [CLS] token vector is used as the aggregate sequence representation for classification with a linear classifier. The remainder of the output vectors are processed into Crisis2Vec embeddings using Mean-Pooling operation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CrisisBERT"
        },
        {
            "text": "As discussed in Section 2.3, Crisis2Vec embedding is a byproduct of CrisisBERT, where the embeddings are constructed based on a pre-trained BERT model, and subsequently fine-tuned with three corpora of crisis-related tweets [6, 31, 32] to be domain-specific for crisis-related tweet representation.",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 227,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 228,
                    "end": 231,
                    "text": "31,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 232,
                    "end": 235,
                    "text": "32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Crisis2Vec"
        },
        {
            "text": "Crisis2Vec leverages the advantages of Transformers, including (1) leveraging a self-attention mechanism to incorporate sentencelevel context bidirectionally, (2) leveraging both word-level and positional information to create contextual representation of words, and (3) taking advantage of the pre-trained models on large relevant corpora.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis2Vec"
        },
        {
            "text": "To the best of our knowledge, we are the first who propose a document-level contextual embedding approach for crisis-related document representation. Upon convergence, we construct the fixed-length tweet vector using a MEAN-Pooling strategy [22] , where we compute the mean of all output vectors, as illustrated in Algorithm 1. ",
            "cite_spans": [
                {
                    "start": 241,
                    "end": 245,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Crisis2Vec"
        },
        {
            "text": "In this work, we conduct two crisis classification tasks, namely Crisis Detection and Crisis Recognition. We formulate the Crisis Detection task as a binary classification model that identifies if a tweet is relevant to a crisis-related event. The Crisis Recognition task on the other hand extends the problem into multi-class classification, where the output is a probability vector that indicate the likelihood of a tweet indicating specific events. Both tasks are modelled as Sequence Classification problems that are formally defined below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis Classification"
        },
        {
            "text": "We define the Crisis Detection task D = (S, \u03a6), which is specified by S = {s 1 , ..., s n } a finite sample space of tweets with size n. Each sample s i is a sequence of tokens at T time steps, i.e., s i = {s 1 i , ..., s T i }. \u03a6 denotes the set of labels that has the same sequence as the sample set, \u03a6 = {\u03c6 1 , ..., \u03c6 n } and \u03c6 i \u2208 {0, 1} where \u03c6 i = 1 indicates that sample s i is relevant to crisis, and \u03c6 i = 0 indicates otherwise. A deterministic classifier C D : S \u2192 \u03c6 specifies the mapping from sample tweets to their flags.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis Detection"
        },
        {
            "text": "Our objective is to train a crisis detector using the provided tweets and labels that minimizes the differences between predicted labels and true labels, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis Detection"
        },
        {
            "text": "where J D denotes some cost function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis Detection"
        },
        {
            "text": "Similarly, we define a Crisis Recognition task R = (S, L), where sample space S is identical to that in Crisis Detection. L denotes a sequence of multi-class labels that have the same sequence as S, i.e., L = {l 1 , ..., l n }, and l i \u2208 R m for m number of classes. A deterministic classifier C R : S \u2192 L specifies the mapping from the sample tweets to the crisis classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis Recognition"
        },
        {
            "text": "The objective of the crisis classification tasks is to train a sequence classifier using the provided tweets and labels that minimizes the differences between predicted labels and true labels, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis Recognition"
        },
        {
            "text": "where J R denotes some cost function for classifier C R . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crisis Recognition"
        },
        {
            "text": "In this section, we discuss the experiments performed and their results in order to propose a highly effective and efficient approach for text classification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "Three datasets of labelled crisis-related tweets [6, 31, 32] are used to conduct crisis classification tasks and evaluate the proposed methods against benchmarks. In total, these datasets consist of close to 8 million tweets, where overall 91.6k are labelled. These data sets are in the form of: (1) 60k labelled tweets on 6 crises [6] , (2) 3.6k labelled tweets for 8 crises [32] , and (3) 27.9k labelled tweets for 26 crises [31] . Table 2 describes more detail about each dataset and their respective classes.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 52,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 53,
                    "end": 56,
                    "text": "31,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 57,
                    "end": 60,
                    "text": "32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 332,
                    "end": 335,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 376,
                    "end": 380,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 427,
                    "end": 431,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [
                {
                    "start": 434,
                    "end": 441,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "For our experimental evaluation, the 91.6k labelled crisis-related tweets are organized into two datasets, annotated as C6 and C36. In particular, C6 consists of 60k tweets from 6 classes of crises, whereas C36 comprises all 91.6k tweets in 36 classes. Both datasets are split into training, validation and test sets that consist of 90%, 5% and 5% of the original sets, respectively. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "CrisisBERT. We evaluate the performance of CrisisBERT against multiple benchmarks, which comprise recently proposed crisis classification models in the literature. These works include linear classifiers, such as Logistic Regression (LR), Support Vector Machine (SVM) and Naive Bayes [33] , and non-linear neural networks, such as Convolutional Neural Network (CNN) [20] and Long Short-Term Memory [34] .",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 287,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 365,
                    "end": 369,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 397,
                    "end": 401,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Proposed Models"
        },
        {
            "text": "Furthermore, we investigate the robustness of CrisisBERT for both detection and recognition tasks. This is achieved by extending the experiments from C6 to C36, which comprise 6 and 36 classes respectively, but with only 51.4% additional data points. We evaluate the robustness of the proposed models against benchmarks by observing the compromise in robustness performance, while realizing the drastically improved classification performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Models"
        },
        {
            "text": "As described in Section 2.3, we use the optimal set of hyper-parameters for CrisisBERT in the experiments, which include the use of a BERT model with distillation (i.e. DistilBERT), an AdamW [30] optimizer, an initial learning rate of 5e-5, a batch size of 32, and a word dropout rate of 0.25.",
            "cite_spans": [
                {
                    "start": 191,
                    "end": 195,
                    "text": "[30]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proposed Models"
        },
        {
            "text": "Crisis2Vec. To evaluate Crisis2Vec, we choose the two classifiers with the aim to represent both traditional Machine Learning approaches and the NN approaches. The two selected models are: (1) a linear Logistic Regression model, denoted as LR c2v , and (2) a non-linear LSTM model, denoted as LST M c2v . We evaluate the performance of Crisis2Vec with the two models by replacing the original embedding to Crisis2Vec, ceteris paribus.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Models"
        },
        {
            "text": "We use two common evaluation metrics, namely Accuracy and F1 score, which are functions of True-Positive (TP), False-Positive (FP), True-Negative (TN) and False-Negative (FN) predictions. Accuracy is calculated by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "For a F1-score of multiple classes, we calculate the unweighted mean for each label, i.e., for n classes of labels as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "where P recision = T P T P + F P",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "and Recall = T P T P + F N",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "We select and implement several crisis classifiers proposed in recent works to serve as benchmarks for evaluating our proposed methods. Concretely, we compare CrisisBERT with the following models:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Benchmark Algorithms"
        },
        {
            "text": "\u2022 LR w2v : Logistic regression model with Word2Vec embedding pre-trained on Google News Corpus [33] \u2022 SV M w2v : Support Vector Machine model with Word2Vec embedding pre-trained on Google News Corpus [33] \u2022 N B w2v : Naive Bayes model assuming Gaussian distribution for features with Word2Vec embedding pre-trained on Google News Corpus [33] \u2022 CN N gv : Convolutional Neural Network model with 2 convolutional layers of 128 hidden units, kernel size of 3, pool size of 2, 250 filters, and GloVe for word embedding [20] \u2022 LST M w2v : Long Short-Term Memory model with 2 layers of 30 hidden states and a Word2Vec-based Crisis Embedding [34] Models ",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 99,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 200,
                    "end": 204,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 337,
                    "end": 341,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 514,
                    "end": 518,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 634,
                    "end": 638,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Benchmark Algorithms"
        },
        {
            "text": "Overall, the experimental results show that both proposed models achieve significant improvement on performance and robustness over benchmarks across all tasks. The experimental results for CrisisBERT and Crisis2Vec are tabulated in Table 4 . Robustness. Comparing Crisis Detection task between C6 and C36, CrisisBERT shows 1.7% and 1.3% decline for F1-score and Accuracy, which is much better than most benchmarks, i.e., from 1.7% to 6.3%, except CNN. However, when we compare the more challenging Crisis Recognition tasks between C6 and C36, the performance of CrisisBERT compromises marginally, i.e., 1.6% for F1-score and 0.7% for Accuracy. On the contrary, all benchmark models record significant decline, i.e. from 6.0% to 67.2%.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 233,
                    "end": 240,
                    "text": "Table 4",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Results"
        },
        {
            "text": "Discussion. Based on experimental results discussed above, we observe that: (1) CrisisBERT's performance exceeds state-of-theart performance for both detection and recognition tasks, with up to 8.2% and 25.0% respectively, (2) CrisisBERT demonstrates higher robustness with marginal decline for performance (i.e. less than 1.7% in F1-score and Accuracy), and (3) Crisis2Vec shows superior performance as compared to conventional Word2Vec embeddings, for both LR and LSTM models across all experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "5 Related Work",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Event detection from tweets has received significant attention in research in order to analyze crisis-related messages for better disaster management and increasing situational awareness [4, 5, 2, 3] . Parilla-Ferrer et al. proposed automatic binary classification of informativeness using Naive Bayes and Support Vector Machine (SVM) [13] . Stowe et al. presented an annotation scheme for tweets to classify relevancy and six [8] . Furthermore, use of pre-trained word2vec reportedly improved SVM for Crisis classification [14] . Lim et al. proposed CLUSTOP algorithm utilizing the community detection approach for automatic topic modelling [15] . Pedrood et al. proposed to transfer-learn classification of one event to the other using a sparse coding model [16] , though the scope was only limited to only two events, i.e. Hurricane Sandy (2012) and Supertyphoon Yolanda (2013).",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 190,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 191,
                    "end": 193,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 194,
                    "end": 196,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 197,
                    "end": 199,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 335,
                    "end": 339,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 427,
                    "end": 430,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 524,
                    "end": 528,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 642,
                    "end": 646,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 760,
                    "end": 764,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Crisis Classification"
        },
        {
            "text": "A substantial number of works focusses on usign Neural Networks (NN) with word embeddings for crisis-related data classification. Manna et al. [33] compared NN models with conventional ML classifiers [33] . ALRashdi and O'Keefe investigated and showed good performance for two deep learning architectures, namely Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural Networks (CNN) with domain-specific GloVe embeddings [17] . However, the study had yet to validate the relevance of model on a different crisis type. Nguyen et al. applied CNN to classify information types using Google News and domain-specific embeddings [18] . Kersten et al. [20] implemented a parallel CNN to detect two disasters, namely hurricanes and floods, which reported a F1-score of 0.83. The CNN architecture was proposed earlier by Kim et al. [19] .",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 147,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 200,
                    "end": 204,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 437,
                    "end": 441,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 639,
                    "end": 643,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 661,
                    "end": 665,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 839,
                    "end": 843,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Crisis Classification"
        },
        {
            "text": "Word-level Embeddings such as Word2Vec [35] and GloVe [23] are commonly used to form the basis of Crisis Embedding [18, 36] in various crisis classification works to improve model performance. For context, Word2Vec uses a Neural Network Language Model (NNLM) that is able to represent latent information on the word level. GloVe achieved better results with a simpler approach, constructing global vectors to represent contextual knowledge of the vocabulary.",
            "cite_spans": [
                {
                    "start": 39,
                    "end": 43,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 54,
                    "end": 58,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 115,
                    "end": 119,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 120,
                    "end": 123,
                    "text": "36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Crisis Embedding"
        },
        {
            "text": "More recently, a series of high quality embedding models, such as FastText [37] and Flair [38] , are proposed and reported to have improved state of the art for multiple NLP tasks. Both word-level contextualization and character-level features are commonly used for these works. Pre-trained models on large corpora of news and tweets collections are also made publicly available to assist in downstream tasks. Furthermore, Transformer-based models are proposed to conduct sentence-level embedding tasks [22] .",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 90,
                    "end": 94,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Crisis Embedding"
        },
        {
            "text": "Social media such as Twitter has become a hub of crowd generated information for early crisis detection and recognition tasks. In this work, we present a transformer-based crisis classification model CrisisBERT, and a contextual crisis-related tweet embedding model Crisis2Vec. We examine the performance and robustness of the proposed models by conducting experiments with three datasets and two crisis classification tasks. Experimental results show that CrisisBERT improves state of the art for both detection and recognition class, and further demonstrates robustness by extending from 6 classes to 36 classes, with only 51.4% additioanl data points. Finally, our experiments with two classification models show that Crisis2Vec enhances classification performance as compared to Word2Vec embeddings, which is commonly used in prior works.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Natural disasters detection in social media and satellite imagery: a survey",
            "authors": [
                {
                    "first": "Naina",
                    "middle": [],
                    "last": "Said",
                    "suffix": ""
                },
                {
                    "first": "Kashif",
                    "middle": [],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Riegler",
                    "suffix": ""
                },
                {
                    "first": "Konstantin",
                    "middle": [],
                    "last": "Pogorelov",
                    "suffix": ""
                },
                {
                    "first": "Laiq",
                    "middle": [],
                    "last": "Hassan",
                    "suffix": ""
                },
                {
                    "first": "Nasir",
                    "middle": [],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "Nicola",
                    "middle": [],
                    "last": "Conci",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Multimedia Tools and Applications",
            "volume": "78",
            "issn": "22",
            "pages": "31267--31302",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Situational awareness enhanced through social media analytics: A survey of first responders",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Luke",
                    "suffix": ""
                },
                {
                    "first": "Morteza",
                    "middle": [],
                    "last": "Snyder",
                    "suffix": ""
                },
                {
                    "first": "Christina",
                    "middle": [],
                    "last": "Karimzadeh",
                    "suffix": ""
                },
                {
                    "first": "David S",
                    "middle": [],
                    "last": "Stober",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ebert",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1909.07316"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Processing social media messages in mass emergency: A survey",
            "authors": [
                {
                    "first": "Muhammad",
                    "middle": [],
                    "last": "Imran",
                    "suffix": ""
                },
                {
                    "first": "Carlos",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                },
                {
                    "first": "Fernando",
                    "middle": [],
                    "last": "Diaz",
                    "suffix": ""
                },
                {
                    "first": "Sarah",
                    "middle": [],
                    "last": "Vieweg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ACM Computing Surveys (CSUR)",
            "volume": "47",
            "issn": "4",
            "pages": "1--38",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Earthquake shakes twitter users: real-time event detection by social sensors",
            "authors": [
                {
                    "first": "Takeshi",
                    "middle": [],
                    "last": "Sakaki",
                    "suffix": ""
                },
                {
                    "first": "Makoto",
                    "middle": [],
                    "last": "Okazaki",
                    "suffix": ""
                },
                {
                    "first": "Yutaka",
                    "middle": [],
                    "last": "Matsuo",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 19th international conference on World wide web",
            "volume": "",
            "issn": "",
            "pages": "851--860",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Crisislex: A lexicon for collecting and filtering microblogged communications in crises",
            "authors": [
                {
                    "first": "Alexandra",
                    "middle": [],
                    "last": "Olteanu",
                    "suffix": ""
                },
                {
                    "first": "Carlos",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                },
                {
                    "first": "Fernando",
                    "middle": [],
                    "last": "Diaz",
                    "suffix": ""
                },
                {
                    "first": "Sarah",
                    "middle": [],
                    "last": "Vieweg",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Eighth international AAAI conference on weblogs and social media",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Semi-supervised discovery of informative tweets during the emerging disasters",
            "authors": [
                {
                    "first": "Shanshan",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Slobodan",
                    "middle": [],
                    "last": "Vucetic",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1610.03750"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Identifying and categorizing disaster-related tweets",
            "authors": [
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Stowe",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Paul",
                    "suffix": ""
                },
                {
                    "first": "Martha",
                    "middle": [],
                    "last": "Palmer",
                    "suffix": ""
                },
                {
                    "first": "Leysia",
                    "middle": [],
                    "last": "Palen",
                    "suffix": ""
                },
                {
                    "first": "Kenneth M",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of The fourth international workshop on natural language processing for social media",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Extracting information nuggets from disaster-related messages in social media",
            "authors": [
                {
                    "first": "Muhammad",
                    "middle": [],
                    "last": "Imran",
                    "suffix": ""
                },
                {
                    "first": "Shady",
                    "middle": [],
                    "last": "Elbassuoni",
                    "suffix": ""
                },
                {
                    "first": "Carlos",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                },
                {
                    "first": "Fernando",
                    "middle": [],
                    "last": "Diaz",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "Meier",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Online public communications by police & fire services during the 2012 hurricane sandy",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Amanda",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hughes",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "Lise",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "St",
                    "suffix": ""
                },
                {
                    "first": "Leysia",
                    "middle": [],
                    "last": "Denis",
                    "suffix": ""
                },
                {
                    "first": "Kenneth M",
                    "middle": [],
                    "last": "Palen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the SIGCHI conference on human factors in computing systems",
            "volume": "",
            "issn": "",
            "pages": "1505--1514",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "On semantics and deep learning for event detection in crisis situations",
            "authors": [
                {
                    "first": "Gr\u00e9goire",
                    "middle": [],
                    "last": "Burel",
                    "suffix": ""
                },
                {
                    "first": "Hassan",
                    "middle": [],
                    "last": "Saif",
                    "suffix": ""
                },
                {
                    "first": "Miriam",
                    "middle": [],
                    "last": "Fernandez",
                    "suffix": ""
                },
                {
                    "first": "Harith",
                    "middle": [],
                    "last": "Alani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Verifying baselines for crisis event information classification on twitter",
            "authors": [
                {
                    "first": "Justin",
                    "middle": [
                        "Michael"
                    ],
                    "last": "Crow",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 17th International Conference on Information Systems for Crisis Response and Management (ISCRAM 2020). ISCRAM Digital Library",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Automatic classification of disaster-related tweets",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "L"
                    ],
                    "last": "Beverly Estephany Parilla-Ferrer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Fernandez",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ballena",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. International conference on Innovative Engineering Technologies (ICIET)",
            "volume": "62",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Distributed representations of words and phrases and their compositionality",
            "authors": [
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [
                        "S"
                    ],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3111--3119",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Clustop: A clustering-based topic modelling algorithm for twitter using word networks",
            "authors": [
                {
                    "first": "Shanika",
                    "middle": [],
                    "last": "Kwan Hui Lim",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Karunasekera",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Harwood",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE International Conference on Big Data (Big Data)",
            "volume": "",
            "issn": "",
            "pages": "2009--2018",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Mining help intent on twitter during disasters via transfer learning with sparse coding",
            "authors": [
                {
                    "first": "Bahman",
                    "middle": [],
                    "last": "Pedrood",
                    "suffix": ""
                },
                {
                    "first": "Hemant",
                    "middle": [],
                    "last": "Purohit",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation",
            "volume": "",
            "issn": "",
            "pages": "141--153",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Deep learning and word embeddings for tweet classification for crisis response",
            "authors": [
                {
                    "first": "Reem",
                    "middle": [],
                    "last": "Alrashdi",
                    "suffix": ""
                },
                {
                    "first": "O&apos;",
                    "middle": [],
                    "last": "Simon",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Keefe",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1903.11024"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Robust classification of crisis-related data on social networks using convolutional neural networks",
            "authors": [
                {
                    "first": "Kamela Ali Al",
                    "middle": [],
                    "last": "Dat Tien Nguyen",
                    "suffix": ""
                },
                {
                    "first": "Shafiq",
                    "middle": [],
                    "last": "Mannai",
                    "suffix": ""
                },
                {
                    "first": "Hassan",
                    "middle": [],
                    "last": "Joty",
                    "suffix": ""
                },
                {
                    "first": "Muhammad",
                    "middle": [],
                    "last": "Sajjad",
                    "suffix": ""
                },
                {
                    "first": "Prasenjit",
                    "middle": [],
                    "last": "Imran",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Eleventh International AAAI Conference on Web and Social Media",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Convolutional neural networks for sentence classification",
            "authors": [
                {
                    "first": "Yoon",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1408.5882"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Robust filtering of crisis-related tweets",
            "authors": [
                {
                    "first": "Jens",
                    "middle": [],
                    "last": "Kersten",
                    "suffix": ""
                },
                {
                    "first": "Anna",
                    "middle": [],
                    "last": "Kruspe",
                    "suffix": ""
                },
                {
                    "first": "Matti",
                    "middle": [],
                    "last": "Wiegmann",
                    "suffix": ""
                },
                {
                    "first": "Friederike",
                    "middle": [],
                    "last": "Klan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ISCRAM 2019 Conference Proceedings-16th International Conference on Information Systems for Crisis Response and Management",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Ashish",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "Llion",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "Aidan",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "\u0141ukasz",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "authors": [
                {
                    "first": "Nils",
                    "middle": [],
                    "last": "Reimers",
                    "suffix": ""
                },
                {
                    "first": "Iryna",
                    "middle": [],
                    "last": "Gurevych",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1908.10084"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Glove: Global vectors for word representation",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Christopher D",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Language models are unsupervised multitask learners",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Rewon",
                    "middle": [],
                    "last": "Child",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Luan",
                    "suffix": ""
                },
                {
                    "first": "Dario",
                    "middle": [],
                    "last": "Amodei",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "OpenAI Blog",
            "volume": "1",
            "issn": "8",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "authors": [
                {
                    "first": "Zhilin",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Zihang",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Yiming",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Jaime",
                    "middle": [],
                    "last": "Carbonell",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Russ",
                    "suffix": ""
                },
                {
                    "first": "Quoc V",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "5754--5764",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Ncuee at mediqa 2019: Medical text inference using ensemble bert-bilstm-attention model",
            "authors": [
                {
                    "first": "Lung-Hao",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Yi",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Po-Han",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Po-Lei",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kuo-Kai",
                    "middle": [],
                    "last": "Shyu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 18th BioNLP Workshop and Shared Task",
            "volume": "",
            "issn": "",
            "pages": "528--532",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Ipod: An industrial and professional occupations dataset and its applications to occupational data mining and analysis",
            "authors": [
                {
                    "first": "Junhua",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Yung",
                    "middle": [
                        "Chuen"
                    ],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "Kristin",
                    "middle": [
                        "L"
                    ],
                    "last": "Wood",
                    "suffix": ""
                },
                {
                    "first": "Kwan Hui",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1910.10495"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Distilling the knowledge in a neural network",
            "authors": [
                {
                    "first": "Geoffrey",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "Oriol",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1503.02531"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "What to expect when the unexpected happens: Social media communications across crises",
            "authors": [
                {
                    "first": "Alexandra",
                    "middle": [],
                    "last": "Olteanu",
                    "suffix": ""
                },
                {
                    "first": "Sarah",
                    "middle": [],
                    "last": "Vieweg",
                    "suffix": ""
                },
                {
                    "first": "Carlos",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 18th ACM conference on computer supported cooperative work & social computing",
            "volume": "",
            "issn": "",
            "pages": "994--1009",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Analysing how people orient to and spread rumours in social media by looking at conversational threads",
            "authors": [
                {
                    "first": "Arkaitz",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Liakata",
                    "suffix": ""
                },
                {
                    "first": "Rob",
                    "middle": [],
                    "last": "Procter",
                    "suffix": ""
                },
                {
                    "first": "Geraldine",
                    "middle": [],
                    "last": "Wong Sak",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Hoi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tolmie",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "PloS one",
            "volume": "11",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Effectiveness of word embeddings on classifiers: A case study with tweets",
            "authors": [
                {
                    "first": "Sukanya",
                    "middle": [],
                    "last": "Manna",
                    "suffix": ""
                },
                {
                    "first": "Haruto",
                    "middle": [],
                    "last": "Nakai",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE 13th International Conference on Semantic Computing (ICSC)",
            "volume": "",
            "issn": "",
            "pages": "158--161",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "A deep multi-modal neural network for informative twitter content classification during emergencies",
            "authors": [
                {
                    "first": "Abhinav",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "Jyoti",
                    "middle": [
                        "Prakash"
                    ],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yogesh",
                    "suffix": ""
                },
                {
                    "first": "Nripendra P",
                    "middle": [],
                    "last": "Dwivedi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rana",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Annals of Operations Research",
            "volume": "",
            "issn": "",
            "pages": "1--32",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "A neural probabilistic language model",
            "authors": [
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "R\u00e9jean",
                    "middle": [],
                    "last": "Ducharme",
                    "suffix": ""
                },
                {
                    "first": "Pascal",
                    "middle": [],
                    "last": "Vincent",
                    "suffix": ""
                },
                {
                    "first": "Christian",
                    "middle": [],
                    "last": "Jauvin",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Journal of machine learning research",
            "volume": "3",
            "issn": "",
            "pages": "1137--1155",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Applications of online deep learning for crisis response using social media information",
            "authors": [
                {
                    "first": "Shafiq",
                    "middle": [],
                    "last": "Dat Tien Nguyen",
                    "suffix": ""
                },
                {
                    "first": "Muhammad",
                    "middle": [],
                    "last": "Joty",
                    "suffix": ""
                },
                {
                    "first": "Hassan",
                    "middle": [],
                    "last": "Imran",
                    "suffix": ""
                },
                {
                    "first": "Prasenjit",
                    "middle": [],
                    "last": "Sajjad",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1610.01030"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Enriching word vectors with subword information",
            "authors": [
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "Edouard",
                    "middle": [],
                    "last": "Grave",
                    "suffix": ""
                },
                {
                    "first": "Armand",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Transactions of the Association for Computational Linguistics",
            "volume": "5",
            "issn": "",
            "pages": "135--146",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Contextual string embeddings for sequence labeling",
            "authors": [
                {
                    "first": "Alan",
                    "middle": [],
                    "last": "Akbik",
                    "suffix": ""
                },
                {
                    "first": "Duncan",
                    "middle": [],
                    "last": "Blythe",
                    "suffix": ""
                },
                {
                    "first": "Roland",
                    "middle": [],
                    "last": "Vollgraf",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "COLING 2018, 27th International Conference on Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1638--1649",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Illustration of CrisisBERT and Crisis2Vec models. During the tokenization phase, word-level and subword-level embeddings are created to produce context embedding for each input word. The vectors are then prefixed with a class header (shown as [CLS] in the",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "illustrates an overview of the proposed architecture. Overall, the architecture of CrisisBERT includes three layers, namely (1) Contextual Tokenization, (2) Transformer Language Model, and (3) Logistic Classifier.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Tokenization. DistrilBERT inherits the embedding layer from BERT, where each word is tokenized contextually with subwordlevel word embeddings and positional encoders. Prefixed with a special token ([CLS]), the vector pairs are concatinated and passed into a DistilBERT LM for training. Hyper-parameter search space and final values used Transformer LM. We perform large-scale model selection and hyperparameter search to fin the best performing transformer LM for the crisis classification tasks. Several transformer models are investigated, including BERT, XLNet, GPT2 and RoBERTa. For each model, we conduct hyper-parameter search, where the search space includes variations of distillation",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Output_V ectors \u2190 CrisisBERT LM (T oken_V ector); T weet_Embedding \u2190 P ool M ean (Output_V ectors); return T weet_Embedding;",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Classes Description",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "describes the statistics of the split sets.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Experimental results of Crisis Classification tasks on C6 and C36 datasets for proposed models and benchmarks, where best performers are emphasized. Results show that CrisisBERT records highest performance across all tasks.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Crisis Detection. For Crisis Detection tasks on C6 dataset, CrisisBERT achieves a 95.5% F1-score and 95.6% Accuracy, which exceeds previous best model results, namely LSTM with pre-trained Word2Vec embeddings, by 3.8% and 3.8% respectively. In terms of embedding, LSTM with Crisis2Vec records 95.1% for both F1-score and Accuracy, which shows 3.4% improvement over LSTM with Word2Vec. Similarly, LR with Crisis2Vec records 93.2% for both F1-score and Accuracy, which shows 1.6% improvement over LR with Word2Vec. LR with Crisis2Vec records 85.1% F1-score and 90.9% Accuracy, which significantly exceed LR with Word2Vec by 13.0% and 8.6% respectively.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}