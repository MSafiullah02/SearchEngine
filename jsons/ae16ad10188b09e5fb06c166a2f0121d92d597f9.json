{
    "paper_id": "ae16ad10188b09e5fb06c166a2f0121d92d597f9",
    "metadata": {
        "title": "Data-Free Adversarial Perturbations for Practical Black-Box Attack",
        "authors": [
            {
                "first": "Zhaoxin",
                "middle": [],
                "last": "Huan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "State Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Yulong",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Ant Financial Services Group",
                    "institution": "",
                    "location": {
                        "settlement": "Hangzhou",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Xiaolu",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Ant Financial Services Group",
                    "institution": "",
                    "location": {
                        "settlement": "Hangzhou",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Lin",
                "middle": [],
                "last": "Shang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "State Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": "shanglin@nju.edu.cn"
            },
            {
                "first": "Chilin",
                "middle": [],
                "last": "Fu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Ant Financial Services Group",
                    "institution": "",
                    "location": {
                        "settlement": "Hangzhou",
                        "country": "China"
                    }
                },
                "email": "chilin.fcl@antfin.com"
            },
            {
                "first": "Jun",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Ant Financial Services Group",
                    "institution": "",
                    "location": {
                        "settlement": "Hangzhou",
                        "country": "China"
                    }
                },
                "email": "jun.zhoujun@antfin.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "Neural networks are vulnerable to adversarial examples, which are malicious inputs crafted to fool pre-trained models. Adversarial examples often exhibit black-box attacking transferability, which allows that adversarial examples crafted for one model can fool another model. However, existing black-box attack methods require samples from the training data distribution to improve the transferability of adversarial examples across different models. Because of the data dependence, fooling ability of adversarial perturbations is only applicable when training data are accessible. In this paper, we present a data-free method for crafting adversarial perturbations that can fool a target model without any knowledge about the training data distribution. In the practical setting of black-box attack scenario where attackers do not have access to target models and training data, our method achieves high fooling rates on target models and outperforms other universal adversarial perturbation methods. Our method empirically shows that current deep learning models are still at a risk even when the attackers do not have access to training data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In recent years, deep learning models demonstrate impressive performance on various machine learning tasks [2, 5, 6] . However, recent works show that deep neural networks are highly vulnerable to adversarial perturbations [4, 16] . Adversarial examples are small, imperceptible perturbations crafted to fool target models. The inherent weakness of lacking robustness to adversarial examples for deep neural networks brings out security concerns, especially for securitysensitive applications which require strong reliability [10] .",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 110,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 111,
                    "end": 113,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 114,
                    "end": 116,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 223,
                    "end": 226,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 227,
                    "end": 230,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 526,
                    "end": 530,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "With the knowledge of the structure and parameters of a given model, many methods can successfully generate adversarial examples in the white-box manner [4, 16] . A more severe issue is that adversarial examples can be transferred across different models, known as black-box attack [4] . This transferability allows for adversarial attacks without the knowledge of the structure and parameters of the target model. Existing black-box attack methods focus on improving the transferability of adversarial examples across different models under the assumption that attackers can obtain the training data on which the target models are trained [3, 4, 7] . Attackers firstly train a substitute model on the same training data, and then generate adversarial examples in the white-box manner. The perturbations crafted for substitute model can thus fool target model, since different models learn similar decision boundaries on the same training set [4, 7] .",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 156,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 157,
                    "end": 160,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 282,
                    "end": 285,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 640,
                    "end": 643,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 644,
                    "end": 646,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 647,
                    "end": 649,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 943,
                    "end": 946,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 947,
                    "end": 949,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In practice, however, attackers can hardly obtain the training data for target model, even the number of categories. For example, the Google Cloud Vision API2 (GCV) only outputs scores for a number of top classes. On this real-world black-box setting, most of existing black-box attack methods can not be applied.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we present a data-free approach for crafting adversarial perturbations to address the above issues. Our method is to craft data-free perturbations that can fool the target model without any knowledge about the data distribution (e.g., the number of categories, type of data, etc.). We utilize such a property that the features extracted from different models are usually similar, since most models are fine-tuned from common pre-trained model weights [8] . Therefore, we establish a mapping connection between fine-tuned model and pre-trained model. Instead of optimizing an objective that reduces the score of the predicted labels [3, 4] , we propose to learn adversarial perturbations that can disturb the internal representation. Our proposed attack method views the logit outputs of pre-trained model as the extracted internal representation, and iteratively maximizes the divergence between clean images and their adversarial examples measured in this representation space. Because of the mapping connection, pre-trained model and fine-tuned model are similar in the internal representation and adversarial examples will successfully mislead target model with high probability.",
            "cite_spans": [
                {
                    "start": 466,
                    "end": 469,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 647,
                    "end": 650,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 651,
                    "end": 653,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We evaluate the proposed method on two public datasets: CIFAR-10 [9] and Caltech-101 [11] and one private dataset with various models including state-ofthe-art classifiers (e.g., ResNet [14] , DenseNet [6] , etc.). Experimental results show that on the real-world black-box setting, our method achieves significant attacking success rates. In this practical setting of black-box attack scenario, only universal adversarial perturbation methods can be applied since they are image-agnostic methods. Compared with universal adversarial perturbations (UAP) [12] and generalizable data-free universal adversarial perturbations (GD-UAP) [13] , the proposed method has the following advantages. First, our method outperforms UAP and GD-UAP by 8.05% and 6.00%. Second, UAP requires a number of training samples to converge when crafting an image-agnostic perturbation and GD-UAP also need to know the distribution of training data to achieve better performance. In contrast, our method generates adversarial perturbations without knowing the data distribution. Third, the proposed method does not need training phase. The perturbation can be obtained by a single back-propagation, whereas UAP and GD-UAP need to train universal perturbation until it converges. [3] integrate the momentum term into the iterative process for fast gradient sign to achieve better attack performance.",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 68,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 85,
                    "end": 89,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 186,
                    "end": 190,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 202,
                    "end": 205,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 554,
                    "end": 558,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 632,
                    "end": 636,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Black-Box Attack. The existing black-box attacks can be classified as querybased and transfer-based. In query-based methods, the attacker iteratively queries the outputs of target model and estimates the gradient of target model [1] . As for transfer-based methods, the existing methods mainly focus on improving the transferability of adversarial examples across different models [7] . They assume the adversary can obtain the training data without the knowledge of the structure and parameters of target model. Because query-based method requires thousands of queries, it is hard to be used in practical attack. In this paper, we focus on transfer-based black-box attack.",
            "cite_spans": [
                {
                    "start": 229,
                    "end": 232,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 381,
                    "end": 384,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Recent work by Moosavi-Dezfooli et al. [12] presents the existence of imageagnostic perturbations, called universal adversarial perturbations (UAP) that can fool the state-of-the-art recognition models on most clean images. Mopuri et al. [13] further proposed a generalizable approach for crafting universal adversarial perturbations, called generalizable data-free universal adversarial perturbations (GD-UAP). These two image-agnostic universal adversarial perturbations can effectively attack under real-world black-box setting. Instead of seeking universal adversarial perturbations, our method is to generate image-specific perturbations without knowing data distribution.",
            "cite_spans": [
                {
                    "start": 39,
                    "end": 43,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 238,
                    "end": 242,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Specifically, we use the output of pre-trained model as internal representation to measure the difference between clean image and adversarial example. By iteratively maximizing the divergence with respect to our objective function Eq. (1), the internal representation becomes much more different. Finally, because of the mapping connection, adversarial examples will successfully mislead target model with high probability. We briefly show our attack framework in Algorithm 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Data-free adversarial attack algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1."
        },
        {
            "text": "A clean image x;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "The target model f (x);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "The pre-trained model t(x); Output:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "The adversarial perturbations x * which misleads target model f (x). 1: Initialize x * with x; 2: Compute the objective function Equation (1) with respect to t(x) for x; 3: Use numerical optimization to iteratively maximize the divergence between x and x * by Equation (1); 4: Get the adversarial perturbations x * generated by t(x); 5: x * misleads target model f (x);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "Let x denote the clean image from a given dataset, and y true denote the class. A target model is a function f (x) = y that accepts an input x \u2208 X and and produces an output y \u2208 Y . f (x) is the outputs of target model including the softmax function, define f l (x) = z to be the output of final layer before the softmax output(z are also called logits), and f (x) = sof tmax(f l (x)) = y. The goal of adversarial attack is to seek an example x * with the magnitude of adversarial perturbation which is misclassified by the target model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "In this paper, we use the definition of real-world black-box: the adversary can not obtain the structure and parameters of target model as well as its data distribution (e.g., the number of categories, type of data, etc.). Moreover, the target model is fine-tuned on pre-trained model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Black-Box Setting"
        },
        {
            "text": "Our objective is to establish a mapping connection between f (x) and t(x) and utilize t(x) to craft data-free perturbations that can fool f (x).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Black-Box Setting"
        },
        {
            "text": "For general image classification tasks, the extracted features are similar. Instead of initializing the model with random weights, initializing it with the pre-trained model can boost performance and reduce training time. Therefore, it is common to use pre-trained model to fine-tune on new tasks [8] . In this paper, we establish the relationship between fine-tuned model f (x) and pre-trained model t(x), called mapping connection. As shown in Fig. 1 , even though the training data distribution between f (x) and t(x) is different (X = X ), we consider the logits output between these two models contain the 'mapping' connection: given an input x, each neuron in f l (x) may be obtained by weighted summation of neurons in t l (x). We will give some experimental explanations in Sect. 4.5. Therefore, by generating the adversarial perturbations from t(x), it will successfully mislead f (x) with high probability. ",
            "cite_spans": [
                {
                    "start": 297,
                    "end": 300,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 446,
                    "end": 452,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Mapping Connection"
        },
        {
            "text": "Given a clean image x \u2208 X, our goal is to utilize t(x) to generate corresponding adversarial example x * which can mislead target model as f (x * ) = y true .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maximizing Divergence"
        },
        {
            "text": "Our objective is to craft data-free perturbations that can fool the target model without any knowledge about the data distribution (e.g., the number of categories, type of data, etc.). Therefore, instead of optimizing an objective that reduces the score to the predicted label or flip the predicted label [3, 4] , we propose to learn perturbations that can maximize feature divergence between clean images and adversarial examples.",
            "cite_spans": [
                {
                    "start": 305,
                    "end": 308,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 309,
                    "end": 311,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Maximizing Divergence"
        },
        {
            "text": "More precisely, x * is formulated as a constrained optimization problem:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maximizing Divergence"
        },
        {
            "text": "where t l (x) is the output at logits (pre-softmax) layer. Eq. (1) measures the divergence between the logits output of x and x . |t l (x)| represents magnitude of each element in t l (x) and t l (x) t l (x ) represents the difference between t l (x) and t l (x ). Intuitively, our objective function in Eq. (1) increases or decreases t l (x ) according to the direction of t l (x). And the magnitude of the change depends on weight |t l (x)|. We will show the effectiveness of our objective function in Sect. 4.6. The constraint on the distance between x and x * is formulated in terms of the L \u221e norm to limit the maximum deviation of single pixel to . The goal is to constrain the degree to which the perturbation is perceptible.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maximizing Divergence"
        },
        {
            "text": "Previous adversarial examples generation methods [3, 4, 16] aim to increase the loss function according to the gradient of f (x) (softmax output). However, due to the deep hierarchy of architectures, the gradients of loss with respect to input may vanish during propagation. To address this issue, we aim to maximize the divergence of logits output f l (x) between input x and adversarial example x * . Empirically, we found that it is inappropriate to directly use objective functions such as Kullback-Leibler divergence to measure the divergence, since the optimization can be hard to converge.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 52,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 53,
                    "end": 55,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 56,
                    "end": 59,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Maximizing Divergence"
        },
        {
            "text": "For implementation details, we first scale the input x into [\u22121, 1] and initialize x = x. Then, we compute the gradients of objective (1) with respect to input x. The adversarial examples will be updated by multiple steps. In each step, we take the sign function of the gradients and clip the adversarial examples into [\u22121, 1] to make valid images. Algorithm 2 presents the details of perturbations generation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation Details"
        },
        {
            "text": "The clean image x;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "The maximum deviation of single pixel ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "The number of iterations n; Output:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "The adversarial perturbations x * generated by pre-trained model t(x); 1: Initialize: x = x, = n , i = 0; 2: while i < n do 3:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "Maximize divergence between x and x by Equation (1): ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input:"
        },
        {
            "text": "In this section, we present the experimental results to demonstrate the effectiveness of our data-free adversarial perturbation method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Throughout experiments, target models are fine-tuned based on ImageNet [2] pre-trained models. We first fine-tune the target model on different datasets to simulate a practical training scenario. Then we only use pre-trained models to generate adversarial perturbations without knowing the training data distribution or the architecture of target models by Algorithm 1.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 74,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "We explore four mainstream deep models: GoogleNet [15] , VGG-16 [14] , ResNet-152 [5] and DenseNet-169 [6] . We compare our method to UAP [12] and GD-UAP [13] . Although some classic attack algorithms such as FGSM [4] and MI-FGSM [3] are data dependence which are not directly comparable to ours, we evaluate their attack performance under this black-box attack scenario. For all the following experiments, perturbation crafted by our method is termed as DF P . The maximum perturbation is set to 10 among all experiments, with pixel value in [0, 255], and the number of iterations is 10.",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 54,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 64,
                    "end": 68,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 82,
                    "end": 85,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 103,
                    "end": 106,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 138,
                    "end": 142,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 154,
                    "end": 158,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 214,
                    "end": 217,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 230,
                    "end": 233,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "CIFAR-10. The CIFAR-10 dataset [9] consists of 60,000 colour images across 10 classes, with size of 32\u00d732. We use training images to fine-tune target models which are pre-trained on ImageNet and use test images to evaluate attack performance. Since UAP and GD-UAP are high resolution perturbations (usually in 224\u00d7224), directly using low-resolution images from CIFAR-10 is inappropriate. Before fine-tuning target models, we resize images to 224 \u00d7 224 without losing recognition performance. Caltech101. Caltech101 [11] consists of objects belonging to 101 categories. The size of each image is roughly 300 \u00d7 200 pixels. Compared with CIFAR-10, Caltech101 is more complicated with higher resolution.",
            "cite_spans": [
                {
                    "start": 31,
                    "end": 34,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 516,
                    "end": 520,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "To fully illustrate the effectiveness of our attack method, we construct another private real-world dataset, called cosmetic insurance dataset. This dataset consists of credentials for customer who are allergic to cosmetic products, including cosmetic products, allergic skin, medical record, etc. This dataset does not involve any Personal Identifiable Information (PPI). The data is only used for academic research and processed by sampling. During the experiment, we conduct adequate data protection to prevent the risk of data leakage and destroy it after the experiment. Table 1 presents the attack performance achieved by our objective on various network architectures on three datasets. Baseline means the model's error rate on clean image (without perturbation). Fooling rate is the percentage of test images for which our crafted perturbations successfully alter the predicted label. Each row in the table indicates one target model and the columns indicate different attack methods. Since UAP and GD-UAP do not provide the perturbation on Densenet-169, we use \"\\\" in the table. Our perturbations result in an average fooling rate of 29.23% on Caltech101 which is 8.05% and 6.00% higher than UAP and GD-UAP. Moreover, compared with UAP and GD-UAP, our method crafts perturbation by one single back propagation without knowing any training data distribution which is much more efficient in practical scenario. Although previous attack methods such as FGSM [4] and MI-FGSM [3] are training-data dependent which are not directly comparable to ours, we evaluate their attack performance under this black-box attack scenario shown in Table 1 . It is clear that the fooling rates of FGSM and MI-FGSM under this practical scenario are significant lower than DF P . Because of the data dependence, fooling ability of the crafted perturbations in FGSM and MI-FGSM is limited to the available training data. Figure 2 shows example data-free perturbations crafted by the proposed method. The top row shows the clean and bottom row shows the corresponding adversarial images. The perturbed images are visually indistinguishable form their corresponding clean images. All the clean images shown in the figure are correctly classified and are successfully fooled by the added perturbation. Corresponding label predicted by the model is shown below each image. The correct labels are shown in black color and the wrong ones in red. ",
            "cite_spans": [
                {
                    "start": 1464,
                    "end": 1467,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1480,
                    "end": 1483,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 576,
                    "end": 583,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1638,
                    "end": 1645,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1907,
                    "end": 1915,
                    "text": "Figure 2",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Cosmetic Insurance Dataset."
        },
        {
            "text": "In Sect. 4.3, we report the attack performance without knowing the training data distribution. In this section, we evaluate the fooling rates of black-box attack across different models. Each row in the Table 2 indicates the target model which generates perturbations and the columns indicate various models attacked using the learned perturbations. The diagonal fooling rates indicate the data-free white-box attack noted in Sect. 4.3, where all the information about the model is known to the attacker except training data distribution. The off-diagonal rates indicate real-world black-box attack, where no information about the model's architecture or training data distribution under attack is revealed to the attacker. Our perturbations cause a mean white-box fooling rate of 25.91% and a mean black-box fooling rate of 15.04%. Given the data-free nature of the optimization, these fooling rates are alarmingly significant.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 203,
                    "end": 210,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Black-Box Attack Transferability"
        },
        {
            "text": "As a further analysis, we reveal the mapping connection between fine-tuned model and pre-trained model noted in Sect. 3.3. Since the categories of cosmetic insurance dataset have no overlap with ImageNet [11] , we evaluate test images from cosmetic insurance dataset with ImageNet pre-trained DenseNet-169 and calculate the frequency occurrence shown in Fig. 3 . The horizontal axis represents categories in ImageNet, and vertical axis represents the proportion of test images in the cosmetic insurance dataset that is classified as categories in horizontal axis. For example, by evaluating test images belonging to chat record category, there are 35% images classified as category \"caldron\" in ImageNet, which has no relationship with chat record. The frequency occurrence of each category in Fig. 3 is higher than 20%. This phenomenon demonstrates that even though fine-tuned model has different categories of pre-trained model, the logits outputs between these two models still contain relationship. Therefore, by disturbing the logits outputs of pre-trained model, it will successfully disturb the logits output of target model with high probability, which can cause the wrong prediction.",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 208,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 354,
                    "end": 360,
                    "text": "Fig. 3",
                    "ref_id": null
                },
                {
                    "start": 794,
                    "end": 800,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Empirical Demonstration of Mapping Connection"
        },
        {
            "text": "To demonstrate the effectiveness of our objective function (1), we compare the logits outputs between a clean image t l (x) (left) and the corresponding adversarial example t l (x * ) (right) after optimizing Eq (1), shown in Fig. 4 . The horizontal axis represents each category in ImageNet (t l (x) i , i = 1, 2, \u00b7 \u00b7 \u00b7 , 1000), and vertical axis represents the value of logits. It can be seen from the figure that t l (x) and t l (x * ) have a significant divergence in magnitude and direction. Combined with mapping connection, it make sense that our objective function dose craft effective data-free perturbations illustrated in Sect. 4.3 and Sect. 4.4. Fig. 3 . Experimental explanation of mapping connection. The horizontal axis represents categories in ImageNet, and vertical axis represents the proportion of test images in the cosmetic insurance dataset that is classified as categories in horizontal axis. Fig. 4 . The left image is t l (x) and right image is t l (x * ). The horizontal axis represents each category in ImageNet (t l (x)i, i = 1, 2, \u00b7 \u00b7 \u00b7 , 1000), and vertical axis represents the value of logits.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 226,
                    "end": 232,
                    "text": "Fig. 4",
                    "ref_id": null
                },
                {
                    "start": 658,
                    "end": 664,
                    "text": "Fig. 3",
                    "ref_id": null
                },
                {
                    "start": 916,
                    "end": 922,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Effectiveness of Objective Function"
        },
        {
            "text": "In this paper, we have proposed a data-free objective to generate adversarial perturbations. Our objective is to craft data-free perturbations that can fool the target model without any knowledge about the data distribution (e.g., the number of categories, type of data, etc.). Our method does not need to utilize any training data sample, and we propose to generate perturbations that can disturb the internal representation. Finally, we demonstrate that our objective of crafting data-free adversarial perturbations is effective to fool target model without knowing training data distribution or the architecture of models. The significant fooling rates achieved by our method emphasize that the current deep learning models are now at an increased risk.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Imagenet: a large-scale hierarchical image database",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Boosting adversarial attacks with momentum",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Explaining and harnessing adversarial examples",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shlens",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "The International Conference on Learning Representations (ICLR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Densely connected convolutional networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Der Maaten",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "Q"
                    ],
                    "last": "Weinberger",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Enhancing adversarial example transferability with an intermediate level attack",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Katsman",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Belongie",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "N"
                    ],
                    "last": "Lim",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The IEEE International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Ternausnet: U-net with VGG11 encoder pre-trained on imagenet for image segmentation",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Iglovikov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shvets",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Learning multiple layers of features from tiny images",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Adversarial examples in the physical world",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kurakin",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "The International Conference on Learning Representations (ICLR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perona",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Computer Vision and Image Understanding",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Universal adversarial perturbations",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Moosavi-Dezfooli",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fawzi",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Fawzi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Frossard",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Generalizable data-free objective for crafting universal adversarial perturbations",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "R"
                    ],
                    "last": "Mopuri",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ganeshan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "V"
                    ],
                    "last": "Babu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "41",
            "issn": "",
            "pages": "2452--2465",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "The International Conference on Learning Representations (ICLR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Going deeper with convolutions",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Intriguing properties of neural networks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "The International Conference on Learning Representations (ICLR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "White-Box Attack. With the knowledge of the structure and parameters of a given model, many methods can successfully generate adversarial examples in the white-box manner. Most white-box algorithms generate adversarial examples based on the gradient of loss function with respect to the inputs. Szegedy et al. [16] first introduce adversarial examples generation by analyzing the instability of deep neural networks. Goodfellow et al. [4] further explain the phenomenon of adversarial examples by analyzing the linear behavior of deep neural network and propose a simple and efficient adversarial examples generating method. Recently, Yinpeng Dong et al.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Mapping connection between fine-tuned model f (x) and pre-trained model t(x). Logits output f l (x) and t l (x) may contain mapping relationship.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "end while 5: return x * = clip(x + sign(x \u2212 x n ), \u22121, 1);",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Original and adversarial image pairs from Caltech101 dataset generated for ResNet. First row shows original images and corresponding predicted labels, second row shows the corresponding perturbed images with their wrong predictions. (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Data-free attack results on CIFAR-10, Caltech101 and cosmetic insurance datasets. Each row in the table shows fooling rates (%) for perturbation generated by different attack methods when attacking various target models (columns). Fooling rate is the percentage of test images for which the crafted perturbations successfully alter the predicted label. Baseline in table means the model's error rate on clean images",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The transferability of our attack method on CIFAR-10, Caltech101 and cosmetic insurance datasets. Each row in the table shows fooling rates (%) for perturbation learned on a specific target model when attacking various other models (columns). Diagonal rates indicate data-free white-box attack and off-diagonal rates represent realworld black-box attack scenario.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This work is supported by the National Natural Science Foundation of China (No. 61672276).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}