{
    "paper_id": "217782f62a776724b853c3f717382b3b540e83a0",
    "metadata": {
        "title": "Balancing Between Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning",
        "authors": [
            {
                "first": "Weiwen",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The Chinese University of Hong Kong",
                    "location": {
                        "settlement": "Sha Tin, Hong Kong"
                    }
                },
                "email": ""
            },
            {
                "first": "Feng",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Harbin Institute of Technology",
                    "location": {
                        "settlement": "Harbin",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Ruiming",
                "middle": [],
                "last": "Tang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The Chinese University of Hong Kong",
                    "location": {
                        "settlement": "Sha Tin, Hong Kong"
                    }
                },
                "email": "tangruiming2015@163.com"
            },
            {
                "first": "Ben",
                "middle": [],
                "last": "Liao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The Chinese University of Hong Kong",
                    "location": {
                        "settlement": "Sha Tin, Hong Kong"
                    }
                },
                "email": ""
            },
            {
                "first": "Guangyong",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Shenzhen",
                        "country": "China"
                    }
                },
                "email": "gychen@link.cuhk.edu.hk"
            },
            {
                "first": "Pheng",
                "middle": [
                    "Ann"
                ],
                "last": "Heng",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The Chinese University of Hong Kong",
                    "location": {
                        "settlement": "Sha Tin, Hong Kong"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Fairness in recommendation has attracted increasing attention due to bias and discrimination possibly caused by traditional recommenders. In Interactive Recommender Systems (IRS), user preferences and the system's fairness status are constantly changing over time. Existing fairness-aware recommenders mainly consider fairness in static settings. Directly applying existing methods to IRS will result in poor recommendation. To resolve this problem, we propose a reinforcement learning based framework, FairRec, to dynamically maintain a longterm balance between accuracy and fairness in IRS. User preferences and the system's fairness status are jointly compressed into the state representation to generate recommendations. FairRec aims at maximizing our designed cumulative reward that combines accuracy and fairness. Extensive experiments validate that FairRec can improve fairness, while preserving good recommendation quality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Interactive Recommender Systems (IRS) have been widely implemented in various fields, e.g., news, movies, and finance [20] . Different from the conventional recommendation settings [11] , IRS consecutively recommend items to individual users and receive their feedback in interactive processes. IRS gradually refine the recommendation policy according to the obtained user feedback in an online manner. The goal of such a system is to maximize the total utility over the whole interaction period. A typical utility of IRS is user acceptance of recommendations. Conversion Rate (CVR) is one of the most commonly used measures of recommendation acceptance, computing the ratio of users performing a system's desired activity to users having viewed recommended items. A desired activity could be downloading from App stores, or making loans for microlending.",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 122,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 181,
                    "end": 185,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, optimizing CVR solely may result in fairness issues, one of which is the unfair allocation of desired activities, like clicks or downloads, over different demographic groups. Under such unfair circumstances, majority (overrepresenting) groups may dominate recommendations, thereby holding a higher proportion of opportunities and resources, while minority groups are largely under-represented or even totally ignored. A fair allocation is a critical objective in recommendation due to the following benefits:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Legal. Recommendation in particular settings are explicitly mandated to guarantee fairness. In the setting of employment, education, housing, or public accommodation, a fair treatment with respect to race, color, religion, etc., is required by the anti-discrimination laws [8] . For job recommendation, it is expected that jobs at minority-owned businesses are being recommended and applied at the same rate as jobs at white-owned businesses. In microlending, loan recommender systems must ensure borrowers of different races or regions have an equal chance of being recommended and funded.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 276,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Financial. Under-representing for some groups leads to the abandonment of the system. For instance, video sharing platforms like YouTube involve viewers and creators. It is desirable to ensure each creator has a fair chance of being recommended and promoted. Otherwise, if the new creators do not get adequate exposure and appreciation, they tend to leave the platform, resulting in less user-generated content. Consequently, users' satisfaction from both viewers and creators, as well as the platform's total income are affected in the long run.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The fairness concern in recommender systems is quite challenging, as accuracy and fairness are usually conflicting goals to be achieved to some extent. On the one hand, to obtain the ideal fairness, one could simply divide the recommendation opportunities equally to each item group, but users' satisfaction will be affected by being persistently presented with unattractive items. On the other hand, existing recommender systems have been demonstrated to favor popular items [5] , resulting in extremely unbalanced recommendation results. Thus, our work aims to answer this question: Can we achieve a fairer recommendation while preserving or just sacrificing a little recommendation accuracy?",
            "cite_spans": [
                {
                    "start": 476,
                    "end": 479,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Most prior works consider fairness for the conventional recommender systems [1, 2] , where the recommendation is regarded as a static process at a certain time instant. A general framework that formulates fairness constraints on rankings in terms of exposure allocation is proposed in [19] . Individual attention fairness is discussed in [3] . [21] models re-ranking with fairness constraints in Multi-sided Recommender Systems (MRS) as an integer linear programming. The balanced neighborhoods method [4] balances protected and unprotected groups by reformulating the Sparse LInear Method (SLIM) with a new regularizer.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 79,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 80,
                    "end": 82,
                    "text": "2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 285,
                    "end": 289,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 338,
                    "end": 341,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 344,
                    "end": 348,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 502,
                    "end": 505,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, it is hard to directly apply those methods to IRS due to:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(i) It is infeasible to impose fairness constraints at every time instant. Forcing the system to be fair at any time and increasing fairness uniformly for all users will result in poor recommendations. In fact, IRS focus on the longterm cumulative utility over the whole interaction session, where the system could focus on improving accuracy for users with particular favor, and the lack of fairness at the time can later be compensated when recommending items to users with diversified interests. As such, we can achieve long-term system's fairness while preserving satisfying recommendation quality. (ii) Existing work only considers the distribution of the number of recommendations (exposure) an item group received. Actually, the distribution of the desired activities that take place after an exposure like clicks or downloads has much larger commercial value and can be directly converted to revenue.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To resolve the problem, we design a Fairness-aware Recommendation framework with reinforcement learning (FairRec) for IRS. FairRec jointly compresses the user preferences and the system's fairness status into the current state representation. A two-fold reward is designed to measure the system gain regarding accuracy and fairness. FairRec is trained to maximize the long-term cumulative reward to maintain an accuracy-fairness balance. The major contributions of this paper are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We formulate a fairness objective for IRS. To the best of our knowledge, this is the first work that balances between accuracy and fairness in IRS. -We propose a reinforcement learning based framework, FairRec, to dynamically maintain a balance between accuracy and fairness in IRS. In FairRec, user preferences and the system's fairness status are jointly compressed into the state representation to generate recommendations. We also design a twofold reward to combine accuracy and fairness. -We evaluate our proposed FairRec algorithm on both synthetic and real-world data. We show that FairRec can achieve a better balance between accuracy and fairness, compared to the state-of-the-art methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we model the fairness-aware recommendation for IRS as a finite time Markov Decision Process (MDP), with an action space A, a state space S, and a reward function r : S \u00d7 A \u2192 R. When a user u arrives at time step t = 1, . . . , T , the system observes the current state s t \u2208 S of the user u and takes an action a t \u2208 A (e.g., recommending an item to the user). The user views the item and provides feedback y at , e.g., clicking or downloading on the recommended item, if she feels interested. Let y at \u2208 {0, 1} denote the user's feedback, with y at = 1 meaning the user performs desired activities, and 0 otherwise. The system then receives a reward r t (a function of y at ), and updates the model. The problem formulation is formally presented as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markov Decision Process for IRS"
        },
        {
            "text": "States S: The state s t is described by user preferences and the system's fairness status. We jointly embed them into the current state representation. The detailed design of the state representation is given in Sect. 3.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markov Decision Process for IRS"
        },
        {
            "text": "Transitions P: The transition of states models the dynamic change of user preferences and the system's fairness. The successor state s t+1 is obtained once the user's feedback at time t is collected.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markov Decision Process for IRS"
        },
        {
            "text": "Action A: An action a t is recommending an item chosen from the available candidate item set A. Our framework can be easily extended to the case of recommending a list of items. To simplify our presentations, we focus on recommending an item at a time in this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markov Decision Process for IRS"
        },
        {
            "text": "Reward R: The reward r t is a scalar measuring the system's gain regarding accuracy and fairness after taking action a t , elaborated in Sect. 3.3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markov Decision Process for IRS"
        },
        {
            "text": "We aim to learn a policy \u03c0, mapping from states to actions a t = \u03c0(s t ), to generate recommendations that are both accurate and fair. The goal is to maximize the sum of discounted rewards (return) from time t onward, which is defined by R \u03b3 t = T k=t \u03b3 k\u2212t r k , and \u03b3 is the discount factor.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markov Decision Process for IRS"
        },
        {
            "text": "Each item is associated with a categorical protected attribute C \u2208 {c 1 , . . . , c l }.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Weighted Proportional Fairness for IRS"
        },
        {
            "text": "Let A c = {a|C = c, a \u2208 A} denote the group of items with an attribute value c. Take loan recommendation for instance, if the protected attribute is the geographical region, then A c with c = \"Oceania\" contains all the loans applied from Oceania. Denote by x t \u2208 R l + the allocation vector, where x i t represents the allocation proportion of group i up to time t,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Weighted Proportional Fairness for IRS"
        },
        {
            "text": "where 1 A (x) equals to 1 if x \u2208 A, and 0 otherwise. Recall that y a k is the user's feedback on recommended item a k . In loan recommendation, x i t denotes the rate of funded loans from the region i over all funded ones up to time t.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Weighted Proportional Fairness for IRS"
        },
        {
            "text": "In this work, we focus on a well-accepted and axiomatically justified metric of fairness, the weighted proportional fairness [9] . Weighted proportional fairness is a generalized Nash solution for multiple groups.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 128,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Weighted Proportional Fairness for IRS"
        },
        {
            "text": "The coefficient w i \u2208 R + is a pre-defined parameter weighing the importance of each group. The optimal solution can be easily solved by standard Lagrangian multiplier methods, namely",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 (Weighted Proportional Fairness). An allocation of desired activities x t is weighted proportionally fair if it is the solution of the following optimization problem,"
        },
        {
            "text": "We aim to improve the weighted proportional fairness",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 (Weighted Proportional Fairness). An allocation of desired activities x t is weighted proportionally fair if it is the solution of the following optimization problem,"
        },
        {
            "text": "This section begins with a brief overview of our proposed FairRec. After that, we introduce the components of FairRec and the learning algorithm in detail. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Model"
        },
        {
            "text": "To balance between accuracy and fairness in the long run, we formulate IRS recommendation as an MDP, which is then solved by reinforcement learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "The previously studied reinforcement learning models can be categorized as follows: Value-based methods approximate the value function, then the action with the largest value is selected [26, 27] . Value-based methods are more sampleefficient and steady, but the computational cost is high when the action space is large. Policy-based methods directly learn a policy that takes as input of the current user state and outputs an action [6, 24] , which generally have a faster convergence. Actor-critic architectures take advantage of both value-based and policy-based methods [15, 25] . Therefore, we design our model following the actor-critic framework.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 191,
                    "text": "[26,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 192,
                    "end": 195,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 435,
                    "end": 438,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 439,
                    "end": 442,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 575,
                    "end": 579,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 580,
                    "end": 583,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "The overall architecture of FairRec is illustrated in Fig. 1 , which consists of an actor network and a critic network. The actor network performs time-varying recommendations according to the dynamic user preferences and the fairness status. The critic network estimates the value of the outputs associated with the actor network to encourage or discourage the recommended items.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 54,
                    "end": 60,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Overview"
        },
        {
            "text": "We propose a personalized fairness-aware state representation to jointly consider accuracy and fairness, which is composed of the the User Preference State (UPS) and the Fairness State (FS). State representation learns a non-linear transformation h t = f s (s t ) that maps the current state s t to a continuous vector h t .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Fairness-Aware State Representation"
        },
        {
            "text": "User Preference State (UPS). UPS represents personalized user preferences. We propose a two-level granularity representation: the item-level and the grouplevel. The item-level representation indicates the user's fine-grained preferences to each item, while the group-level representation shows the user's coarse-grained interests in each item group. Such two-level granularity representation provides more information on the propensity of different users towards diverse recommendation. Therefore, the agent could focus on accuracy for the users with particular favor, and the lack of fairness at a point in time can later be compensated when recommending items to users with diverse interests.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Fairness-Aware State Representation"
        },
        {
            "text": "The input of UPS is the sequence of the user u's N most recent positively interacted items, as well as the corresponding group IDs that the items belong to at t. Items belonging to the same group share the same protected attribute value c. Each item a is mapped to a continuous embedding vector e a \u2208 R d . The embedding vector of each group ID e g is the average of the embedding vectors of all items belonging to the group g. Then each item is represented by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Fairness-Aware State Representation"
        },
        {
            "text": "where a \u2208 R d , and item a belongs to group g. The group embedding e g is added to serve as a global bias (or a regularizer), allowing items belonging to the same group to share the same group information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Fairness-Aware State Representation"
        },
        {
            "text": "As for a specific user u, the affects of different historical interactions on her future interest may vary significantly. To capture this sequential dependencies among the historical interacted items, we apply an attention mechanism [23] to weigh each item in the interacted item sequence. The attention net learns a weight vector \u03b2 of size N , \u03b2 = Softmax(\u03c9 1 \u03c3(\u03c9 2 [ a1 , . . . , aN ] + b 2 ) + b 1 ), where \u03c9 1 , b 1 , \u03c9 2 , b 2 are the network parameters and \u03c3(\u00b7) is the ReLU activation function. The user preference state representation m t is obtained by multiplying the attention weights with the corresponding item representations as m t = [\u03b2 1 a1 , . . . , \u03b2 N aN ], where m t is of dimension N \u00d7 d and \u03b2 i denotes the i-th entry in the weight vector \u03b2. Therefore, the items currently contributing more to the outcome are assigned with higher weights.",
            "cite_spans": [
                {
                    "start": 233,
                    "end": 237,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Personalized Fairness-Aware State Representation"
        },
        {
            "text": "The input of FS is the current allocation distribution of the desired activities at time t. As a complementary for UPS, FS provides evidence of the current fairness status and helps the agent to promote items belonging to under-represented groups. In particular, we deploy a Multi-Layer Perceptron (MLP) to map the allocation vector x t to a latent space, n t = MLP(x t ). Then we concatenate m t and n t to obtain the final state representation,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fairness State (FS)."
        },
        {
            "text": "with || denotes concatenation operation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fairness State (FS)."
        },
        {
            "text": "The reward is designed to measure the system's gain regarding accuracy and fairness. Existing reinforcement learning frameworks for recommendation only consider the recommendation accuracy, and one commonly used definition of reward is r = 1 if the user performs desired activities and \u22121 otherwise [15, 25] .",
            "cite_spans": [
                {
                    "start": 299,
                    "end": 303,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 304,
                    "end": 307,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Reward Function Design"
        },
        {
            "text": "To incorporate the fairness measure into IRS, we propose a two-fold reward by first examining whether the user performs the desired activities on the recommended item, and then evaluating the fairness gain of performing such a desired activity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reward Function Design"
        },
        {
            "text": "As discussed in Sect. 2, to achieve the weighted proportional fairness, the optimal allocation vector is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reward Function Design"
        },
        {
            "text": ", with w i the pre-defined target allocation proportion of group i. Therefore, we incorporate the deviation from the optimal solution x i * \u2212 x i t into the reward as the fairness indicator:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reward Function Design"
        },
        {
            "text": "where 1 A (x) is the indicator function and is 1 when x \u2208 A, 0 otherwise, x i t is the allocation proportion of group i at time t. The constant \u03bb > 1 is the penalty value for inaccurate recommendations and manages the accuracy-fairness tradeoff. A larger \u03bb means that the agent focuses more on accuracy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reward Function Design"
        },
        {
            "text": "Since the fairness metric (Eq. (1) and Eq. (2)) is computed according to the number of the desired activities, only positive y at influences fairness. Therefore, we simply give a negative reward \u2212\u03bb for y at = 0 to punish the undesired activities. When y at = 1, we compute the fairness score x i * \u2212 x i t , which is the difference between the optimal distribution and current allocation. Suppose the user performs a desired activity on the item a t \u2208 A ci . Then the fairness score x i * \u2212 x i t is negative if the i-th group is over-representing (x i t > x i * ), and is more negative if A ci already has a higher rate of the desired activity, indicating that the system should focus more on other groups. Similarly, the fairness score x i * \u2212x i t is positive if the i-th group is currently under-representing (x i t < x i * ), and is more positive if A ci is more lacking in the desired activity. We add 1 to the fairness score to ensure the reward is positive if y at = 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reward Function Design"
        },
        {
            "text": "To sum up, the agent receives a large positive reward if the user performs a desired activity on the item and the item belongs to an under-representing group. Whereas the reward is a smaller positive number if the activity is desired, but the item belongs to an over-representing (majority) group. We punish the most severely with y at = 0, as it neither contributes to accuracy nor fairness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reward Function Design"
        },
        {
            "text": "Actor Network. The actor network extracts latent features from s t and outputs a ranking strategy vector z t . The recommendation is performed according to the ranking vector by a t = arg max a\u2208A e a z t . In particular, we first embed s t to h t following the architecture described in Sect. 3.2, then we stack fully-connected layers on top of h t to learn the nonlinear transformation and generate z t , as presented in Fig. 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 422,
                    "end": 428,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Model Update"
        },
        {
            "text": "Suppose the policy \u03c0 \u03b8 (s) learned by the actor is parameterized by \u03b8. The actor is trained according to Q \u03b7 (s t , z t ) from the critic, and updated by the sampled policy gradient [18] with \u03b1 \u03b8 as the learning rate, B as the batch size,",
            "cite_spans": [
                {
                    "start": 182,
                    "end": 186,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Model Update"
        },
        {
            "text": "Critic Network. The critic adopts a deep neural network Q \u03b7 (s t , z t ), parameterized by \u03b7, to estimate the expected total discounted reward E[R \u03b3 t |s t , z t ; \u03c0], given the state s t and the ranking strategy vector z t under the policy \u03c0. Specifically for this problem, the network structure is designed as follows",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Update"
        },
        {
            "text": "by first mapping h t to the same space as z t with a fully-connected layer and then concatenating it with z t , while MLP(\u00b7) denotes a mutli-layer perceptron, and h t = f s (s t ) is the state representation as presented in Sect. 3.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Update"
        },
        {
            "text": "We use the temporal-difference (TD) learning [22] to update the critic. The loss function is the mean square error",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Model Update"
        },
        {
            "text": "\u03b7 and \u03b8 are the parameters of the target critic and actor network that are periodically copied from \u03b7, \u03b8 and kept constant for a number of iterations to ensure the stability of the training [14] . The parameter \u03b8 is updated by gradient descent, with \u03b1 \u03b7 the learning rate and B the batch size:",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 194,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Model Update"
        },
        {
            "text": "We evaluate the proposed FairRec algorithm on both synthetic and real-world data, comparing with the state-of-the-art recommendation methods in terms of fairness and accuracy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "Datasets. We use MovieLens 1 and Kiva.org datasets for evaluation. Movie-Lens is a public benchmark dataset for recommender systems, with 943 users, 1,602 items and 100,000 user-item interactions. Since the MovieLens data do not have protected attributes, we created 10 groups to represent differences among group inventories, and randomly assigned movies to each of such groups following a geometric distribution. An interaction with the rating (ranging from 1 to 5) larger than 3 is defined as a desired activity in calculating CVR. Kiva.org is a proprietary dataset obtained from Kiva.org, consisting of lending transactions over a 6-month period. We followed the pre-processing technique used in [16] to densify the dataset. The retained dataset has 1,589 loans, 589 lenders and 43,976 ratings. The geographical region of loans is selected as the protected attribute, as Kiva.org has a stated mission of equalizing access to capital across different regions so that loans from each region have a fair chance to be funded. We define a transaction amount greater than USD25 as the desired activity for Kiva.",
            "cite_spans": [
                {
                    "start": 700,
                    "end": 704,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "Evaluation Metrics. We evaluate the recommendation accuracy by the Conversion Rate (CVR):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "and measure the fairness by Weighted Proportional Fairness (PropFair) 2 : [17] between FairRec and the strongest baseline DRR, where * means the p-value is smaller than 0.05.",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 78,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "Moreover, we propose a Unit Fairness Gain (UFG) to jointly consider accuracy and fairness,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "UFG indicates the fairness of the system under unit accuracy budget. For any recommendation system, the ideal maximum CVR, namely CVR max , equals to 1. Thus UFG can be interpreted as the slope of fairness versus accuracy-the fairness gain if we decrease a unit accuracy from CVR max . A larger UFG means a higher value of PropFair can be achieved with unit deviation from CVR max , namely, the larger, the better.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "We randomly sample 80% of the user with associated rating sequences for training, and 10% for validation, 10% for testing, so that the item dependencies within each session can be learned. We use grid search to select the hyper-parameters for all the methods to maximize the hybrid metric UFG: the embedding dimension in {10, 30, 50, 100}, the learning rate in {0.0001, 0.001, 0.01}. Embedding vectors are pre-trained using standard matrix factorization [11] following the traditional processing as in [15, 25] . For the proposed FairRec, we set the number of recent interacted items N = 5, discount factor \u03b3 = 0.9, the width of each hidden layer of the actor-critic network is 1000. The batch size is set to 1024, and the optimization method is Adam. Without loss of generality, we set w i = 1, i = 1, . . . , l. All results are averaged from multiple independent runs.",
            "cite_spans": [
                {
                    "start": 454,
                    "end": 458,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 502,
                    "end": 506,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 507,
                    "end": 510,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Reproducibility."
        },
        {
            "text": "Comparison with Existing Methods. We compare our proposed FairRec with six representative recommendation algorithms: (i) NMF. Non-negative Matrix Factorization (NMF) [12] estimates the rating matrix with positive user and item factors; (ii) SVD. Singular Value Decomposition (SVD) [10] is the classic matrix factorization based method that decomposes the rating matrix via a singular value decomposition; (iii) DeepFM. DeepFM [7] is the state-ofthe-art deep learning model in recommendation that combines the factorization machines and deep neural networks; (iv) LinUCB. LinUCB [13] is the state-ofthe-art contextual bandits algorithm that sequentially selects items and balances between exploitation and exploration in IRS; (v) DRR. DRR [15] is a deep reinforcement learning framework designed for IRS to maximize the long-term reward; (vi) MRPC. Multi-sided Recommendation with Provider Constraints (MRPC) [21] is the state-of-the-art fairness-aware method by formulating the fairness problem as an integer programming. Table 1 shows the results. Bold numbers are the best results and underlined numbers are the strongest baselines. We have the following observations:",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 170,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 281,
                    "end": 285,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 426,
                    "end": 429,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 578,
                    "end": 582,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 738,
                    "end": 742,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 908,
                    "end": 912,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 1022,
                    "end": 1029,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "First, the deep learning based method (DeepFM) outperforms matrix factorization based methods (NMF and SVD) in CVR, while PropFair of DeepFM is lower. This is consistent with our expectation that DeepFM combines low-order and high-order feature interactions and has great fitting capability, yet it solely maximizes the accuracy, with fairness issues overlooked.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Second, LinUCB and DRR generally achieve better CVR than matrix factorization and deep learning methods. It is because LinUCB and DRR consider the IRS setting, and aims to maximize the long-term reward. Compared Lin-UCB to DRR, LinUCB underperforms DRR since LinUCB assumes states of the system remain unchanged and fails to tailor the recommendation to match the dynamic user preferences. DRR is the strongest baseline as it achieves the best tradeoff between accuracy and fairness, with UFG = 6.0177 on MovieLens and UFG = 2.5183 on Kiva, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Third, MRPC considers fairness by adding fairness constraints for static recommendation. Therefore, MRPC generates the fairest recommendation on both datasets, but the CVR significantly decreases as MRPC ignores the dynamic change of user preferences and the fairness status.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Fourth, FairRec consistently yields the best performance in terms of CVR, PropFair, and UFG on both datasets, demonstrating FairRec is effective in maintaining the accuracy-fairness tradeoff over time. FairRec outperforms the strongest baselines, DRR, by 1.3%, 2.3%, and 11% in CVR, PropFair, and UFG on MovieLens, and 5.1%, 2.2%, and 13.4% on Kiva. Considering UFG, with unit accuracy loss, FairRec achieves the most fairness gain. FairRec observes the current user preferences and the fairness status, and estimates the long-term discounted cumulative reward. Therefore, FairRec is capable of long-term planning to manage the balance between accuracy and fairness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Analysis"
        },
        {
            "text": "Influence of Embedding Dimension. Embedding dimension d is an important factor for FairRec. We study how the embedding dimension d influences the performance of FairRec. We vary d in {10, 30, 50}, and run 2500 epochs. The cumulative reward and the test performance are plotted in Fig. 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 280,
                    "end": 286,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "We observe that when d is large (d = 30 and d = 50), the algorithm benefits from sufficient expressive power and the reward converges at a high level. As for d = 10, the cumulative reward converges fast at a relatively low value, indicating that the model suffers from the limited fitting capability. In terms of UFG value, UFG = 6.68 when d = 50, which is slightly better than 6.6 as d = 30. Similar results can be found on Kiva, which is omitted for limited space. Therefore, we select d = 50 in FairRec for all the experiments. Ablation Study. To evaluate the effectiveness of different components (i.e., the state representation and the reward function) in FairRec, we replace a component of FairRec with the standard setting in RL at each time, and compare the performance with the full-fledged FairRec. Experimental results are presented in Table 2 . We design two variants: FairRec(reward-) with standard reward as in [15, 25] ; and FairRec(state-) with simple concatenation of item embeddings as the state representation as in [15] . Results show that FairRec(reward-) generally has high CVR, as no punishment on unfair recommendation. Moreover, the model simply optimizes accuracy, failing to balance between accuracy and fairness. As for FairRec(state-), CVR is downgraded significantly, validating the importance of our designed state representation. Overall, UFG of FairRec is the largest, confirming that all the components of FairRec work together yield the best results.",
            "cite_spans": [
                {
                    "start": 925,
                    "end": 929,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 930,
                    "end": 933,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 847,
                    "end": 854,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "In this work, we propose a fairness-aware recommendation framework in IRS to dynamically balance between accuracy and fairness in the long run with reinforcement learning. In the proposed state representation component, UPS models both personalized preference and propensity to diversity; FS is utilized to describe the current fairness status of IRS. A two-fold reward is designed to combine accuracy and fairness. Experimental results demonstrate the effectiveness in the balance of accuracy and fairness of our proposed framework over the state-of-the-art models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Beyond personalization: Research directions in multistakeholder recommendation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Abdollahpouri",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Multi-stakeholder recommendation and its connection to multi-sided fairness",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Abdollahpouri",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Burke",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1907.13158"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Equity of attention: amortizing individual fairness in rankings",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Biega",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Balanced neighborhoods for multi-sided fairness in recommendation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Burke",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "In: FAT*",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "From hits to niches? Or how popular artists can bias music recommendation and discovery",
            "authors": [
                {
                    "first": "\u00d2",
                    "middle": [],
                    "last": "Celma",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cano",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 2nd KDD Workshop on Large-Scale Recommender Systems and the Netflix Prize Competition",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Large-scale interactive recommendation with tree-structured policy gradient",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "DeepFM: a factorization-machine based neural network for CTR prediction",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Antidiscrimination rights without equality",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Holmes",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Mod. Law Rev",
            "volume": "",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Rate control for communication networks: shadow prices, proportional fairness and stability",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "P"
                    ],
                    "last": "Kelly",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "JORS",
            "volume": "49",
            "issn": "",
            "pages": "237--252",
            "other_ids": {
                "DOI": [
                    "10.1057/palgrave.jors.2600523"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Factor in the neighbors: Scalable and accurate collaborative filtering",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Koren",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "TKDD",
            "volume": "4",
            "issn": "1",
            "pages": "1--1",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Matrix factorization techniques for recommender systems",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Koren",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Computer",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Algorithms for non-negative matrix factorization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "D"
                    ],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A contextual-bandit approach to personalized news article recommendation",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Continuous control with deep reinforcement learning",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "P"
                    ],
                    "last": "Lillicrap",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1509.02971"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Deep reinforcement learning based recommendation with explicit user-item interactions modeling",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.12027"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Personalizing fairness-aware re-ranking",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Burke",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1809.02921"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "The unequal variance t-test is an underused alternative to student's t-test and the mann-whitney u test",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "D"
                    ],
                    "last": "Ruxton",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Behav. Ecol",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Deterministic policy gradient algorithms",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Silver",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ICML",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Fairness of exposure in rankings",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Joachims",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Interactive recommender systems: tutorial. In: RecSys",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Steck",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Multistakeholder recommendation with provider constraints",
            "authors": [
                {
                    "first": "\u00d6",
                    "middle": [],
                    "last": "S\u00fcrer",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Introduction to Reinforcement Learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "S"
                    ],
                    "last": "Sutton",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "A reinforcement learning framework for explainable recommendation",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Deep reinforcement learning for page-wise recommendations",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Recommendations with negative feedback via pairwise deep reinforcement learning",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "DRN: a deep reinforcement learning framework for news recommendation",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The architecture of FairRec.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Experimental results with embedding dimension d on MovieLens: cumulative reward (left) and CVR, PropFair, and UFG (right).",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Experimental results on MovieLens and Kiva.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Ablation study on MovieLens and Kiva.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This work is supported by National Natural Science Foundation of China (No. U1813204).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}