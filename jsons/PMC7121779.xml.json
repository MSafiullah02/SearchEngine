{
    "paper_id": "PMC7121779",
    "metadata": {
        "title": "Generating Fake but Realistic Headlines Using Deep Neural Networks",
        "authors": [
            {
                "first": "Djamal",
                "middle": [],
                "last": "Benslimane",
                "suffix": "",
                "email": "djamal.benslimane@univ-lyon1.fr",
                "affiliation": {}
            },
            {
                "first": "Ernesto",
                "middle": [],
                "last": "Damiani",
                "suffix": "",
                "email": "ernesto.damiani@kustar.ac.ae",
                "affiliation": {}
            },
            {
                "first": "William",
                "middle": [
                    "I."
                ],
                "last": "Grosky",
                "suffix": "",
                "email": "wgrosky@umich.edu",
                "affiliation": {}
            },
            {
                "first": "Abdelkader",
                "middle": [],
                "last": "Hameurlain",
                "suffix": "",
                "email": "abdelkader.hameurlainr@irit.fr",
                "affiliation": {}
            },
            {
                "first": "Amit",
                "middle": [],
                "last": "Sheth",
                "suffix": "",
                "email": "amit.sheth@wright.edu",
                "affiliation": {}
            },
            {
                "first": "Roland",
                "middle": [
                    "R."
                ],
                "last": "Wagner",
                "suffix": "",
                "email": "rwagner@faw.jku.at",
                "affiliation": {}
            },
            {
                "first": "Ashish",
                "middle": [],
                "last": "Dandekar",
                "suffix": "",
                "email": "ashishdandekar@u.nus.edu",
                "affiliation": {}
            },
            {
                "first": "Remmy",
                "middle": [
                    "A.",
                    "M."
                ],
                "last": "Zen",
                "suffix": "",
                "email": "remmy.augusta@ui.ac.id",
                "affiliation": {}
            },
            {
                "first": "St\u00e9phane",
                "middle": [],
                "last": "Bressan",
                "suffix": "",
                "email": "steph@nus.edu.sg",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "In the Digital News Report 20161, Reuters Institute for the Study of Journalism claims that 51% of the people in their study indicate the use of social media platforms as their primary source of news. This transition of social media platforms to news sources further accentuates the issue of the trustworthiness of the news which is published on the social media platforms. In order to address this, social media platform like Facebook has already started working with five fact-checking organizations to implement a filter which can flag fake news on the platform2.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Starting from the traditional problem of spam filtering to a more sophisticated problem of anomaly detection, machine learning techniques provide a toolbox to solve such a spectrum of problems. Machine learning techniques require a good quality training data for the filters to be robust and effective. To train fake news filters, they need a large amount of fake but realistic news. Fake news, which are generated by a juxtaposition of a couple of news without any context, do not lead to robust filtering. Therefore, there is a need of a tool which automatically generates a large amount of good quality fake but realistic news.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we propose a deep learning model that automatically generates news headlines given a seed and the context. For instance, for a seed \u201cobama says that\u201d, typical news headlines generated under technology context reads \u201cobama says that google is having new surface pro with retina display design\u201d whereas the headline generated under business context reads \u201cobama says that facebook is going to drop on q1 profit\u201d. For the same seed with medicine and entertainment as the topics, typical generated headlines are \u201cobama says that study says west africa ebola outbreak has killed million\u201d and \u201cobama says that he was called out of kim kardashian kanye west wedding\u201d respectively.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We expect that the news headlines generated by the model should not only adhere to the provided context but also to conform to the structure of the sentence. In order to catch the attention of the readers, news headlines follow the structure which deviates from the conventional grammar to a certain extent. We extend the architecture of Contextual Long Short Term Memory (CLSTM), proposed by Ghosh et al. [9], to learn the part-of-speech model for news headlines. We compare Recurrent Neural Networks (RNNs) variants towards the effectiveness of generating news headlines. We qualitatively and quantitatively compare the topical coherence and the syntactic quality of the generated headlines and show that the proposed model is competitively efficient and effective.",
            "cite_spans": [
                {
                    "start": 407,
                    "end": 408,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Section 2 presents the related work. Section 3 delineates the proposed model along with some prerequisites in the neural network. We present experiments and evaluation in Sect. 4. Section 5 concludes the work by discussing the insights and the work underway.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In the last four-five years, with the advancement in the computing powers, neural networks have taken a rebirth. Neural networks with multiple hidden layers, dubbed as \u201cDeep Neural Networks\u201d, have been applied in many fields starting from classical fields like multimedia and text analysis [11, 18, 28, 29] to more applied fields [7, 32]. Different categories of neural networks have been shown to be effective and specific to different kinds of tasks. For instance, Restricted Boltzmann Machines are widely used for unsupervised learning as well as for dimensionality reduction [13] whereas Convolutional Neural Networks are widely used for image classification task [18].",
            "cite_spans": [
                {
                    "start": 291,
                    "end": 293,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 295,
                    "end": 297,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 299,
                    "end": 301,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 303,
                    "end": 305,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 331,
                    "end": 332,
                    "mention": "7",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 334,
                    "end": 336,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 580,
                    "end": 582,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 669,
                    "end": 671,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Recurrent Neural Networks [28] (RNNs) are used learn the patterns in the sequence data due to their ability to capture interdependence among the observations [10, 12]. In [5], Chung et al. show that the extensions of RNN, namely Long Short Term Memory (LSTM) [14] and Gated Recurrent Unit (GRU) [3], are more effective than simple RNNs at capturing longer trends in the sequence data. However, they do not conclude which of these gated recurrent model is better than the other. Readers are advised to refer to [22] for an extensive survey of RNNs and their successors.",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 29,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 159,
                    "end": 161,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 163,
                    "end": 165,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 172,
                    "end": 173,
                    "mention": "5",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 260,
                    "end": 262,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 296,
                    "end": 297,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 511,
                    "end": 513,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Recurrent neural networks and their extensions are widely used by researchers in the domain of text analysis and language modeling. Sutskever et al. [29] have used multiplicative RNN to generate text. In [10], Graves has used LSTM to generate text data as well as images with cursive script corresponding to the input text. Autoencoder [13] is a class of neural networks which researchers have widely used for finding latent patterns in the data. Li et al. [19] have used LSTM-autoencoder to generates text preserving the multi-sentence structure in the paragraphs. They give entire paragraph as the input to the system that outputs the text which is both semantically and syntactically closer to the input paragraph. Tomas et al. [24, 25] have proposed RNN based language models which have shown to outperform classical probabilistic language models. In [26], Tomas et al. provide a context along with the text as an input to RNN and later predict the next word given the context of preceding text. They use LDA [2] to find topics in the text and propose a technique to compute topical features of the input which are fed to RNN along with the input. Ghosh et al. [9] have extended idea in [26] by using LSTM instead of RNN. They use the language model at the level of a word as well as at the level of a sentence and perform experiments to predict next word as well as next sentence given the input concatenated with the topic. There have been evidences of LSTM outperforming GRU for the task of language modeling [15, 16]. Nevertheless, we compare our proposed model using both of these gated recurrent building blocks. We use the simple RNN as our baseline for the comparison.",
            "cite_spans": [
                {
                    "start": 150,
                    "end": 152,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 205,
                    "end": 207,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 337,
                    "end": 339,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 458,
                    "end": 460,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 732,
                    "end": 734,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 736,
                    "end": 738,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 856,
                    "end": 858,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1014,
                    "end": 1015,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1166,
                    "end": 1167,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1192,
                    "end": 1194,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1517,
                    "end": 1519,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1521,
                    "end": 1523,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Despite these applications of deep neural networks on the textual data, there are few caveats in these applications. For instance, although in [9] authors develop CLSTM which is able to generate text, they evaluate its predictive properties purely using objective metric like perplexity. The model is not truly evaluated to see how effective it is towards generating the data. In this paper, our aim is to use deep neural networks to generate the text and hence evaluate the quality of synthetically generated text against its topical coherence as well as grammatical coherence.",
            "cite_spans": [
                {
                    "start": 144,
                    "end": 145,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Recurrent Neural Network (RNN) is an adaptation of the standard feedforward neural network wherein connections between hidden layers form a loop. Simple RNN architecture consists of an input layer (x), a hidden layer (h), and an output layer (y). Unlike the standard feedforward networks, the hidden layer of RNN receives an additional input from the previous hidden layer. These recurrent connections give RNN the power to learn sequential patterns in the input. We use the many-to-many variant of RNN architecture which outputs n-gram given the previous n-gram as the input. For instance, given {(hello, how, are)} trigram as the input, RNN outputs {(how, are, you)} as the preceding trigram.",
            "cite_spans": [],
            "section": "Background: Recurrent Neural Network ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Bengio et al. [1] show that learning the long-term dependencies using gradient descent becomes difficult because the gradients eventually either vanish or explode. The gated recurrent models, LSTM [14] and GRU [3], alleviate these problems by adding gates and memory cells (in the case of LSTM) in the hidden layer to control the information flow. LSTM introduces three gates namely forget gate (f), input gate (i), and output gate (o). Forget gate filters the amount of information to retain from the previous step, whereas input and output gate defines the amount of information to store in the memory cell and the amount of information to transfer to the next step, respectively. Equation 1 shows the formula to calculate the forget gate activations at a certain step t. For given layers or gates m and n, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_{mn}$$\\end{document} denotes the weight matrix and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b_{m}$$\\end{document} is the bias vector for the respective gate. h is the activation vector for the hidden state and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma (\\cdot )$$\\end{document} denotes the sigmoid function. Readers are advised to refer to [14] for the complete formulae of each gate and layer in LSTM.1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} f_{t} = \\sigma (W_{fx} x_{t} + W_{fh}h_{t-1} + b_{f}) \\end{aligned}$$\\end{document}GRU simplifies LSTM by merging the memory cell and the hidden state, so there is only one output in GRU. It uses two gates which are update and reset gate. Update gate unifies the input gate and the forget gate in LSTM to control the amount of information from the previous hidden state. The reset gate combines the input with the previous hidden state to generate the current hidden state.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 16,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 198,
                    "end": 200,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 211,
                    "end": 212,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1882,
                    "end": 1884,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Background: Recurrent Neural Network ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Simple RNNs predict the next word solely based on the word dependencies which are learnt during the training phase. Given a certain text as a seed, the seed may give rise to different texts depending on the context. Refer to the Sect. 1 for an illustration. [9] extends the standard LSTM to Contextual Long Short Term Memory (CLSTM) model which accepts the context as an input along with the text. For example, an input pair {(where, is, your), (technology)} generates an output like {(is, your, phone)}. CLSTM is a special case of the architecture shown in Fig. 1a using LSTM as the gated recurrent model.",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 260,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Proposed Syntacto-Contextual Architecture ::: Methodology",
            "ref_spans": [
                {
                    "start": 563,
                    "end": 564,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "In order to use the model for the purpose of text generation, contextual information is not sufficient to obtain a good quality output. A good quality text is coherent not only in terms of its semantics but also in terms of its syntax. By providing the syntactic information along with the text, we extend the contextual model to Syntacto-Contextual (SC) models. Figure 1b shows the general architecture of the proposed model. We encode the patterns in the syntactic meta information and input text using the gated recurrent units and, later, merge them with the context. The proposed model not only outputs text but also corresponding syntactic information. For instance, an input {(where, is, your), (adverb, verb, pronoun), (technology)} generates output like {(is, your, phone), (verb, pronoun, noun)}.\n",
            "cite_spans": [],
            "section": "Proposed Syntacto-Contextual Architecture ::: Methodology",
            "ref_spans": [
                {
                    "start": 370,
                    "end": 371,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Mathematically, the addition of context and syntactic information amounts to learning a few extra weight parameters. Specifically, in case of LSTM, Eq. 1 will be modified to Eqs. 2 and 3, for CLSTM and SCLSTM respectively. In Eqs. 2 and 3, p represents topic embedding and s represents embedding of the syntactic information.2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} f_{t}&= \\sigma (W_{fx} x_{t} + W_{fh}h_{t-1} + b_{f} \\mathbf {+ W_{pf}p_{t}}) \\end{aligned}$$\\end{document}\n3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} f_{t}&= \\sigma (W_{fx} x_{t} + W_{fh}h_{t-1} + b_{f} \\mathbf {+ W_{pf}p_{t} + W_{sf}s_{t}} ) \\end{aligned}$$\\end{document}For the current study, we annotate the text input with the part-of-speech tags using Penn Treebank tagset [23]. We learn the parameters of the model using stochastic gradient descent by minimizing the loss for both output text and output tags. We, also, work on a variation of the contextual architecture which does not accept topic as an input and uses conventional RNN instead of LSTM. This model is treated as the baseline against which all of the models will be compared.",
            "cite_spans": [
                {
                    "start": 1234,
                    "end": 1236,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Proposed Syntacto-Contextual Architecture ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "For each of the model, we embed the input in a vector space. We merge the inputs by column wise concatenation of the vectors. We perform experiments using both LSTM and GRU as the gated recurrent units. The output layer is a softmax layer that represents the probability of each word or tag. We sample from that probability to get the next word and tag output.",
            "cite_spans": [],
            "section": "Proposed Syntacto-Contextual Architecture ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "We use the News Aggregator dataset3 consisting of the news headlines collected by the news aggregator from 11,242 online news hostnames, such as time.com, forbes.com, reuters.com, etc. between 10 March 2014 to 10 August 2014. The dataset contains 422,937 news articles divided into four categories, namely business, technology, entertainment, and health. We randomly select 45000 news headlines, which contain more than three words, from each category because we give trigram as the input to the models. We preprocess the data in two steps. Firstly, we remove all non alpha-numeric characters from the news titles. Secondly, we convert all the text into lower case. After the preprocessing, the data contains 4,274,380 unique trigrams and 39,461 unique words.",
            "cite_spans": [],
            "section": "Dataset ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "All programs are run on Linux machine with quad core 2.40 GHz Intel\u00ae Core i7\u2122processor with 64 GB memory. The machine is equipped with two Nvidia GTX 1080 GPUs. Python\u00ae 2.7.6 is used as the scripting language. We use a high-level neural network Python library, Keras [4] which runs on top of Theano [30]. We use categorical cross entropy as our loss function and use ADAM [17] as an optimizer to automatically adjust the learning rate.",
            "cite_spans": [
                {
                    "start": 268,
                    "end": 269,
                    "mention": "4",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 300,
                    "end": 302,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 373,
                    "end": 375,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Experimental Setup ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "We conduct experiments and comparatively evaluate five models. We refer to those models as, baseline - a simple RNN model, CLSTM - contextual architecture with LSTM as the gated recurrent model, CGRU - contextual architecture with GRU as the gated recurrent model, SCLSTM - syntacto-contextual architecture with LSTM as the gated recurrent model, SCGRU - syntacto-contextual architecture with GRU as the gated recurrent model, in the rest of the evaluation. All inputs are embedded into a 200-dimensional vector space. We use recurrent layers each with 512 hidden units with 0.5 dropout rate to prevent overfitting. To control the randomness of the prediction, we set the temperature parameter in our output softmax layer to 0.4. We use the batch size of 32 to train the model until the validation error stops decreasing.",
            "cite_spans": [],
            "section": "Experimental Setup ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "In this section, we present different evaluation metrics that we use for the quantitative analysis. Along with purely objective quantitative metrics such as perplexity, machine translation quality metric, and topical precision, we use metrics like grammatical correctness, n-gram repetition for a finer effectiveness analysis. Additionally, we devise a novelty metric to qualitatively analyse the current use case of news headline generation.",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "\nPerplexity is commonly used as the performance measure [9, 10, 15, 16] to evaluate the predictive power of a language model. Given N test data with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_{t}$$\\end{document} as the target outputs, the perplexity is calculated by using Eq. 4, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_{w_{t}}^{i}$$\\end{document} is the probability of the target output of sample i. A good language model assigns a higher probability to the word that actually occurs in the test data. Thus, a language model with lower perplexity is a better model.4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} perplexity = 2^{-\\frac{1}{N}\\sum _{i=1}^{N} \\log p_{w_{t}}^{i}} \\end{aligned}$$\\end{document}As it happens, the exponent in the Eq. 4 is the approximation of cross-entropy4, which is the loss function we minimize to train the model, given a sequence of fixed length.",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 58,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 60,
                    "end": 62,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 64,
                    "end": 66,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 68,
                    "end": 70,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "Although the task under consideration of the presented work is not of a word or a topic prediction, we simply use perplexity as a purely objective baseline metric. We complement it by using various application specific measures in order to evaluate the effectiveness of the quality of the generated text.",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "\nTopical coherence refers to the extent to which the generated text adheres to the desired topic. In order to evaluate the topical coherence, one requires a faithful classifier which predicts the topic of generated text. We treat the topics predicted by the classifier as the ground truth to quantitatively evaluate the topical coherence. The proposed method generates a news headline given a seed and a topic of the news. People have widely used Multinomial naive Bayes classifier to deal with text data due to independence among the words given a certain class5. We train a Multinomial naive Bayes classifier with Laplace smoothing on the news dataset consisting of 45000 news from each of the four categories. We hold out 20% of the data for validation. By proper tuning of the smoothing parameter, we achieve 89% validation accuracy on the news dataset. We do not use this metric for the baseline model.",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "Taking existing text as a reference, a quality metric evaluates the effectiveness of the generated text in correspondence to the reference. Such a metric measures the closeness of the generated text to the reference text. Metrics such as BLEU [27], Rouge [8], NIST [21] are widely used to evaluate the quality of machine translation. All of these metrics use \u201cgold standard\u201d, which is either the original text or the text written by the domain experts, to check the quality of the generated text. We use BLEU as the metric to evaluate the quality of generated text. For a generated news headline, we calculate its BLEU score by taking all the sentences in the respective topic from the dataset as the reference. Interested readers should refer to [33] for a detailed qualitative and quantitative interpretation of BLEU scores.",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 246,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 256,
                    "end": 257,
                    "mention": "8",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 266,
                    "end": 268,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 748,
                    "end": 750,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "With the motivation of the current work presented in the Sect. 1, we want the generated text from our model to be as novel as possible. So as to have a robust fake news filter, the fake news, which is used to train the model, should not be a juxtaposition of few existing news headlines. More the patterns it learns from the training data to generate a single headline, more novel is the generated headline. We define novelty of the generated output as the number of unique patterns the model learns from the training data in order to generate that output. We realize this metric by calculating longest common sentence common to the generated headline and each of the headline in the dataset. Each of these sentences stands as a pattern that the model has learned to generate the text. Novelty of a generated headline is taken as the number of unique longest common sentences.",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "The good quality generated text should be both novel and grammatically correct. Grammatical correctness refers the judgment on whether the generated text adheres to the set of grammatical rules defined by a certain language. Researchers either employ experts for evaluation or use advanced grammatical evaluation tools which require the gold standard reference for the evaluation [6]. We use an open-source grammar and spell checker software called LanguageTool6 to check the grammatical correctness of our generated headlines. LanguageTool uses NLP based 1516 English grammar rules to detect syntactical errors. Aside from NLP based rules, it used English specific spelling rules to detect spelling errors in the text. To evaluate grammatical correctness, we calculate the percentage of grammatically correct sentences as predicted by the LanguageTool.",
            "cite_spans": [
                {
                    "start": 381,
                    "end": 382,
                    "mention": "6",
                    "ref_id": "BIBREF29"
                }
            ],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "We find that LanguageTool only recognizes word repetition as an error. Consider a generated headline beverly hills hotel for the first in the first in the world as an example. In this headline, there is a trigram repetition - the first in - that passes LanguageTool grammatical test. Such headlines are not said to be good quality headlines. We add new rules with a regular expression to detect such repetitions. We count n-gram repetitions within a sentence for values of n greater than two.",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "To generate the output, we need an initial trigram as a seed. We randomly pick the initial seed from the set of news headlines from the specified topic. We use windowing technique to generate the next output. We remove the first word and append the output to the back of the seed to generate the next output. The process stop when specified sentence length is generated. We generate 100 sentences for each topic in which each sentence contains 3 seed words and 10 generated words.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "\nQuantitative Evaluation. Table 1 summarizes the quantitative evaluation of all the models using metrics described in Sect. 4.3. Scores in bold numbers denote the best value for each metric. We can see that for Contextual architecture, GRU is a better gated recurrent model. Conversely, LSTM is better for Syntacto-Contextual architecture.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": [
                {
                    "start": 32,
                    "end": 33,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "For Syntacto-Contextual architecture, we only consider the perplexity of the text output to make a fair comparison with the Contextual architecture. We analyze that our Syntacto-Contextual architecture has a higher perplexity score because the model jointly minimizes both text and syntactical output losses. On the other hand, the baseline model has a low perplexity score because it simply predicts the next trigram with control on neither the context nor the syntax.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "A high score on classification precision substantiates that all of these models generate headlines which are coherent with the topic label with which they are generated. We observe that all of the models achieve a competitive BLEU score. Although Contextual architecture performs slightly better in terms of BLEU score, Syntacto-Contextual architecture achieves a higher novelty score. In the qualitative evaluation, we present a more detailed comparative analysis of BLEU scores and novelty scores.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "We observe that the news headlines generated by Syntacto-Contextual architecture are more grammatically correct than other models. Figure 2 shows the histogram of n-gram repetitions in the generated news headline. We see that the Syntacto-Contextual architecture gives rise to news headlines with less number of n-gram repetitions.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": [
                {
                    "start": 138,
                    "end": 139,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Lastly, we have empirically evaluated, but not presented here, the time taken by different models for one epoch. CLSTM takes 2000 s for one epoch whereas SCLSTM takes 2131 s for one epoch. Despite the Syntacto-Contextual architecture being a more complex architecture than Contextual architecture, it shows that it is competitively efficient.\n",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "\n\n\n",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "\nQualitative Evaluation. Table 2 presents the samples of generated news from CLSTM proposed by [9] and SCLSTM, which outweighs the rest of the models in the quantitative analysis.\n",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 97,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": [
                {
                    "start": 31,
                    "end": 32,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "In Table 1, we see that the Contextual architecture models receive a higher BLEU score than the proposed architecture models. BLEU score is calculated using n-gram precisions with the news headlines as the reference. It is not always necessary that the higher BLEU score leads towards a good quality text generation. Qualitative analysis of generated headlines shows that the higher BLEU score, in the most cases, is the result of the juxtaposition of the existing news headlines. For instance, consider a headline generated by CLSTM model as an example - \u201cjustin bieber apologizes for racist joke in new york city to take on\u201d - which receives a BLEU score of 0.92. When we search for the same news in the dataset, we find that this generated news is a combination of two patterns from the following two headlines, \u201cjustin bieber apologizes for racist joke\u201d and \u201cuber temporarily cuts fares in new york city to take on city cabs\u201d. Whereas the headline generated by SCLSTM with the same seed is quite a novel headline. In the training dataset there is mention of neither Justin Bieber joking on Twitter nor joke for gay fans. Similar observation can be made with the news related to Fukushima. In the training data set there is no news headline which links Fukushima with climate change. Additionally, there is no training data which links higher growth risk to climate change as well. Thus, we observe that the headlines generated using SCLSTM are qualitatively better than CLSTM.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": [
                {
                    "start": 9,
                    "end": 10,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "All of the models presented in the work are probabilistic models. Text generation being a probabilistic event, on the one hand it is possible that contextual architecture generates a good quality headline at a certain occasion. For instance, we see that CLSTM also generates some good quality news headlines such as \u201cthe fault in our stars trailer for the hunger games mockingjay part teaser\u201d. On the other hand, it is possible that Syntacto-Contextual architecture generates some news headline with poor quality or repetitions, such as \u201cobama warns google apple to make android support for mobile for mobile\u201d. In order to qualitatively analyse the novelty of generated sentence, we need to observe how likely such events occur. Figure 3 shows the boxplot of novelty numbers we calculate for each of 400 generated news headlines using different models. As discussed earlier, we want our model to generate novel news headlines. So, we prefer higher novelty scores. Although the mean novelty of all of the models lie around 24, we see that SCLSTM is more likely to generate the novel headlines. Additionally, we observe that contextual and Syntacto-Contextual architectures performs better than the baseline model.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": [
                {
                    "start": 736,
                    "end": 737,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "As mentioned in the quantitative evaluation, Contextual architecture gives rise to news headlines with a large number of n-gram repetitions. In an extreme case, CLSTM model generates the following headline, \u201clorillard inc nyse wmt wal mart stores inc nyse wmt wal mart stores\u201d, that contains 6-gram repetition. The news headline generated by CLSTM - \u201cSamsung sues newspaper for anti vaccine and other devices may be the best\u201d- exemplifies the smaller topical coherence observed for the Contextual architecture models.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "In order to garner the opinion of real-world users, we use CrowdFlower7 to conduct a crowdsource based study. In this study, we generate two news headlines using CLSTM and SCLSTM using the same seed and ask the workers to choose a more realistic headline between two. We generate such a pair of headlines for 200 different seeds. Each pair is evaluated by three workers and majority vote is used to choose the right answer. At the end of the study, 66% workers agree that SCLSTM generates more realistic headlines than CLSTM.",
            "cite_spans": [],
            "section": "Results ::: Experimentation and Results",
            "ref_spans": []
        },
        {
            "text": "In [9], Ghosh et al. proposed a deep learning model to predict the next word or sentence given the context of the input text. In this work, we adapted and extended their model towards automatic generation of news headlines. The contribution of the proposed work is two-fold. Firstly, in order to generate news headlines which are not only topically coherent but also syntactically sensible, we proposed an architecture that learns part-of-speech model along with the context of the textual input. Secondly, we performed thorough qualitative and quantitative analysis to assess the quality of the generated news headlines using existing metrics as well as a novelty metric proposed for the current application. We comparatively evaluated the proposed models with [9] and a baseline. To this end, we show that the proposed approach is competitively better and generates good quality news headlines given a seed and the topic of the interest.",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 5,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 763,
                    "end": 764,
                    "mention": "9",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Discussion and Future Work",
            "ref_spans": []
        },
        {
            "text": "Through this work, we direct our methodology for data-driven text generation towards a \u201cconstraint and generate\u201d paradigm from a more brute-force way of \u201cgenerate and test\u201d. Quality assessment of the generated data using generative model remains an open problem in the literature [31]. We use the measure of quality, which in our case is the grammatical correctness, as an additional constraint for the model in order to generate the good quality data. The usage of POS tags as the syntactic element is mere a special case in this application. We can think of more sophisticated meta information to enrich the quality of text generation. Ontological categories can be an alternative option.",
            "cite_spans": [
                {
                    "start": 281,
                    "end": 283,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Discussion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Quantitative evaluation.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Generated news headlines\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Contextual and syntacto-contextual architectures",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: n-gram repetition analysis",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Boxplot for novelty metric",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Learning long-term dependencies with gradient descent is difficult",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Simard",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Frasconi",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "IEEE Trans. Neural Netw.",
            "volume": "5",
            "issn": "2",
            "pages": "157-166",
            "other_ids": {
                "DOI": [
                    "10.1109/72.279181"
                ]
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "Reducing the dimensionality of data with neural networks",
            "authors": [
                {
                    "first": "GE",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "RR",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Science",
            "volume": "313",
            "issn": "5786",
            "pages": "504-507",
            "other_ids": {
                "DOI": [
                    "10.1126/science.1127647"
                ]
            }
        },
        "BIBREF5": {
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural Comput.",
            "volume": "9",
            "issn": "8",
            "pages": "1735-1780",
            "other_ids": {
                "DOI": [
                    "10.1162/neco.1997.9.8.1735"
                ]
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Latent dirichlet allocation",
            "authors": [
                {
                    "first": "DM",
                    "middle": [],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "AY",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "MI",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res.",
            "volume": "3",
            "issn": "",
            "pages": "993-1022",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Building a large annotated corpus of English: the penn treebank",
            "authors": [
                {
                    "first": "MP",
                    "middle": [],
                    "last": "Marcus",
                    "suffix": ""
                },
                {
                    "first": "MA",
                    "middle": [],
                    "last": "Marcinkiewicz",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Santorini",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Comput. Linguist.",
            "volume": "19",
            "issn": "2",
            "pages": "313-330",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "Context dependent recurrent neural network language model",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zweig",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "SLT",
            "volume": "12",
            "issn": "",
            "pages": "234-239",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}