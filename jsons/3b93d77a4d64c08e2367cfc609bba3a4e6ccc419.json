{
    "paper_id": "3b93d77a4d64c08e2367cfc609bba3a4e6ccc419",
    "metadata": {
        "title": "Supplementary Material for Discretization and Feature Selection Based on Bias Corrected Mutual Information Considering High-Order Dependencies",
        "authors": [
            {
                "first": "Puloma",
                "middle": [],
                "last": "Roy",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Dhaka",
                    "location": {
                        "country": "Bangladesh"
                    }
                },
                "email": "pulomaa92@gmail.com"
            },
            {
                "first": "Sadia",
                "middle": [],
                "last": "Sharmin",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Islamic University of Technology",
                    "location": {
                        "country": "Bangladesh"
                    }
                },
                "email": "sharmin@iut-dhaka.edu"
            },
            {
                "first": "Amin",
                "middle": [
                    "Ahsan"
                ],
                "last": "Ali",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Independent University of Bangladesh",
                    "location": {
                        "country": "Bangladesh"
                    }
                },
                "email": "aminali@iub.edu.bd"
            },
            {
                "first": "Mohammad",
                "middle": [],
                "last": "Shoyaib",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Dhaka",
                    "location": {
                        "country": "Bangladesh"
                    }
                },
                "email": "shoyaib@du.ac.bd"
            }
        ]
    },
    "abstract": [],
    "body_text": [
        {
            "text": "For continuous distribution H(X, Y, Z) can be defined using Eq. 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "Let us divide the xyz-space into (M \u00d7 J \u00d7 I) equally sized (\u2206x \u00d7 \u2206y \u00d7 \u2206z) cells with coordinates (m, j, i). The number of samples observed in cell (m, j, i) is n mji and the total number of samples is N . Then, the estimator function of entropy H(x, y, z) will b\u00ea",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "If the samples are independent, the stochastic variables n mji are multinomially distributed. Then, the expectations E{n mji } and variances V AR{n mji } are defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "Let us define p 1 = nmji N and q 1 = nmji N and assume f (p 1 ) = nmji N ln nmji N = p 1 ln p 1 as a function of p 1 . Substituting these value in 3 and expanding f (p 1 ) through Taylor series at p 1 = q 1 , we find",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "Applying the value of p 1 and q 1 into Eq. 6, Eq. 3 can be written a\u015d",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "In Eq. 7, R 3 mji (n mji ) is the higher order term of the Taylor expansion. Replacing the formal parameters n mji by the stochastic variables n mji , we take the expectations of\u0124(X, Y, Z) assuming independent samples n mji is multinomially distributed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "In Eq. 8, the 2 nd term vanishes and the 3 rd term can be written in the following form using the value of expectations and variance from Eq. 4 and 5",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "Considering Eq. 8, 9 and ignoring the term (E{R 3 mji (n mji )}), we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "Now, using Eq. 10 conditional mutual information can be expressed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": "Eq. 11 indicates that the bias for Interaction is (M\u22121)(J \u22121)I",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorems"
        },
        {
            "text": ". Now, based on Theorem 1 and 2 in [2] and Eq. 11, we can state that the score of DSbM is Proof. Conditional mutual information between two independent random variable X and Y for a given Z can be defined as",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 38,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "2N ln 2"
        },
        {
            "text": "Here, p(x, y, z) is the joint probability and p(x | z), p(y | z) are the conditional probability. Let q(x, y, z) \u2261 p(x | z)p(y | z)p(z).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "2N ln 2"
        },
        {
            "text": "For simplicity, assume p 1 = p(x, y, z), q 1 = q(x, y, z) and f (p 1 ) = p 1 ln p1 q1 as a function of p 1 . Expanding f (p 1 ) into a Taylor series at p 1 = q 1 yields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "2N ln 2"
        },
        {
            "text": "where f r (p 1 ) is the r th derivative of f (p 1 ). The first term vanishes, the second term is (p 1 \u2212 q 1 ) and the third term is (p1\u2212q1) 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "2N ln 2"
        },
        {
            "text": ". Ignoring the higher order term, we can write Eq. 14 as At this point, this expression of conditional mutual information is related to a standard test of independence in statistics, the \u03c7 2 test. The \u03c7 2 test variable is defined as ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "2q1"
        },
        {
            "text": "with (X \u2212 1)(Y \u2212 1)Z degrees of freedom if X, Y and Z are statistically independent [1] . Hence, I(f m ; f j | f i ) follows \u03c7 2 distribution.",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 87,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "2q1"
        },
        {
            "text": "As Interaction follows \u03c7 2 distribution, its critical value can be calculated using Eq. 19",
            "cite_spans": [],
            "ref_spans": [],
            "section": "2q1"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Some aspects of multivariate analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "E"
                    ],
                    "last": "Myers",
                    "suffix": ""
                }
            ],
            "year": 1988,
            "venue": "Quantitative Analysis of Mineral and Energy Resources",
            "volume": "",
            "issn": "",
            "pages": "669--687",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Simultaneous feature selection and discretization based on mutual information",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sharmin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shoyaib",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "Ali",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A H"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chae",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Pattern Recognition",
            "volume": "91",
            "issn": "",
            "pages": "162--174",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Bias is (M\u22121)(J \u22121)I 2N ln 2 for Interaction I(f m ; f j | f i ) among the features f m and f j given feature f i , where I, J and M are the number of intervals in feature f i , f j and f m respectively. Proof. Conditional mutual information between X, Y given Z can be represented as Eq. 1 I(X; Y | Z) = H(X, Z) + H(Y, Z) \u2212 H(X, Y, Z) \u2212 H(Z)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "I(f m ; f j | f i ) follows \u03c7 2 distribution with (M \u22121)(J \u22121)I degrees of freedom if f m , f i and f j are statistically independent.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}