{
    "paper_id": "3d8ecb8a971ade120ad011b3e1d93d588d2ae81a",
    "metadata": {
        "title": "Analysing ProB's Constraint Solving Backends What Do They Know? Do They Know Things? Let's Find Out!",
        "authors": [
            {
                "first": "Jannik",
                "middle": [],
                "last": "Dunkelau",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf",
                    "location": {
                        "addrLine": "Universit\u00e4tsstra\u00dfe 1",
                        "postCode": "40225",
                        "settlement": "D\u00fcsseldorf",
                        "country": "Germany"
                    }
                },
                "email": "jannik.dunkelau@hhu.de"
            },
            {
                "first": "Joshua",
                "middle": [],
                "last": "Schmidt",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf",
                    "location": {
                        "addrLine": "Universit\u00e4tsstra\u00dfe 1",
                        "postCode": "40225",
                        "settlement": "D\u00fcsseldorf",
                        "country": "Germany"
                    }
                },
                "email": "joshua.schmidt@hhu.de"
            },
            {
                "first": "Michael",
                "middle": [],
                "last": "Leuschel",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf",
                    "location": {
                        "addrLine": "Universit\u00e4tsstra\u00dfe 1",
                        "postCode": "40225",
                        "settlement": "D\u00fcsseldorf",
                        "country": "Germany"
                    }
                },
                "email": "michael.leuschel@hhu.de"
            }
        ]
    },
    "abstract": [
        {
            "text": "We evaluate the strengths and weaknesses of different backends of the ProB constraint solver. For this, we train a random forest over a database of constraints to classify whether a backend is able to find a solution within a given amount of time or answers unknown. The forest is then analysed in regards of feature importances to determine subsets of the B language in which the respective backends excel or lack for performance. The results are compared to our initial assumptions over each backend's performance in these subsets based on personal experiences. While we do employ classifiers, we do not aim for a good predictor, but are rather interested in analysis of the classifier's learned knowledge over the utilised B constraints. The aim is to strengthen our knowledge of the different tools at hand by finding subsets of the B language in which a backend performs better than others.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Besides its native CLP(FD)-based backend, the validation tool ProB [30] offers various backends for solving constraints, e.g. encountered during symbolic verification. In previous work [18, 19] , we trained neural networks to decide for a given constraint which backend should be used. We compared two approaches: one based on feature vectors derived from domain knowledge, and one based on encoding constraints as images. While we achieved promising results with the image-based approach, it was not possible to extract a comprehensible explanation about how the predictions were made. In follow-up work [34] the experiment was replicated with decision trees [5] using the same feature sets as before. This was motivated by the fact that decision trees are a transparent machine learning algorithm allowing to extract and interpret the learned decision rules and thus the acquired knowledge.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 71,
                    "text": "[30]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 185,
                    "end": 189,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 190,
                    "end": 193,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 605,
                    "end": 609,
                    "text": "[34]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 660,
                    "end": 663,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper we will expand on the decision tree approach and further analyse the relative importances of the features used for deciding whether the different backends of ProB will be successful or not. Moreover, we will compare these results with our a priori assumptions about the subdomains in which each backend should work well. While we will display achievable classification performances for our predictors, we do not aim for a good performance, but instead for an analysis over the whole dataset. In particular, we are not interested in replacing the decision function in ProB with a predictor presented in this paper. The goal is to find subsets of the B language in which a backend performs better than others to strengthen our knowledge of the different tools at hand. With the gathered information we may be able to improve the ProB constraint solver and to obtain more suitable features sets for related machine learning tasks for B in the future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "ProB [29, 30] is an animator, model checker, and constraint solver for the formal specification language B [1] . The B language allows to specify, design, and code software systems as well as to perform formal proof of their properties. When using ProB, properties can be checked exhaustively on a state space using various model checking techniques. B is rooted in predicate logic with arithmetic and set theory. At the heart of ProB is a constraint solver for the B language. ProB's constraint solver is used for many tasks. During animation it has to find suitable parameters for the B operations and compute the effect of executing an operation, during disproving [26] it is used to find counter examples to proof obligations. The constraint solver is also used for test case generation, symbolic model checking or program synthesis.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[29,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 10,
                    "end": 13,
                    "text": "30]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 107,
                    "end": 110,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 668,
                    "end": 672,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Primer on ProB and its Backends"
        },
        {
            "text": "ProB has actually not one but three constraint solving backends and each backend has a variety of options. In Sects. 2.1, 2.2 and 2.3 we will introduce each backend, outline their differences, and summarise our a priori assumptions about their performances on subdomains of the B language.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primer on ProB and its Backends"
        },
        {
            "text": "ProB's kernel [29] is implemented in SICStus Prolog [11] using features such as co-routines for delayed constraint propagation, or mutable variables for its constraint store. The CLP(FD) finite domain library [10] is used for integers and enumerated set elements. The library has a limited precision of 59 bits. ProB handles overflows by custom implementations and also supports unbounded domains as well as symbolic representations for infinite or large sets. Some specific features of the ProB constraint solver are that it computes all solutions to a constraint using backtracking. This is important as constraints are often used within set comprehensions. It is also important for model checking to ensure that the entire state space is constructed. ProB can deal with higher-order sets, relations and functions.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 18,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 52,
                    "end": 56,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 209,
                    "end": 213,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "The Native CLP(FD) Backend"
        },
        {
            "text": "First and foremost, the CLP(FD) backend of ProB is the only backend supporting all constructs available in B. It is thus the default backend. It performs best for constraints arising in animation, where usually a small number of variables (operation parameters) have to be enumerated. In this context, it can deal well with large data values.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Subdomains in Which CLP(FD) Presumably Performs Better."
        },
        {
            "text": "Generally speaking, ProB performs well on constraints using enumerated sets, booleans and/or bounded integers as base types. It performs reasonably well on unbounded intervals if interval reasoning can be applied. While ProB is very good at model finding, it can only detect unsatisfiability by exhaustively enumerating all values remaining after deterministic propagation. In case of unbounded data structures, ProB cannot exhaustively enumerate all cases and is much less powerful. While CLP(FD) cannot natively handle unbounded domains or quantifiers, ProB's backend contains several custom extensions to do so. A key limitation of the CLP(FD) backend is that it has no features such as backjumping, conflict-driven clause learning, or random restarts. In consequence, the backend can get stuck in the search space repeatedly enumerating invalid values which SAT or SMT solvers would rule out by learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Subdomains in Which CLP(FD) Presumably Performs Better."
        },
        {
            "text": "An alternative backend [35] for ProB makes use of Alloy's Kodkod library [38] to translate constraints to propositional logic, which are then solved by a SAT solver. For instance, sets are translated as bit vectors. In particular, a subset x of the interval 0..2 would be translated into three propositional logic variables",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 27,
                    "text": "[35]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 73,
                    "end": 77,
                    "text": "[38]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "The Kodkod Backend"
        },
        {
            "text": "2} \u2286 x can then be translated to the propositional logic formula x 1 \u2227 x 2 . As Kodkod does not allow higher-order values, any such constraint is not passed to Kodkod and is instead dealt with by ProB's default CLP(FD) backend after Kodkod has found a solution for the other constraints.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Kodkod Backend"
        },
        {
            "text": "When using this backend, ProB will first perform an interval analysis and determine which variables have a finite scope and a first-order type. The constraint is then partitioned into a part sent to Kodkod and a part solved by ProB. During solving, the SAT solver is called first. For every solution obtained by the SAT solver, ProB's CLP(FD) backend solves the remaining constraints. By default, Kodkod's Sat4j [28] SAT solver is selected.",
            "cite_spans": [
                {
                    "start": 412,
                    "end": 416,
                    "text": "[28]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "The Kodkod Backend"
        },
        {
            "text": "Presumably Performs Better. The strengths and weaknesses of the backend based on Kodkod stem from its internal reliance on SAT solving. While modern SAT solvers are very fast when it comes to solving very large boolean formulae, encoding B into propositional logic underlies certain restrictions. SAT encodings can only be used for data types known to be finite. In particular, one has to assign an upper and lower bound for integers and set sizes. Thus, integer overflows might occur and it is hard to ensure soundness and completeness. Furthermore, arithmetic operations have to be encoded in propositional logic as well such as binary adders. This leads to additional overhead when generating a conjunctive normal form, especially for large bit widths. The designers of Alloy argue [25] that lack of integers is not disadvantageous in general, as integer constraints are often of secondary nature. In B models, this is not the case. In summary, this backend is not good for arithmetic, large relations, infinite domains, higher-order constraints, or data structures.",
            "cite_spans": [
                {
                    "start": 785,
                    "end": 789,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Subdomains in Which Kodkod"
        },
        {
            "text": "In contrast, SAT solving is ideal for problems involving relations as those can be expressed in a way suitable for Kodkod's backends [35] . Furthermore, given that Kodkod is originally used as a backend for analysing Alloy it has been tuned towards constraints involving operations on relations. For instance, the relational image or transitive closure operations of B are handled efficiently by the translation to SAT using Kodkod.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 137,
                    "text": "[35]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Subdomains in Which Kodkod"
        },
        {
            "text": "The third backend of ProB translates B constraints to SMT-LIB formulae and targets the SMT solvers Z3 [13] and CVC4 [3] . Here we focus on the Z3 binding [27] only. The translation works by rewriting the B constraints into a normal form using a core subset of the B operators which can be mapped to SMT-LIB. Additional variables, set comprehensions, and quantifiers are introduced for those operators which have no counterpart in SMT-LIB or Z3, e.g. cardinality, or minimum and maximum of an integer set. Functions and relations are translated to the Array theory of SMT-LIB. The DPLL(T) [21] algorithm underlying SMT solvers is fundamentally different from CLP(FD). Just like for the SAT translation, SMT solvers can perform backjumping and conflict-driven clause learning.",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 106,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 116,
                    "end": 119,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 154,
                    "end": 158,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 588,
                    "end": 592,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "The Z3 Backend"
        },
        {
            "text": "Better. SMT solvers such as Z3 are very good at proof for B and Event-B (cf. [14, 15] ). Our experience in the context of model finding is that Z3 is good at detecting inconsistencies, in particular on infinite domains. For example, Z3 is able to detect that the constraint x < y \u2227 y < x is unsatisfiable. The other two backends are unable to detect this using their default settings. Note that ProB is able to detect this inconsistency if one enables an additional set of propagation rules based on CHR.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 81,
                    "text": "[14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 82,
                    "end": 85,
                    "text": "15]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Subdomains in Which Z3 Presumably Performs"
        },
        {
            "text": "On the downside, Z3 often has difficulties to deal with quantifiers. Moreover, the translation from B to SMT-LIB does not yet support various operators such as general union or general sum nor does it support iteration and closure operators. Constraints using one of these operators are not translated to SMT-LIB at all and the backend returns unknown. In summary, the Z3 backend is good at detecting inconsistencies and reasoning over infinite domains, but for constraints involving quantifiers, larger data values or cardinality computations it often answers unknown.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Subdomains in Which Z3 Presumably Performs"
        },
        {
            "text": "We utilise techniques of supervised machine learning to train a classifier for B constraints, which we will then further analyse in Sect. 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primer on Decision Trees and Random Forests"
        },
        {
            "text": "The notion of machine learning covers a family of algorithms which are able to improve their predictions using a dataset processed at a so called training time. For supervised machine learning, this dataset consists of tuples (x, y) \u2208 D \u2286 X \u00d7 Y , where x \u2208 X represents the input data and y \u2208 Y is the corresponding ground truth, which is the correct class label to be predicted by the employed algorithm. For instance, for a binary classification task, the ground truth can be either 0 or 1. Usually, X = R d corresponds to a d-dimensional feature space, where each problem instance to be classified is represented as a feature vector x = x 1 , . . . , x d . Each x i hereby refers to a specific characterisation, i.e. feature, of the problem instance. During training time, the algorithm is supposed to learn the mapping x \u2192 y for each (x, y) \u2208 D by generalising over recurring patterns in the input data X. It is important that this learned mapping is accurate yet as general as possible, so as to cover yet unseen problem instances. A classifier is said to overfit on the training data if its performance in classifying unseen data is significantly worse. To detect possible overfitting, the resulting classifier's performance is evaluated on a separate test set, i.e. a data set which was not experienced during training time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primer on Decision Trees and Random Forests"
        },
        {
            "text": "In this article, we employed decision trees as the machine learning algorithm of choice. They correspond to a supervised learning method where the training data at the root of the decision tree is progressively split into smaller subsets using a feature-based splitting criterion. At the leaves of the decision tree only subsets with the same ground truth remain. Such subsets are referred to as pure subsets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primer on Decision Trees and Random Forests"
        },
        {
            "text": "A variety of splitting criteria exist. For example, the CART algorithm [5] is based on the Gini impurity i(t) [31] of a node t defined as",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 74,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 110,
                    "end": 114,
                    "text": "[31]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Primer on Decision Trees and Random Forests"
        },
        {
            "text": "with C being the set of possible classes, and p c (t) is the relative frequency of the elements in t belonging to the class c. For a pure subset t of a class c, p c (t ) will be 0 for c = c and 1 for c = c . Hence i(t ) = 0. For an evenly distributed node t we have i(t ) = 1 \u2212 1 |C| , where |C| is the cardinality of C. The goal of the decision tree learning algorithm is to reach an impurity of 0 with as few splits as possible. For any split of t into two sub-nodes t L and t R , we thus measure the impurity decrease by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primer on Decision Trees and Random Forests"
        },
        {
            "text": "The split which maximises the impurity decrease is finally chosen and the algorithm is called recursively on t L and t R respectively. A decision tree is shown in Fig. 1 , where leaves represent actual classes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 163,
                    "end": 169,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Primer on Decision Trees and Random Forests"
        },
        {
            "text": "Random forests [7] are a bagging approach [6] to decision trees, i.e. instead of only training a single decision tree, a set of k decision trees (T i ) 1\u2264i\u2264k is trained. [20] . The species iris setosa, iris versicolor, or iris virginica is classified based on petal length and width.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 18,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 42,
                    "end": 45,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 170,
                    "end": 174,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Random Forests"
        },
        {
            "text": "Each tree is trained on a random subset of the training samples as well as a random subset of features. This randomisation ensures most trees in the set to be distinct from each other. For example, the impurity decrease of common features will vary between the training samples, leading to different choices of splitting. Due to bagging, the relatively unstable nature of decision trees is countered and the technique is less prone to overfitting. A measure for the relative importances of each feature in a random forest is the mean decrease importance [7] . The mean decrease importance of a feature averages the impurity decrease per feature over each decision tree in the forest. Hence, it is a measure of the average impurity decrease the feature offers [2, 37] .",
            "cite_spans": [
                {
                    "start": 554,
                    "end": 557,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 759,
                    "end": 762,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 763,
                    "end": 766,
                    "text": "37]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Random Forests"
        },
        {
            "text": "While we had multiple classification algorithms to choose from, we finally settled on random forests. This choice was motivated by our need for a strong and interpretable classifier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rationale for Using Random Forests"
        },
        {
            "text": "In previous work [19] we used convolutional neural networks, but we were unable to extract the knowledge accumulated by the classifciation due to the black box nature of the neural networks. Hence, we started to use decision trees, as one can easily extract classification rules after the training phase. These rules are comprehensible and can be interpreted by non-experts as well. Decision trees also offer insights about the relevancy of features: the closer to the root a split over a specific feature is done, the more impact it has for the decision process.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 21,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Rationale for Using Random Forests"
        },
        {
            "text": "Alternate machine learning approaches are linear regression and clustering approaches. For linear regression the relevance of features could be extracted by examining the relative differences in their coefficients. However, this would not yield direct rules describing why a particular prediction was made. As we are particularly interested in extractable knowledge from trained classifiers and reasoning for the given predictions, we favoured decision trees over linear regression. On a similar note, we decided against clustering. However, a clustering approach for grouping similar constraints together presents an interesting alternative approach to be studied in future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Rationale for Using Random Forests"
        },
        {
            "text": "In the end, we decided to utilise random forests for the present article. Although they are again blackbox algorithms, they consist of interpretable pieces, which can be analysed for more general rules [16, 23] .",
            "cite_spans": [
                {
                    "start": 202,
                    "end": 206,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 207,
                    "end": 210,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Rationale for Using Random Forests"
        },
        {
            "text": "The related work in the field is split into two categories: machine learning powered algorithm portfolios for SMT solving, and knowledge extraction from tree ensemble learners such as random forests. To the best of our knowledge, no intersection of both categories exists yet in literature, as we do in this article.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Healy et al. [24] conducted a solver portfolio for the Why3 platform [4] . The solver selection was done via decision trees which predicted the anticipated runtime of a proof obligation for each solver, and choosing the fastest one. James P. Bridge [8] used support vector machines for automating the heuristic selection for the E theorem solver [36] . While he was able to improve the already implemented auto-mode in E, he also investigated picking a minimal feature set which ultimately consisted of only two to three features.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 69,
                    "end": 72,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 249,
                    "end": 252,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 346,
                    "end": 350,
                    "text": "[36]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Yang et al. [39] analysed decision trees to extract minimal feature subsets which need to be flipped to achieve a more favourable outcome. Their application area was customer relationship management with focus on increasing the amount of loyal customers, i.e. detect what needs to be done to turn a regular customer into a loyal customer. Similarly, Cui et al. [12] proposed an integer linear program on random forests for finding the minimal subset of features to change for obtaining a different classification. Deng [16] proposed interpretable trees (inTrees) for interpreting tree ensembles. In their paper, they propose a set of metrics to extract learned knowledge from a tree ensemble such as a random forest. This includes the actual rules learned in an ensemble as well as frequent variable interactions. Narayanan et al. [32] extracted the most common patterns for failing solid state drives in datacenters using inTrees. In their work, they found that these extracted patterns match with previously made observations.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "text": "[39]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 361,
                    "end": 365,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 519,
                    "end": 523,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 831,
                    "end": 835,
                    "text": "[32]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this section, we briefly outline the training data and the feature set in use.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "For acquiring the constraints for the training data, we extracted B predicates from the public ProB examples repository 1 and constructed more complex constraints inspired by ProB's enabling analysis [17] or discharging proof obligations [26] . Each backend was given a timeout of 25 s to decide whether the constraint has a solution or is a contradiction. Constraints for which a definite answer was found build up the positive class for a solver. The negative class is made up of the other outcomes: timeouts, errors, or the answer unknown. Overall, the class distribution was imbalanced, as for instance only about 35% of samples belonged to the negative class for the CLP(FD) backend. Yet, we do not deem this as a problem because the decision trees are trained with respect to a weighted training set.",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 204,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 238,
                    "end": 242,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "The Training Data"
        },
        {
            "text": "The choice of the 25 s timeout was arbitrary. However, we evaluated how much more constraints are assigned to the positive class compared to using ProB's default timeout of 2.5 s. The CLP(FD) backend is able to solve 65.47% of the constraints using a timeout of 2.5 s, while the Kodkod and Z3 backends solve 64.65% and 21.52% respectively. When increasing the timeout by factor 10 to 25 s, these percentages increase to 65.48% for CLP(FD) (+0.01%), 64.67% for Kodkod (+0.02%), and 21.53% for Z3 (+0.01%). As the percentage of solvable constraints for each backend only increased by a rather insignificant amount, we deemed the unsolvable constraints as complex enough for our analysis approach. We did not test with higher timeouts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Training Data"
        },
        {
            "text": "For each backend's analysis we had around 170,000 unique samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Training Data"
        },
        {
            "text": "For training the decision trees, we created a manually selected set of 109 features (further referred to as F109) which mainly consists of characteristics such as the amount of arithmetic operations per top level conjunct, or the ratio of intersections of all used set operators. Further features consist of maximum and mean nesting depths for certain language constructs such as negations and powersets, or the amount of unique identifiers per top level conjunct and number of interactions between them. Additionally, identifiers are grouped into unbounded, semi-bounded (only upper or lower bound), and fully bounded (both, upper and lower bound) identifiers. This grouping is sensitive to whether the boundaries are explicitly set (e.g. a < 5) or only bounded by another identifier (a < b).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Feature Set"
        },
        {
            "text": "As we are interested in the knowledge gathered by the random forests over the whole corpus of B constraints at our disposal, we will not split the dataset into sets for training and testing for our final analysis as is common for classification tasks aiming for a good predictor. However, as a sanity check that the selected features are indeed discriminatory enough to actually learn weaknesses and strengths of each backend, we still analysed the predictive performances of a random forest for each backend on a classical split into datasets for training and testing. For measuring performance, we utilised the metrics accuracy, balanced accuracy [9] , and the F 1 -score [22] .",
            "cite_spans": [
                {
                    "start": 649,
                    "end": 652,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 674,
                    "end": 678,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "The Feature Set"
        },
        {
            "text": "Each prediction of a classifier can either be a true positive (tp), true negative (tn), false positive (fp) or false negative (fn), i.e. the prediction can be either correct or false corresponding to either the positive or negative classes 1 and 0. The utilised performance metrics are defined as follows: accuracy = tp + tn tp + tn + fp + fn , balanced acc. = 1 2 tp tp + fn + tn tn + fp .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Feature Set"
        },
        {
            "text": "Accuracy describes the percentage of the test data which were classified correctly. Balanced accuracy is most suitable for an unbalanced dataset in which the distribution of classes is not equal. It averages the percentage of correctly predicted samples per class. The F 1 -score is defined as the harmonic mean over the notions precision and recall [22] :",
            "cite_spans": [
                {
                    "start": 350,
                    "end": 354,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "The Feature Set"
        },
        {
            "text": "Precision describes the probability of a positive prediction to be correct. Recall describes the probability for samples of the positive class to be classified as such. Table 1 shows the results of this sanity check. We used 80% of the data for training, whereas the performance measures were taken on the remaining 20%. Each classification task was concerned with whether a backend would return a definitive answer for a given constraint (satisfiable or unsatisfiable) or would yield unknown. As the performance scores are all higher than 0.9, we deem the feature set F109 to be suitable for our purposes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 169,
                    "end": 176,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "The Feature Set"
        },
        {
            "text": "For each backend, we trained a random forest with 50 trees using the Gini impurity decrease splitting criterion to predict whether the respective backend can find an answer or leads to unknown. As machine learning framework, we employed the Scikit-learn Python library [33] .",
            "cite_spans": [
                {
                    "start": 269,
                    "end": 273,
                    "text": "[33]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Analysis and Results"
        },
        {
            "text": "For the following analysis, we utilised the whole dataset of 109 features (F109) as the training set and did not make use of a test set, as we are more interested in an analysis of random forests containing information of all the data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis and Results"
        },
        {
            "text": "Please note that this work only considered the default settings of each backend. It is relevant to mention that multiple settings exist which could influence the respective outcomes of the analysis. For instance, although the CLP(FD) backend has problems with detecting inconsistencies over unbounded domains such as x < y \u2227 y < x, one can activate an additional CHR propagation which improves detection of inconsistencies in general as mentioned in Sect. 2.3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis and Results"
        },
        {
            "text": "In order to gain a deeper insight in the feature set we compute the Gini importance which is the mean decrease importance of a feature within a random forest using the Gini impurity as the splitting criterion. Table 2 shows the top ten features that are necessary to classify the data at hand for each backend. The common features of the three subsets are highlighted. These indicate a particularly high importance as they are used for each of the three backends' decisions.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 210,
                    "end": 217,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Feature Importances"
        },
        {
            "text": "The CLP(FD) backend and the Kodkod backend have the most features in common. The most important feature for both backends is the presence of function applications. Indeed, a function application is a complex operation for a constraint solver since it entails for example the well-definedness condition that the applied value is an element of the function's domain. Both backends' classifiers favour the presence of nested logic formulae with further possibly nested conjunctions, disjunctions, and implications, indicating more involved constraint as well. The initial assumption that the Kodkod backend is better suited to solve constraints over relations is strengthened by the high ranking of the ratio of relational compositions and overrides in the top 10 features. Of course, the overall higher similarity of the top ranked features for the CLP(FD) and Kodkod backend is influenced by the fact that constraints that cannot be translated to SAT are solved by ProB.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Importances"
        },
        {
            "text": "The gathered feature set for the classifier of the Z3 backend favours the presence of relational operations, in particular, the presence of domain operations. As initially expected, the feature representing the presence of unbounded domains has a high importance as well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Importances"
        },
        {
            "text": "While this analysis allows for selection of features for the sole purpose of classification, it does not yet give us info as of why a feature ranks high. For instance, it remains uncertain whether the presence of relational operators correlates to Z3's positive or negative class. This will be analysed in Sect. 6.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Importances"
        },
        {
            "text": "Classifying on Reduced Feature Sets. While we are mostly interested in analysing which language subsets are hard for a backend to solve, we can evaluate the significance of the most relevant features (determined via Gini relevance as done above) by conducting a regular classification over only these relevant features. When using the ranked feature sets to find a minimal set of features, we have to consider that at least one feature exists for each B data type or group of operations, e.g. relational operators, as the dataset might be biased to specific operations. For instance, the fact that the presence of arithmetic operations is not ranked high does not mean there should be no such feature at all in general. We created two features sets containing the top 10 and 50 ranked features for each backend, referred to as F10 and F50 respectively. The results presented in Table 3 show that the minimised subsets of 50 features capture the problem domain as well as the larger set containing 109 features presented in Table 1 . The smaller subset containing 10 features already shows good performance but does not perform as well as the one using 50 features, indicating that the problem at hand is complicated at least.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 878,
                    "end": 885,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1023,
                    "end": 1030,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Feature Importances"
        },
        {
            "text": "Our main goal is to determine how the backends perform on different subsets of the B language. For this we performed an association rule analysis using the inTrees framework [16] , thereby identifying frequent feature interactions as well as determining those syntax elements which increase the chance of unsolvability for each backend. For the analysis, we interpret paths from the root to the leaves of each decision tree in the forest as a single rule. Each node in these trees corresponds to a feature along with a threshold value for deciding which path to follow. An example based on the decision tree from Fig. 1 is given in Fig. 2 . Different paths might be identical up to the respective threshold values. In our analysis, we discard the threshold values and only consider the tendency (below or above threshold) for each rule. This way we can compare rules without having to worry about mismatching threshold values while still accounting for the feature's tendency. Table 4 displays several rules that were collected from the random forest trained for each backend.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 613,
                    "end": 619,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 632,
                    "end": 638,
                    "text": "Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 977,
                    "end": 984,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Association Rule Analysis"
        },
        {
            "text": "Deng [16] uses two metrics for the association rules, support and confidence.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "are the respective conditions and Y a , Y b the respective outcomes. Rule b is said to be in the support of rule a iff C a \u2286 C b . That is, each feature used in C a is also used in C b (with equal threshold tendency). Let \u03c3(a) = {r | r is in the support of a} denote the support set of a. The confidence of an association rule a is then defined as c(a) = |{{C r \u21d2 Y r } \u2208 \u03c3(a) | Y r = Y a }|/|\u03c3(a)|, i.e. the ratio of rules in the support of a with the same outcome as a.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "For a deeper analysis of the subproblems' performances for each backend, we calculated the support and confidence of the respectively 250,000 shortest rules of the corresponding random forests. Table 4 . Exemplary association rules with their corresponding support and confidence values (Supp. and Conf. respectively). The operators < and > indicate whether the feature value is above or below the learned threshold. Analysis for CLP(FD). For ProB's native backend, most rules with high support only had a confidence of 50%, rendering them insignificant for our analysis. While higher confidence rules had less support such as the one presented in Table 4 , they allowed for a look on certain subareas in the problem domain in which the backend struggles to find an answer for. Main concern for the backend appears to be function applications because they are the most relevant feature for deciding whether the CLP(FD) backend is able to satisfy or reject a constraint according to the analysis in Sect. 6.1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 194,
                    "end": 201,
                    "text": "Table 4",
                    "ref_id": null
                },
                {
                    "start": 648,
                    "end": 655,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Association Rule Analysis"
        },
        {
            "text": "The implementation of function applications in ProB consists of many special cases such as different treatment for partial or total functions. Moreover, function applications entail a well-definedness condition leading to more involved constraints and possibly weaker propagation. In particular, the constraint solver has to deduce that the values applied to a function are part of its domain which increases complexity drastically if domains are (semi-)unbounded. The multitude of such cases might emphasise the overall complexity for constraint solving and be the reason for function applications leading to negative predictions. This finding suggests the need for a more involved statical analysis of constraints with function types by means of discarding well-definedness constraints early to allow for a more aggressive propagation of function applications. Thus the solver would not need to wait for verification of whether an element actually resides in a function's domain or not.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "Further findings show that the use of implications, equivalences, nested powersets as well as operations on powersets contribute to the probability for the backend to answer unknown for a given constraint, as do operations concerning multiple variables representing functions and unbounded domains.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "Comparing this to our initial presumptions made in Sect. 2.1, the particular difficulty associated with function application was mostly unexpected. Furthermore, while we did not anticipate implications or equivalences to have such significance, their role for unsolvability might be caused by a lot of backtracking inside the constraint solver for satisfying these constraints. The analysis did not bring up further results mismatching our assumptions from Sect. 2.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "Analysis for Kodkod. The Kodkod backend struggles with arithmetic and powersets, which was to be expected. As already observed with the native backend, we also found an increase in logical operators to increase the constraint complexity significantly. An increase in logical operators naturally increases the nesting depth of the top-level conjuncts, leading to much more involved constraints. The use of functions only appears to be a problem for Kodkod if these are not manipulated by relational operators, rendering Kodkod as a more suitable choice over CLP(FD) in these cases. We generally found our expectations from Sect. 2.2 met regarding Kodkod's handling of relations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "Most positive rules favouring relational operators only showed a small support but had high confidence values and mostly differed in a single feature describing a different relational operator. If one was to generalise these rules into a singular one which is independent of the particular relational operator, these rules should be able to support each other while maintaining their high confidence. This suggests the use of relational operators for the Kodkod backend.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "Note again that the Kodkod backend has a fallback to the CLP(FD) backend for non-translated structures, hence both backends perform similar overall.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "Analysis for Z3. Contrary to the two backends presented above, the Z3 backend's association rule analysis delivered many high-support/high-confidence rules for the positive class. Table 4 shows one such rule with high support and confidence. Since the analysis did not provide rules with high support and confidence for the negative class, we compared absence of syntax elements in the positive rules to their existence in low-support negative rules for analysis of areas where Z3 does not perform well.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 180,
                    "end": 187,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Association Rule Analysis"
        },
        {
            "text": "The results suggest that Z3 handles unbounded domains well and favours integer variables and inequality constraints. This is in line with our expectations from Sect. 2.3. However, we observed good performance for relational operators as well which goes against initial presumptions, although this is correlated to the amount of domain restrictions in use. Otherwise, Z3 lacks performance with quantifiers, set comprehensions, powersets, or set operations (as was expected).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "The main issues for the Z3 backend are the non-translated operators as well as highly involved translations as outlined in Sect. 2.3. Revisiting these translations and comparing their implementations to those of well-performing syntax elements might allow to increase the backend's performance on further language subsets significantly. For instance, the translation of relational operators might inspire the translation of certain set operators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Association Rule Analysis"
        },
        {
            "text": "In this article, we identified subproblems of the B language for which the individual ProB constraint solving backends performed better or worse respectively. While our findings generally matched our expections stated in Sects. 2.1, 2.2 and 2.3, we found certain results which we did not explicitly expect. For instance, our evidence suggests a difficulty for dealing with function applications as well as implications and equivalences. Involved constraints containing many nested conjunctions and disjunctions also increased the chance for the backends to return unknown. Surprisingly, the Z3 backend performed much better on relational operators as expected. As a consequence, our analysis identified the need for a more sophisticated handling of function application and nested logic operators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "As by-product of this work, we were also able to train well-performing classifiers for each backend, which can be used for automated backend selection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The experimental data as well as corresponding Jupyter notebooks are available on GitHub: https://github.com/jdnklau/prob-backend-analysis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Acknowledgements. Computational support and infrastructure was provided by the \"Centre for Information and Media Technology\" (ZIM) at the University of D\u00fcsseldorf (Germany).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "The B-Book: Assigning Programs to Meanings",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Abrial",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Empirical characterization of random forest variable importance measures",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Archer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "V"
                    ],
                    "last": "Kimes",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Comput. Stat. Data Anal",
            "volume": "52",
            "issn": "4",
            "pages": "2249--2260",
            "other_ids": {
                "DOI": [
                    "10.1016/j.csda.2007.08.015"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "CAV 2011",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Barrett",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "6806",
            "issn": "",
            "pages": "171--177",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-22110-1_14"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Why3: Shepherd your herd of provers",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Bobot",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Filli\u00e2tre",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "March\u00e9",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Paskevich",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Boogie 2011: First International Workshop on Intermediate Verification Languages",
            "volume": "",
            "issn": "",
            "pages": "53--64",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Classification and Regression Trees",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Olshen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Stone",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Bagging predictors",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Mach. Learn",
            "volume": "24",
            "issn": "2",
            "pages": "123--140",
            "other_ids": {
                "DOI": [
                    "10.1007/BF00058655"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Random forests",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Mach. Learn",
            "volume": "45",
            "issn": "1",
            "pages": "5--32",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Machine learning and automated theorem proving",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Bridge",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "The balanced accuracy and its posterior distribution",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "H"
                    ],
                    "last": "Brodersen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "S"
                    ],
                    "last": "Ong",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "E"
                    ],
                    "last": "Stephan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Buhmann",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "2010 International Conference on Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "3121--3124",
            "other_ids": {
                "DOI": [
                    "10.1109/ICPR.2010.764"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "An open-ended finite domain constraint solver",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Carlsson",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Ottosson",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Carlson",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "PLILP 1997",
            "volume": "1292",
            "issn": "",
            "pages": "191--206",
            "other_ids": {
                "DOI": [
                    "10.1007/BFb0033845"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "SICStus Prolog User's Manual",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Carlsson",
                    "suffix": ""
                }
            ],
            "year": 1988,
            "venue": "",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Optimal action extraction for random forests and boosted trees",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cui",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Conference on Knowledge Discovery and Data Mining KDD 2015",
            "volume": "",
            "issn": "",
            "pages": "179--188",
            "other_ids": {
                "DOI": [
                    "10.1145/2783258.2783281"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Z3: An efficient SMT solver",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "De Moura",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Bj\u00f8rner",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "TACAS 2008",
            "volume": "4963",
            "issn": "",
            "pages": "337--340",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-540-78800-3_24"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "SMT solvers for Rodin",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "D\u00e9harbe",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fontaine",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guyot",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Voisin",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ABZ 2012",
            "volume": "7316",
            "issn": "",
            "pages": "194--207",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-30885-7_14"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Interpreting tree ensembles with intrees",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. J. Data Sci. Anal",
            "volume": "7",
            "issn": "4",
            "pages": "277--287",
            "other_ids": {
                "DOI": [
                    "10.1007/s41060-018-0144-8"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Enabling analysis for Event-B",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Dobrikov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Leuschel",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Sci. Comput. Program",
            "volume": "158",
            "issn": "",
            "pages": "81--99",
            "other_ids": {
                "DOI": [
                    "10.1016/j.scico.2017.08.004"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Machine learning and AI techniques for automated tool selection for formal methods",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dunkelau",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the PhD Symposium at iFM'18 on Formal Methods: Algorithms, Tools and Applications",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.18154/RWTH-CONV-236485"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Automated backend selection for ProB using deep learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dunkelau",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Krings",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidt",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NFM 2019",
            "volume": "11460",
            "issn": "",
            "pages": "130--147",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-20652-9_9"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "The use of multiple measurements in taxonomic problems",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Fisher",
                    "suffix": ""
                }
            ],
            "year": 1936,
            "venue": "Ann. Eugen",
            "volume": "7",
            "issn": "2",
            "pages": "179--188",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "DPLL(T ): fast decision procedures",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ganzinger",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hagen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Nieuwenhuis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oliveras",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Tinelli",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "CAV 2004",
            "volume": "3114",
            "issn": "",
            "pages": "175--188",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-540-27813-9_14"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A probabilistic interpretation of precision, recall and F -score, with implication for evaluation",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Goutte",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gaussier",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "ECIR 2005",
            "volume": "3408",
            "issn": "",
            "pages": "345--359",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-540-31865-1_25"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Making tree ensembles interpretable",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hara",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hayashi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ICML Workshop on Human Interpretability in Machine Learning (WHI 2016",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Evaluating the use of a general-purpose benchmark suite for domain-specific SMT-solving",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Healy",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Monahan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Power",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Symposium on Applied Computing SAC 2016",
            "volume": "",
            "issn": "",
            "pages": "1558--1561",
            "other_ids": {
                "DOI": [
                    "10.1145/2851613.2851975"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Alloy: a lightweight object modelling notation",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jackson",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Trans. Softw. Eng. Methodol",
            "volume": "11",
            "issn": "2",
            "pages": "256--290",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "From failure to proof: the ProB disprover for B and Event-B",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Krings",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bendisposto",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Leuschel",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "SEFM 2015",
            "volume": "9276",
            "issn": "",
            "pages": "199--214",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-22969-0_15"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "SMT solvers for validation of B and Event-B models",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Krings",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Leuschel",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IFM 2016",
            "volume": "9681",
            "issn": "",
            "pages": "361--375",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "The Sat4J library, release 2.2",
            "authors": [
                {
                    "first": "Le",
                    "middle": [],
                    "last": "Berre",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Parrain",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J. Satisf. Boolean Model. Comput",
            "volume": "7",
            "issn": "",
            "pages": "59--64",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "From animation to data validation: the ProB constraint solver 10 years on",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Leuschel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bendisposto",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Dobrikov",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Krings",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Plagge",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Formal Methods Applied to Complex Systems: Implementation of the B Method",
            "volume": "",
            "issn": "",
            "pages": "427--446",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "ProB: a model checker for B",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Leuschel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Butler",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "FME 2003",
            "volume": "2805",
            "issn": "",
            "pages": "855--874",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-540-45236-2_46"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Classification and regression tree methods",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Loh",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Wiley StatsRef: Statistics Reference Online",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1002/9781118445112.stat03886"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "SSD failures in datacenters: what? when? and why? In: Systems and Storage Conference",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Narayanan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Scikit-learn: machine learning in Python",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pedregosa",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Mach. Learn. Res",
            "volume": "12",
            "issn": "",
            "pages": "2825--2830",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "The decision does not fall far from the tree: automatic configuration of predicate solving",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Petrasch",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Validating B,Z and TLA + Using ProB and Kodkod",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Plagge",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Leuschel",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "FM 2012",
            "volume": "7436",
            "issn": "",
            "pages": "372--386",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Bias in random forest variable importance measures: Illustrations, sources and a solution",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Schulz",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Strobl",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Boulesteix",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zeileis",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hothorn",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "BMC Bioinf",
            "volume": "15",
            "issn": "1",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1186/1471-2105-8-25"
                ]
            }
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Kodkod: a relational model finder",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Torlak",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jackson",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "TACAS 2007",
            "volume": "4424",
            "issn": "",
            "pages": "632--647",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-540-71209-1_49"
                ]
            }
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Postprocessing decision trees to extract actionable knowledge",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "X"
                    ],
                    "last": "Ling",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "International Conference on Data Mining",
            "volume": "",
            "issn": "",
            "pages": "685--688",
            "other_ids": {
                "DOI": [
                    "10.1109/ICDM.2003.1251008"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "petal length (<) \u21d2 setosa petal length (>) \u2227 petal width (<) \u21d2 versicolor petal length (>) \u2227 petal width (>) \u21d2 virginicaFig. 2. Association rules extracted from the decision tree inFig. 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "FD) Function applications (<) \u2227 conjunctions (<) \u2227 quantifiers (>) \u2227 logic operators (>) \u2227 functions (>) applications (<) \u2227 conjunctions (<) \u2227 disjunctions (>) \u2227 implications (<) \u2227 powersets (<) \u2227 inequality (>) \u2227 quantifiers (<) \u2227 lambda-expression ratio (<) \u2227 relational inversions (<) \u2227 sequences (<) \u2227 =\u21d2 negative 2413 0.79 Z3 Relational operations (<) \u2227 functions (<) \u2227 unbounded variables (>) \u2227 set inclusions (member, subset) (<) \u2227 sequences (<) \u2227 set operations (<) =\u21d2 positive 24,479 0.69",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Fig. 1. Decision tree classifier over a set of iris flowers",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Random Forest classification performances over the set of 109 features.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Top ten features for each backend ranked by the Gini importance.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Random forest classification performances for minimised feature sets.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}