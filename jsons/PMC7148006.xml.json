{
    "paper_id": "PMC7148006",
    "metadata": {
        "title": "SentiInc: Incorporating Sentiment Information into Sentiment Transfer Without Parallel Data",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Kartikey",
                "middle": [],
                "last": "Pant",
                "suffix": "",
                "email": "kartikey.pant@research.iiit.ac.in",
                "affiliation": {}
            },
            {
                "first": "Yash",
                "middle": [],
                "last": "Verma",
                "suffix": "",
                "email": "yash.verma@students.iiit.ac.in",
                "affiliation": {}
            },
            {
                "first": "Radhika",
                "middle": [],
                "last": "Mamidi",
                "suffix": "",
                "email": "radhika.mamidi@iiit.ac.in",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Esoteric methods in sequence to sequence tasks use massive amounts of parallel data. However, in many style transfer tasks such as sentiment-to-sentiment transfer, such data is not readily available. Therefore, most of the recent work in language style transfer focuses on solving this task in an unsupervised setting [2, 6]. Unsupervised learning involves learning a latent representation of data in a shared latent space, which provides fine control over the latent attributes in the data. Autoencoders have been used for generating sentences with controllable attributes by the disentangled latent representations [1, 4]. Most of the work done previously on the task uses adversarial training for learning this latent representation [3, 4, 12, 18].",
            "cite_spans": [
                {
                    "start": 319,
                    "end": 320,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 322,
                    "end": 323,
                    "mention": "6",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 618,
                    "end": 619,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 621,
                    "end": 622,
                    "mention": "4",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 737,
                    "end": 738,
                    "mention": "3",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 740,
                    "end": 741,
                    "mention": "4",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 743,
                    "end": 745,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 747,
                    "end": 749,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The task of sentiment-to-sentiment transfer is equivalent to finding target sentence y that maximizes the conditional probability of y given a source sentence x and sentiment s, i.e., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\max p(y | x, s)$$\\end{document}. Sentiment-to-sentiment transfer can be seen as a special style transfer task. It involves changing the underlying sentiment of the source text while preserving the underlying non-semantic content.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this work, we propose an architecture that extends the model proposed by [6], which performs machine translation relying on monolingual corpora in each language. We form language models for each sentiment and use iterative back-translation [10], thereby converting this unsupervised task into a semi-supervised task.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 78,
                    "mention": "6",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 244,
                    "end": 246,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To summarize, the following are the contributions1:We propose a novel approach for sentiment-to-sentiment transfer incorporating sentiment-specific information in an unsupervised setting.The proposed method encodes sentiment-specific information in the target sentence by incorporating sentiment-based loss in the iterative back-translation algorithm.The proposed method also gives finer control over the trade-off between content-preservation and sentiment-transfer, thus making it adaptable for various applications.We extensively evaluate the performance of our model on a real-world dataset, and results reveal that it outperforms the state-of-the-art methods.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The most closely related works are on the areas of neural unpaired sentiment-to-sentiment translation and encoding sentiment-specific information in training neural networks.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Generating sentences with controllable attributes has been addressed in [4] by learning disentangled latent representations [1]. Their model builds on variational auto-encoders (VAEs) and uses independency constraints to enforce reliable inference of attributes back from generated sentences. [12] focuses on separating the underlying content from style information, [19] employs an iterative back-translation algorithm by using a pseudo-parallel corpus created using a word-to-word transfer table built by cross-domain word embeddings and style specific language models. Use of back translation to facilitate unsupervised machine translation is addressed in [6]. This model was extended by [13] to address the limitations of attribute transfer by performing attribute conditioning and latent representation pooling.",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 74,
                    "mention": "4",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 125,
                    "end": 126,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 294,
                    "end": 296,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 368,
                    "end": 370,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 660,
                    "end": 661,
                    "mention": "6",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 692,
                    "end": 694,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "On the other hand, sentiment-specific information is encoded with lexicon-based approaches [14, 16, 17], mostly sentiment-polarity pairs, incorporating negation and intensification to compute sentence polarity for each sentence. [15] proposed learning sentiment-specific word embeddings (SSWE) which encoded the sentiment information into a continuous representation of words by incorporating a sentiment-specific loss function in the training process.",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 94,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 96,
                    "end": 98,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 100,
                    "end": 102,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 230,
                    "end": 232,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "To the best of our knowledge, there is no previous work that facilitates sentiment-to-sentiment transfer while encoding sentiment-specific information, in an unsupervised setting.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Instead of considering words, we use BPE tokens [11] as they reduce the vocabulary size and eliminate the presence of unknown words in the output. The data of both the sentiment domains (positive and negative) is jointly processed to form the BPE tokens which are shared across sentiment domains. Token embeddings [8] are now learned to initialize the lookup tables for encoder and decoder. We accomplish language modeling via denoising autoencoders [6], by minimizing the following loss:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{lm} = E_{x \\sim S} [-log P_{s \\rightarrow s}(x|C(x))] + E_{y \\sim T} [-log P_{t \\rightarrow t}(y|C(y))] \\end{aligned}$$\\end{document}where, C is a noise model with some words dropped and swapped. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ P_{s \\rightarrow s}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P_{t \\rightarrow t}$$\\end{document} are the composition of encoder and decoder both operating on the source and target sides, respectively.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 51,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 315,
                    "end": 316,
                    "mention": "8",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 451,
                    "end": 452,
                    "mention": "6",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Style Transfer as Unsupervised Machine Translation ::: Sentiment Transfer Using Sentiment-Specific Loss",
            "ref_spans": []
        },
        {
            "text": "For converting this unsupervised task to a semi-supervised setting, we use iterative back-translation. We consider \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\forall x \\in S $$\\end{document}, let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v^{\\star }(x) = \\mathop {\\text {arg max}}\\limits P_{s \\rightarrow t} (v|x)$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\forall y \\in T $$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u^{\\star }(y) = \\mathop {\\text {arg max}}\\limits P_{t \\rightarrow s} (x, v^{\\star }(x))$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(u^{\\star }(y), y)$$\\end{document} which forms automatically-generated parallel sentences. Using this, we train two transfer models by minimizing the following loss:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{back} = E_{y \\sim T} [-log P_{s \\rightarrow t}(y|u^{\\star }(y))] + E_{x \\sim S} [-log P_{t \\rightarrow s}(x|v^{\\star }(x))] \\end{aligned}$$\\end{document}\n",
            "cite_spans": [],
            "section": "Style Transfer as Unsupervised Machine Translation ::: Sentiment Transfer Using Sentiment-Specific Loss",
            "ref_spans": []
        },
        {
            "text": "For incorporating the sentiment-specific loss into the training, we use pretrained fastText classifiers [5], which provide polarities from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-2$$\\end{document} to 2, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-2$$\\end{document} being the most negative. Let the score predicted by fastText classifier be denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta (t)$$\\end{document}. Now, let f(t) be the indicator of the target sentiment, given a score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$+1$$\\end{document} if the target sentiment is negative and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-1$$\\end{document} if it is positive. The sentiment-specific loss is modeled as:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{s(t)}= exp(n^2 \\cdot k) \\cdot max(0, f(t) \\cdot \\delta (t)) \\end{aligned}$$\\end{document}Here, n denotes the number of epochs and k is a hyperparameter and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$exp(n^2 \\cdot k)$$\\end{document} is used to make the sentiment-specific information more dominant as the model learns to generate content-preserved sentences with an increase in n.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 106,
                    "mention": "5",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Sentiment-Specific Loss ::: Sentiment Transfer Using Sentiment-Specific Loss",
            "ref_spans": []
        },
        {
            "text": "SentiInc uses latent representation for both language modeling and style transfer as it ensures inductive transfer across both tasks. To share the encoder representation, we share all the encoder as well as decoder parameters across the two sentiment domains.",
            "cite_spans": [],
            "section": "Shared Latent Representation ::: Sentiment Transfer Using Sentiment-Specific Loss",
            "ref_spans": []
        },
        {
            "text": "While minimizing the loss function, backpropagation is not performed through the reverse model which generated the data since as observed by [6], no significant improvement were observed by doing so. The final loss function to be minimized is as follows:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L = L_{lm} + L_{back} + L_{s(t)} \\end{aligned}$$\\end{document}\n",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 143,
                    "mention": "6",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Shared Latent Representation ::: Sentiment Transfer Using Sentiment-Specific Loss",
            "ref_spans": []
        },
        {
            "text": "We conduct experiments on the publicly available Yelp food review dataset as previously used by [7, 12, 13]. Unlike most work in the area, we operate at the scale of complete reviews and do not assume that every sentence of the review inherits the sentiment of the complete review. We, therefore, relax constraints enforced in prior works [7, 12] that discard reviews with more than 15 words and only consider the 10k most frequent words. Instead, we consider full reviews with up to 100 words and use byte-pair encodings (BPE) [11] eliminating the presence of unknown words.",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 98,
                    "mention": "7",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 100,
                    "end": 102,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 104,
                    "end": 106,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 340,
                    "end": 341,
                    "mention": "7",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 343,
                    "end": 345,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 529,
                    "end": 531,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Dataset ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Small Yelp Review Dataset (SYelp): It is used by many of the previous works conducted in this area [3, 7, 12], and contains sentences instead of complete reviews. It is based on the assumption that each sentence in a review inherits the sentiment of the complete review. Finally, the data is encoded in 10k BPE Codes.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 101,
                    "mention": "3",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 103,
                    "end": 104,
                    "mention": "7",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 106,
                    "end": 108,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Dataset ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Big Yelp Review Dataset (BYelp): It contains full reviews instead of individual sentences. Following previous work on reviews spanning multiple reviews [13], we preprocess this data to remove reviews that are not written in English according to a fastText classifier [5] which achieves competitive performance. Based on the rating associated with the review, we classify it as either positive or negative. Finally, the data is encoded in 60k BPE Codes.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 155,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 268,
                    "end": 269,
                    "mention": "5",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Dataset ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In this work, we use a combination of multiple automatic evaluation criteria informed by our desiderata. We want our model to generate sentences that have the target sentiment while preserving the structure and content of the input.",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Therefore, we evaluate samples from different models along the following two different criteria:Transfer of Sentiment (Accuracy): We measure the extent to which the sentiment is converted using the pretrained fastText classifier for the polarity of the reviews. The fastText classifier [5] achieves a high accuracy of 95.7% in determining the polarity of the review.Content preservation (BLEU): We measure the extent to which a model preserves the content present in a given input using n-gram statistics, by measuring the BLEU score [9] between generated text and the input itself.\n",
            "cite_spans": [
                {
                    "start": 287,
                    "end": 288,
                    "mention": "5",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 535,
                    "end": 536,
                    "mention": "9",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Evaluation Metrics ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "It has been observed by [7, 13] that as the BLEU score increases, the sentiment transfer accuracy decreases. As we want our model to produce sentences that have the target sentiment while preserving the content, we use the Geometric mean (G-score) of Accuracy and BLEU as an evaluation metric to evaluate the overall performance [18].",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 26,
                    "mention": "7",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 28,
                    "end": 30,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 330,
                    "end": 332,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Evaluation Metrics ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We compare our proposed model with the following baselines: Style Embedding [3]: In this method, the model learns a representation for the input sentence that only contains the content information after which it learns style embeddings in addition to the content representations.Multi-Decoder [3]: This method uses a multi-decoder model with adversarial learning which uses different decoders, one for each style, to learn generation of sentences in each corresponding style.Cross-Alignment Auto-Encoder (CAE) [12]: This model uses refined alignment of latent representations in hidden layers.Retrieval, DeleteOnly, and DeleteAndRetrieve [7]: DeleteOnly works on extracting content words by deleting phrases associated with the sentence\u2019s original style. Retrieval works on retrieving new phrases associated with the target style. DeleteAndRetrieve is a neural model that smoothly combines the DeleteOnly and Retrieval method into a final output.\n",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 78,
                    "mention": "3",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 294,
                    "end": 295,
                    "mention": "3",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 511,
                    "end": 513,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 639,
                    "end": 640,
                    "mention": "7",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Table 1 compares the results of the baselines with our model on SYelp dataset where accuracy evaluates transfer of sentiment, BLEU evaluates semantic content preservation, and G-score is the geometric mean of accuracy and BLEU. Our models outperform the current state-of-the-art by a G-score of 11%.",
            "cite_spans": [],
            "section": "Results and Analysis",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "In SYelp dataset, StyleEmbedding achieves a high BLEU score of 67.63; however, it is unable to transfer the sentiment significantly, showing a low sentiment transfer accuracy of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$14.5\\%$$\\end{document}. MultiDecoder achieves a BLEU score of 40.22 showing sentiment transfer accuracy of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$50.40\\%$$\\end{document}. CAE achieves a BLEU score of 20.28 and shows a sentiment transfer accuracy of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$73.7\\%$$\\end{document}, obtaining a G-score of 38.66. Retrieval achieves a much lower BLEU score of 2.62; however, it shows a high sentiment transfer accuracy of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$83.8\\%$$\\end{document}. DeleteOnly and DeleteAndRetrieve show competitive performances among the baselines, having BLEU scores of 37.45 and 35.55 respectively and sentiment transfer accuracy of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$82.6\\%$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$84\\%$$\\end{document} respectively.\n",
            "cite_spans": [],
            "section": "Results and Analysis",
            "ref_spans": []
        },
        {
            "text": "We also compare SentiInc with and without sentiment loss. With sentiment loss, it shows the maximum G-score of 66.25, and obtains sentiment transfer accuracy of 73.7% and BLEU score of 59.56. This denotes that SentiInc produces better sentiment-accurate and content-preserved sentences than all the baselines. Without sentiment loss, we obtain a G-score of 65.29, observing a drop of 0.96 on the SYelp dataset.",
            "cite_spans": [],
            "section": "Results and Analysis",
            "ref_spans": []
        },
        {
            "text": "The ablation study conducted on the BYelp dataset shows a 1.76 increase in G-score upon incorporation of sentiment loss. We also observe a decrease in the trade-off between BLEU and sentiment transfer accuracy with respect to the baseline without the sentiment loss. This shows a direct effect of the sentiment loss in reducing the limitations by the BLEU-accuracy trade-off, which makes the target sentences content-preserved while maintaining sentiment accuracy.",
            "cite_spans": [],
            "section": "Results and Analysis",
            "ref_spans": []
        },
        {
            "text": "In this paper, we focus on unpaired sentiment-to-sentiment translation and proposed our model SentiInc based on back-translation and sentiment analysis. SentiInc incorporates sentiment-based loss that enables training through only mono-sentiment data. Our experiments on review datasets (with varied maximum sentence length) show that our method substantially outperforms the state-of-the-art models in overall performance. We further show that by incorporating sentiment-loss into the back-translation based model, it is possible to decrease the limitations of the trade-off between content preservation and sentiment transfer accuracy. In the future, we would like to experiment with converting offensive text and hate-speech into polite forms and increasing the degrees of polarity in the sentiment transfer.",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Results for the SYelp dataset.\n",
            "type": "table"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Lexicon-based methods for sentiment analysis",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Taboada",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Brooke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tofiloski",
                    "suffix": ""
                },
                {
                    "first": "KD",
                    "middle": [],
                    "last": "Voll",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Stede",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Comput. Linguist.",
            "volume": "37",
            "issn": "2",
            "pages": "267-307",
            "other_ids": {
                "DOI": [
                    "10.1162/COLI_a_00049"
                ]
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Sentiment strength detection for the social web",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Thelwall",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Buckley",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Paltoglou",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "J. Am. Soc. Inf. Sci. Technol.",
            "volume": "63",
            "issn": "1",
            "pages": "163-173",
            "other_ids": {
                "DOI": [
                    "10.1002/asi.21662"
                ]
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}