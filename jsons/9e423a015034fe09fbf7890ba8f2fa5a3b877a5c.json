{
    "paper_id": "9e423a015034fe09fbf7890ba8f2fa5a3b877a5c",
    "metadata": {
        "title": "",
        "authors": [
            {
                "first": "Firoj",
                "middle": [],
                "last": "Alam",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Shaden",
                "middle": [],
                "last": "Shaar",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Alex",
                "middle": [],
                "last": "Nikolov",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sofia University",
                    "location": {
                        "settlement": "Sofia",
                        "country": "Bulgaria"
                    }
                },
                "email": ""
            },
            {
                "first": "Hamdy",
                "middle": [],
                "last": "Mubarak",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Giovanni",
                "middle": [],
                "last": "Da",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "San",
                "middle": [],
                "last": "Martino",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Ahmed",
                "middle": [],
                "last": "Abdelali",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Fahim",
                "middle": [],
                "last": "Dalvi",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Nadir",
                "middle": [],
                "last": "Durrani",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Hassan",
                "middle": [],
                "last": "Sajjad",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Kareem",
                "middle": [],
                "last": "Darwish",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Preslav",
                "middle": [],
                "last": "Nakov",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Disinformation, i.e., information that is both false and means harm, thrives in social media. Most often, it is used for political purposes, e.g., to influence elections or simply to cause distrust in society. It can also target medical issues, most notably the use of vaccines. With the emergence of the COVID-19 pandemic, the political and the medical aspects merged as disinformation got elevated to a whole new level to become the first global infodemic. Fighting this infodemic is now ranked second on the list of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. The fight requires solving a number of problems such as identifying tweets containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. These are challenging problems, and some of them have been studied previously, but typically in isolation. Here, we design, annotate, and release to the research community a new dataset for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society as a whole, and (iii) covers both English and Arabic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Social media have become one of the major communication channels for information dissemination and consumption. For many people, social media have become more important news source than traditional news media (Perrin, 2015) . In the time of the present COVID-19 pandemic, social media serve as an effective means to disseminate information to a large number of people, and they are closely monitored by government organizations.",
            "cite_spans": [
                {
                    "start": 209,
                    "end": 223,
                    "text": "(Perrin, 2015)",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Unfortunately, the democratic nature of social media, where anybody can easily become a news producer, has raised questions about the quality and the factuality of the information shared there. Social media have become the main platforms to spread disinformation, to influence people's opinions, and to mislead the society with false claims (Kumar and Shah, 2018; Alzanin and Azmi, 2018) . Figures 1 and 2 show some examples of tweets that demonstrate how online users discuss topics related to COVID-19 in social media. We can see that the problem is much broader than simply looking at factuality, and that the tweets we show could be of interest to journalists, fact-checkers, social media platforms, policymakers, and the society in general. The examples include tweets spreading rumors (see Figures 1b and 1c) , promoting conspiracy theories (see Figure 1d ), making jokes (see Figure 1a ), starting panic (see Figure 2a ), promoting fake cures (see Figure 1e , or spreading xenophoby, racism and prejudices (see Figure 1f) . Other examples tweets contain information that could be potentially useful and might deserve the attention and some action/reaction by government entities and policymakers. For example, the tweet in Figure 2b blames the authorities for their (in)action regarding COVID-19 testing. The tweets in the Figures 2c and 2d are also useful for authorities as well as for the general public as they discuss actions some government of some country has taken to fight the pandemic, and suggests actions that probably should be taken.",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 363,
                    "text": "(Kumar and Shah, 2018;",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 364,
                    "end": 387,
                    "text": "Alzanin and Azmi, 2018)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 390,
                    "end": 405,
                    "text": "Figures 1 and 2",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 796,
                    "end": 814,
                    "text": "Figures 1b and 1c)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 852,
                    "end": 861,
                    "text": "Figure 1d",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 883,
                    "end": 892,
                    "text": "Figure 1a",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 916,
                    "end": 925,
                    "text": "Figure 2a",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 955,
                    "end": 964,
                    "text": "Figure 1e",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1018,
                    "end": 1028,
                    "text": "Figure 1f)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1230,
                    "end": 1239,
                    "text": "Figure 2b",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "For the tweets in the Figures 1 and 2 , it is necessary to understand whether the information is correct, whether it is harmful to anyone, whether it needs some organization to react to it or to act based on this information, etc. Rapid answers to these questions are crucial to help organizations direct their efforts and to counter the spread of disinformation that may cause panic, mistrust, and other problems in the society. There has been a lot of manual effort by a number of fact-checking organizations 1 , but their efforts do not scale. Some manual effort can be saved by automatic systems for check-worthiness estimation, i.e., identifying which claims are worth factchecking by the professional fact-checkers; notable works include ClaimBuster (Hassan et al., 2017) , Storyzy 2 , LazyTruth 3 and ClaimRank (Karadzhov et al., 2017a) . Moreover, all the above effort has focused exclusively on factuality, but there is a need to look at a broader range of problems as the above examples have shown.",
            "cite_spans": [
                {
                    "start": 756,
                    "end": 777,
                    "text": "(Hassan et al., 2017)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 818,
                    "end": 843,
                    "text": "(Karadzhov et al., 2017a)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [
                {
                    "start": 22,
                    "end": 37,
                    "text": "Figures 1 and 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Thus, here we address the problem as a multifaceted one, which needs to be addressed taking several actors (e.g., individual, society, or government entities) into consideration. Hence, here we aim at focusing on three goals: (i) if there is a claim in a tweet, then is it worth fact-checking by professionals, (ii) is the tweet harmful to the society, and (iii) is there something a government-entity should take notice of. We define seven fine-grained sub-tasks to achieve these goals, which are instantiated as seven questions. For the former goal, we defined five questions, and for the latter two goals, we specified two questions. Each question can also serve as an independent task. Our methodological steps consist of (i) defining comprehensive annotation guidelines, (ii) collecting tweets targeting the ongoing pandemic all over the world and sampling tweets for the annotation, and (iii) manually annotating tweets and making them publicly available.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our comprehensive guidelines can serve as an annotation standard to encourage community effort in this direction. The diversity of the annotations enables interesting modeling solutions while at the same time helping the society by providing correct information, helping journalists, fact-checking organizations, and social media platforms, as well as policy makers and government entities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "From the modeling point of view, each question serves as an independent task, while they can also be considered in relation to each other. For example, the fifth question can be analyzed in relation to the first four questions. Similarly, all tasks can be combined in a multitask setting for building one model that serves all the above-described purposes. Another interesting research frontier to be explored is how to integrate additional information, such 1 http://tiny.cc/zd1fnz 2 http://storyzy.com/ 3 http://www.lazytruth.com/ as images, videos, emoticons, or links to external websites that users post as part of their tweets to support their claims, in the modeling process. Note that the annotations were carried out looking at this supplementary information, even the tweets posted as a reply to the tweet.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To tackle the disinformation in social media compared to the current literature, our study differs in the following respects:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 we target COVID-19 rather than political messages; \u2022 we develop comprehensive guidelines to annotate social media data that combine the perspectives and the interests of journalists, factcheckers, social media platforms, policy makers, and society as a whole; \u2022 we covers two languages: English and Arabic; \u2022 we focus on social media rather than on debates; \u2022 we take the entire tweet context into account rather than just its text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest of the paper is organized as follows: Section 2 offers a brief overview of previous work. Section 3 presents the annotation guidelines. Section 4 describes our annotated dataset. Section 5 reports details and findings about our dataset. Finally, Section 6 concludes and points to possible directions for future work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Journalists, online users, and researchers are wellaware of the proliferation of false information, and thus topics such as credibility and fact-checking have become important research topics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The interested reader can learn more about \"fake news\" from the overview by Shu et al. (2017) , which adopted a data mining perspective and focused on social media. Another survey, by , took a fact-checking perspective on \"fake news\" and related problems. Yet another survey was performed by Li et al. (2016) , covering truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 93,
                    "text": "Shu et al. (2017)",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 292,
                    "end": 308,
                    "text": "Li et al. (2016)",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "(2018) offered a general overview and discussion on the science of \"fake news\", while Vosoughi et al. (2018) focused on the process of proliferation of true and false news online. In particular, they analyzed 126K stories tweeted by 3M people more than 4.5M times, and confirmed that \"fake news\" spread much wider than true news.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 108,
                    "text": "Vosoughi et al. (2018)",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Veracity of information has been studied at different levels: (i) claim-level (e.g., fact-checking), (ii) article-level (e.g., \"fake news\" detection), (iii) user-level (e.g., hunting for trolls), and (iv) medium-level (e.g., source reliability estimation). Our primary interest here is claim-level.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015 Ma et al., , 2016 Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018) . The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016 Popat et al., , 2017 Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018) .",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 205,
                    "text": "(Canini et al., 2011;",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 206,
                    "end": 228,
                    "text": "Castillo et al., 2011;",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 229,
                    "end": 244,
                    "text": "Ma et al., 2015",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 245,
                    "end": 262,
                    "text": "Ma et al., , 2016",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 263,
                    "end": 284,
                    "text": "Zubiaga et al., 2016;",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 285,
                    "end": 301,
                    "text": "Ma et al., 2017;",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 302,
                    "end": 321,
                    "text": "Dungs et al., 2018;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 322,
                    "end": 344,
                    "text": "Kochkina et al., 2018)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 401,
                    "end": 429,
                    "text": "(Mukherjee and Weikum, 2015;",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 430,
                    "end": 448,
                    "text": "Popat et al., 2016",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 449,
                    "end": 469,
                    "text": "Popat et al., , 2017",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 470,
                    "end": 494,
                    "text": "Karadzhov et al., 2017b;",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 495,
                    "end": 518,
                    "text": "Mihaylova et al., 2018;",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 519,
                    "end": 537,
                    "text": "Baly et al., 2018)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Fact-Checking"
        },
        {
            "text": "Relevant shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification , and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019) .",
            "cite_spans": [
                {
                    "start": 180,
                    "end": 204,
                    "text": "(Mihaylova et al., 2019)",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Fact-Checking"
        },
        {
            "text": "One of the earlier efforts in check-worthiness estimation is the ClaimBuster system (Hassan et al., 2015) , which has been developed using the transcripts of 30 historical US election debates with a total of 28,029 transcribed sentences. The annotation includes non-factual, unimportant factual, and check-worthy factual class labels and has been carried out by students, professors, and journalists. The study by Gencheva et al. (2017) also focused on the debates of the 2016 US Presidential Campaign for which they obtained annotations from different fact-checking organizations. An extension of this work resulted in the development of ClaimRank, where the authors used more data and also included Arabic content Jaradat et al. (2018) .",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 105,
                    "text": "(Hassan et al., 2015)",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 414,
                    "end": 436,
                    "text": "Gencheva et al. (2017)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 716,
                    "end": 737,
                    "text": "Jaradat et al. (2018)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Check-worthiness"
        },
        {
            "text": "Some notable research outcomes came from shared tasks. For example, the CLEF CheckThat! labs' shared tasks Elsayed et al., 2019b,a) in the past few years featured challenges on automatic identification and verification Hasanain et al., 2019) of claims in political debates.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 131,
                    "text": "Elsayed et al., 2019b,a)",
                    "ref_id": null
                },
                {
                    "start": 219,
                    "end": 241,
                    "text": "Hasanain et al., 2019)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Check-worthiness"
        },
        {
            "text": "In this study we mainly focus on social media content, i.e., tweets. We specifically focused on disinformation posted on Twitter related to the COVID-19 pandemic. A recent effort related to the pandemic uses social media, i.e., Weibo, to study different situational information types such as \"caution and advice\", \"donations of money, goods, or services\", \"help seeking\", \"counter-rumor\", etc (Li et al., 2020) . In another study, the authors report media bias and rumor amplification patterns for COVID-19 (Cinelli et al., 2020) using five different social media platforms. They identified posts containing URLs to other websites and categorized them into a political bias (i.e., right, right-center, least-biased, left-center and left), and also categorized them as questionable or reliable news outlets. For example, out of 2,637 news outlets, 800 are classified as questionable and 1,837 of them are labeled as reliable. Related to the COVID-19 pandemic, the study by Medford et al. (2020) analyzes tweets to understand different content types such as emotional, racially prejudiced, xenophobic or content that causes fear. The study stresses the fact that it is necessary to identify the tweet that instills fear and identify the fearful users to reassure and educate them.",
            "cite_spans": [
                {
                    "start": 393,
                    "end": 410,
                    "text": "(Li et al., 2020)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 507,
                    "end": 529,
                    "text": "(Cinelli et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 972,
                    "end": 993,
                    "text": "Medford et al. (2020)",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "COVID-19 Research"
        },
        {
            "text": "Unlike the above work, here we combine the perspectives and the interests of journalists, factcheckers, social media platforms, policy makers, and society as a whole; we also cover two languages: English and Arabic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "COVID-19 Research"
        },
        {
            "text": "Task description: Given a tweet, the task is to determine whether it contains a factual claim, as well as its veracity, harmfulness (to the society, to a person, to an organization, or to a product), whether it requires verification, and how interesting it is for a government entity to pay attention to. These aspects are covered by seven questions. These questions are grouped into three main objectives: (i) is it worth fact-checking by professionals? (Q1-5), (ii) is it harmful to the society (Q6), and (iii) does it contain information that should get the attention of policy makers? (Q7). Although the questions within a problem set are correlated, the annotation instructions are designed, so that the dataset can be used independently for different similar tasks. In the following, we provide detailed annotation instructions with respect to each question.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotation Instructions"
        },
        {
            "text": "Questions 2-4 are designed as both categorical and numerical (i.e., using a Likert scale) in order to enable their use both in classification and in regression tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotation Instructions"
        },
        {
            "text": "General Instructions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotation Instructions"
        },
        {
            "text": "1. For each tweet, the annotator needs to read the text including the hashtags, and also to look for the tweet itself when necessary by going to the link (i.e., for Q2-Q7 it might be required to open the tweet link). 4 3. The annotators may look at the images and the videos, to the Web pages that the tweet links to, as well as to the tweets in the same thread when making a judgment, if required. 4. The annotators are not required to complete questions Q2-Q5 if the answer to question Q1 is NO.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotation Instructions"
        },
        {
            "text": "Question 1: Does the tweet contain a verifiable factual claim? A verifiable factual claim is a sentence claiming that something is true, and this can be verified using factual, verifiable information such as statistics, specific examples, or personal testimony.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "Factual claims include the following: 5",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "\u2022 Stating a definition;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "\u2022 Mentioning quantity in the present or the past;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "\u2022 Making a verifiable prediction about the future; \u2022 Reference to laws, procedures, and rules of operation; \u2022 References to images or videos (e.g., \"This is a video showing a hospital in Spain.\"); \u2022 Statements about correlations or causations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "Such a correlation or causation needs to 4 The reason for not going to the tweet link for Q1 is that we wanted to reduce the complexity of the annotation task and to focus on the content of the tweet only. As for Q2, it might be important to check whether the tweet was posted by an authoritative source, and thus it might be useful for the annotator to open the tweet to get more context; after all, this is how real users perceive the tweet. Since the annotators would open the tweet's link for Q2, they can use that information for the rest of the questions as well (even though this is not required). 2. The annotators should assume the time when the tweet was posted as a reference when making judgments, e.g. \"Trump thinks, that for the vast majority of Americans, the risk is very, very low.\" would be true when he made the statement but false by the time annotations were carried out for this tweet. The annotator should consider the time when the tweet was posted. 5 Inspired by (Konstantinovskiy et al., 2018) .",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 42,
                    "text": "4",
                    "ref_id": null
                },
                {
                    "start": 988,
                    "end": 1019,
                    "text": "(Konstantinovskiy et al., 2018)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "be explicit, i.e., sentences like \"This is why the beaches haven't closed in Florida. https://t.co/8x2tcQeg21\" is not a claim because it does not explicitly say why, thus it is not verifiable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "Tweets containing personal opinions and preferences are not factual claims. Note that if a tweet is composed of multiple sentences or clauses, at least one full sentence or clause needs to be a claim in order for the tweet to contain a factual claim. If a claim exist in a sub-sentence or sub-clause, then the tweet is not considered to have a factual claim. For example, \"My new favorite thing is Italian mayors and regional presidents LOSING IT at people violating quarantine\" is not a claim, however, it is an opinion. Moreover, if we consider \"Italian mayors and regional presidents LOSING IT at people violating quarantine\" it would be a claim. In addition, when answering this question, annotators should not open the tweet URL. Since this is a binary decision task, the answer of this question consists of two labels as defined below. Labels:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "\u2022 YES: if it contains a verifiable factual claim;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "\u2022 NO: if it does not contain a verifiable factual claim;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "\u2022 Don't know or can't judge: the content of the tweet does not have enough information to make a judgment. It is recommended to categorize the tweet using this label when the content of the tweet is not understandable at all. For example, it uses a language (i.e., non-English) or references that it is difficult to understand;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "Examples:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "1. Please don't take hydroxychloroquine (Plaquenil) plus Azithromycin for #COVID19 UNLESS your doctor prescribes it. Both drugs affect the QT interval of your heart and can lead to arrhythmias and sudden death, especially if you are taking other meds or have a heart condition. Label: YES Explanation: There is a claim in the text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "2. Saw this on Facebook today and its a must read for all those idiots clearing the shelves #coronavirus #toiletpapercrisis #auspol Label: NO Explanation: There is no claim in the text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verifiable Factual Claim"
        },
        {
            "text": "Question 2: To what extent does the tweet appear to contain false information?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "False Information"
        },
        {
            "text": "The stated claim may contain false information. This question labels the tweets with the categories mentioned below. False Information appears on social media platforms, blogs, and news-articles to deliberately misinform or deceive the readers (Kumar and Shah, 2018). Labels: The labels for this question are defined with a five point Likert scale (Albaum, 1997) . A higher value means that it is more likely to be false:",
            "cite_spans": [
                {
                    "start": 348,
                    "end": 362,
                    "text": "(Albaum, 1997)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "False Information"
        },
        {
            "text": "1. NO, definitely contains no false information 2. NO, probably contains no false information 3. Not sure",
            "cite_spans": [],
            "ref_spans": [],
            "section": "False Information"
        },
        {
            "text": "To answer this question it is recommended to open the link of the tweet and to look for additional information for the veracity of the claim identified in question 1. For example, if the tweet contains a link to an article from a reputable information source (e.g., Reuters, Associated Press, France Press, Aljazeera English, BBC) then the answer could be \". . . contains no false info\". Examples:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "YES, probably contains false information 5. YES, definitely contains false information"
        },
        {
            "text": "1. \"Dominican Republic found the cure for Covid-19 https://t.co/1CfA162Lq3\" Label: 5. YES, definitely contains false information Explanation: This is not correct information at the time of this tweet is posted.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "YES, probably contains false information 5. YES, definitely contains false information"
        },
        {
            "text": "2. This is Dr. Usama Riaz. He spent past weeks screening and treating patients with Corona Virus in Pakistan. He knew there was no PPE. He persisted anyways. Today he lost his own battle with coronavirus but he gave life and hope to so many more. KNOW HIS NAME https://t.co/flSwhLCPmx Label: 2. NO, probably contains no false info Explanation: The content of the tweet states correct information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "YES, probably contains false information 5. YES, definitely contains false information"
        },
        {
            "text": "Question 3: Will the tweet's claim have an effect on or be of interest to the general public?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Interest to General Public"
        },
        {
            "text": "Most often people do not make interesting claims, which can be verified by our general knowledge. For example, \"The sky is blue\" is a claim, however, it is not interesting to the general public. In general, topics such as healthcare, political news, and current events are of higher interest to the general public. Using the five point Likert scale the labels are defined below. Labels: The information is not interesting for the general public as it is an opinion and providing statement of someone else.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Interest to General Public"
        },
        {
            "text": "Question 4: To what extent does the tweet appear to be harmful to society, person(s), company(s) or product(s)?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmfulness"
        },
        {
            "text": "The purpose of this question is to determine if the content of the tweet aims to and can negatively affect society as a whole, specific person(s), company(s), product(s), or spread rumors about them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmfulness"
        },
        {
            "text": "The content intends to harm or weaponize the information 6 (Broniatowski et al., 2018) . A rumor involves a form of a statement whose veracity is not quickly or ever confirmed 7 .",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 86,
                    "text": "6 (Broniatowski et al., 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Harmfulness"
        },
        {
            "text": "Labels: To categorize the tweets we defined the following labels based on the Likert scale. A higher value means a higher degree of harm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmfulness"
        },
        {
            "text": "1. NO, definitely not harmful 2. NO, probably not harmful 3. Not sure 4. YES, probably harmful 5. YES, definitely harmful Examples:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmfulness"
        },
        {
            "text": "1. How convenient but not the least bit surprising from Democrats! As usual they put politics over American citizens. @Speaker-Pelosi withheld #coronavirus bill so DCCC could run ads AGAINST GOP candidates! #tcot Label: 5. YES, definitely harmful Explanation: This tweet is weaponized to target Nancy Pelosi and the Democrats in general.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmfulness"
        },
        {
            "text": "2. As we saw over the wkend, disinfo is being spread online about a supposed national lockdown and grounding flights. Be skeptical of rumors. Make sure youre getting info from legitimate sources. The @WhiteHouse is holding daily briefings and @cdcgov is providing the latest. Label: 1. NO, definitely not harmful Explanation: This tweet is informative and gives advice. It does not attack anyone and is not harmful.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmfulness"
        },
        {
            "text": "Question 5: Do you think that a professional fact-checker should verify the claim in the tweet? It is important to verify a factual claim by a professional fact-checker which can cause harm to society, specific person(s), company(s), product(s), or some government entities. However, not all factual claims are important or worthwhile to be fact-checked by a professional fact-checker as it is a time-consuming procedure. Therefore, the 6 The use of information as a weapon to spread misinformation and mislead people. 7 https://en.wikipedia.org/wiki/Rumor purpose is to categorize the tweet using the labels defined below. While doing so, the annotator can rely on the answers to the previous questions. For this question, we defined the following labels to categorize the tweets. Labels:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Need of Verification"
        },
        {
            "text": "1. NO, no need to check: the tweet does not need to be fact-checked, e.g., because it is not interesting, a joke, or does not contain any claim. 2. NO, too trivial to check: the tweet is worth fact-checking, however, this does not require a professional fact-checker, i.e., a non-expert might be able to fact-check the claim. For example, one can verify the information using reliable sources such as the official website of the WHO, etc. An example of a claim is as follows: \"The GDP of the USA grew by 50% last year.\" 3. YES, not urgent: the tweet should be fact-checked by a professional fact-checker, however, it is not urgent or critical; 4. YES, very urgent: the tweet can cause immediate harm to a large number of people, therefore, it should be verified as soon as possible by a professional fact-checker; 5. Not sure: the content of the tweet does not have enough information to make a judgment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Need of Verification"
        },
        {
            "text": "1. Things the GOP has done during the Covid-19 outbreak: -Illegally traded stocks -Called it a hoax -Blamed it on China -Tried to bailout big business without conditions What they havent done: -Help workers -Help small businesses -Produced enough tests or ventilators Label: 2. YES, very urgent Explanation: Clearly, the content of the tweet blames authority, hence, it is important to verify this claim immediately by a professional fact-checker. In addition, the attention of government entities might be required in order to take necessary actions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples:"
        },
        {
            "text": "The corona virus can be spread through internationaly printed albums. If you have any albums at home, put on some gloves, put all the albums in a box and put it outside the front door tonight. I'm collecting all the boxes tonight for safety. Think of your health.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "ALERT"
        },
        {
            "text": "Label: 5. NO, no need to check Explanation: This is a joke and does not need to be checked by a professional fact checker.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "ALERT"
        },
        {
            "text": "Question 6: Is the tweet harmful for society and why?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmful to Society"
        },
        {
            "text": "The purpose of this question is to categorize if the content of the tweet is intended to harm or weaponized to mislead the society. To identify that we defined the following labels for the categorization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Harmful to Society"
        },
        {
            "text": "A. NO, not harmful: the content of the tweet would not harm the society (e.g., \"I like corona beer\").",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "B. NO, joke or sarcasm: the tweet contains a joke (e.g., \"If Corona enters Spain, itll enter from the side of Barcelona defense\") or sarcasm (e.g., \"'The corona virus is a real thing.' -Wow, I had no idea!\").",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "C. Not sure: if the content of the tweet is not understandable enough to judge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "D. YES, panic: the tweet spreads panic. The content of the tweet can cause sudden fear and anxiety for a large part of the society (e.g., \"there are 50,000 cases ov COVID-19 in Qatar\").",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "E. YES, xenophobic, racist, prejudices, or hate-speech: the tweet reports xenophobia, racism or prejudiced expression(s). According to the dictionary 8 Xenophobic refers to fear or hatred of foreigners, people from different cultures, or strangers. Racism is the belief that groups of humans possess different behavioral traits corresponding to physical appearance and can be divided based on the superiority of one race over another. 9 It may also refer to prejudice, discrimination, or antagonism directed against other people because they are of a different race or ethnicity. Prejudice is an unjustified or incorrect attitude (i.e., typically negative) towards an individual based solely on the individual's membership of a social group. 10 An example of a xenophobic statement is \"do not buy cucumbers from Iran\". F. YES, bad cure: the tweet reports a questionable cure, medicine, vaccine or prevention procedures (e.g., \". . . drinking bleach can help cure coronavirus\"). G. YES, rumor, or conspiracy: the tweet reports or spreads a rumor. It is defined as a \"specific (or topical) proposition for belief passed along from person to person usually by word of mouth without secure standards of evidence being present\" (Allport and Postman, 1947). For example, \"BREAKING: Trump could still own stock in a company that, according to the CDC, will play a major role in providing coronavirus test kits to the federal government, which means that Trump could profit from coronavirus testing. #COVID-19 #coronavirus https://t.co/Kwl3ylMZRk\" H. YES, other: if the content of the tweet does not belong to any of the above categories, then this category can be chosen to label the tweet.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "Question 7: Do you think that this tweet should get the attention of any government entity? Most often people tweet by blaming authorities, providing advice, and/or calls for action. Sometimes that information might be useful for any government entity to make a plan, respond or react on it. The purpose of this question is to categorize such information. It is important to note that not all information requires attention from a government entity. Therefore, even if the tweet's content belongs to any of the positive categories, it is important to understand whether that requires government attention. For the annotation, it is mandatory to first decide on whether attention is necessary from government entities (i.e., YES/NO). If the answer is YES, it is obligatory to select a category from the YES sub-categories mentioned below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Require attention"
        },
        {
            "text": "A. NO, not interesting: if the content of the tweet is not important or interesting for any government entity to pay attention to. B. Not sure: if the content of the tweet is not understandable enough to judge. C. YES, categorized as in question 6: if some government entities need to pay attention to this tweet as it is harmful for society, i.e., it is labeled as any of the YES sub-categories in question 6. D. YES, other: if the tweet cannot be labeled as any of the above categories, then this label should be selected. E. YES, blame authorities: the tweet contains information that blames some government entities or top politician(s), e.g., \"Dear @VP Pence: Is the below true? Do you have a plan? Also, when are local jurisdictions going to get the #Coronavirus test kits you promised?\". F. YES, contains advice: the tweet contains advice about social, political, national, or international issues that requires attention from some government entities (e.g., ) . We ran our queries in two time epochs, namely: March 9-10, 2020 and March 20-25, 2020. We filtered out all non-Arabic or non-English tweets. For each time epoch, we generated two lists of tweets and merged them. The lists were as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 966,
                    "end": 967,
                    "text": ")",
                    "ref_id": null
                }
            ],
            "section": "Labels:"
        },
        {
            "text": "1. The top 500 most retweeted tweets. The rationale behind this is that they might contain valuable information. A sample of those tweets are used for the annotations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "2. The top 500 most retweeted tweets containing words may indicate rumors. These words are: hoax, conspiracy, rumor, fake, cabal, reportedly, allegedly, (no truth), (deny), The goals here is to increase the chances of seeing potential rumors or harmful tweets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "Based on the annotation instructions above, in the first phase, we annotated 504 tweets in English and 218 in Arabic using seven annotators, where a subset of tweets were categorized by two or three annotators. 12 As the social media data is noisy and the annotation tasks are highly subjective, disagreement is a typical scenario. The disputed labels were resolved in a consensus meeting. Such an approach has also been used in a similar study (Zubiaga et al., 2015) . In the cases where disagreements were not resolved among the annotators' group working on the tweets, another consensus meeting was carried out among all the annotators to work out the labels and improve the annotation guidelines.",
            "cite_spans": [
                {
                    "start": 445,
                    "end": 467,
                    "text": "(Zubiaga et al., 2015)",
                    "ref_id": "BIBREF44"
                }
            ],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "In Table 1 , we present a sample tweet, annotated for all questions. Tweet 1 negates the claim \"Young people aren't at risk\" through personal testimony of experience of being a COVID-19 patient. So Question 1 is marked as Yes. The tweet probably contains no false information as a verified user is providing information about himself. The tweet is of interest to the general population as it clears the misconception about \"young people not at risk\". The tweet is not harmful to society but it blames the authorities.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Labels:"
        },
        {
            "text": "Tweet 2 is a joke and does not contain a factual claim. Tweet 3 contains a claim with a causal argument. Because the content of the tweet attacks government officials, it requires to be fact-checked immediately by a professional fact-checker. Figure 4 shows the geographical distribution of the annotated tweets for English and Arabic. We consider the country of the tweet author or the original author in case of retweeting. It's observed that most English tweets came from US, India and UK (\u223c60%), while most Arabic tweets came from 12 Note that our annotation task is currently ongoing and we expect to annotate more tweets in the coming future.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 243,
                    "end": 251,
                    "text": "Figure 4",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Labels:"
        },
        {
            "text": "We will make them available on https://github.com/firojalam/ COVID-19-tweets-for-check-worthiness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "KSA and Qatar (\u223c70%). For both languages, there are tweets from a large number of countries which indicates a good diversity of interests, topics, styles, etc. that strengthens our study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Labels:"
        },
        {
            "text": "In Figures 5a and 5b , we report the distribution of class labels of the annotated English tweets. We found that the class distribution for Q1 is quite balanced, (YES:59% and NO:41% -See Figure 5a ). 59% of the tweets labeled as factual claims were also annotated for Q2-Q5. For the question Q2, the label \"NO, probably contains no false info\" shows a higher distribution comparatively, which entails that in the majority of cases the identified claims are probably true. Out of 295 tweets labeled for Q2, in about 74% of the cases it contains no false information, whereas 14% were categorized as \"not sure\" and 13% as \"contains false information\". While computing the statistics, we combined \"probably\" and \"definitely\" into one set for both positive and negative answers, respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 20,
                    "text": "Figures 5a and 5b",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 187,
                    "end": 196,
                    "text": "Figure 5a",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Data Statistics for English Tweets"
        },
        {
            "text": "For Q3, the distribution of people's general interest is higher compared to the identified claims, which is 80%. For Q4, on average the claims of the tweets vary from not harmful to harmful. For Q5, the majority of the cases are either \"YES, not urgent\" (38%) or \"No, no need to check\" (26%). It appears that only in a small fraction of cases a professional fact-checker should verify the claims mentioned in the tweets immediately (17%). For Questions 3-5, on average the \"Not sure\" cases are very few. However, they are substantially larger in the case of Q2. The false information identification (Q2) is a challenging task, as it requires further probing into external information. When annotating Q2, we only relied on the content of the tweets (i.e., user identifier, threads, videos, and images), which makes it difficult to judge credibility and resulted in more \"Not sure\" cases comparatively. However, we encouraged the annotators to examine the whole tweet at its original URL. For example, in the following tweet Epidemiologist Marc Lipsitch, director of Harvard's Center for Communicable Disease Dynamics: \"In the US it is the opposite of contained.' https://t.co/IPAPagz4Vs\" it was difficult to determine whether it contains false information without looking at the tweet in its entirety. See Figure 3 as an example.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1306,
                    "end": 1314,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Data Statistics for English Tweets"
        },
        {
            "text": "For Q6, most of the tweets are classified as \"not harmful\" for society and as \"joke or sarcasm\". ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Statistics for English Tweets"
        },
        {
            "text": "Expl: This has a factual claim, in which user posted his personal testimony, mentioning his experience as a COVID-19 patient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q1 Yes"
        },
        {
            "text": "No: probably contains no false info Expl: As the twitter user himself is providing his testimony, therefore, it might be correct information. In addition, the user is a verified user, which makes us to believe that it has a less chance of misinformation. Q3 Yes: probably of interest Expl: General population might get interest in this how it is like to be a COVID-19 patient. Q4 NO: probably not harmful Expl: As it would not harm to anyone, therefore it is not harmful. Q5 YES: not urgent Expl: It is a factual claim and worthwhile to fact-check, however, it is less important for the fact-checker. Q6 NO: not harmful Expl: It is not harmful for the society as it does not express anything that can affect society.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q2"
        },
        {
            "text": "Expl: Upon reading the whole threads it seems that user explicitly blames authorities by mentioning \". . . The government has failed us. Im lucky, others wont be. Its far past the time to take action. Not words, ACTION.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q7 YES: blame authorities"
        },
        {
            "text": "Step the fuck up, and protect the people of this country. If they wont, we need to. Stay inside, be smart. No death is worth you being ignorant. We can do this.\". From the critical classes, 8% of the tweets are classified as containing \"xenophobic, racist, prejudices or hate speech\" and 4% for \"spreading panic\". For Q7, it is clear that in the majority of cases (63%) the tweets are not of interest to government entities, however, 16% of the cases blame authorities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q7 YES: blame authorities"
        },
        {
            "text": "The distribution of Arabic tweets is reported in Figure 6 . We can mostly observe a similar distribution to the one found in the English tweets. For example, the percentage of factual claims is higher (64%) than no factual claims (36%) and the number of tweets containing no false information is higher than the tweets containing false information (Q2).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 49,
                    "end": 57,
                    "text": "Figure 6",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Data Statistics for Arabic Tweets"
        },
        {
            "text": "For Q3, a higher general public interest is observed for the tweets containing factual claims (91%). The content in the tweets is not harmful (65%) which can be seen from Q4. From Q5, we notice that 10% of the cases urgently need a professional factchecker to verify the claim(s) in the tweets. The findings from Q6 suggest that the tweets contain mostly non-harmful content (74%), whereas in 26% of the cases the content either spreads panic, a rumor, or a conspiracy. Only 6% of the cases blame authorities, as can be seen from Q7. Figure 7 : Distribution of datasets for all the questions associated with user accounts. NA refers to tweets that have not been labeled for those questions, they are identical to the tweets categorized with the label NO in Q1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 534,
                    "end": 542,
                    "text": "Figure 7",
                    "ref_id": null
                }
            ],
            "section": "Data Statistics for Arabic Tweets"
        },
        {
            "text": "In this subsection, we study the correlation between tweet labels and whether or not the original author of a tweet has a verified account. Verified accounts include government entities, public figures, celebrities, etc. which have a large number of followers, so their tweets typically have a high impact on society. Figure 7 shows that verified accounts tend to post more tweets that contain factual claims than unverified accounts (Q1), and their tweets are more likely to not contain false information (Q2), be of higher interest to the general public (Q3), be less harmful to society (Q6, Arabic), and attract greater attention from a government entity than tweets from unverified accounts (Q7, English). These are general observations from the current small number of annotated tweets, and there are some differences between the English and Arabic annotations. Quantitative study can be held later using a larger dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 318,
                    "end": 326,
                    "text": "Figure 7",
                    "ref_id": null
                }
            ],
            "section": "Verified and Unverified Accounts"
        },
        {
            "text": "This correlation can be one the features that clas-sifiers use to predict labels for unseen tweets, also can help in speeding up the annotation process by providing initial default values before manual revision. In addition, in some cases, verified accounts can be used to check annotation quality, for example tweets from @WHO should not be labeled as harmful to the society or weaponized.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verified and Unverified Accounts"
        },
        {
            "text": "In Figure 8 , we report contingency and correlation tables in a form of heatmap for different questions pairs obtained from English tweets dataset. For questions Q2-3, it appears that there is a high association 13 between \". . . no false info\" to general public interest as shown in Figure 8a . For questions Q2 and Q4 (Figure 8b) , the high association can be observed between \". . . no false info\" and \". . . not harmful\" (65%) compared to \"harmful\" (34%) for either individual, products or government entities. By analyzing questions Q2 and Q5 (Figure 8c ), we observe that \". . . no false info\" is associated with either \"no need to check\" or \"too trivial to check\", highlighting the fact that professional factchecker does not need to spend time on them. From questions Q3 and Q4 (Figure 8d ), it appears that general public interest is higher when contents of the tweets are \"not harmful\" (61%) than \"harmful\" (39%). From question Q3 and Q5 (Figure 8e) , we see an interesting phenomenon, when tweets with high general public interest have a higher association with professional fact-checker to verify them (61%) compared to either \"too trivial to check\" or \"no need to check\" (39%). The questions Q4 and Q5 (Figure 8f) shows that \"harmful\" tweets requires more attention (53%) for the professional fact-checker than \"not harmful\" tweets (45%). Our finding for Q6 and Q7 (Figure 8g) suggests that the majority of the tweets are not harmful for the society, which also requires less attention for government entities. The second majority tweets in Q7 blames authorities though they are mostly not harmful for the society.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 8",
                    "ref_id": null
                },
                {
                    "start": 284,
                    "end": 293,
                    "text": "Figure 8a",
                    "ref_id": null
                },
                {
                    "start": 320,
                    "end": 331,
                    "text": "(Figure 8b)",
                    "ref_id": null
                },
                {
                    "start": 548,
                    "end": 558,
                    "text": "(Figure 8c",
                    "ref_id": null
                },
                {
                    "start": 786,
                    "end": 796,
                    "text": "(Figure 8d",
                    "ref_id": null
                },
                {
                    "start": 948,
                    "end": 959,
                    "text": "(Figure 8e)",
                    "ref_id": null
                },
                {
                    "start": 1215,
                    "end": 1226,
                    "text": "(Figure 8f)",
                    "ref_id": null
                },
                {
                    "start": 1378,
                    "end": 1389,
                    "text": "(Figure 8g)",
                    "ref_id": null
                }
            ],
            "section": "English Tweets Dataset"
        },
        {
            "text": "In Figure 8h , we report the correlation between questions Q2-Q4 for English tweets to understand their association. We computed such correlation using the Likert scale values (i.e., 1-5) that we defined for these questions. We observed that overall Q2 and Q3 are negatively correlated, which infers that if the claim contains no false information, it is of high interest to the general public. This can be also observed in Figure 8a . The question Q2 and Q4 shows a positive correlation, which might be due to their high association with \". . . no false info\" and \". . . not harmful\".",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Figure 8h",
                    "ref_id": null
                },
                {
                    "start": 424,
                    "end": 433,
                    "text": "Figure 8a",
                    "ref_id": null
                }
            ],
            "section": "English Tweets Dataset"
        },
        {
            "text": "In Figure 9 , we report heatmaps to report the association across questions using Arabic tweets. From Q2 and Q3 (Figure 9a ), we observe that the association between \". . . contains no false info\" and general public interest is higher (67%) than \". . . contains false info\" (29%). From questions Q2 and Q4 (Figure 9b ), we observe that \". . . contains no false info\" is associated with \". . . not harmful\" and \". . . contains false info\" is associated with \". . . harmful\", which can also seen with its high correlation of 0.74 in Figure 9h . From the relation between Q2 and Q5 (Figure 9c) , it can be seen that majority cases \". . . contains no false info\" is associated with either \"no need to check\" or \"too trivial to check\", means that professional fact-checker does not need to verify them. The analysis be-tween questions Q3 and Q4 suggests that general public interest is higher when the contents of the tweets are not harmful (68%) than harmful (30%) (Figure 9d ). From questions Q3 and Q5, we observe that general public interest is higher when the claim(s) in the tweets are either \"no need to check\" or \"too trivial to check\" (Figure 9e ). The analysis between question Q4 and Q5 shows that \"not harmful\" tweets are either \"no need to check\" or \"too trivial to check\" by the professional factchecker (Figure 9f ). From the questions Q6 and Q7, we observe that in majority cases tweets are not harmful for the society and hence they are not interesting for the government entities (Figure 9g ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 9",
                    "ref_id": null
                },
                {
                    "start": 112,
                    "end": 122,
                    "text": "(Figure 9a",
                    "ref_id": null
                },
                {
                    "start": 306,
                    "end": 316,
                    "text": "(Figure 9b",
                    "ref_id": null
                },
                {
                    "start": 531,
                    "end": 540,
                    "text": "Figure 9h",
                    "ref_id": null
                },
                {
                    "start": 579,
                    "end": 590,
                    "text": "(Figure 9c)",
                    "ref_id": null
                },
                {
                    "start": 961,
                    "end": 971,
                    "text": "(Figure 9d",
                    "ref_id": null
                },
                {
                    "start": 1139,
                    "end": 1149,
                    "text": "(Figure 9e",
                    "ref_id": null
                },
                {
                    "start": 1313,
                    "end": 1323,
                    "text": "(Figure 9f",
                    "ref_id": null
                },
                {
                    "start": 1493,
                    "end": 1503,
                    "text": "(Figure 9g",
                    "ref_id": null
                }
            ],
            "section": "Arabic Tweets Dataset"
        },
        {
            "text": "We have presented an annotation scheme and a corresponding manually annotated dataset of COVID-19 tweets, aiming to help in the fight against the first global infodemic, which emerged as a result of the COVID-19 pandemic. The dataset combines the perspectives and the interests of journalists, factcheckers, social media platforms, policy makers, and the society as a whole. It includes annotations in English and Arabic, and is made freely available to the research community. We further provided detailed analysis of the annotations, reporting the label distribution for different questions, as well as correlation with between different questions, among with other statistics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        },
        {
            "text": "We will be expanding the annotations, and we will make them available on the URL (see footnote 10). We plan to recruit professional annotators to be able to expand the size of the dataset significantly. We would also allow people to contribute to these annotations using crowd-sourcing (again, check the URL at footnote 10 for detail). Once we accumulate enough annotations, we will build systems for predicting the different kinds of labels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "The likert scale revisited. Market Research Society",
            "authors": [
                {
                    "first": "Gerald",
                    "middle": [],
                    "last": "Albaum",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Journal",
            "volume": "39",
            "issn": "2",
            "pages": "1--21",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "The psychology of rumor",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "Leo",
                    "middle": [],
                    "last": "Allport",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Postman",
                    "suffix": ""
                }
            ],
            "year": 1947,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Detecting rumors in social media: A survey",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Samah",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Alzanin",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Azmi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Procedia computer science",
            "volume": "142",
            "issn": "",
            "pages": "294--300",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims, task 1: Check-worthiness",
            "authors": [
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Tamer",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                },
                {
                    "first": "Reem",
                    "middle": [],
                    "last": "Suwaileh",
                    "suffix": ""
                },
                {
                    "first": "Wajdi",
                    "middle": [],
                    "last": "Zaghouani",
                    "suffix": ""
                },
                {
                    "first": "Spas",
                    "middle": [],
                    "last": "Kyuchukov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CLEF 2018 Working Notes. Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum, CEUR Workshop Proceedings",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims. Task 1: Check-Worthiness",
            "authors": [
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Georgi",
                    "middle": [],
                    "last": "Karadzhov",
                    "suffix": ""
                },
                {
                    "first": "Mitra",
                    "middle": [],
                    "last": "Mohtarami",
                    "suffix": ""
                },
                {
                    "first": "Giovanni Da San",
                    "middle": [],
                    "last": "Martino",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CLEF 2019 Working Notes",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Integrating stance detection and fact checking in a unified corpus",
            "authors": [
                {
                    "first": "Ramy",
                    "middle": [],
                    "last": "Baly",
                    "suffix": ""
                },
                {
                    "first": "Mitra",
                    "middle": [],
                    "last": "Mohtarami",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Glass",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Alessandro",
                    "middle": [],
                    "last": "Moschitti",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18",
            "volume": "",
            "issn": "",
            "pages": "21--27",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims, task 2: Factuality",
            "authors": [
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Tamer",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                },
                {
                    "first": "Reem",
                    "middle": [],
                    "last": "Suwaileh",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                },
                {
                    "first": "Wajdi",
                    "middle": [],
                    "last": "Zaghouani",
                    "suffix": ""
                },
                {
                    "first": "Spas",
                    "middle": [],
                    "last": "Kyuchukov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CLEF 2018 Working Notes. Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum, CEUR Workshop Proceedings",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Weaponized health communication: Twitter bots and russian trolls amplify the vaccine debate",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "Amelia",
                    "middle": [
                        "M"
                    ],
                    "last": "Broniatowski",
                    "suffix": ""
                },
                {
                    "first": "Sihua",
                    "middle": [],
                    "last": "Jamison",
                    "suffix": ""
                },
                {
                    "first": "Lulwah",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Alkulaib",
                    "suffix": ""
                },
                {
                    "first": "Adrian",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Benton",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sandra",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Quinn",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dredze",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "American journal of public health",
            "volume": "108",
            "issn": "10",
            "pages": "1378--1384",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Finding credible information sources in social networks based on content and social structure",
            "authors": [
                {
                    "first": "Kevin",
                    "middle": [
                        "R"
                    ],
                    "last": "Canini",
                    "suffix": ""
                },
                {
                    "first": "Bongwon",
                    "middle": [],
                    "last": "Suh",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [
                        "L"
                    ],
                    "last": "Pirolli",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the IEEE International Conference on Privacy, Security, Risk, and Trust, and the IEEE International Conference on Social Computing, SocialCom/PASSAT '11",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Information credibility on Twitter",
            "authors": [
                {
                    "first": "Carlos",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                },
                {
                    "first": "Marcelo",
                    "middle": [],
                    "last": "Mendoza",
                    "suffix": ""
                },
                {
                    "first": "Barbara",
                    "middle": [],
                    "last": "Poblete",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 20th International Conference on World Wide Web, WWW '11",
            "volume": "",
            "issn": "",
            "pages": "675--684",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Fabiana Zollo, and Antonio Scala. 2020. The covid-19 social media infodemic",
            "authors": [
                {
                    "first": "Matteo",
                    "middle": [],
                    "last": "Cinelli",
                    "suffix": ""
                },
                {
                    "first": "Walter",
                    "middle": [],
                    "last": "Quattrociocchi",
                    "suffix": ""
                },
                {
                    "first": "Alessandro",
                    "middle": [],
                    "last": "Galeazzi",
                    "suffix": ""
                },
                {
                    "first": "Carlo",
                    "middle": [
                        "Michele"
                    ],
                    "last": "Valensise",
                    "suffix": ""
                },
                {
                    "first": "Emanuele",
                    "middle": [],
                    "last": "Brugnoli",
                    "suffix": ""
                },
                {
                    "first": "Ana",
                    "middle": [
                        "Lucia"
                    ],
                    "last": "Schmidt",
                    "suffix": ""
                },
                {
                    "first": "Paola",
                    "middle": [],
                    "last": "Zola",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Can rumour stance alone predict veracity?",
            "authors": [
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Dungs",
                    "suffix": ""
                },
                {
                    "first": "Ahmet",
                    "middle": [],
                    "last": "Aker",
                    "suffix": ""
                },
                {
                    "first": "Norbert",
                    "middle": [],
                    "last": "Fuhr",
                    "suffix": ""
                },
                {
                    "first": "Kalina",
                    "middle": [],
                    "last": "Bontcheva",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, COLING '18",
            "volume": "",
            "issn": "",
            "pages": "3360--3370",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Automatic identification and verification of claims",
            "authors": [
                {
                    "first": "!",
                    "middle": [],
                    "last": "Checkthat",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Clef",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Information Retrieval, ECIR '19",
            "volume": "",
            "issn": "",
            "pages": "309--315",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Overview of the CLEF-2019 CheckThat!: Automatic identification and verification of claims",
            "authors": [
                {
                    "first": "Tamer",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Maram",
                    "middle": [],
                    "last": "Hasanain",
                    "suffix": ""
                },
                {
                    "first": "Reem",
                    "middle": [],
                    "last": "Suwaileh",
                    "suffix": ""
                },
                {
                    "first": "Giovanni",
                    "middle": [],
                    "last": "Da San",
                    "suffix": ""
                },
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Martino",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "A context-aware approach for detecting worthchecking claims in political debates",
            "authors": [
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Gencheva",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Ivan",
                    "middle": [],
                    "last": "Koychev",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP '17",
            "volume": "",
            "issn": "",
            "pages": "267--276",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims. Task 2: Evidence and Factuality",
            "authors": [
                {
                    "first": "Maram",
                    "middle": [],
                    "last": "Hasanain",
                    "suffix": ""
                },
                {
                    "first": "Reem",
                    "middle": [],
                    "last": "Suwaileh",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CLEF 2019",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum",
            "authors": [],
            "year": null,
            "venue": "CEUR Workshop Proceedings",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Toward automated factchecking: Detecting check-worthy factual claims by claimbuster",
            "authors": [
                {
                    "first": "Naeemul",
                    "middle": [],
                    "last": "Hassan",
                    "suffix": ""
                },
                {
                    "first": "Fatma",
                    "middle": [],
                    "last": "Arslan",
                    "suffix": ""
                },
                {
                    "first": "Chengkai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Tremayne",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "1803--1812",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Detecting check-worthy factual claims in presidential debates",
            "authors": [
                {
                    "first": "Naeemul",
                    "middle": [],
                    "last": "Hassan",
                    "suffix": ""
                },
                {
                    "first": "Chengkai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Tremayne",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM '15",
            "volume": "",
            "issn": "",
            "pages": "1835--1838",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Claim-Rank: Detecting check-worthy claims in Arabic and English",
            "authors": [
                {
                    "first": "Israa",
                    "middle": [],
                    "last": "Jaradat",
                    "suffix": ""
                },
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Gencheva",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT '18",
            "volume": "",
            "issn": "",
            "pages": "26--30",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Fully automated fact checking using external sources",
            "authors": [
                {
                    "first": "Georgi",
                    "middle": [],
                    "last": "Karadzhov",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Ivan",
                    "middle": [],
                    "last": "Koychev",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the Conference on Recent Advances in Natural Language Processing, RANLP '17",
            "volume": "",
            "issn": "",
            "pages": "344--353",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Fully automated fact checking using external sources",
            "authors": [
                {
                    "first": "Georgi",
                    "middle": [],
                    "last": "Karadzhov",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede No",
                    "suffix": ""
                },
                {
                    "first": "Ivan",
                    "middle": [],
                    "last": "Koychev",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 International Conference on Recent Advances in Natural Language Processing",
            "volume": "17",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "All-in-one: Multi-task learning for rumour verification",
            "authors": [
                {
                    "first": "Elena",
                    "middle": [],
                    "last": "Kochkina",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Liakata",
                    "suffix": ""
                },
                {
                    "first": "Arkaitz",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, COLING '18",
            "volume": "",
            "issn": "",
            "pages": "3402--3413",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Towards automated factchecking: Developing an annotation schema and benchmark for consistent automated claim detection",
            "authors": [
                {
                    "first": "Lev",
                    "middle": [],
                    "last": "Konstantinovskiy",
                    "suffix": ""
                },
                {
                    "first": "Oliver",
                    "middle": [],
                    "last": "Price",
                    "suffix": ""
                },
                {
                    "first": "Mevan",
                    "middle": [],
                    "last": "Babakar",
                    "suffix": ""
                },
                {
                    "first": "Arkaitz",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "False information on web and social media: A survey",
            "authors": [
                {
                    "first": "Srijan",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "Neil",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.08559"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "The science of fake news",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [
                        "A"
                    ],
                    "last": "Lazer",
                    "suffix": ""
                },
                {
                    "first": "Yochai",
                    "middle": [],
                    "last": "Baum",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [
                        "J"
                    ],
                    "last": "Benkler",
                    "suffix": ""
                },
                {
                    "first": "Kelly",
                    "middle": [
                        "M"
                    ],
                    "last": "Berinsky",
                    "suffix": ""
                },
                {
                    "first": "Filippo",
                    "middle": [],
                    "last": "Greenhill",
                    "suffix": ""
                },
                {
                    "first": "Miriam",
                    "middle": [
                        "J"
                    ],
                    "last": "Menczer",
                    "suffix": ""
                },
                {
                    "first": "Brendan",
                    "middle": [],
                    "last": "Metzger",
                    "suffix": ""
                },
                {
                    "first": ";",
                    "middle": [],
                    "last": "Nyhan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Steven",
                    "suffix": ""
                },
                {
                    "first": "Cass",
                    "middle": [
                        "R"
                    ],
                    "last": "Sloman",
                    "suffix": ""
                },
                {
                    "first": "Emily",
                    "middle": [
                        "A"
                    ],
                    "last": "Sunstein",
                    "suffix": ""
                },
                {
                    "first": "Duncan",
                    "middle": [
                        "J"
                    ],
                    "last": "Thorson",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [
                        "L"
                    ],
                    "last": "Watts",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Zittrain",
            "volume": "359",
            "issn": "6380",
            "pages": "1094--1096",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Characterizing the propagation of situational information in social media during covid-19 epidemic: A case study on weibo",
            "authors": [
                {
                    "first": "Lifang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Qingpeng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Xiao",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Tian-Lu",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "Kelvin",
                    "middle": [
                        "Kamfai"
                    ],
                    "last": "Tsoi",
                    "suffix": ""
                },
                {
                    "first": "Fei-Yue",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Transactions on Computational Social Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "A survey on truth discovery",
            "authors": [
                {
                    "first": "Yaliang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Jing",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Chuishi",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "Qi",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Lu",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Jiawei",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "SIGKDD Explor. Newsl",
            "volume": "17",
            "issn": "2",
            "pages": "1--16",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Detecting rumors from microblogs with recurrent neural networks",
            "authors": [
                {
                    "first": "Jing",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Prasenjit",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                },
                {
                    "first": "Sejeong",
                    "middle": [],
                    "last": "Kwon",
                    "suffix": ""
                },
                {
                    "first": "Bernard",
                    "middle": [
                        "J"
                    ],
                    "last": "Jansen",
                    "suffix": ""
                },
                {
                    "first": "Kam-Fai",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "Meeyoung",
                    "middle": [],
                    "last": "Cha",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence, IJCAI '16",
            "volume": "",
            "issn": "",
            "pages": "3818--3824",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Detect rumors using time series of social context information on microblogging websites",
            "authors": [
                {
                    "first": "Jing",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Zhongyu",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Yueming",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Kam-Fai",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM '15",
            "volume": "",
            "issn": "",
            "pages": "1751--1754",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Detect rumors in microblog posts using propagation structure via kernel learning",
            "authors": [
                {
                    "first": "Jing",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Kam-Fai",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL '17",
            "volume": "",
            "issn": "",
            "pages": "708--717",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "An \"infodemic\": Leveraging high-volume twitter data to understand public sentiment for the covid-19 outbreak",
            "authors": [
                {
                    "first": "Richard",
                    "middle": [
                        "J"
                    ],
                    "last": "Medford",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sameh",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Saleh",
                    "suffix": ""
                },
                {
                    "first": "Trish",
                    "middle": [
                        "M"
                    ],
                    "last": "Sumarsono",
                    "suffix": ""
                },
                {
                    "first": "Christoph",
                    "middle": [
                        "U"
                    ],
                    "last": "Perl",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1101/2020.04.03.20052936"
                ]
            }
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "SemEval-2019 task 8: Fact checking in community question answering forums",
            "authors": [
                {
                    "first": "Tsvetomila",
                    "middle": [],
                    "last": "Mihaylova",
                    "suffix": ""
                },
                {
                    "first": "Georgi",
                    "middle": [],
                    "last": "Karadzhov",
                    "suffix": ""
                },
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                },
                {
                    "first": "Ramy",
                    "middle": [],
                    "last": "Baly",
                    "suffix": ""
                },
                {
                    "first": "Mitra",
                    "middle": [],
                    "last": "Mohtarami",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation, SemEval '19",
            "volume": "",
            "issn": "",
            "pages": "860--869",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Fact checking in community forums",
            "authors": [
                {
                    "first": "Tsvetomila",
                    "middle": [],
                    "last": "Mihaylova",
                    "suffix": ""
                },
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Mitra",
                    "middle": [],
                    "last": "Mohtarami",
                    "suffix": ""
                },
                {
                    "first": "Georgi",
                    "middle": [],
                    "last": "Karadjov",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Glass",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI '18",
            "volume": "",
            "issn": "",
            "pages": "5309--5316",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Leveraging joint interactions for credibility analysis in news communities",
            "authors": [
                {
                    "first": "Subhabrata",
                    "middle": [],
                    "last": "Mukherjee",
                    "suffix": ""
                },
                {
                    "first": "Gerhard",
                    "middle": [],
                    "last": "Weikum",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM '15",
            "volume": "",
            "issn": "",
            "pages": "353--362",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims",
            "authors": [
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Alberto",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "Tamer",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                },
                {
                    "first": "Reem",
                    "middle": [],
                    "last": "Suwaileh",
                    "suffix": ""
                },
                {
                    "first": "Llu\u00eds",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "Wajdi",
                    "middle": [],
                    "last": "Zaghouani",
                    "suffix": ""
                },
                {
                    "first": "Pepa",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                },
                {
                    "first": "Spas",
                    "middle": [],
                    "last": "Kyuchukov",
                    "suffix": ""
                },
                {
                    "first": "Giovanni Da San",
                    "middle": [],
                    "last": "Martino",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Ninth International Conference of the CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and Interaction",
            "volume": "",
            "issn": "",
            "pages": "372--387",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Social media usage. Pew research center",
            "authors": [
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Perrin",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "52--68",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Credibility assessment of textual claims on the web",
            "authors": [
                {
                    "first": "Kashyap",
                    "middle": [],
                    "last": "Popat",
                    "suffix": ""
                },
                {
                    "first": "Subhabrata",
                    "middle": [],
                    "last": "Mukherjee",
                    "suffix": ""
                },
                {
                    "first": "Jannik",
                    "middle": [],
                    "last": "Str\u00f6tgen",
                    "suffix": ""
                },
                {
                    "first": "Gerhard",
                    "middle": [],
                    "last": "Weikum",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16",
            "volume": "",
            "issn": "",
            "pages": "2173--2178",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Where the truth lies: Explaining the credibility of emerging claims on the web and social media",
            "authors": [
                {
                    "first": "Kashyap",
                    "middle": [],
                    "last": "Popat",
                    "suffix": ""
                },
                {
                    "first": "Subhabrata",
                    "middle": [],
                    "last": "Mukherjee",
                    "suffix": ""
                },
                {
                    "first": "Jannik",
                    "middle": [],
                    "last": "Str\u00f6tgen",
                    "suffix": ""
                },
                {
                    "first": "Gerhard",
                    "middle": [],
                    "last": "Weikum",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 26th International Conference on World Wide Web Companion, WWW '17",
            "volume": "",
            "issn": "",
            "pages": "1003--1012",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Fake news detection on social media: A data mining perspective",
            "authors": [
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Shu",
                    "suffix": ""
                },
                {
                    "first": "Amy",
                    "middle": [],
                    "last": "Sliva",
                    "suffix": ""
                },
                {
                    "first": "Suhang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Jiliang",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Huan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "SIGKDD Explor. Newsl",
            "volume": "19",
            "issn": "1",
            "pages": "22--36",
            "other_ids": {
                "DOI": [
                    "10.1145/3137597.3137600"
                ]
            }
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Automated fact checking: Task formulations, methods and future directions",
            "authors": [
                {
                    "first": "James",
                    "middle": [],
                    "last": "Thorne",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [],
                    "last": "Vlachos",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, COLING '18",
            "volume": "",
            "issn": "",
            "pages": "3346--3359",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "FEVER: a large-scale dataset for fact extraction and verification",
            "authors": [
                {
                    "first": "James",
                    "middle": [],
                    "last": "Thorne",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [],
                    "last": "Vlachos",
                    "suffix": ""
                },
                {
                    "first": "Christos",
                    "middle": [],
                    "last": "Christodoulopoulos",
                    "suffix": ""
                },
                {
                    "first": "Arpit",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18",
            "volume": "",
            "issn": "",
            "pages": "809--819",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "The spread of true and false news online",
            "authors": [
                {
                    "first": "Soroush",
                    "middle": [],
                    "last": "Vosoughi",
                    "suffix": ""
                },
                {
                    "first": "Deb",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "Sinan",
                    "middle": [],
                    "last": "Aral",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Science",
            "volume": "359",
            "issn": "6380",
            "pages": "1146--1151",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Towards detecting rumours in social media",
            "authors": [
                {
                    "first": "Arkaitz",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Liakata",
                    "suffix": ""
                },
                {
                    "first": "Rob",
                    "middle": [],
                    "last": "Procter",
                    "suffix": ""
                },
                {
                    "first": "Kalina",
                    "middle": [],
                    "last": "Bontcheva",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Tolmie",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Analysing how people orient to and spread rumours in social media by looking at conversational threads",
            "authors": [
                {
                    "first": "Arkaitz",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Liakata",
                    "suffix": ""
                },
                {
                    "first": "Rob",
                    "middle": [],
                    "last": "Procter",
                    "suffix": ""
                },
                {
                    "first": "Geraldine",
                    "middle": [],
                    "last": "Wong Sak",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Hoi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Tolmie",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "PLoS ONE",
            "volume": "11",
            "issn": "3",
            "pages": "1--29",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Tweets of potential interest to journalists, fact-checkers, social media platforms, and the society.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Tweets of potential interest to policy makers, government entities, and the society as a whole.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "about), and (rumor). The keywords were determined based on a manual examination of the tweets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "An example of a tweet for which we needed to open the URL to see the whole tweet and to confirm the veracity of the claim.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Country distribution for English and Arabic tweets",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Distribution of class labels for English tweets (a) Questions (Q1-5).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Distribution of class labels for Arabic tweets",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "NO, definitely not of interest 2. NO, probably not of interest 3. Not sure 4. YES, probably of interest 5. YES, definitely of interest Examples: 1. Germany is conducting 160k Covid-19 tests a week. It has a total 35k ventilators, 10k ordered to be made by the govt. It has converted a new 1k bed hospital in Berlin. Its death rate is tiny bcos its mass testing allows quarantine and bcos it has fewer non reported cases. Label: 4. YES: probably of interest Explanation: This information is relevant and of high interest for the general population as it reports how a country deals with COVID-19. 2. Fake news peddler Dhruv Rathee had said: \"Corona virus won't spread outside China, we need not worry\" Has this guy ever spoke something sensible? https://t.co/siBAwIR8Pn Label: 2. NO, probably not of interest Explanation:",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Example of annotated tweets, their labels and explanation.Tweet 1: So, the last week I have been battling COVID-19 &amp; Pneumonia. Never in my life have I been this ill. \"Young people arent at risk, theyll only have mild symptoms\" Wrong. I want to open up about the difficulties Ive gone through these past days, what it was like in the ICU...",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "NO: probably contains no false info Expl: It may not contains false info as it came from an authentic person.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}