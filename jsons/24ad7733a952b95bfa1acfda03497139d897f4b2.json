{
    "paper_id": "24ad7733a952b95bfa1acfda03497139d897f4b2",
    "metadata": {
        "title": "Supplementary Paper for Online Algorithms for Multiclass Classification using Partial Labels 1 Proof of Theorem 1",
        "authors": []
    },
    "abstract": [
        {
            "text": "Proof. Assume that at the round t, the algorithm fails to classify (x t , Y t ) with the proper margin using the weight matrix W t , that is, 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Where we used the fact that and W 1 = 0 d\u00d7K . Let W * .W T +1 be the Frobenius inner product between W * and W T +1 . Then, using Cauchy-Schwartz inequality, we get the following.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "From Eq.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "(2) and (3) and using the assumption that ||W * || = 1, we get:",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Now, we derive upper bound on ||W T ||. We know that at t th trial, example x t is misclassified. Thus,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "(5)",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "(5), we get the following.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We know that W 1 2 = 0 and there are m mistakes. Summing the above equation over t = 1 to T , we get,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Where, c = min t |Y t |. Thus, combining the upper and lower bound from Eq.(4) and (6), we get the following.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Proof. If D = 0, it reduces to linearly separable case and thus, we assume",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Thus, the data is not linearly separable with respect to W . We now transform the linearly non-separable data to separable data. We extend each instance x t \u2208 R d to z t \u2208 R d+T as follows. The first d coordinates of z t are set to x t . The (d + t)th coordinate of z t is set to \u2206 whose value will be determined later while the rest of the coordinates of z t are set to 0. We extend weight matrix W to M \u2208 R (d+T )\u00d7K as follows. We set the first d columns of M to be 1 Z W (where Z is a constant whose value will be determined). For the rest of the columns, we set the (d + t, t) th position in M to d t Z \u2206 if r \u2208 Y t and to 0 otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We choose the value of Z such that ||M || 2 = 1 and hence,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "This gives us,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Let m r be the r th column of M , then m r .x t = 1 Z w r .x t + I r\u2208Y t d t \u2206 \u2206 . We now show that M linearly separates all the examples z t with a margin at least \u03b3 Z as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "1",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We also observe that ||z t || 2 2 = ||x t || 2 2 + \u2206 2 \u2264 R 2 + \u2206 2 . Thus, using Theorem 1, the number of mistakes made by the algorithm Avg Perceptron on the sequence (z 1 , Y 1 ), . . . , (z T , Y T ) is bounded above as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Minimizing RHS expression in Eq.(7) over \u2206, we get that the optimal value of",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Using this value of \u2206, we get the mistake bound as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Finally, to complete the proof we need to show that classifying the original partially labeled sequence with matrices W 1 , . . . , W T is the same as classifying as the extended sequence with the extended matrices M 1 , . . . , M T . That is, they both produce same sequence of predictions. This can be accomplished if we can show the following holds for all t \u2208 [T ].",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "1. The first d columns of M t are equal to W t 2. The (d+t)th column of M t is zero.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The proof of the above conditions is straightforward by induction on t (by initializing M 1 and W 1 as zero matrices).",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Proof. The theorem and the proof is almost same as Theorem 1 and its proof in the Pegasos paper [1] . The main idea in the proof is to upper bound ||\u2207 t || where \u2207 t is given by Eq. 6. Thus, using triangle inequality we can write:",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 99,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Proof of Theorem 3"
        },
        {
            "text": "We note that the L2 norm of the weight matrix W t can be written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorem 3"
        },
        {
            "text": "From the updates of Avg Perceptron, we get:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorem 3"
        },
        {
            "text": "So we get,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorem 3"
        },
        {
            "text": "So, using the above result along with Equation 8, we can write:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorem 3"
        },
        {
            "text": "Thus, if c = min t |Y t |, we get the following bound:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof of Theorem 3"
        },
        {
            "text": "The rest of the proof is exactly same as the one given in [1] .",
            "cite_spans": [
                {
                    "start": 58,
                    "end": 61,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Proof of Theorem 3"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Pegasos: Primal estimated sub-gradient solver for svm",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shalev-Shwartz",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Srebro",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proc. International Conference on Machine Learning (ICML)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {},
    "back_matter": []
}