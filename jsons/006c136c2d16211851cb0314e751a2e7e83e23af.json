{
    "paper_id": "006c136c2d16211851cb0314e751a2e7e83e23af",
    "metadata": {
        "title": "CLEF eHealth Evaluation Lab 2020",
        "authors": [
            {
                "first": "Hanna",
                "middle": [],
                "last": "Suominen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "The Australian National University",
                    "location": {
                        "postCode": "2601",
                        "settlement": "Acton",
                        "region": "ACT",
                        "country": "Australia"
                    }
                },
                "email": "hanna.suominen@anu.edu.au"
            },
            {
                "first": "Liadh",
                "middle": [],
                "last": "Kelly",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Maynooth University",
                    "location": {
                        "settlement": "Kildare",
                        "country": "Ireland"
                    }
                },
                "email": "liadh.kelly@mu.ie"
            },
            {
                "first": "Lorraine",
                "middle": [],
                "last": "Goeuriot",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Grenoble Alpes, CNRS, Grenoble INP, LIG",
                    "institution": "Univ",
                    "location": {
                        "postCode": "38000",
                        "settlement": "Grenoble",
                        "country": "France"
                    }
                },
                "email": "lorraine.goeuriot@imag.fr"
            },
            {
                "first": "Martin",
                "middle": [],
                "last": "Krallinger",
                "suffix": "",
                "affiliation": {},
                "email": "martin.krallinger@bsc.es"
            }
        ]
    },
    "abstract": [
        {
            "text": "Laypeople's increasing difficulties to retrieve and digest valid and relevant information in their preferred language to make healthcentred decisions has motivated CLEF eHealth to organize yearly labs since 2012. These 20 evaluation tasks on Information Extraction (IE), management, and Information Retrieval (IR) in 2013-2019 have been popular-as demonstrated by the large number of team registrations, submissions, papers, their included authors, and citations (748, 177, 184, 741, and 1299, respectively, up to and including 2018)-and achieved statistically significant improvements in the processing quality. In 2020, CLEF eHealth is calling for participants to contribute to the following two tasks: The 2020 Task 1 on IE focuses on term coding for clinical textual data in Spanish. The terms considered are extracted from clinical case records and they are mapped onto the Spanish version of the International Classification of Diseases, the 10th Revision, including also textual evidence spans for the clinical codes. The 2020 Task 2 is a novel extension of the most popular and established task in CLEF eHealth on CHS. This IR task uses the representative web corpus used in the 2018 challenge, but now also spoken queries, as well as textual transcripts of these queries, are offered to the participants. The task is structured into a number of optional subtasks, covering ad-hoc search using the spoken queries, textual transcripts of the spoken queries, or provided automatic speech-to-text conversions of the spoken queries.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "substantial community interest in the tasks and their resources has led to CLEF eHealth maturing as a primary venue for all interdisciplinary actors of the ecosystem for producing, processing, and consuming electronic health information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Keywords: eHealth \u00b7 Medical informatics \u00b7 Information extraction \u00b7 Information storage and retrieval \u00b7 Speech recognition",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Improving the legibility of Electronic Health Record (EHR) can contribute to patients' right to be informed about their health and health care. The requirement to ensure that patients can understand their own privacy-sensitive, official health information in their EHR are stipulated by policies and laws. For example, the Declaration on the Promotion of Patients' Rights in Europe by World Health Organization (WHO) from 1994 obligates health care workers to communicate in a way appropriate to each patient's capacity for understanding and give each patient a legible written summary of these care guidelines. This patient education must capture the patient's health status, condition, diagnosis, and prognosis, together with the proposed and alternative treatment/non-treatment with risks, benefits, and progress. Patients' better abilities to understand their own EHR empowers them to take part in the related health/care judgment, leading to their increased independence from health care providers, better health/care decisions, and decreased health care costs [11] . Improving patients' ability to digest this content could mean enriching the EHR-text with hyperlinks to term definitions, paraphrasing, care guidelines, and further supportive information on patientfriendly and reliable websites, and the enabling methods for such reading aids can also release health care workers' time from EHR-writing to, for example, longer patient-education discussions [14] .",
            "cite_spans": [
                {
                    "start": 1066,
                    "end": 1070,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Information access conferences have organized evaluation labs on related Electronic Health (eHealth) Information Extraction (IE), Information Management (IM), and Information Retrieval (IR) tasks for almost 20 years. Yet, with rare exception, they have targeted the health care experts' information needs only [1, 2, 6] . Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series 1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4, 5, [8] [9] [10] 13, 16, 17] . In 2012, the inaugural scientific CLEF workshop took place, and from 2013-2019 this annual workshop has been supplemented with a lead-up evaluation lab, consisting of, on average, three shared tasks each year (Fig. 1) . Although the tasks have been centered around the patients and their families' needs in accessing and understanding eHealth information, also Automatic Speech Recognition (ASR) and IE to aid clinicians in IM were considered in 2015-2016 and in 2017-2019, tasks on technology assisted reviews to support health scientists and health care policymakers' information access were organized. This paper presents first an overview of CLEF eHealth lab series from 2012 to 2019 and introduces its 2020 evaluation tasks. Then, it concludes by presenting our vision for CLEF eHealth beyond 2020.",
            "cite_spans": [
                {
                    "start": 310,
                    "end": 313,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 314,
                    "end": 316,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 317,
                    "end": 319,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 503,
                    "end": 506,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 507,
                    "end": 509,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 510,
                    "end": 513,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 514,
                    "end": 517,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 518,
                    "end": 522,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 523,
                    "end": 526,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 527,
                    "end": 530,
                    "text": "16,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 531,
                    "end": 534,
                    "text": "17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 746,
                    "end": 754,
                    "text": "(Fig. 1)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "CLEF eHealth tasks offered yearly from 2013 have brought together researchers working on related information access topics, provided them with resources to work with and validate their outcomes, and accelerated pathways from scientific ideas to societal impact. In 2013, 2014, 2015, 2016, 2017, 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4, 5, [8] [9] [10] 16, 17] . 2 According to our analysis of the impact of CLEF eHealth labs up to 2017 [15] , the submitting teams have achieved statistically significant improvements in the processing quality in at least 1 out of the top-3 methods submitted to the following eight tasks: 3 [14] . CLEF eHealth 2012 lab workshop has resulted in 16 papers and each year CLEF eHealth 2013-2017 evaluation labs have increased this number from 31 to 35. In accordance with the CLEF eHealth mission to foster teamwork, the number of co-authors per paper has been from 1 to 15 (the mean and standard deviation of 4 and 3, respectively). In about a quarter of the papers, this co-authoring collaboration has been international, and sometimes even intercontinental.",
            "cite_spans": [
                {
                    "start": 565,
                    "end": 568,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 569,
                    "end": 571,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 572,
                    "end": 575,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 576,
                    "end": 579,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 580,
                    "end": 584,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 585,
                    "end": 588,
                    "text": "16,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 589,
                    "end": 592,
                    "text": "17]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 595,
                    "end": 596,
                    "text": "2",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 669,
                    "end": 673,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 857,
                    "end": 861,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "CLEF eHealth Contributions and Growth in 2012-2019"
        },
        {
            "text": "This substantial community interest in the CLEF eHealth tasks and their resources has led to the evaluation campaign maturing and establishing its presence over the years. In 2020, CLEF eHealth is one of the primary venues for all interdisciplinary actors of the ecosystem for producing, processing, and consuming eHealth information [1, 2, 6] . Its niche is addressing health information needs of laypeople-and not health care experts only-in retrieving and digesting valid and relevant eHealth information to make health-centered decisions.",
            "cite_spans": [
                {
                    "start": 334,
                    "end": 337,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 338,
                    "end": 340,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 341,
                    "end": 343,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "CLEF eHealth Contributions and Growth in 2012-2019"
        },
        {
            "text": "The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 [4, 5, 8, 10, 16] that have already addressed the analysis of biomedical text in English, French, Hungarian, Italian, and German. This year, the CodiEsp task, will focus on the International Classification of Diseases, the 10th Revision (ICD10) coding for clinical case data in Spanish using the Spanish version of ICD10 (CIE10). The CodiEsp task will explore the automatic assignment of CIE10 codes to clinical case documents in Spanish, namely of two categories: procedure and diagnosis (known as 'Procedimiento' and 'Diagnostico' in Spanish). The following three subtasks will be posed: (1) CodiEsp Diagnosis Coding will consist of automatically assigning diagnosis codes to clinical cases in Spanish. (2) CodiEsp Procedure Coding will focus on assigning procedure codes to clinical cases in Spanish. (3) CodiEsp Explainable Artificial Intelligence (AI) will evaluate the explainability/interpretability of the proposed systems, as well as their performance by requesting to return the text spans supporting the assignment of CIE10 codes.",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 211,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 212,
                    "end": 214,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 215,
                    "end": 217,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 218,
                    "end": 221,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 222,
                    "end": 225,
                    "text": "16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "CLEF eHealth 2020 Information Extraction and Retrieval Tasks"
        },
        {
            "text": "The CodiEsp corpus used for this task consists of a total of 1, 000 clinical cases that were manually annotated by clinical coding professionals with clinical procedure and diagnosis codes from the Spanish version of ICD10 together with the actual minimal text spans supporting the clinical codes. The CodiEsp corpus has around 18, 000 sentences, and contains about 411, 000 words and 19, 000 clinical codes. Code annotations will be released in a separate file together with the respective document code and the span of text that leads to the codification (the evidence). Additional data resources including medical literature abstracts in Spanish indexed with ICD10 codes, linguistic resources, gazetteers, and a background set of medical texts in Spanish will also be released to complement the CodiEsp corpus, together with annotation guidelines and details.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CLEF eHealth 2020 Information Extraction and Retrieval Tasks"
        },
        {
            "text": "For the CodiEsp Diagnosis and Procedure Coding subtasks, participants will submit their coding predictions returning ranked results. For every document, a list of possible codes will be submitted, ordered by confidence or relevance. Since these subtasks are designed to be ranking competitions, they will be evaluated on a standard ranking metric: Mean Average Precision. For the CodiEsp Explainable AI subtask, explainability of the systems will be considered, in addition to their performance on the test set. Systems have to provide textual evidence from the clinical case documents that supports the code assignment and thus can be interpreted by humans. This automatically returned evidence will be evaluated against manually annotated text spans. True positive evidence texts are those that consist in a sub-match of the manual annotations. F 1 will be used as the primary evaluation metric.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CLEF eHealth 2020 Information Extraction and Retrieval Tasks"
        },
        {
            "text": "The 2020 CLEF eHealth Task 2 on IR builds on the tasks that have run at CLEF eHealth since its inception in 2012. This Consumer Health Search (CHS) task follows a standard IR shared challenge paradigm from the perspective that it provides participants with a test collection consisting of a set of documents and a set of topics to develop IR techniques for. Runs submitted by participants are pooled, and manual relevance assessments conducted. Performance measures are then returned to participants.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CLEF eHealth 2020 Information Extraction and Retrieval Tasks"
        },
        {
            "text": "In the 2017 CLEF eHealth CHS task, similarly to 2016, we used the ClueWeb 12 B13 4 document collection [12, 18] . This consisted of a collection of 52.3 million medically related web pages. Given the scale of this document collection participants reported that it was difficult to store and manipulate the document collection. In response, the 2018 CHS task introduced a new document collection, named clefehealth2018. This collection consists of over 5 million medical webpages from selected domains acquired from the CommonCrawl [7] . Given the positive feedback received for this document collection, it will be used again in the 2020 CHS task.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 107,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 108,
                    "end": 111,
                    "text": "18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 531,
                    "end": 534,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "CLEF eHealth 2020 Information Extraction and Retrieval Tasks"
        },
        {
            "text": "Historically the CLEF eHealth IR task has released text queries representative of layperson information needs in various scenarios. In recent years, query variations issued by multiple laypeople for the same information need have been offered. In this year's task we extend this to spoken queries. These spoken queries are generated by 6 individuals using the information needs derived for the 2018 challenge [7] . We also provide textual transcripts of these spoken queries and ASR translations.",
            "cite_spans": [
                {
                    "start": 409,
                    "end": 412,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "CLEF eHealth 2020 Information Extraction and Retrieval Tasks"
        },
        {
            "text": "Given the query variants for an information need, participants are challenged in the 2020 task with retrieving the relevant documents from the provided document collection. This is divided into a number of subtasks which can be completed using the spoken queries or their textual transcripts by hand or ASR. Similar to the 2018 CHS tasks, subtasks explored this year are: adhoc/personalized search, query variations, and search intent with Binary Preference, Mean Reciprocal Rank, Normalized Discounted Cumulative Gain@1-10, and (Understandability-biased) Rank-biased Precision as subtask-dependent evaluation measures. Participants can submit multiple runs for each subtask.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CLEF eHealth 2020 Information Extraction and Retrieval Tasks"
        },
        {
            "text": "The general purpose of CLEF eHealth throughout the years, as its 2020 IE and IR tasks demonstrate, has been to assist laypeople in finding and understanding health information in order to make enlightened decisions. Breaking language barriers has been our priority over the years, and this will continue in our multilingual tasks. Text has been our major media of interest, but speech has been, and continues to be, included in tasks as a major new way of interacting with systems. Each year of the labs has enabled the identification of difficulties and challenges in IE, IM, and IR which have shaped our tasks. For example, popular IR tasks have considered multilingual, contextualized, and/or spoken queries and query variants. However, further exploration of query construction, aiming at a better understanding of CHS are still needed. The task into the future will also further explore relevance dimensions, and work toward a better assessment of readability and reliability, as well as methods to take these dimensions into consideration. As lab organizers, our purpose is to increase the impact and the value of the resources, methods and the community built by CLEF eHealth. Examining the quality and stability of the lab contributions will help the CLEF eHealth series to better understand where it should be improved and how. As future work, we intend continuing our analyses of the influence of the CLEF eHealth evaluation series from the perspectives of publications and data/software releases [3, 14, 15] .",
            "cite_spans": [
                {
                    "start": 1507,
                    "end": 1510,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "text": "14,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1515,
                    "end": 1518,
                    "text": "15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "A Vision for CLEF eHealth Beyond 2020"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Aspiring to unintended consequences of natural language processing: a review of recent developments in clinical and consumergenerated text processing",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Demner-Fushman",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Elhadad",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Yearb. Med. Inform",
            "volume": "1",
            "issn": "",
            "pages": "224--233",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Advancing the state of the art in clinical natural language processing through shared tasks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Filannino",
                    "suffix": ""
                },
                {
                    "first": "\u00d6",
                    "middle": [],
                    "last": "Uzuner",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Yearb. Med. Inform",
            "volume": "27",
            "issn": "01",
            "pages": "184--192",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "An analysis of evaluation campaigns in ad-hoc medical information retrieval: CLEF eHealth",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Goeuriot",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Inf. Retrieval J",
            "volume": "21",
            "issn": "6",
            "pages": "507--540",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-018-9331-4"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Overview of the CLEF eHealth evaluation lab",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Goeuriot",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "CLEF 2015",
            "volume": "9283",
            "issn": "",
            "pages": "429--443",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-24027-5_44"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "CLEF 2017 eHealth evaluation lab overview",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Goeuriot",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "10456",
            "issn": "",
            "pages": "291--303",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-65813-1_26"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Community challenges in biomedical text mining over 10 years: success, failure and the future",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Brief. Bioinf",
            "volume": "17",
            "issn": "1",
            "pages": "132--144",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Overview of the CLEF 2018 consumer health search task",
            "authors": [
                {
                    "first": "Zuccon",
                    "middle": [],
                    "last": "Jimmy",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Palotti",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Overview of the CLEF eHealth evaluation lab",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kelly",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Goeuriot",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suominen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "N\u00e9v\u00e9ol",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Palotti",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zuccon",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CLEF 2016",
            "volume": "9822",
            "issn": "",
            "pages": "255--266",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-44564-9_24"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Overview of the ShARe/CLEF eHealth evaluation lab",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kelly",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "CLEF 2014",
            "volume": "8685",
            "issn": "",
            "pages": "172--191",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-11382-1_17"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Overview of the CLEF eHealth evaluation lab",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kelly",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CLEF 2019",
            "volume": "11696",
            "issn": "",
            "pages": "322--339",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-28577-7_26"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Patient empowerment: the need to consider it as a measurable patient-reported outcome for chronic conditions",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mcallister",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dunn",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Payne",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Davies",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Todd",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "BMC Health Serv. Res",
            "volume": "12",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Palotti",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "CLEFeHealth2012 -the CLEF 2012 workshop on cross-language evaluation of methods, applications, and resources for ehealth document analysis",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suominen",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Working Notes",
            "authors": [],
            "year": 2012,
            "venue": "CEUR Workshop Proceedings (CEUR-WS.org)",
            "volume": "1178",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Scholarly influence of the conference and labs of the evaluation forum eHealth initiative: review and bibliometric study of the 2012 to 2017 outcomes",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suominen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kelly",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Goeuriot",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "JMIR Res. Protoc",
            "volume": "7",
            "issn": "7",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Information Retrieval Evaluation in a Changing World: Lessons Learned from 20 Years of CLEF",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suominen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kelly",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Goeuriot",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "333--363",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-22948-1_14"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Overview of the CLEF eHealth evaluation lab",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suominen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction",
            "volume": "",
            "issn": "",
            "pages": "286--301",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-98932-7_26"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Overview of the ShARe/CLEF eHealth evaluation lab",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Suominen",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "CLEF 2013",
            "volume": "8138",
            "issn": "",
            "pages": "212--231",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-40802-1_24"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "The IR task at the CLEF eHealth evaluation lab 2016: usercentred health information retrieval",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zuccon",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CLEF 2016 Evaluation Labs and Workshop: Online Working Notes. CEUR-WS",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Timeline of the CLEF eHealth tasks in 2013-2020",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "Task 2 on English shorthand extension with respect to the Unified Medical Language System (UMLS) codes with the accuracy and random shuffling (P = .009) as the performance measure and statistical significance test, respectively, on independent sets of 200 and 100 annotated EHRs for training and testing. The top-3 submissions had on the test set the accuracy of 0.719, 0.683, and 0.664, while the worst accuracy was 0.426. 4. 2013 Task 3 on English IR with the Precision at 10 (P@10) and Wilcoxon test (P = .04) as the performance measure and statistical significance test, respectively, on 50 test queries and the matching result set. The top-3 submissions had P@10 of 0.518, 0.504, and 0.484, while the worst P@10 was 0.006. 5. 2015 Task 1 on English nursing handover ASR with the error and Wilcoxon test (P = .04) as the performance measure and statistical significance test, respectively, on independent sets of 100 and 100 annotated EHRs for training and testing. The top-3 submissions had on the test set the error of 0.385, 0.523, and 0.528, while the worst error was 0.954. 6. 2016 Task 1 on English nursing handover IE with F 1 and Wilcoxon test (P = .04) as the performance measure and statistical significance test, respectively, on independent sets of 200 and 100 annotated EHRs for training and testing. The top-3 submissions had on the test set F papers for the 741 included authors from 33 countries across the world, and the papers have attracted nearly 1, 300 citations, creating h-index and i10-index of 18 and 35, respectively, on Google Scholar",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}