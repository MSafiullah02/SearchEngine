{
    "paper_id": "PMC7148007",
    "metadata": {
        "title": "Identifying Notable News Stories",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Antonia",
                "middle": [],
                "last": "Saravanou",
                "suffix": "",
                "email": "antoniasar@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Giorgio",
                "middle": [],
                "last": "Stefanoni",
                "suffix": "",
                "email": "giorgio.stefanoni@gmail.com",
                "affiliation": {}
            },
            {
                "first": "Edgar",
                "middle": [],
                "last": "Meij",
                "suffix": "",
                "email": "emeij@bloomberg.net",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "With the rise in popularity of social media and the increase in citizen journalism, news is increasing in volume and coverage all around the world. As a result, news consumers run the risk of either being overwhelmed due to the sheer amount of news being produced, or missing out on news stories due to heavy filtering. To deal with the information overload, it is crucial to develop systems that can filter the noise in an intelligent fashion. Due to the highly condensed language used in news, automated systems have been developed to process them and generate well-defined structured representations from their content [9]. Each structure is a so-called triple that represents an event in the form of who did what to whom, with additional metadata information about when and where this happened. Such representations (triples) form a knowledge graph (KG). There are multiple computational benefits when searching, labeling, and processing KGs due to their clean and simple structure [11, 14].",
            "cite_spans": [
                {
                    "start": 623,
                    "end": 624,
                    "mention": "9",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 987,
                    "end": 989,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 991,
                    "end": 993,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A common approach to measure notability of a news event is to track it through a proxy metric. For example, Naseri et al. [7] decide whether an article describes a notable event by counting the user interactions, while Setty et al. [10] cluster together similar news articles and then use the cluster size to decide if the common theme is notable. Wang et al. [12] propose a recommendation framework that takes as input a stream of news and predicts the user\u2019s click-through rate.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 124,
                    "mention": "7",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 233,
                    "end": 235,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 361,
                    "end": 363,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we approach the problem of identifying notable news stories as a ranking task, i.e., we rank structured news stories represented as triples against notable events. We use Wikipedia\u2019s Current Events Portal (WCEP) [2] as curated notable events and, using a combination of textual and semantic features, we build a learning to rank (LTR) model to solve the ranking problem.\n",
            "cite_spans": [
                {
                    "start": 228,
                    "end": 229,
                    "mention": "2",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {Q} = [q_0, \\dots , q_{k}]$$\\end{document} denote a stream of events, where each query event\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i\\in \\mathcal {Q}$$\\end{document} is a notable event composed of a textual description and of a publication date. Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {C} = [c_0, \\dots , c_{l}]$$\\end{document} denote a stream of candidate events. Each \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_j \\in \\mathcal {C}$$\\end{document} is a structured representation of a news story that consists of a triple of the form (s, p, o), where s is the subject, p is the predicate, and o is the object, together with information about the location (city, country) and the date of the news story.",
            "cite_spans": [],
            "section": "Problem Statement",
            "ref_spans": []
        },
        {
            "text": "Given a query \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i \\in \\mathcal {Q}$$\\end{document} and a stream of candidates \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {C}$$\\end{document}, we aim to rank each candidate \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_j \\in \\mathcal {C}$$\\end{document} by its relevance to the query \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document}. A pair \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(q_i, c_j)$$\\end{document} is considered as very relevant when the information from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_j$$\\end{document} matches completely; it is considered as relevant when some of the information matches; otherwise, it is considered as not relevant. Table 1 shows a query \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_0$$\\end{document} and two candidates \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_0$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_1$$\\end{document}. The pair \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(q_0, c_0)$$\\end{document} is very relevant because \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_0$$\\end{document} matches \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_0$$\\end{document} completely; in contrast, the pair \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(q_0, c_1)$$\\end{document} is relevant because \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_0$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_1$$\\end{document} disagree only on the location of the event.",
            "cite_spans": [],
            "section": "Problem Statement",
            "ref_spans": [
                {
                    "start": 2382,
                    "end": 2383,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "In this section we present our method to identify notable news stories which consists of three steps: (1) creating (query, candidate) pairs, (2) extracting textual and semantic features, and (3) training a learning to rank (LTR) model.",
            "cite_spans": [],
            "section": "Method",
            "ref_spans": []
        },
        {
            "text": "(1) Creating Pairs. We create the set of all possible (query, candidate) pairs where (i) the query and the candidate have the same publication date, and (ii) the query and the candidate have at least one word in common as a pair is unlikely to be relevant if they share no words.",
            "cite_spans": [],
            "section": "Method",
            "ref_spans": []
        },
        {
            "text": "(2) Extracting Features. We extract a set of features for each constructed pair. Our features can be classified into three groups as follows.",
            "cite_spans": [],
            "section": "Method",
            "ref_spans": []
        },
        {
            "text": "(i)\nFeatures related to a component. We compute the size of the query or the candidate (i.e., the number of terms in the query/candidate).",
            "cite_spans": [],
            "section": "Method",
            "ref_spans": []
        },
        {
            "text": "(ii)\nFeatures related to the pair. We calculate the Okapi BM25 score, the term frequency (TF) and the term frequency\u2013inverse document frequency (TF\u2013IDF) for the query/candidate in the pair. We calculate these scores using the stemmed versions of the query/candidate (using the Porter Stemmer [8]). We further define a similarity score, element match, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${EM}(q_i, ele_{c_j}) = |q_i \\cap ele_{c_j}| / |ele_{c_j}|$$\\end{document} where an element \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ele_{c_j}$$\\end{document} is one of the: subject, predicate, object, description of the predicate, location, and the date in the candidate \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_j$$\\end{document}. For each of those, we calculate the fraction of the number of common terms between the element \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ele_{c_j}$$\\end{document} and the query \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} to the total number of terms in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ele_{c_j}$$\\end{document}. In addition, we compute all EM scores using the stemmed versions of the pair components. We also extract similarity scores for combinations of elements, as for example EM(\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document}, subject\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\cap $$\\end{document}\npredicate\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\cap $$\\end{document}\nobject) and EM(\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document}, city\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\cap $$\\end{document}\ncountry).",
            "cite_spans": [
                {
                    "start": 293,
                    "end": 294,
                    "mention": "8",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Method",
            "ref_spans": []
        },
        {
            "text": "(iii)\nSemantic features. An entity is a well-defined, meaningful and unique way to characterize a word/phrase. We therefore apply entity linking using the TagMe API [5] to identify entities (an example is shown in Table 1). Given the tagged query and the tagged candidate, we calculate the number of common entities using the Jaccard similarity.",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 167,
                    "mention": "5",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Method",
            "ref_spans": [
                {
                    "start": 220,
                    "end": 221,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "(3) Ranking Pairs. We then use our features to train a learning to rank model in order to obtain a ranking of pairs. More details on the training and the tuning can be found in Sect. 4.",
            "cite_spans": [],
            "section": "Method",
            "ref_spans": []
        },
        {
            "text": "For the candidate news stories, we use the Integrated Crisis Early Warning System (ICEWS) [1] dataset which contains events that are automatically extracted from news articles using TABARI [3, 9]. This system uses grammatical parsing to identify events (who did what to whom, when and where) using human-generated rules. The events are triples consisting of coded actions between socio-political actors. The actors refer to individuals, groups, sectors and nation states. The actions are coded into 312 categories. Geographical and temporal metadata are also associated with each triple (examples are shown in Table 1).",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 92,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 190,
                    "end": 191,
                    "mention": "3",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 193,
                    "end": 194,
                    "mention": "9",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Experimental Setup",
            "ref_spans": [
                {
                    "start": 616,
                    "end": 617,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "In our experiments, we use the same two weeks of data from ICEWS and WCEP. We remove triples with the generic action type \u201cMake statement\u201d as they do not convey any meaningful information. We then create pairs as described in Sect. 3. We build a crowdsourcing task (see below) to get golden truth labels. From the resulting annotated dataset, we only keep queries with at least one relevant ICEWS triple as there are, e.g., sports events in the WCEP dataset but not in the ICEWS dataset. In total, the resulting dataset contains 9.1K pairs; 74 queries and 123 candidates per query on average. To evaluate our method in a real-world setting we split the dataset by date and use the first ten days for training, the next two days for validation, and the last two for testing.\n",
            "cite_spans": [],
            "section": "Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "Golden Truth. We employ crowdsourcing on the Figure-eight platform and ask annotators to judge the relevance of each pair on a 3-point scale (very relevant, relevant, not relevant).1 Each pair \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(q_i, c_j)$$\\end{document} is annotated by at least three annotators and we use majority voting to obtain the gold labels. Our task obtains a inter-annotator agreement of 96.57%. Table 2 shows the distribution of relevance labels among pairs. The resulting dataset is highly skewed; with 3% annotated as very relevant, 1% as relevant, and 96% as not relevant.",
            "cite_spans": [],
            "section": "Experimental Setup",
            "ref_spans": [
                {
                    "start": 647,
                    "end": 648,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Models. We explore various LTR algorithms and include results from RankBoost (RB) [6], lambdaMART (LM) [13], and Random Forest (RF) [4]. We experiment using different sets of features: all features (All), all\nexcept entity-related features (All\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{-}$$\\end{document}), selected features (Sel) and baseline features (B). Sel features include BM25 and TF\u2013IDF scores calculated from the original/stemmed versions, EM scores for subject, predicate, object and location, and the number of entities in common and Jaccard similarity between the query and the candidate. For B features, we only consider BM25 and TF\u2013IDF scores calculated from the original/stemmed versions of the query and the candidate. We evaluate using MAP, Precision@k, NDCG@k and MRR.",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 84,
                    "mention": "6",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 104,
                    "end": 106,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 133,
                    "end": 134,
                    "mention": "4",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "We compare the three LTR models on the All and B feature sets and show the results in Table 3. Our method (using All features) achieves better results than using just the baseline B features. For each model and feature set, we only show the best tuned model on the validation set. Our method consistently outperforms all baselines, achieving 5\u201312% improvements on NDCG@10. These improvements are statistically significant with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p\\le 0.01$$\\end{document} using paired t-test.\n",
            "cite_spans": [],
            "section": "Overall Performance ::: Results and Discussion",
            "ref_spans": [
                {
                    "start": 92,
                    "end": 93,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "We tune the parameters for each model on the validation set using NDCG@10. Figure 1 (left) shows the performance quartile plot using different parameter settings. RB and RF models show less sensitivity in the parameters tuning compared to LM. We evaluate the models when ranking pairs using all annotations (VR, R, and NR). We perform the same experiment using only the VR and NR labeled pairs. Figure 1 (right) shows that the model achieves better results when excluding the R labeled pairs. This is expected as the relevant label is very rare (only 1%, see Table 2) and the models tend to consider it as noise.",
            "cite_spans": [],
            "section": "Overall Performance ::: Results and Discussion",
            "ref_spans": [
                {
                    "start": 82,
                    "end": 83,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 402,
                    "end": 403,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 565,
                    "end": 566,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Our next step is to evaluate different combinations of features (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textsc {All}, \\textsc {All}^-, \\textsc {Sel}$$\\end{document}, B). We show our findings in Table 4. First, we compare our method using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\textsc {All}^-}$$\\end{document} and B feature sets. We show that using the proposed features (Sect. 3) we achieve better performance for all LTR models. Second, we evaluate the performance of the models when we add the entity features by comparing All and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\textsc {All}^-}$$\\end{document}. In Table 4, we show that there is a statistically significant improvement (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p\\le $$\\end{document} 0.01) on MRR (+7%) when we add the entity\u2013related features.\n",
            "cite_spans": [],
            "section": "Overall Performance ::: Results and Discussion",
            "ref_spans": [
                {
                    "start": 431,
                    "end": 432,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1309,
                    "end": 1310,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "In this section, we show examples of the output from our best performing setting, i.e., RF with All features using the VR and NR labeled pairs. We show our best and worst per\u2013query NDCG@10 performance. The best one achieves a score of 1, which indicates that our method was able to rank all pairs in the right order. The top\u20131 ranked pair is the query \u201cAt least 15 children are killed and 45 more are injured after a school bus collides with a truck in Etah, India. Date: 20 Jan. 2017\u201d and the candidate <Attacker (from India), Kill by physical assault, Children (from India)> with metadata <Etah, India, 20 Jan. 2017>. The item with the worst per\u2013query NDCG@10 performance is \u201cMexican drug lord Joaquin Guzman is extradited to the USA, where he will face charges for his role as leader of the Sinaloa Cartel. Date: 20 Jan. 2017\u201d paired with the candidate <USA, Host a visit, Narendra Modi> with metadata <-, USA, 20 Jan. 2017>. This query is about the extradition of a drug lord, while the candidate is about a visit of the Prime Minister of India. However, among the top\u201310 ranked candidates, the most relevant one is the triple <USA, Arrest, detain, or charge with legal action, Men (from Mexico)> with metadata <Kansas City, USA, 20 Jan. 2017>, ranked 9th. This shows that even in the worst ranking per\u2013query, our method ranks a relevant candidate in the top\u201310.",
            "cite_spans": [],
            "section": "Analysis ::: Results and Discussion",
            "ref_spans": []
        },
        {
            "text": "In summary, we provide quantitative and qualitative performance analyses of our proposed method and we conclude that learning to rank is a viable method to determine notability of news stories. Among the key steps of our method are: (i) the extraction of textual and semantic features, and (ii) the exclusion of the pairs that do not convey strong signal, i.e., the ones labeled as \u2018relevant\u2019. The RF model outperforms all baselines and it is also more robust with respect to hyperparameter settings. These findings show that our approach to detect notable news through ranking is a promising one. Although our method obtains high performance (MRR = 81%), we believe we can attain further improvements by leveraging relations of the identified entities to discover implicitly relevant ones, such as <Narendra_Modi, isPrimeMinisterOf, India>.",
            "cite_spans": [],
            "section": "Analysis ::: Results and Discussion",
            "ref_spans": []
        },
        {
            "text": "In this paper, we present a method to rank notable news representations which leverages textual and semantic features. Our evaluation on labeled pairs from WCEP and the ICEWS shows that our method is effective. In the future, we intend to include features based on the relations of the tagged entities from external KGs, such as DBPedia and Freebase.",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Example of a query \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_0$$\\end{document} and two candidate events \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_0$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_1$$\\end{document}.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Distribution of the relevance labels in the dataset.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Main results of the LTR models on our dataset.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Results using binary relevance labels.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: (Left) Results for each model on the validation set. Each box shows the median and upper/lower quartiles. (Right) Performance using RB with selected features on two datasets.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "Adapting boosting for information retrieval measures",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "CJ",
                    "middle": [],
                    "last": "Burges",
                    "suffix": ""
                },
                {
                    "first": "KM",
                    "middle": [],
                    "last": "Svore",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Inf. Retrieval",
            "volume": "13",
            "issn": "",
            "pages": "254-270",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-009-9112-1"
                ]
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Random forests",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Mach. Learn.",
            "volume": "45",
            "issn": "1",
            "pages": "5-32",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1010933404324"
                ]
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "An efficient boosting algorithm for combining preferences",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Iyer",
                    "suffix": ""
                },
                {
                    "first": "RE",
                    "middle": [],
                    "last": "Schapire",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res.",
            "volume": "4",
            "issn": "",
            "pages": "933-969",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "An algorithm for suffix stripping",
            "authors": [
                {
                    "first": "MF",
                    "middle": [],
                    "last": "Porter",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Readings in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}