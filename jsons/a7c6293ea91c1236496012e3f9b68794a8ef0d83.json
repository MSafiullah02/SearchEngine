{
    "paper_id": "a7c6293ea91c1236496012e3f9b68794a8ef0d83",
    "metadata": {
        "title": "Using Rough Set to Find the Factors That Negate the Typical Dependency of a Decision Attribute on Some Condition Attributes",
        "authors": [
            {
                "first": "Honghai",
                "middle": [],
                "last": "Feng",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Hebei Agricultural University",
                    "location": {
                        "postCode": "071001",
                        "settlement": "Baoding, Hebei",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Xu",
                "middle": [],
                "last": "Hao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Science and Technology",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing, Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Liu",
                "middle": [],
                "last": "Baoyan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "China Academy of Traditional Chinese Medicine",
                    "location": {
                        "postCode": "100700",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Yang",
                "middle": [],
                "last": "Bingru",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Science and Technology",
                    "location": {
                        "postCode": "100083",
                        "settlement": "Beijing, Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Gao",
                "middle": [],
                "last": "Zhuye",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Beijing Sino-Japen Friendship Hospital",
                    "location": {
                        "postCode": "100029",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Li",
                "middle": [],
                "last": "Yueli",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Hebei Agricultural University",
                    "location": {
                        "postCode": "071001",
                        "settlement": "Baoding, Hebei",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "In real world, there are a lot of knowledge such as the following: most human beings that are infected by a kind of virus suffer from a corresponding disease, but a small number human beings do not. Which are the factors that negate the effects of the virus? Standard rough set method can induce simplified rules for classification, but cannot generate this kind of knowledge directly. In this paper, we propose two algorithms to find the factors. In the first algorithm, the typical rough set method is used to generate all the variable precision rules firstly; secondly reduce attributes and generate all the non-variable precision rules; lastly compare the variable precision rules and non-variable precision rules to generate the factors that negate the variable precision rules. In the second algorithm, firstly, induce all the variable precision rules; secondly, select the examples corresponding to the variable precision rules to build decernibility matrixes; thirdly, generate the factors that negate the variable precision rules. Three experimental results show that using the two algorithms can get the same results and the computational complexity of the second algorithm is largely less than the firs one.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In real world, there are a lot of knowledge such as the following: most people suffer from hyperpyrexia when they take a heavy cold, whereas some people do not; most earthquakes in the sea cannot cause a ground sea, but in 2005 the earthquake in Indian Ocean cause a ground sea and cause thousands upon thousands people death. Which are the factors that negate the effects of virus, or a heavy cold? And which factors make an earthquake cause the Indian Ocean ground sea? Standard rough set method [1] can induce simplified rules for classification, but cannot generate this kind of knowledge directly. Other machine learning theories such as SVM [2] , ANN [3] and Bayesian networks have not been found that they can be used to induce this kind of knowledge.",
            "cite_spans": [
                {
                    "start": 498,
                    "end": 501,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 647,
                    "end": 650,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 657,
                    "end": 660,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "All the disasters such as floods, dam collapses, terror events, epidemics etc are exceptional cases. The factors that cause these disasters are significant to us. Additionally, the exceptional students education, exceptional customers service, exceptional patients therapy and nurse etc need the factors that cause exceptional rules.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The decernibility matrix in rough set theory is a valid method for attribute reduction and rule generation whose main idea is to compare the examples that are not in the same class. However, generally, the decernibility matrix is used to the whole data set [4] , can it be used to partial data to induce the knowledge with which we can find the factors that negate the typical dependency of a decision attribute on some condition attributes? The answer is affirmative.",
            "cite_spans": [
                {
                    "start": 257,
                    "end": 260,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A decision table is composed of a 4-tuple DT= , , ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "is a nonempty, finite set called the universe; A is a nonempty, finite set of attributes; x . The construction of elementary sets is the first step in classification with rough sets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "By a discernibility matrix of B \u2286 A denoted M (B) a n n \u00d7 matrix is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "Thus entry c ij is the set of all attributes that discern objects x i and x j .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "It is easily seen that the core is the set of all single element entries of the discernibility matrix M (B), i.e., Where \u220f denotes the Boolean multiplication.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "The following property establishes the relationship between disjunctive normal form of the function f (B) and the set of all reducts of B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "All constituents in the minimal disjunctive normal form of the function f (B) are all reducts of B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "In order to compute the value core and value reducts for x we can also use the discernibility matrix as defined before and the discernibility function, which must be slightly modified: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Basic Concepts of Rough Set Theory"
        },
        {
            "text": "to get the factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithms"
        },
        {
            "text": "We have generated the feature vectors of 4 hand-written Chinese characters, where the values 1, 2, 3 and 4 of Y denote the 4 different hand written Chinese characters ( Figure 1 ), examples 1 and 2, 3 and 4, 5 and 6, 7 and 8 are the same hand-written Chinese characters respectively, after discretization we get Table 1 . (1) Using algorithm 1 to induce the factors (a) The variable precision rules are: ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 169,
                    "end": 177,
                    "text": "Figure 1",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 312,
                    "end": 319,
                    "text": "Table 1",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Hand-Written Chinese Characters Recognition"
        },
        {
            "text": "(e) For variable precision rule 1) A=17\u2192Y=2 with \u03b1 =2/3, rule 4) A=17 \u2227 B=8\u2192Y=1 and rule 15) A=17 \u2227 D=4\u2192Y=1 should be selected, and after comparing rule 1) with rule 4) and rule 15) respectively, we can get that: B=8 and D=4 are the factors that negate rule 1) A=17\u2192Y=2 with \u03b1 =2/3. For rule 2) A=18\u2192Y=3 with \u03b1 =2/3, rule 5) A=18 \u2227 D=3\u2192Y=1 and rule 20) A=18 \u2227 C=5\u2192Y=1 should be selected, and after comparing it with rule 5) rule 20) respectively, we can get that: D=3 and C=5 are the factors that negate the rule A=18\u2192Y=3 with \u03b1 =2/3. For rule 3) E=3 \u2192Y=4 with \u03b1 =2/3, rule 12) A=18 \u2227 E=3\u2192Y=3 should be selected, and after comparing rule 3) with rule12), we can get that: A=18 is the factor that negate E=3 \u2192Y=4 with \u03b1 =2/3. and D=4 are the factors that negate rule 1) A=17\u2192Y=2 with \u03b1 =2/3, or make A=17 \u2227 B=8\u2192Y=1 and A=17 \u2227 D=4\u2192Y=1. Namely we get the following knowledge:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hand-Written Chinese Characters Recognition"
        },
        {
            "text": "B=8 and D=4 are the factors that negate the rule A=17\u2192Y=2 with \u03b1 =2/3 ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hand-Written Chinese Characters Recognition"
        },
        {
            "text": "(1) SARS Data Set and Discretization The SARS data are the experimental results of micronutrients that are essential in minute amounts for the proper growth and metabolism of human beings. Among them, examples 31~60 are the results of SARS patients and 61~90 are the results of healthy human beings. Attributes \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\" denote micronutrient Zn, Cu, Fe, Ca, Mg, K and Na respectively, and decision attribute \"C\" denotes the class \"SARS\" and \"healthy\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Micronutrient Data Set Experiments"
        },
        {
            "text": "{0,1} C V = , where \"0\" denotes \"SARS\", \"1\" denotes \"healthy\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Micronutrient Data Set Experiments"
        },
        {
            "text": "After discretization, some examples become a repeat. The amount of the total examples is reduced from 60 to 39. , 78 and 88, we can conclude that \"Fe=2 \u2192 C=1\", whereas for example 52, we hold \"Fe=2 \u2192 C=0\". Namely \"Fe=2 \u2192 C=1\" with \u03b1 =7/8, and \"Fe=2 \u2192 C=0\" with \u03b1 =1/8 can be held. Which are the attribute value that negate the rule of \"Fe=2 \u2192 C=1\"? With the partial decernibility matrix we can find the factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Micronutrient Data Set Experiments"
        },
        {
            "text": "Firstly, the examples with Fe=2 are selected. Secondly, generate the decernibility matrix with these examples. Thirdly, use the decernibility matrix to induce the rules that include the factors negating the rule \"Fe=2 \u2192 C=1\". Table 5 gives the results of the decernibility matrix. The attribute value that negates the rule \"Fe=2 \u2192 C=1\" can be induced as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 226,
                    "end": 233,
                    "text": "Table 5",
                    "ref_id": "TABREF8"
                }
            ],
            "section": "Micronutrient Data Set Experiments"
        },
        {
            "text": "So for example 52, the following knowledge can be gotten:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Micronutrient Data Set Experiments"
        },
        {
            "text": "with \u03b1 =1/8; \"Fe=2 \u2227 Ca=0\u2192 C=0\" with \u03b1 =1; \"Ca=0\"is the factor that negate the rule \"Fe=2 \u2192 C=1\"with \u03b1 =7/8 (see Table 1 ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 113,
                    "end": 120,
                    "text": "Table 1",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Micronutrient Data Set Experiments"
        },
        {
            "text": "with \u03b1 =1/18; \"Fe=0\" is the factor that negate the rule \"Ca=2 \u2192 C=1\" with \u03b1 =17/18.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Micronutrient Data Set Experiments"
        },
        {
            "text": "For examples 49 and 53, we can induce that \"Ca=1 \u2192 C=0 \", whereas for examples 62, 68, 71 and 73 we can get that \"Ca=1 \u2192 C=1\". Table 6 gives the results of the decernibility matrix. With 7 \u2227 7 \u2227 (3 \u22287) \u2227 (3 \u22285 \u22287) =7 (Na) for example 49 and with 6 \u2227 (6 \u22287) \u2227 (3 \u22286) \u2227 (3 \u22285 \u22286)=6 (K) for example 53, we can get the following knowledge:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 127,
                    "end": 134,
                    "text": "Table 6",
                    "ref_id": "TABREF10"
                }
            ],
            "section": "3)"
        },
        {
            "text": "\"Na=2\" (see example 49) and \"K=0\" (example 53) are the factors that negate the rule \"Ca=1 \u2192 C=1\" with \u03b1 =4/6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3)"
        },
        {
            "text": "We have gotten 441 coronary heart disease cases from Beijing, and among all the 441 cases there are 161 ones who suffer from heart failure with 638 records in the course of being in hospital. Among all the 638 records we get the following rules:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coronary Heart Disease Data Experiments"
        },
        {
            "text": "(1) Heart failure=true \u2192 Breath sounds in lungs = gruff (not sharp) with 192/638 and souffles in hearts =true with only 4/192; (2) Heart failure=true \u2192 Breath sounds in lungs = decrease in soundness with 205/638 and souffles in hearts =true with 111/192;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Coronary Heart Disease Data Experiments"
        },
        {
            "text": "1) Three experimental results show that using the two algorithms can get the same results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Discussions"
        },
        {
            "text": "2) Since algorithm 1 contains the step of attribute reduction and the computational complexity of attribute reduction is NP hard, the computational complexity of algorithm 1 is NP hard too, whereas algorithm 2 does not need the step of attribute reduction, especially the amount of selected examples for building a decernibility matrix will be very small even there are only several ones. So the computational complexity of algorithm 2 is largely less than the one of algorithm 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Discussions"
        },
        {
            "text": "3) The factors that negate a typical dependency embody the correlation between two rules, i.e., the factors negate a rule (dependency) whereas support another rule (dependency). This kind of knowledge differs from the exceptional rules, since the factors can give us the information of two rules, which is a kind of comparative knowledge, whereas the exceptional rule can only give us the information of one rule. 4) ANN, SVM, etc models can be viewed as \"population based\" as a single model is formed for the entire population (test data set), while the rough set approach follows an \"individual (data object) based\" paradigm. The \"population based\" tools determine features that are common to a population (training data set). The models (rules) created by rough set are explicit and easily understood. So for inducing easily understood knowledge, the rough set theory has an advantage over the black-box based machine learning methods such as ANN, SVM etc. 5) This kind of knowledge give us the knowledge that how a typical pattern change to the exceptional pattern.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Discussions"
        },
        {
            "text": "6) The idea of decernibility matrix can be used to not only the whole data set but also the partial data. The current use of the decernibility matrix need whole data set, i.e., whether a part of data of the whole data set can be selected to be applied to the decernibility matrix for inducing particular knowledge has not been offered up to now.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions and Discussions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Rough sets",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Pawlak",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Int. J. of Computer and Information Science",
            "volume": "1",
            "issn": "",
            "pages": "341--356",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Statistical Learning Theory",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "N"
                    ],
                    "last": "Vapnik",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Pattern recognition with neural networks combined by genetic algorithm\", Fuzzy sets and systems",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "B"
                    ],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "",
            "volume": "103",
            "issn": "",
            "pages": "339--347",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "On optimal decision rules in decision tables",
            "authors": [
                {
                    "first": "S K",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ziarko",
                    "suffix": ""
                }
            ],
            "year": 1985,
            "venue": "Bulletin of Polish Academy of Sciences",
            "volume": "33",
            "issn": "",
            "pages": "357--362",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Every discernibility matrix M (B) defines uniquely a discernibility (boolean) function f (B) defined as follows.Let us assign to each attribute B a \u2208 a binary Boolean variable a Then the discernibility function can be defined by the",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Generate all the variable precision rules. (b) Attribute reduction. (c) Select the reduced attribute set with the attributes that are in the variable precision rules. (d) Generate the non-variable precision rules. (e) Select the non-variable precision rules whose preconditions are the same as the variable precision rules whereas the postcondition is not the same as the variable precision rules. (f) Compare the variable precision rules and their corresponding to generate the factors that negate the variable precision rules. For every condition attribute A, (or select anyone among all the condition attributes) calculate its equivalence classes or partition IND(A)={A 1 , A 2 , \u2026\u2026, A n ,}. And for the decision attribute D, calculate IND(D)={D 1 , D 2 , \u2026\u2026, D m ,}. (b) For (i=0; i<n; i ++)// n: amount of equivalence classes of condition attribute //A (c) For (j=0; j<m; j++)// m: amount of equivalence classes of decision attributeD.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "4 hand-written Chinese characters",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "After attribute reduction we get the following new condition attributes combinations: {A, B, C}, {A, B, E}, {A, B, F}, {A, D}, {B, C, E}, {B, C, F}. (c) Select {A, B, C}, {A, B, E}, {A, B, F}, {A, D}, {B, C, E}, {A, C, E, F}, since the attribute sets {B, C, F} and {B, C, D, F} do not contain attribute A and E that are in the variable precision rules. (d) For the selected attribute sets, the following rules are generated:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Using algorithm 2 to induce the factors (a) The partitions of all the attributes can be gotten as follows: IND(A)={A 1 ,A 2 ,A 3 ,A 4 }={{1,3,4},{2,5,6},{7},{8}}, IND(B)={B 1 ,B 2 ,B 3 }={{1,2,6,7,8},{3,4},{5}}, IND(C)={C 1 ,C 2 } }={{1,2,3,4,7,8},{5,6}}, IND(D)={D 1 ,D 2 ,D 3 }={{1,2,6,7,8},{3,4},{5}}, IND(E)={E 1 ,E 2 }={{1,2, 3,4, 5},{6,7,8}}, IND(F)={F 1 ,F 2 }={{1,2,4,5},{3,6,7,8}}, IND(Y)={ Y 1 , Y 2 , Y 3 ,Y 4 }={{1,2},{3,4},{5,6},{7,8}}. for A 1 ={1,3,4}, we select examples 1, 3, and 4 to build the decernibility matrix, since the example 1 belongs to class 1, and examples 3, 4 belong to class 2; they should in different places in decernibility matrix, and for example 1 we using Boolean multiplication and Boolean sum to induce the factors that negate the variable rule.FromTable 2, since (B \u2228 D \u2228 F) \u2227 (B \u2228 D)= B \u2228 D, we can conclude that B=8",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Left",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "is a finite set of condition attributes and D is a finite set of",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The equivalence class of ( ) IND B is called elementary set in B because it presents the smallest discernible groups of objects. For any element i x of U , the equivalence class of i",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "For the examples whose decision attribute values are not included in D j to use Boolean multiplication and Boolean sum to induce the factors that negate the rule A i \u2192 D j with \u03b1 . That is, use",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "The information table of 4 hand-written Chinese characters",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Decernibility matrix for the examples in A 1",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Decernibility matrix for the examples in A 2",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "we get that D=3 and C=5 are the factors that negate the rule A=18\u2192Y=3 with \u03b1 =2/3, or make A=18 \u2227 C=5\u2192Y=1 and A=18 \u2227 D=3\u2192Y=1.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "describes the left 39 examples after discretization.",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Decernibility matrix for the examples with Fe=2",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "Decernibility matrix for the examples with Ca=1",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}