{
    "paper_id": "PMC7148069",
    "metadata": {
        "title": "Towards a Better Contextualization of Web Contents via Entity-Level Analytics",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Amit",
                "middle": [],
                "last": "Kumar",
                "suffix": "",
                "email": "amit.kumar@unicaen.fr",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Even in the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$30^{\\text {th}}$$\\end{document} year of World Wide Web, we can still observe the enormous amount of growth in the Web contents being created and subsequently available to any internet user. Because of the inconsistency of the data being generated, it is very hard for an ordinary user to distinguish the Web contents according to their societal relevance. With the availability of NER techniques [5] and LOD [1, 11, 13], we have access to a lot of information about the named entities described in a Web content. This contextualization of the entities contained in a text can help us to deal with Web contents. We observe that, for a text describing an event, there are specific recurring patterns of entity types appearing together. For instance, in the case of \u2018natural disasters\u2019, entities like organizations, countries, presidents appear together whereas in the case of \u2018political events\u2019, entities like parties, leaders, business-persons appear together. The availability of tools like AIDA [16] or DBPedia Spotlight [12], which can interlink text documents to LOD has provided us efficient means to capture the semantics of a plain text using the entity-level.",
            "cite_spans": [
                {
                    "start": 678,
                    "end": 679,
                    "mention": "5",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 690,
                    "end": 691,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 693,
                    "end": 695,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 697,
                    "end": 699,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1278,
                    "end": 1280,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1304,
                    "end": 1306,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction and Motivation",
            "ref_spans": []
        },
        {
            "text": "The purpose of this study is to analyze those large amounts of data and to help the user in getting a better semantic understanding of a Web document. To this end, we aim to study a Web document semantically using entity-level analytics. Ultimately, we plan to exploit and aggregate external knowledge using LOD for the proper contextualization of a Web content.",
            "cite_spans": [],
            "section": "Introduction and Motivation",
            "ref_spans": []
        },
        {
            "text": "Knowledge bases (KBs) are an effective way to store Web documents semantically in a structured format. Because of easy accessibility, these KBs are fruitful resources for many tasks in information retrieval [4] and natural language processing [9]. Recently, researchers from different domains have developed different knowledge acquisition approaches for the creation of knowledge graphs. This results in an emanation of large publicly accessible KBs such as Freebase [1], DBPedia [11], YAGO [13], which accommodate spatial and temporal information in addition to structural knowledge. Many applications such as complex event detection [17], named entities disambiguation [14] or social media topic classification [3] from various domains have acquired the benefit by integrating knowledge from LOD.",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 209,
                    "mention": "4",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 244,
                    "end": 245,
                    "mention": "9",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 469,
                    "end": 470,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 482,
                    "end": 484,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 493,
                    "end": 495,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 637,
                    "end": 639,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 673,
                    "end": 675,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 715,
                    "end": 716,
                    "mention": "3",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Background and Related Work",
            "ref_spans": []
        },
        {
            "text": "Entity-level analytics aggregates semantic information by incorporating knowledge about an entity or its types. The problem of event diffusion prediction into foreign language communities [8] has shown encouraging results with the assimilation of knowledge about the entities contained in a document. Here the introduced framework ELEVATE only utilizes the information about the entities in the document and resources from YAGO [13]. In [7], the authors address the task of Web content fine-grained hierarchical classification. They hypothesize that a document is symbolized by the named entities it comprised. They propose the idea of the \u2018semantic fingerprinting\u2019 method that expresses the overall semantics of a Web document by a compact vector. Entity-level analytics is also effective in computational fact checking of information [2]. The authors claim that human fact checking can be achieved by finding the shortest path on a conceptually or semantically defined network such as knowledge graphs (KGs).",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 190,
                    "mention": "8",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 429,
                    "end": 431,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 438,
                    "end": 439,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 837,
                    "end": 838,
                    "mention": "2",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Background and Related Work",
            "ref_spans": []
        },
        {
            "text": "Entity-level analytics provide a depth insight into Web contents. KGs carry a lot of information about entities, but all the information is not equally important for a given text. The novelty of this thesis is to discriminate between interesting and uninteresting semantic information about entities w.r.t. the context of a text.",
            "cite_spans": [],
            "section": "Background and Related Work",
            "ref_spans": []
        },
        {
            "text": "The idea of semantic fingerprinting [6] - as an approach towards Web analytics was well acknowledged by researchers in the semantic Web community. Thus, we presented the CALVADOS system [7] as an extension of semantic fingerprinting. At first, this system filters all the named entities present in a given text. By utilization of type information for all these entities from YAGO, it creates a representative vector (called semantic fingerprint) for the text. In last, it predicts the fine-grained type of the content using machine learning techniques. Moreover, it reports the semantic building block of the text. Figure 1 outlines the conceptual pipeline. The notable contributions of the mentioned scientific article are:employ semantic fingerprint to represent document\u2019s semanticsexploration and visualization of dependencies among entities compriseddata digestion supported by providing contextual KB data (e.g., types)\n\n",
            "cite_spans": [
                {
                    "start": 37,
                    "end": 38,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 187,
                    "end": 188,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "CALVADOS : For Entity-Level Content Analysis ::: Current Work",
            "ref_spans": [
                {
                    "start": 622,
                    "end": 623,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Exploitation of named entities and their types are always valuable in getting a better contextualization of a Web document [7]. But, sometimes we can be easily drowned by too much information. For example, recent articles involving Donald Trump deal with his position of president. But Trump has 76 facets (considering Wordnet types) like communicator, president, business-person, etc., which are not equally relevant. For some entities, it is even more complicated. For instance, Arnold Schwarzenegger is famous to be an actor, a politician and a bodybuilder. When an article deals with him, the context of the article makes us understand which facet is relevant, e.g., \u2018actor\u2019 if the article deals with a film release.",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 125,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Concise Entity-Type Extraction ::: Current Work",
            "ref_spans": []
        },
        {
            "text": "Hence, it arises two research questions:RQ1: What are the most relevant type(s) for an entity in general (i.e., without context)?RQ2: What are the most relevant type(s) for an entity in a given context?\n",
            "cite_spans": [],
            "section": "Concise Entity-Type Extraction ::: Current Work",
            "ref_spans": []
        },
        {
            "text": "Computational Model: Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_i \\epsilon D$$\\end{document} represents a document. The named entities associated with a document \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_i$$\\end{document} is given by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N(d_i)$$\\end{document}. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T(n_j)$$\\end{document} represents all the k types associate with any entity \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n_j$$\\end{document}. Entity-level document type is represented by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_{i}^t$$\\end{document} as shown in Eq. 4. Our task is to select m number of types from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T(n_j)$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m<< k$$\\end{document}. We define two types of models - First, for the calculation of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{Gen}$$\\end{document} (i.e.\nRQ1, Eq. 5) and second, for the calculation of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{Con}$$\\end{document} (i.e.\nRQ2, Eq. 6).1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} D = {d_1,d_2.....d_p} \\end{aligned}$$\\end{document}\n2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} N(d_i) = {n_{1}^i,n_{2}^i,.....n_{q}^i} \\end{aligned}$$\\end{document}\n3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} T(n_j) = {t_{1}^j,t_{2}^j,.....t_{k}^j} \\end{aligned}$$\\end{document}\n4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\text {Entity-level typed document}, d_{i}^t =\\varphi ({n_{1}^i,n_{2}^i,.....n_{q}^i}) \\end{aligned}$$\\end{document}\n5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} T_{Gen} = f_{gen}(T(n_j)) \\end{aligned}$$\\end{document}\n6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} T_{Con} = f_{con}(T(n_j), text) \\end{aligned}$$\\end{document}Currently, we have focus on our first research question (RQ1). Our first challenge is to find or develop the appropriate data set for the aforementioned task.",
            "cite_spans": [],
            "section": "Concise Entity-Type Extraction ::: Current Work",
            "ref_spans": []
        },
        {
            "text": "Gold Standard Creation: It is not easy to find the most relevant type(s) for an entity in general. We create the gold standard based on the Wordnet hierarchy mentioned in YAGO (1981 types). We consider that the most relevant type(s) for any entity is mentioned in its Wikipedia page. Precisely, we extract the types that are mentioned in the first or second sentence of entity\u2019s Wikipedia page and map them to the mentioned hierarchy.",
            "cite_spans": [],
            "section": "Concise Entity-Type Extraction ::: Current Work",
            "ref_spans": []
        },
        {
            "text": "Example \u2013 Extracted Wikipedia labels for \u2018Arnold Schwarzenegger\u2019 are actor, filmmaker, businessman, author, bodybuilder and politician. After mapping of these labels to Schwarzenegger\u2019s Wordnet hierarchy in YAGO, the ground truths are actor, film-maker, businessman, bodybuilder and politician.",
            "cite_spans": [],
            "section": "Concise Entity-Type Extraction ::: Current Work",
            "ref_spans": []
        },
        {
            "text": "Experimental Pipeline: Our next challenge is to find the suitable mechanism for concise entity-type prediction in general. We rely only on the structural information, which we get by exploring knowledge graph of entity in YAGO. We implemented several techniques as baselines but none of these techniques show promising results. These models are:Based on Leaf Node: We had the intuition that the most specific or relevant type of an entity should be at the deepest in the YAGO Wordnet entity\u2019s hierarchy. So, we picked the type that is at the deepest in the hierarchy.Based on Branching Factor: While implementing the model based on leaf node, we observe that sometimes, we were selecting a too specific type, e.g., forward (child of football-player in the hierarchy) instead of football-player. So, we decided to pick the node that has the highest branching factor (number of direct children) and at the deepest in the hierarchy.Based on ML Classifier: We developed a model based on random forest. We used all the Wordnet types present in the entity\u2019s hierarchy as features.\n",
            "cite_spans": [],
            "section": "Concise Entity-Type Extraction ::: Current Work",
            "ref_spans": []
        },
        {
            "text": "We aim to develop a relevant types prediction model based on Graph Neural Network [15]. More specifically, we utilize the concept of Graph Convolutional Network (GCN) [10]. While implementation, we faced the following challenges:In GCN, Readout function [15] is used to get embedding for the graph based on an aggregation of node features from the final iteration. Finding the suitable Readout function for our task is one of the main challenges.Entity\u2019s graph is a sub-graph of YAGO Wordnet hierarchy. Only delivering the structure of the sub-graph is not sufficient. It needs label information along with the structure of sub-graph. Encoding node label is our next challenge. One hot encoding is one of the solutions for giving the label information.\n",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 85,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 168,
                    "end": 170,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 255,
                    "end": 257,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Concise Entity-Type Extraction ::: Current Work",
            "ref_spans": []
        },
        {
            "text": "Based on our proposed research and current work progress, we find the following challenges to handle in the near future:In the early stage of our experiments, we realise that some of the types within a category are very hard to predict, e.g., in person category - there are entities with ground truth types \u2018intellectual\u2019 or \u2018military officer\u2019 where the model fails to predict it correctly. Our challenge is to find the common patterns among these sub-categories and to propose the solution for this failure.Our next challenge is to develop a gold standard for task 2 (RQ2).Our last challenge is to develop method for types prediction in a given context.\n",
            "cite_spans": [],
            "section": "Challenges and Next Steps",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "FIGREF0": {
            "text": "Fig. 1.: Conceptual overview of the CALVADOS pipeline",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "DBpedia-a large-scale, multilingual knowledge base extracted from Wikipedia",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Semant. Web J.",
            "volume": "6",
            "issn": "",
            "pages": "167-195",
            "other_ids": {
                "DOI": [
                    "10.3233/SW-140134"
                ]
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "YAGO: a multilingual knowledge base from Wikipedia, Wordnet, and Geonames",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Rebele",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Suchanek",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hoffart",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Biega",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kuzey",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Weikum",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The Semantic Web \u2013 ISWC 2016",
            "volume": "",
            "issn": "",
            "pages": "177-185",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "AGDISTIS - graph-based disambiguation of named entities using linked data",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Usbeck",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "The Semantic Web \u2013 ISWC 2014",
            "volume": "",
            "issn": "",
            "pages": "457-471",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Event oriented dictionary learning for complex event detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Image Process.",
            "volume": "24",
            "issn": "6",
            "pages": "1867-1878",
            "other_ids": {
                "DOI": [
                    "10.1109/TIP.2015.2413294"
                ]
            }
        },
        "BIBREF9": {
            "title": "Computational fact checking from knowledge networks",
            "authors": [
                {
                    "first": "GL",
                    "middle": [],
                    "last": "Ciampaglia",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "PLoS ONE",
            "volume": "10",
            "issn": "6",
            "pages": "1-13",
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0128193"
                ]
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Semantic fingerprinting: a novel method for entity-level content classification",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Govind",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Alec",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Spaniol",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ICWE 2018",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "CALVADOS: a tool for the semantic analysis and digestion of web contents",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Govind",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Alec",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Spaniol",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ESWC 2019",
            "volume": "",
            "issn": "",
            "pages": "1-6",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}