{
    "paper_id": "PMC7148207",
    "metadata": {
        "title": "Recognizing Semantic Relations: Attention-Based Transformers vs. Recurrent Models",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Dmitri",
                "middle": [],
                "last": "Roussinov",
                "suffix": "",
                "email": "dmitri.roussinov@strath.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Serge",
                "middle": [],
                "last": "Sharoff",
                "suffix": "",
                "email": "s.sharoff@leeds.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Nadezhda",
                "middle": [],
                "last": "Puchnina",
                "suffix": "",
                "email": "np486061@tlu.ee",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "During the last few years, Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have resulted in major breakthroughs and are behind the current state-of-the-art algorithms in language processing, computer vision, and speech recognition [9]. Meanwhile, modeling higher level abstract knowledge still remains a challenging problem even for them. This includes classification of semantic relations: given a pair of concepts (words or word sequences) to identify the best semantic label to describe their relationship. The possible labels are typically \u201cis a\u201d, \u201cpart-of\u201d, \u201cproperty-of\u201d, \u201cmade-of\u201d, etc. This information is useful in many applications. For example, knowing that London is a city is needed for a Question Answering system to answer the question What cities does the River Thames go through? Information retrieval benefits from query expansion with more specific words, e.g. transportation disasters\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}\nrailroad disasters. For the task of database federation, an attribute in one database (e.g. with values France, Germany, and UK) often needs to be automatically matched with an attribute called country in another database. Knowing the semantic relations allows large-scale knowledge base construction [11, 23, 33], automated inferencing [6, 29], query understanding [31], post-search navigation [7], and personalized recommendation [34]. The connection between word meanings and their usage is prominent in the theories of human cognition [12] and human language acquisition [2]. While manually curated dictionaries exist, they are known to be out-of-date, not covering specialized domains, designed to be used by people, and exist for only a few well resourced languages (English, German, etc.). Therefore, here we are interested in methods for automated discovery (knowledge acquisition, taxonomy mining, etc.). As our Sect. 2 elaborates, this problem has been a subject of extensive exploration for more than three decades. Our results here suggest that knowledge transfer, that was recently demonstrated to be useful for the other tasks, can be also successfully applied to recognizing semantic relations leading to substantial performance improvements and needing much less training data.",
            "cite_spans": [
                {
                    "start": 258,
                    "end": 259,
                    "mention": "9",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 1531,
                    "end": 1533,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1535,
                    "end": 1537,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1539,
                    "end": 1541,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1567,
                    "end": 1568,
                    "mention": "6",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1570,
                    "end": 1572,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1596,
                    "end": 1598,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1625,
                    "end": 1626,
                    "mention": "7",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1662,
                    "end": 1664,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1769,
                    "end": 1771,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1805,
                    "end": 1806,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The automated approaches to detecting semantic relations between concepts (words or phrases) can be divided into two major groups: (1) path-based and (2) distributional methods. Path-based approaches (e.g. [25]) look for certain patterns in the joint occurrences of words (phrases, concepts, etc.) in the corpus. Thus, every word pair of interest (x,y) is represented by the set of word paths that connect x and y in a raw text corpus (e.g. Wikipedia). Distributional approaches (e.g. [30]) are based on modeling the occurrences of each word, x or y, separately, not necessary in the proximity to each other. Our goal here is to improve and compare both classes of approaches.",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 209,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 486,
                    "end": 488,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Attention-based transformers (e.g. [28]) have been recently shown more effective than convolutional and recurrent neural models for several natural text applications, leading to new state-of-the-art results on several benchmarks including GLUE, MultiNLI, and SQuAD [4, 8]. At the same time, we are not aware of any applications of attention-based transformers to the task of recognizing semantic relations, so we are the first to successfully apply them to this task. Thus, our contributions are as follows:",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 38,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 266,
                    "end": 267,
                    "mention": "4",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 269,
                    "end": 270,
                    "mention": "8",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "(1) We develop a novel neural path-based model that combines useful properties of convolutional and recurrent networks. Our approach resolves several shortcomings of the prior models within that type. As a result, it outperforms the state-of-the art path-based approaches on 3 out of 6 well known benchmarking datasets, and on par on the other 3. (2) Our distributional approach worked better than our neural path-based model and outperformed current state-of-the-art by 1\u201312% points (15\u201340% error reduction) on 4 out of 6 (same) standard datasets, and on par on the remaining 2. (3) We show that the datasets that are not improved are those where we have already reached the human performance. (4) We illustrate that even our best model still has certain limitations which are not always revealed by the standard datasets.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We make our code and data publicly available.1 The next section overviews the prior related work. It is followed by the description of the models, followed by our empirical results.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The approaches to automatically classifying semantic relations between words can be divided into two major groups: (1) path-based and (2) distributional. Path-based approaches look for certain patterns in the joint occurrences of words (phrases, concepts, etc.) in some validation text corpus. Thus, every word pair of interest (x,y) is represented by the set of word paths that connect x and y in a raw text corpus (e.g. Wikipedia). The earliest path-based approach is typically attributed to \u201cHearst Patterns\u201d [5] \u2013 a set of 6 regular expressions to detect \u201cis-a\u201d relations (e.g. Y such as X). Later works successfully involved trainable templates and larger texts (e.g. [18, 27]). However, a major limitation in relying on patterns in the word paths is the sparsity of the feature space [14]. Distributed representation do not have such limitations, thus with deep neural representations (\u201cembeddings\u201d, e.g. [13, 17]) becoming popular, a number of successful models were developed that used word embeddings as features (concatenation, dot product or difference) and surpassed the path-based methods in performance [15, 20], to the point that the path-based approaches were perceived not be adding anything to the distributional ones.",
            "cite_spans": [
                {
                    "start": 513,
                    "end": 514,
                    "mention": "5",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 674,
                    "end": 676,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 678,
                    "end": 680,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 791,
                    "end": 793,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 912,
                    "end": 914,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 916,
                    "end": 918,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1118,
                    "end": 1120,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1122,
                    "end": 1124,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "However, [10] noted that supervised distributional methods tend to perform \u201clexical memorization:\u201d instead of learning a relation between the two terms, they learn an independent property of a single term in the pair. For example, if the training set contains pairs such as (dog, animal), (cat, animal), and (cow, animal), the algorithm learns to classify any new (x, animal) pair as true, regardless of x. Shwartz et al. [24, 25] (one of our baselines) successfully combined distributional and path-based approaches to improve the state-of-the performance, and thus proving that path-based information is also crucial for that. In their approach, each word path connecting a pair of concepts (X, Y) is mapped by an RNN into a context vector. Those vectors are averaged across all existing paths and fed to a two-layer fully connected network.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 12,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 423,
                    "end": 425,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 427,
                    "end": 429,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "There have been several related studies following [24]: [26] did extensive comparison of supervised vs. unsupervised approaches to detecting \u201cis-a\u201d relation. [32] looked at how additional word paths can be predicted even if they are not in the corpus [19] also looked at \u201cis-a\u201d relation and confirmed the importance of modeling word paths in addition to purely distributional methods. Still, the results in [24, 25] remain unsurpassed within the class of word-path models. Our baseline for distributional approaches is [30], who suggested using hyperspherical relation embeddings and improved the results of [24] on 3 out of 4 datasets.\n",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 53,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 57,
                    "end": 59,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 159,
                    "end": 161,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 252,
                    "end": 254,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 408,
                    "end": 410,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 412,
                    "end": 414,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 520,
                    "end": 522,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 609,
                    "end": 611,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Intuitive Description. Our proposed path-based neural model combines useful properties of convolutional and recurrent networks, while resolving several shortcomings of the current state-of-the-art model [25] as we explain below. Figure 1 presents an informal intuitive illustration. We jointly train our semantic classification along with an unsupervised language modeling (LM) task.",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 206,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": [
                {
                    "start": 236,
                    "end": 237,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The output of LM is the probability of occurrence of any input word sequence. We use some of those probabilities as features for our relation classification model. Inspired by the success of convolutional networks (CNNs), we use a fixed set of trainable filters (also called kernels), which learn to respond highly to certain patterns that are indicative of specific semantic relations. For example, a specific filter \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} can learn to respond highly to is a (and similar) patterns. At the same time, our recurrent LM may suggest that there is a high probability of occurrence of the sequence green is a color in raw text corpus. Combining those two facts suggests that green belongs to the category color (true is-a relation between them). Figure 1 shows only three convolutional filters (and the probabilities of the sequences \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P_1,P_2,P_3$$\\end{document}), while in our current study we used up to 16.",
            "cite_spans": [],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": [
                {
                    "start": 1032,
                    "end": 1033,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Thus, the LM probabilities act as approximate (\u201csoft\u201d) pattern matching scores: (1) similar patterns receive similar scores with the same filter and (2) similar filters produce similar scores for the same pattern. LM also reduces the need for using many filters as explained by the following intuitive example: While training, LM can encounter many examples of sequences like green is a popular color and green is a relaxing color. By modeling the properties of a language, LM learns that removing an adjective in front of a noun does not normally result in a large drop of the probability of occurrence, so the sequence green is a color also scores highly even if it never occurs in the corpus.",
            "cite_spans": [],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "Since the current state-of-the art path-based approach [25] aggregates the word paths connecting each target pair by averaging the context vectors representing all the paths, we believe their approach has two specific drawbacks that our approach does not: (1) when averaging is applied, the different occurrences of word patterns are forced to compete against each other, so the more rare occurrences can be dominated by more common ones and their impact on classification decision neglected as a result. By using LM we avoid facing the question how to aggregate the context vectors representing each path existing in the corpus. (2) The other relative strength of our approach over the baseline comes from the fact that our model does not \u201canonymize\u201d the word paths unlike [25], which uniformly uses \u201cx\u201d and \u201cy\u201d for the path ends regardless of which words the target pair (x,y) actually represents. Without the use of LM, this anonymizing is unavoidable to generalize to the previously unseen (x,y) pairs, but it also misses the opportunity for the model to transfer knowledge from similar words.",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 58,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 775,
                    "end": 777,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "Formal Definitions. Language Model (LM) is a probability distribution over sequences of words: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p(w_1,...,w_m)$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_1, ...,w_m$$\\end{document} is any arbitrary sequence of words in a language. We train LM jointly with our semantic relation classification task by minimizing cross-entropy costs, equally weighted for both tasks. As nowadays de-facto standard for a LM, we use a recurrent neural network (specifically a GRU variation [3], which works as well as LSTM while being faster to train). Thus, the probability of a word \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_m$$\\end{document} in the language to follow a sequence of words \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_1,...,w_{m-1}$$\\end{document} is determined by using the RNN to map the sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_1,...,w_{m-1}$$\\end{document} into its context vector:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\overrightarrow{v}_{w_1,...,w_{m-1}}=\\text {RNN}(w_1,...,w_{m-1}) \\end{aligned}$$\\end{document}and then applying a linear mapping and the softmax function:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} p(w_m|w_1,...,w_{m-1}) =&\\nonumber \\\\ \\text {softmax}\\,{{(W \\cdot \\overrightarrow{v}_{w_1,...,w_{m-1}} +b)}}&\\end{aligned}$$\\end{document}where W is a trainable matrix, b is a trainable bias, and softmax is a standard function to scale any given vector of scores to probabilities.",
            "cite_spans": [
                {
                    "start": 990,
                    "end": 991,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "As any typical neural LM, our LM also takes distributed representations of words as inputs: all the words are represented by their trainable embedding vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_1,...,v_m$$\\end{document}.2 This is important for our model and allows us to treat LM as a function defined over arbitrary vectors\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p(v_m|v_1,...,v_{m-1})$$\\end{document}\nrather than over words.",
            "cite_spans": [],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "To classify semantic relations, we only look at the word paths that connect the target word pairs. Thus, we only make use of probabilities of the form \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p(v_y|v_x,v_1,...,v_k)$$\\end{document}, where (x, y) is one of the target pairs of words - those in the dataset that are used in training or testing the semantic relations, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(v_x, v_y)$$\\end{document} are their embedding vectors. The sequence of vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_1,...,v_k$$\\end{document} defines a trainable filter, and k is its size. While vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_1,...,v_k$$\\end{document} have the same dimensions as the word embeddings, they are additional parameters in the model that we introduce. They are trained with the other ones (word embeddings + RNN matrices + the decision layer) by back propagation. It is possible since due to the smoothness of a neural LM, the entire model is differentiable.",
            "cite_spans": [],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "Thus, we formally define the score of each of our convolutional filters (kernels) the following way:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} f_i=\\log p(v_y|v_x, v_1^i,...,v_k^i) \\end{aligned}$$\\end{document}where p() is determined by our language model as the probability of the word with the embedding vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_y$$\\end{document} to follow the sequence of words with the vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_x, v_1^i,...,v_k^i$$\\end{document}. We apply log in order to deal with high variation in the orders of magnitude of p(). Finally, we define the vector of filter scores by concatenating the individual scores: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{f} = [f_1,f_2,f_3,..f_N]$$\\end{document}, where N is the total number of filters (16 in our study here).",
            "cite_spans": [],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "Filter scores \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{f}$$\\end{document} are mapped into a semantic relation classification decision by using a neural network with a single hidden layer. Thus, we define:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\overrightarrow{h_1} = \\tanh ({W_2\\cdot \\overrightarrow{f}+b_2}) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_2$$\\end{document} is a trainable matrix and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b_2$$\\end{document} is a trainable \u201cbias\u201d vector. The classification decision is made based on the output activations:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} c = \\text {argmax}\\,{(W_3\\cdot \\overrightarrow{h_1}+b_3)} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_3$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b_3$$\\end{document} are also trainable parameters. As traditional with neural networks, we train to minimize the cross-entropy cost:6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} cost = - \\log (( \\text {softmax}\\,{{(W_3\\cdot \\overrightarrow{h_1}+b_3)) [c_l] ) }} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_l$$\\end{document} is the correct (expected) class label. We used stochastic gradient descent for cost minimization.\n",
            "cite_spans": [],
            "section": "Path-Based Neural Model ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "The diagram on Fig. 2 illustrates how attention-based transformer [28] operates. Instead of recurrent units with \u201cmemory gates\u201d essential for RNN-s, attention-based transformers use additional word positional embeddings which allows them to be more flexible and parallelizable than recurrent mechanisms which have to process a sequence in a certain direction. The conversions from the inputs to the outputs are performed by several layers, which are identical in their architecture, varying only in their trained parameters. In order to obtain the vectors on the layer above, the vectors from layer immediately below are simply weighted and added together. After that, they are transformed by a standard nonlinearity function. We use tanh:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\overrightarrow{v_i}' = \\tanh (W \\cdot \\sum _{t=1}^{k} \\alpha _t \\overrightarrow{v_{t}}) \\end{aligned}$$\\end{document}here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{v_i}'$$\\end{document} is the vector in the i-th position on the upper layer, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{v_{t}}$$\\end{document} is the vector in the t-th position on the lower layer, W is a trainable matrix (same regardless of i but different at different layers), and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _t$$\\end{document} is a trainable function of vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{v_i}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{v_t}$$\\end{document}, such as the weights for all \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{v_t}$$\\end{document} add up to 1. We use a scaled dot product of the vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{v_i}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{v_t}$$\\end{document}:8\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\alpha _t = \\overrightarrow{v_i} \\cdot W' \\cdot \\overrightarrow{v_t} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W'$$\\end{document} is a trainable matrix (also same regardless of i and t at the same layer but different at different layers). The normalization to 1 is accomplished by using a softmax function.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 69,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Distributional Model: Attention-Based Transformer ::: Compared Models for Semantic Relations",
            "ref_spans": [
                {
                    "start": 20,
                    "end": 21,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "This mechanism allows rich vector representations to be formed at the highest layers that can capture the entire content of a word sequence (e.g. a sentence or a word pair) so it can be effectively used for any AI applications such as text classification or generation. As it is commonly done with the transformers, we make our output classification decision based on the first vector on the top level. We do not use a hidden layer here, so we apply our formula 5 above to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_1$$\\end{document} defined as the following:9\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\overrightarrow{h_1} = \\overrightarrow{{v_0^{u}}} \\end{aligned}$$\\end{document}where {\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{{v_t^{u}}}\\}$$\\end{document} is the vector sequence produced by the transformer for the top level.\n",
            "cite_spans": [],
            "section": "Distributional Model: Attention-Based Transformer ::: Compared Models for Semantic Relations",
            "ref_spans": []
        },
        {
            "text": "Table 1 summarizes general statistics of the datasets. We used the same datasets as our baselines: the first two are from [25] and were built using a similar methodology: the relations used in them have been primarily taken from various sources including WordNet, DBPedia, Wikidata and Yago. Thus, their x-s are primarily named entities (places, films, music albums and groups, people, companies, etc.). The important difference is that in order to create the split between training, testing and validation sets for HypeNet Lexical, the lexical separation procedure was followed [10], so that there is no overlap in words (neither x nor y) between them. This reduces \u201clexical memorization\u201d effect mentioned above. The last four datasets are from [24], which originate from various preceding studies: K&H+N [15], BLESS [1], ROOT09 [20], EVALution [21]. Most of the relations for them were also taken WordNet. BLESS dataset also contains event and attribute relations, connecting a concept with a typical activity/property, e.g. (alligator, swim) and (alligator, aquatic). EVALution dataset contains the largest number of semantic relations including antonyms, e.g. (good, bad). To make our comparison more direct, we used exactly the same splits into training, development (validation) and testing subsets as in the baselines. We also used exactly the same word paths data, as it is made publicly available by the authors.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 125,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 580,
                    "end": 582,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 747,
                    "end": 749,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 807,
                    "end": 809,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 819,
                    "end": 820,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 831,
                    "end": 833,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 847,
                    "end": 849,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "The Datasets ::: Empirical Evaluation",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Since we sought to keep the number of hyper-parameters to the minimum, we set the word embedding size, the RNN context vector size, and the hidden layer size to be the same within all our path-based models. We tested their values in the {50,100,500,1000} set. This size is the only hyper-parameter that was varied in our experiments. We used the static learning rate of 0.01. As it is commonly done, we report the results computed on the test sets with the hyper-parameter and the number of training iterations that maximize the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_1$$\\end{document} scores on the validation sets, thus using exactly the same metrics and procedures as were used to obtained the baseline results: scikit-learn [16] with the \u201cweighted\u201d set-up, which computes the metrics for each relation, and reports their average, weighted by support (the number of true instances for each relation). For HypeNet datasets, that was accordingly set to \u201cbinary\u201d. We also verified through personal communications with the authors of [24] that our metrics are numerically identical for the same sets of predicted labels.",
            "cite_spans": [
                {
                    "start": 961,
                    "end": 963,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1266,
                    "end": 1268,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Experimental Setups ::: Empirical Evaluation",
            "ref_spans": []
        },
        {
            "text": "For our path-based models, all the trainable parameters were initialized by a normal distribution around 0 average and standard deviation of 1. We used the same transformer architecture and hyper-parameters as in [4] (BERT mono-lingual English uncased version) which has 12 layers and the output vector size of 768, resulting in the total number of trainable parameters of 110 million. As it is commonly done when using a pre-trained transformer, we initialize our weights to those that were already trained by [4] for a language model and next sentence prediction tasks on a copy of English Wikipedia text and the BookCorpus. For consistency with the data used during pre-training, we add the same special markers before, between and after our input word sequences x and y.\n",
            "cite_spans": [
                {
                    "start": 214,
                    "end": 215,
                    "mention": "4",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 512,
                    "end": 513,
                    "mention": "4",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Experimental Setups ::: Empirical Evaluation",
            "ref_spans": []
        },
        {
            "text": "Table 2 presents our results. For additional comparison, we also include \u201cBefore Baseline\u201d row, which lists the baselines used in [24, 25]. For HypeNet Random and Evalution datasets, we put the larger values that we obtained in our re-implementation of the distributional methods that they used rather than their reported values. The following can be observed: Our neural word path model has been able to improve the state-of-the-art on three (3) out of six (6) datasets: Hypenet L, Bless and Root09. The differences are statistically significant at the level of .01. On the remaining three (3) datasets (HypeNet Random, K&H+N and Evalution), our results are the same as with the baseline performance (no statistically significant difference at the level .05). The baseline did not improve on those datasets over the prior work either. The scores for HypeNet Random and K&H+N are already high due to \u201clexical memorization\u201d mentioned above. Since the compared models used exactly the same data, the obtained results clearly suggest that our neural model is better than the current state-of-the-art word-path model [24, 25].Our transformer-based model has also demonstrated tangible gains over state-of-the-art baselines regardless of the class of the approach (both path-based and distributional) on four (4) out of six (6) datasets by 1\u201312% points (15\u201340% error reduction). Those differences are statistically significant at the level of .01. There are no statistically significant differences on HypeNet Random and K&H+N. This suggests that an attention-based transformer is a very powerful mechanism for modeling semantic relations. Although they have been shown to be very effective in many other applications where knowledge transfer between tasks is essential, this is the first study that has used them for semantic relations.On four (4) out of six (6) datasets, our distributional model worked better than our neural word path model. The differences are statistically significant at the level of .01. There are no statistically significant differences on the remaining two.\n\n",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 133,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 135,
                    "end": 137,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1114,
                    "end": 1116,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1118,
                    "end": 1120,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Comparing Against the Baselines ::: Empirical Evaluation",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "We estimated the human performance on our datasets by giving 100 randomly selected word pairs to 3 independent graders, who were allowed to look up the meanings online (last row). It can be seen that the state-of-the-art approaches have already achieved the human level on the datasets where no improvement was detected (HypeNet Random and K&H+N), so this may explain why our approaches did not substantially improve them any further. Fig. 3 illustrates the effect of error reduction on the four datasets on which our approaches improved the state-of-the-art. For comparison, we plot the semantic relation classification error calculated as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$100 - F_1$$\\end{document} score rounded to the nearest integer. It can be seen that our approaches have approximately reduced the errors on those \u201cunsolved\u201d datasets half-way from the baseline to the human level. We believe that this result is truly remarkable!\n",
            "cite_spans": [],
            "section": "Comparing Against the Baselines ::: Empirical Evaluation",
            "ref_spans": [
                {
                    "start": 440,
                    "end": 441,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "For additional comparison, we also include our results along with the results of other recent works that looked at semantic relations classification even though those works did not claim to exceed the state-of-the-art approaches presented in [24, 25]. We report the metric of Average Precision (AP) used in those studies and the results on the two datasets (Bless and Evalution) also commonly used in them. We did not use the other datasets for comparison since they are much smaller and relying on manual part of speech tags (nouns, verbs, adjectives etc.). As Table 3 illustrates, our path-based model outperforms all but one, and our transformer-based model sizably exceeds all of those results reported.",
            "cite_spans": [
                {
                    "start": 243,
                    "end": 245,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 247,
                    "end": 249,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Comparing Against the Baselines ::: Empirical Evaluation",
            "ref_spans": [
                {
                    "start": 568,
                    "end": 569,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "We have also tested the influence of training size on the model by comparing its performance with 5%, 10%, 25%, 50% and 75% of randomly selected training subsets. Due to the size limit, we show only the results on Root09 (Fig. 4). The results suggest the importance of the dataset size and the possibility of further improvements when more training data is available for the path-based. At the same time, out transformer-based model needs much less training to reach its top possible performance. We also verified that all the components of our path-based model here are essential to exceed the baselines, specifically: using a hidden layer, using all the available word paths, using all 16 filters. Larger number of filters did not result in any gains, but increased the training time.\n",
            "cite_spans": [],
            "section": "Comparing Against the Baselines ::: Empirical Evaluation",
            "ref_spans": [
                {
                    "start": 227,
                    "end": 228,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "We also tried to play an adversarial role and fed more challenging pairs to the trained models to see when they are starting to fail. Our attention-based transformer model trained for HypeNet Lexical dataset (named entities mostly) erroneously classified all the 100 examples created by combining random general words and the word \u201cair\u201d (e.g. \u201ccar air\u201d, \u201ccircle air\u201d, \u201cnew air\u201d) as \u201cairline.\u201d It also erroneously classified all the 30 correct airline names that we tried as \u201cairports\u201d in addition to correctly classifying them as \u201cairline.\u201d The proportion of correct airline names classified as \u201crecording label\u201d was 60%, which is lower than for the correct category, but still alarmingly high. Meanwhile, general words (like \u201ccar\u201d, \u201cbook\u201d, \u201cnew\u201d, etc.) are very rarely classified as members of any categories in this dataset since the model correctly sees that they are not named entities. Those observations suggest that what the transformer actually learns for this datasets is to use the combined properties of a word sequence (n-gram) to check if it can possibly be a named entity, and then if it topically fits the category (e.g. \u201caviation\u201d in general). Those two conditions are sufficient to make a positive classification and to obtain high scores since very few test categories in the dataset are closely related (e.g. \u201cairport\u201d and \u201cairline\u201d). While our neural path models don\u2019t make such mistakes, their mistakes are primarily due to no word paths existing between the candidates in the training corpus, which was already noted in the related prior work. This suggests that a combination of those two approaches may provide additional gains over each. We have left more formal exploration of those observations for future studies.",
            "cite_spans": [],
            "section": "Comparing Against the Baselines ::: Empirical Evaluation",
            "ref_spans": []
        },
        {
            "text": "We have considered the task of automatically recognizing semantic relations between words (phrases, concepts, etc.) such as \u201cis a\u201d, \u201cpart of\u201d, \u201cproperty of\u201d, \u201copposite of\u201d etc., which is an important task affecting many applications including knowledge base construction, inference, query understanding, personalized recommendation and post-search navigation. Using six standard datasets, we have demonstrated that both distributional and word path state-of-the-art approaches can be tangibly improved. Out of those two approaches that we suggested, the transformer-based distributional approach worked significantly better. It has decreased the gap between the current strong baselines and human performance by roughly 50% for those datasets that still had room for improvement. We are not aware of any other work applying a pre-trained attention-based transformer (ABT) for this task. Since ABT-s are currently known to be the first practically useful mechanism for knowledge transfer between natural language tasks, our work paves the way to making knowledge transfer to be a default feature in any modern NLP tool.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "It will also lead to integrating training of the transformer with the semantic classification task on a deeper level, which can be accomplished by customizing its pre-training (weight-initialization) algorithm to include word semantic information available from existing taxonomies, which we are planning to undertake in future, along with experimenting with cross-lingual knowledge transfer (e.g. [22]), when a model uses English data to predict semantic relations in other, less resourced, languages.",
            "cite_spans": [
                {
                    "start": 399,
                    "end": 401,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: The relation types and statistics in each dataset.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_1$$\\end{document} scores of our tested models compared to the state-of-the-art baselines. Datasets: HyperNet Lexical (HL), Hypenet Random (HR), BLESS (B), ROOT09 (R), EVALution(E).\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Average Precision (AP) scores of our tested models compared to other recent strong baselines on binary category verification (\u201cis-a\u201d relation only).\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Our path-based neural approach to semantic relationship classification.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Attention-based transformer used in our distributional approach to semantic relationship classification.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Visual illustration of the error reduction relatively to the baseline and human performance on the tasks.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 4.: Using only portion of Root dataset for training.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Scikit-learn: machine learning in Python",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pedregosa",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Mach. Learn. Res.",
            "volume": "12",
            "issn": "",
            "pages": "2825-2830",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Finding next of kin: cross-lingual embedding spaces for related languages",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sharoff",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Nat. Lang. Eng.",
            "volume": "26",
            "issn": "2",
            "pages": "163-182",
            "other_ids": {
                "DOI": [
                    "10.1017/S1351324919000354"
                ]
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Hyperlex: a large-scale evaluation of graded lexical entailment",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Vulic",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gerz",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kiela",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Korhonen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Comput. Linguist.",
            "volume": "43",
            "issn": "4",
            "pages": "781-835",
            "other_ids": {
                "DOI": [
                    "10.1162/COLI_a_00301"
                ]
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "Deep learning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "LeCun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "GE",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Nature",
            "volume": "521",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/nature14539"
                ]
            }
        }
    }
}