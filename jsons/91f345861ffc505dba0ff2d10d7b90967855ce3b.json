{
    "paper_id": "91f345861ffc505dba0ff2d10d7b90967855ce3b",
    "metadata": {
        "title": "Personalized Video Summarization Based Exclusively on User Preferences",
        "authors": [
            {
                "first": "Costas",
                "middle": [],
                "last": "Panagiotakis",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Hellenic Mediterranean University",
                    "location": {
                        "postCode": "72100",
                        "settlement": "Agios Nikolaos",
                        "country": "Greece"
                    }
                },
                "email": ""
            },
            {
                "first": "Harris",
                "middle": [],
                "last": "Papadakis",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Hellenic Mediterranean University",
                    "location": {
                        "postCode": "71004",
                        "settlement": "Heraklion",
                        "country": "Greece"
                    }
                },
                "email": ""
            },
            {
                "first": "Paraskevi",
                "middle": [],
                "last": "Fragopoulou",
                "suffix": "",
                "affiliation": {},
                "email": "fragopou@ics.forth.gr"
            }
        ]
    },
    "abstract": [
        {
            "text": "We propose a recommender system to detect personalized video summaries, that make visual content interesting for the subjective criteria of the user. In order to provide accurate video summarization, the video segmentation provided by the users and the features of the video segments' duration are combined using a Synthetic Coordinate based Recommendation system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Video summarization is an application of recommender systems [9, 13] that generally aims at providing users with targeted information about items that might interest them. Recommender systems are also used to provide users with suggestions for various entities such as e-shop items, web pages, news, articles, movies, music, hotels, television shows, books, restaurants, friends, etc.",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 64,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 65,
                    "end": 68,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we study the problem of personalized video summarization without an priori knowledge of the video categories. According to our knowledge, this is the first work that solves the personalized video summarization based exclusively on user preferences for a given dataset of videos. In order to solve this problem, we propose a video segmentation method that yields global video segments. The main contribution of this work is the proposed video segmentation method and the efficient combination of the video segments' duration attribute with the Synthetic Coordinate based Recommendation system (SCoR) [12] without the use complex audiovisual features.",
            "cite_spans": [
                {
                    "start": 613,
                    "end": 617,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The problem of content recommendation can be described as follows. Given a set U of users, a set I of items and a set R of user ratings for items, we need to predict ratings for user-item pairs which are not in R. One of the main recommender system techniques is similarity-based Collaborative Filtering [1] . Such algorithms are based on a similarity function which takes into account user preferences and outputs a similarity degree between pairs of users. Another important approach in recommender systems is Dimensionality Reduction. Each user or item in the system is represented by a vector. A user's vector is the set of his ratings for all items in the system (even those that have not been rated by the specific user). The Matrix Factorization method [5] that characterizes both items and users by vectors of latent factors inferred from item rating patterns, is also a Dimensionality Reduction technique. High correlation between item and user factors leads to a recommendation.",
            "cite_spans": [
                {
                    "start": 304,
                    "end": 307,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 760,
                    "end": 763,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In [12] , the SCoR recommender system has been proposed that assigns synthetic coordinates to users and items (nodes). SCoR assigns synthetic coordinates (vectors) to users and items as proposed in [2] , but instead of using the dot product, SCoR uses the Euclidean distance between a user and an item in the Euclidean space, so that, when the system converges, the distance between a user-item pair provides an accurate prediction of that user's preference for the item. SCoR has been also successfully applied to the distributed community detection problem [11] and to the interactive image segmentation problem [10] .",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 7,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 198,
                    "end": 201,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 559,
                    "end": 563,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 614,
                    "end": 618,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "A video summary usually includes the most important scenes and events from a video, with the shortest possible description. Many traditional video summarization approaches, which are not personalized, [8, 16] find a global optimal representation of a given video taking into account only its audiovisual features. As the given, video synopsis datasets and annotations increase, the computer vision community realized that the problem of video summarization can be also defined and solved separately for each user taking into account his preferences. Thus, the research on personalized video summarization is gaining increased attention recently [19] .",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 204,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 205,
                    "end": 208,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 645,
                    "end": 649,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "There exist supervised methods based on complex audiovisual features that can become personalized by training on annotations coming from a single user [18] . Other personalized methods use text queries [17] . They suffer, however, from the cold start problem, not being able to provide recommendations for users that are not in the training set. In addition, only a small number of examples per user are often available. This limits the class of possible methods to simple models that can be trained from a handful of examples [6] . More recent methods use a ranking formulation, where the goal is to score interesting video segments higher than non-interesting ones [4, 6, 14, 19] while combining audiovisual representation and user preferences. In [19] , a novel pairwise deep ranking model is proposed that employs deep learning in order to learn the relationship between highlighted and non-highlighted video segments. A two-stream network structure is developed by representing video segments from complementary information on the appearance of video frames and temporal dynamics across frames for video highlight detection. Rather than training one model per user, the model proposed in [6] is personalized via its inputs, which allows to effectively adapt its predictions, given only a few user-specific examples. To train this model, a large- scale dataset of users and GIFs is created, providing an accurate indication of their interests. In this work, we use the same dataset and a ranking formulation.",
            "cite_spans": [
                {
                    "start": 151,
                    "end": 155,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 202,
                    "end": 206,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 527,
                    "end": 530,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 667,
                    "end": 670,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 671,
                    "end": 673,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 674,
                    "end": 677,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 678,
                    "end": 681,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 750,
                    "end": 754,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1193,
                    "end": 1196,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this Section, the proposed personalized video summarization method is described. Figure 1 depicts the two stages of the proposed framework. In the first stage, each video is segmented into non overlapping segments according to the preferences of the users. In the second stage, the personalized rankings of the video segments are provided.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 92,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Personalized Video Summarization"
        },
        {
            "text": "The goal of video segmentation is to provide the candidate video segments that are included in the video summarization, significantly reducing the problem search space from the set of frames to the set of video segments. The simplest video segmentation is to use fixed segments (e.g. of 5 s duration) [6] . Several audiovisual based video summarization methods use shot detection [3] or other more complex temporal segmentation approaches [7, 19] to provide accurate (non-overlapping) video segmentation. In this work, since the audiovisual data are not taken into account, we take advantage of the user preferences in the training set to derive the video segmentation.",
            "cite_spans": [
                {
                    "start": 301,
                    "end": 304,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 380,
                    "end": 383,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 439,
                    "end": 442,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 443,
                    "end": 446,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Video Segmentation"
        },
        {
            "text": "Let F v be the union of segment borders (frames) in ascending order, that the users provide in the training set according to the proposed video highlights of video v. As the number of users increases, the frames of F v correspond to an over-segmentation of the given video v. So, in this work we simplify set F v , so that there is a minimum duration for each video segment, e.g. at least 1 s. To do so, we repetitively remove the frame f from F v according to Eq. 1, until the minimum segment length is at least 1 s.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Video Segmentation"
        },
        {
            "text": "and |v| is the video length. This equation selects the frame that corresponds to the shortest segment. In order to decide which of the two border frames of a segment should be eliminated, we also take into account the size of the longest neighbor segment (max(\u03b4 v (i), \u03b4 v (i + 1))), so that the frame in between the two shorter in duration video segments is selected. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Video Segmentation"
        },
        {
            "text": "Generally, it holds that the users select short video segments to be included in the proposed video synopsis (e.g. less than 20 s). In this work, we apply a statistical analysis approach with personalized components taking into account the average segment duration of a user (d u ), of a video (d v ), for dataset (d) and the standard deviation of the video segment duration in dataset (\u03c3). So, for a user u and an unseen (for that user) video v, the ranking function D(x i ) (see Fig. 2 ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 481,
                    "end": 487,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Video Segments Duration"
        },
        {
            "text": "where CDF \u03bc,\u03c3 is the Cumulative Gamma distribution function with mean value \u03bc = du+dv+d 3 + 3 \u00b7 \u03c3 and standard deviation \u03c3. The popular two-parameter Gamma distribution is selected, since it is defined only for positive values, such as the duration attribute. The positive parameter \u03bb (e.g. \u03bb = 0.05) and the addition of 3 \u00b7 \u03c3 is used to relax the effect of the duration attribute to the whole ranking process, since it is a complementary feature in the final decision process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Video Segments Duration"
        },
        {
            "text": "In the final stage of the proposed method, the video segments are ranked by combining the segment duration based on the ranked function D(x i ) and the ranking of video segments provided by the SCoR system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ranking Video Segments"
        },
        {
            "text": "Similarly to [12] , in order to train SCoR, we get all video segments (see Sect. 3.1) of each video v that have been summarized by user u. Let [F v (i), F v (i \u2212 1)] be the video segment i of video v, then the recommendation R u (i) of user u for this segment, that is used to train the SCoR, is given by the percentage of the video segment frames [F v (i), F v (i \u2212 1)] that belong to the video summary that user u provides. This means that R u (i) \u2208 [0, 1].",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Ranking Video Segments"
        },
        {
            "text": "SCoR [12] assigns synthetic coordinates to users and items (video segments), so that the distance between a user and a video segment provides an accurate prediction of the user preference for that video segment. The lowest ranking value (recommendation) is assigned a distance of 1, whereas the highest ranking value is assigned a distance of 0. When the system converges, users and video segments have been placed in the same multi-dimensional Euclidean space. Let p(u) and p(i), be the position of user u and video segment i in this space. Then, for a pair of user u and video segment i, SCoR is able to provide a recommendation SCOR u (i) = max(0, 1 \u2212 ||p(u) \u2212 p(i)|| 2 ). The final personalized recommendation F R u (i) \u2208 [0, 1] is given by the product of SCoR and the duration based recommendations:",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Ranking Video Segments"
        },
        {
            "text": "The denomination of Eq. 3, normalizes the final recommendation F R u (i) so that its maximum value is one. Figure 2 (b) depicts an example of SCOR u (i) (left) and F R u (i) (right) recommendation for a given video.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 107,
                    "end": 115,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Ranking Video Segments"
        },
        {
            "text": "In our experimental results, we included the proposed method (SCOR \u2212 D) and two methods from the literature (P HD \u2212 CA + SV M \u2212 D [6] and V ideo2GIF [4] ) and the following three variants of the proposed method: To obtain personalized video highlight data, we have used the large scale dataset proposed in [6] , that contains 13,822 users and 222,015 annotations on 119,938 YouTube videos. Due to the fact that our method is only based on user preferences, we keep users and videos with at least five annotations in order to be able to provide recommendations (cold start problem). The resulting dataset consists of 1822 users and 6347 annotations on 381 videos with 129,890 candidate video segments under the proposed video segmentation with variable segment lengths, and 199,462 video segments with fixed, 5 s, segment length. The dataset was randomly separated into training and test sets, as proposed in [6] . In the test set, we included annotations from 191 users concerning their last (191) annotated videos (50% of the given videos).",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 133,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 149,
                    "end": 152,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 306,
                    "end": 309,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 908,
                    "end": 911,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "To evaluate the performance of the video summarization methods, we report the mean Average Precision (mAP ) [14] and the Normalized Meaningful Summary Duration (NMSD) [6] . NMSD rates how much of the video has to be watched before the majority of the ground truth selection is shown, given that the frames in the video are re-arranged in descending order of their predicted recommendation scores. In addition, we report the F 1 score that is computed by comparing the ground truth selection with the video summary of the same length (recall = precision) that is created by adding frames in descending order of their predicted recommendation scores. Thus, the F 1 score measures the percentage of the video summary that belongs to the ground truth selection. Table 1 presents the average mAP , nM SD and F 1 score. It holds that the proposed method SCOR \u2212 D clearly outperforms all the remaining methods under any evaluation metric. The importance of the duration attribute and the proposed variable length video segmentation is verified by comparing the results of the proposed method against SCOR and SCOR \u2212 F IX, respectively. The F 1 score of the proposed method is 9% and 13% higher than the F 1 score of SCOR and SCOR \u2212 F IX, respectively. SCOR is the second method in performance, while SCOR \u2212 F IX is the third one, under any evaluation metric. Finally, it should be noted that the performances of P HD \u2212 CA + SV M \u2212 D and V ideo2GIF have been obtained in the whole dataset of [6] , so they are not directly comparable with the other methods.",
            "cite_spans": [
                {
                    "start": 108,
                    "end": 112,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 167,
                    "end": 170,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1484,
                    "end": 1487,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 758,
                    "end": 765,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "In this work, we presented a methodology to detect personalized video highlights without taking into account audiovisual features. The proposed method efficiently uses known user preferences to derive a video segmentation and it combines the segment duration attribute with the SCoR recommender system [12] , yielding accurate personalized video summarization. According to our experimental results, the proposed system outperforms other variants and methods from literature. The proposed methodology can be extended to include rich audiovisual features [15] , in order to be able to provide personalized user summaries even for unseen videos.",
            "cite_spans": [
                {
                    "start": 302,
                    "end": 306,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 554,
                    "end": 558,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Improving aggregate recommendation diversity using ranking-based techniques",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Adomavicius",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kwon",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "24",
            "issn": "5",
            "pages": "896--911",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Generalized Hebbian algorithm for incremental singular value decomposition in natural language processing",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gorrell",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference",
            "volume": "",
            "issn": "",
            "pages": "3--7",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Ridiculously fast shot boundary detection with fully convolutional neural networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gygli",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 International Conference on Content-Based Multimedia Indexing (CBMI)",
            "volume": "",
            "issn": "",
            "pages": "1--4",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Video2gif: automatic generation of animated gifs from video",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gygli",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1001--1009",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Matrix factorization techniques for recommender systems",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Koren",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bell",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Volinsky",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Computer",
            "volume": "42",
            "issn": "8",
            "pages": "30--37",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Phd-gifs: personalized highlight detection for automatic gif creation",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Del Molino",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gygli",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.06604"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Video segmentation using minimum ratio similarity measurement",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Acharjee",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rudrapaul",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Ashour",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dey",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Image Min",
            "volume": "1",
            "issn": "1",
            "pages": "87--110",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Equivalent key frames selection based on iso-content principles",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Panagiotakis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Doulamis",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tziritas",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Trans. Circuits Syst. Video Technol",
            "volume": "19",
            "issn": "3",
            "pages": "447--451",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Detection of hurriedly created abnormal profiles in recommender systems",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Panagiotakis",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Papadakis",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fragopoulou",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Intelligent Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Interactive image segmentation based on synthetic graph coordinates",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Panagiotakis",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Papadakis",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Grinias",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Komodakis",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fragopoulou",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tziritas",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Pattern Recogn",
            "volume": "46",
            "issn": "11",
            "pages": "2940--2952",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Distributed detection of communities in complex networks using synthetic coordinates",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Papadakis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Panagiotakis",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fragopoulou",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "J. Stat. Mech: Theory Exp",
            "volume": "2014",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Scor: a synthetic coordinate based system for recommendations",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Papadakis",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Panagiotakis",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fragopoulou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Expert Syst. Appl",
            "volume": "79",
            "issn": "",
            "pages": "8--19",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Recommender systems: introduction and challenges",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ricci",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Rokach",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Shapira",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Recommender Systems Handbook",
            "volume": "",
            "issn": "",
            "pages": "1--34",
            "other_ids": {
                "DOI": [
                    "10.1007/978-1-4899-7637-6_1"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Ranking domain-specific highlights by analyzing edited videos",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Seitz",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ECCV 2014",
            "volume": "8689",
            "issn": "",
            "pages": "787--802",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-10590-1_51"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Learning spatiotemporal features with 3d convolutional networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bourdev",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Torresani",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Paluri",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "4489--4497",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Video abstraction: a systematic review and classification",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "T"
                    ],
                    "last": "Truong",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Venkatesh",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "ACM Trans. Multimedia Comput. Commun. Appl. (TOMM)",
            "volume": "3",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Query-adaptive video summarization via quality-aware relevance estimation",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "B"
                    ],
                    "last": "Vasudevan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gygli",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Volokitin",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 ACM on Multimedia Conference",
            "volume": "",
            "issn": "",
            "pages": "582--590",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Gaze-enabled egocentric video summarization via constrained submodular maximization",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mukherjee",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Warner",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Rehg",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2235--2244",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Highlight detection with pairwise deep ranking for firstperson video summarization",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mei",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Rui",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "982--990",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The schema of the proposed system architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "(a) An example of the ranking function D(xi). (b) An example of SCORu(i) (left) and F Ru(i) (right) recommendations on a given video.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "SCOR: The variant of the proposed method that only uses the SCoR system. -SCOR \u2212 F IX: The variant of the proposed method that combines SCoR with fixed length (5 s, as proposed in [6]) video segmentation. -RAN DOM : Random summaries based on the proposed video segmentation.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Comparison with the state-of-the-art comparison Criteria PHD-CA + SVM-D Video2GIF SCOR-D SCOR SCOR-FIX RANDOM",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This research has been co-financed by the European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH -CREATE -INNO-VATE (project code: T1EDK-02147).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}