{
    "paper_id": "PMC7148224",
    "metadata": {
        "title": "Relevance Ranking Based on Query-Aware Context Analysis",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Ali",
                "middle": [],
                "last": "Montazeralghaem",
                "suffix": "",
                "email": "montazer@cs.umass.edu",
                "affiliation": {}
            },
            {
                "first": "Razieh",
                "middle": [],
                "last": "Rahimi",
                "suffix": "",
                "email": "rahimi@cs.umass.edu",
                "affiliation": {}
            },
            {
                "first": "James",
                "middle": [],
                "last": "Allan",
                "suffix": "",
                "email": "allan@cs.umass.edu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "In basic retrieval models such as BM25 [30] and the language modeling framework [29], the relevance score of a document is estimated based on explicit matching of query and document terms. These retrieval models have been improved in several directions; in this study, we focus on two of them: (1) semantic matching, and (2) simulating human relevance decision making.",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 42,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 81,
                    "end": 83,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "First, different choices of words between the authors of documents and users interested in those documents impose the long-standing challenge of term mismatch between query and documents. Basic retrieval models suffer from the term mismatch problem, since semantically related terms do not contribute to the relevance scores of documents.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Several techniques have been developed to address the term mismatch problem, including query expansion [10, 24, 31, 37, 39], latent models [4, 8, 14], and retrieval using distributed representations of words [13]. Query expansion techniques using global or local analysis of documents have shown improvements in the performance of retrieval models; however, these techniques suffer from query-independent analysis of documents in a large corpus or query drift [7], respectively. Latent models have been used for matching queries and documents represented in latent semantic space. Although semantic matching is required for information retrieval, exact matching, especially when query terms are new or rare, still provides strong evidence of relevance [23]. Thus, these latent models alone do not perform well for information retrieval [3]. Translation models were initially proposed to address the term mismatch problem in the language modeling framework for information retrieval. These models estimate the likelihood that a query can be generated as a translation of a given document. Ganguly et al. [13] used word embedding [22, 28] to estimate document language models through a noisy channel to address the term mismatch problem. Although there is a large body of research on semantic matching of terms in queries and documents, many studies fail to capture important IR heuristics such as proximity and term dependencies [21].",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 106,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 108,
                    "end": 110,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 112,
                    "end": 114,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 116,
                    "end": 118,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 120,
                    "end": 122,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 140,
                    "end": 141,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 143,
                    "end": 144,
                    "mention": "8",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 146,
                    "end": 148,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 209,
                    "end": 211,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 461,
                    "end": 462,
                    "mention": "7",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 753,
                    "end": 755,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 837,
                    "end": 838,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1104,
                    "end": 1106,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1129,
                    "end": 1131,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1133,
                    "end": 1135,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1429,
                    "end": 1431,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The second direction considers how people actually make relevance decisions. Relevance in almost all retrieval models is measured by comparing query terms with terms in the entire text of a document. This fundamental choice of input to scoring functions is not compatible with how a person perceives a document as relevant or non-relevant to his/her information need. This mismatch can lead to non-optimal performance of retrieval systems [18, 19]. Kong et al. [38] described that a person first tries to locate pieces of a document that are likely to be related to the query. For each piece, the person then makes relevance decision based on the local context of the piece. If the piece is found to be relevant to the query, the document is judged as relevant, otherwise other pieces are considered for evaluation. Surprisingly little attention has been given to relevance ranking based on simulating how a person makes relevance decisions. Wu et al. [36] proposed a retrieval model simulating human relevance decision making and using context of query terms, however their model is not based on semantic similarity between query and the context of query terms.",
            "cite_spans": [
                {
                    "start": 440,
                    "end": 442,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 444,
                    "end": 446,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 462,
                    "end": 464,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 953,
                    "end": 955,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Simulating human relevance decision making, we propose a novel model for semantic matching in information retrieval. The document\u2019s relevance to a query is thus estimated based on local contexts in the document. Local contexts for determining relevance consist of query terms\u2019 window-based pieces of text. These local contexts reduce the amount of texts considered for estimation of relevance to a query, while no information related to the query will be missed. Having local contexts, we compare each piece of text with the query based on both exact and semantic term matching. The proposed semantic matching model relaxes the assumption of independence between query terms to some extent, in that semantic similarity of terms in the local context of a query term is weighted by similarity of the query term with other query terms.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Our model can thus produce document ranking effectively and efficiently. Finally, our proposed model for relevance ranking provides the basis for natural integration of semantic term matching and local document context analysis into any retrieval model.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this section, we review the existing models for semantic term matching in information retrieval.",
            "cite_spans": [],
            "section": "Semantic Matching ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Query Expansion. Query expansion has a long history in information retrieval, where a query is expanded with terms relevant to query terms. Query expansion can be done using global or/and local analysis [37]. Global expansion methods use word associations obtained independent of queries, such as corpus-wide information [16] or external resources such as WordNet [33]. Local query expansion methods mainly analyze the top retrieved documents for a query. Pseudo-relevance feedback is a well-known model of automatic query expansion, and has shown improvements in the performance of retrieval. Several models for pseudo-relevance feedback has been developed [2, 20, 24\u201326, 31].",
            "cite_spans": [
                {
                    "start": 204,
                    "end": 206,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 322,
                    "end": 324,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 365,
                    "end": 367,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 659,
                    "end": 660,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 662,
                    "end": 664,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 666,
                    "end": 668,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 669,
                    "end": 671,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 673,
                    "end": 675,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Semantic Matching ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "In addition, some models for query expansion use word embeddings [1, 15]. Kuzi et al. find terms that are similar to the entire query or its terms using word embeddings and use them to expand query in the relevance model [17]. Diaz et al. [10] propose to use locally-trained embeddings for query expansion, where documents sampled from the top retrieved documents for queries are used to train word embeddings.",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 67,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 69,
                    "end": 71,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 222,
                    "end": 224,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 240,
                    "end": 242,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Semantic Matching ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Latent Models. Latent models represent queries and documents in a latent space of reduced dimensionality using the term-document matrix, such as LSA [9], PLSA [14], and LDA [4]. However, these models do not perform well for information retrieval [3]. Wei and Croft [35] use LDA topics to estimate document language models in the language modeling framework, and shown improvements in the performance of retrieval. Therefore, we compare our model with this LDA-based language model as a representative of this group.",
            "cite_spans": [
                {
                    "start": 150,
                    "end": 151,
                    "mention": "9",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 160,
                    "end": 162,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 174,
                    "end": 175,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 247,
                    "end": 248,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 266,
                    "end": 268,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Semantic Matching ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Embedding-Based Retrieval Models. Vuli\u0107 and Moens [34] use (bilingual) word embeddings for monolingual and bilingual information retrieval, where queries and documents are represented as an average of the embeddings of their terms. However, the proposed model did not improve the performance of retrieval, unless it is combined with a basic retrieval model. Zheng and Callan [41] proposed a supervised model for re-weighting query terms in traditional retrieval models, BM25 and language modeling framework, based on embeddings of query terms. Although this model has shown to improve the performance of traditional retrieval models, the retrieval is still based on only exact matching of query and document terms.",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 53,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 376,
                    "end": 378,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                }
            ],
            "section": "Semantic Matching ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Ganguly et al. [13] proposed a generalized estimate of document language models using a noisy channel, which captures semantic term similarities computed using word embeddings. In this model, words that are semantically related to any word of a document contribute to the new estimate of document language models, while we only consider words that are semantically related to terms in local contexts of documents. Zamani and Croft [39] show that their query expansion model outperforms the generalized language model [13], therefore we only report the result of embedding-based estimation of query language models.",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 18,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 432,
                    "end": 434,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 518,
                    "end": 520,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Semantic Matching ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Wu et al. [36] proposed a model for information retrieval by simulating how human makes relevance decisions. They consider context of query terms in documents to propose a novel retrieval method. They used related terms in context of query terms in document as expansion terms.",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 13,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                }
            ],
            "section": "Local Context Analysis ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Kong et al. [38] introduced three principles for combining relevance evidence from different pieces of a document to make the final relevance decision: (1) Disjunctive Relevance Decision (DRD) principle, (2) Aggregate Relevance (AR) principle, and (3) Conjunctive Relevance Decision (CRD) principle. Following TREC guideline for relevance judgment stating that a document is relevant if any piece of it is relevant to the query.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 15,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Local Context Analysis ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "We briefly introduce the information-based retrieval model based on the Log-logistic distribution, which is used in our proposed ranking model. Document scores in this model is computed as follow:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathrm {RSV}(Q,D) = \\sum _{w\\in Q} \\mathrm {count}(w,Q) \\log (\\frac{\\mathrm {tf}(w,D) + \\lambda _w}{\\lambda _w}), \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {tf}(w,D) = \\mathrm {count}(w,D) \\times \\log (1+c \\frac{\\mathrm {avdl}}{|D|}) $$\\end{document}, c is a free parameter, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {avdl}$$\\end{document} is average document length in the collection and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda _w = \\frac{N_w}{N}$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_w$$\\end{document} is the number of documents containing w and N is the number of documents in the collection.",
            "cite_spans": [],
            "section": "Log-Logistic Retrieval Model ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "The relevance ranking problem is to score a document \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${D=\\{d_1, d_2, \\dots , d_n\\}}$$\\end{document} with respect to a query \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q = \\{q_1, q_2, \\dots , q_m\\}$$\\end{document} based on a combination of exact and semantic matching between query and document terms.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "According to the TREC relevance judgment process, a document is relevant to a query if any piece of the document is relevant to the query1. Taking this definition, we first need to determine locations in the document that are likely related to the query.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "Determining Query-Related Locations. Wu et al. [36] proposed the query-centric assumption, which states that relevant information only occurs in the contexts around query terms in documents. The validity of this assumption is confirmed with a user study [18, 19]. Following this assumption, a local context of query term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} inside document D is denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C(q_i, D)$$\\end{document} and is determined by a window around one occurrence of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} in D. A symmetric window of size h centred at the occurrence of query term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} in the document, gives a local context of following document terms.2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} C(q_i, D) = [d_{j-h}, \\dots , d_j = q_i, \\dots , d_{j+h}]. \\end{aligned}$$\\end{document}Thus, each local context of a query term has a length of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2h+1$$\\end{document}. For simplicity, we refer to the local context of a query term in a document as the document context.",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 50,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 255,
                    "end": 257,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 259,
                    "end": 261,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "The next step is to estimate the relevance score of each query-centric context \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c(q_i, D)$$\\end{document} with respect to query Q. For this purpose, a scoring function based on exact and semantic matching is desired. In addition, the scoring function should satisfy the constraints defined on information retrieval models so that ranking based on these scores provides reasonable rankings [12]. The Log-logistic model [5] has shown to be effective for information retrieval [6]. Therefore, we derive our local context scoring function based on the Log-logistic model as follows:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} S(Q, C(q_i, D)) = \\sum _{q_j \\in Q} \\log (\\frac{\\mathrm {sim}(q_j, C(q_i, D)) + \\lambda _{q_j}}{\\lambda _{q_j}}), \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda _{q_j} = {N_{q_{j}}} \\big / {N}$$\\end{document} is computed based on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_{q_{j}}$$\\end{document} representing the number of documents in the collection containing \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_j$$\\end{document} and the total number of documents in the collection, N. This scoring function is obtained by replacing the normalized frequency of a query term in a document used in the Log-logistic scoring function of Eq. (1) by semantic similarity of the query term with terms in the document context. The similarity of term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_j$$\\end{document} with respect to the local context \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C(q_i, D)$$\\end{document} which is a short text, is then estimated based on4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathrm {sim}(q_j, C(q_i, D)) = \\sum _{w\\in C(q_i, D)} \\mathrm {sim}({q_j}, {w}) \\times \\mathbbm {1}_{(\\mathrm {sim}({q_j}, {w}) > \\theta )}, \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbbm {1}_{(.)}$$\\end{document} represents the indicator function taking on a value of 1 if the similarity between two terms is above the threshold parameter \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta $$\\end{document}, and 0 otherwise. This indicator function is added to filter out the impacts of words unrelated to the query. Using this estimation of similarity, exact occurrences of query terms as well as words semantically related to query terms contribute to the relevance scores of local contexts in Eq. (3). And when parameter \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta $$\\end{document} is set to the similarity value of a term to itself, the scoring function reduces to exact term matching.",
            "cite_spans": [
                {
                    "start": 658,
                    "end": 660,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 687,
                    "end": 688,
                    "mention": "5",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 743,
                    "end": 744,
                    "mention": "6",
                    "ref_id": "BIBREF37"
                }
            ],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "The underlying assumption in the scoring function of Eq. (3) as well as many well-established retrieval models is independence between query terms. However, the similarity between query-centric (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document}) and current query term (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document}) does not consider in this function. As an example, consider the query 303 of TREC Robust dataset, \u201cHubble telescope achievements\u201d. And assume that we want to score a local context of query term \u201cHubble\u201d which is a space telescope in a document. Thus, term \u201cHubble\u201d is in the center of this local context. Although terms \u201cHubble\u201d and \u201ctelescope\u201d should logically have a higher similarity degree than terms \u201cHubble\u201d and \u201cachievements\u201d in any term association resource, we believe occurrences of terms related to \u201cachievements\u201d in the local context makes it more likely to be relevant to the query than those of \u201ctelescope\u201d, because we already know that \u201cHubble\u201d exists in the context and occurring \u201ctelescope\u201d can not have much information of relevance compared to \u201cachievements\u201d. Therefore, to account for this observation, we proposed the following function to score a document context with respect to a query.5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} S(Q, C(q_i, D)) = \\sum _{q_j \\in Q} \\log (\\frac{\\mathrm {sim}(q_j, C(q_i, D)) + \\lambda _{q_j}}{\\lambda _{q_j}}) \\times \\mathrm {dis}(q_i, q_j), \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {dis}(q_i, q_j)$$\\end{document} denotes the semantic difference between the query term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} in the center of the local context and the current query term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_j$$\\end{document}. We add \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {dis}(q_i, q_j)$$\\end{document} to this function because we want to promote occurring query terms that semantically are far from query-centric. We compute \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {dis}(q_i, q_j) = a - \\mathrm {sim} ({q_i}, {q_j}), $$\\end{document} where a is a constant that its value is obtained as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a = 1 + \\mathrm {sim}(t,t)$$\\end{document}.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "Herein, we use word embeddings to compute term similarities. Therefore, the similarity of a query term with a document context in Eq. (4) is computed as follows:6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathrm {sim}(q_j, C(q_i, D)) = \\sum _{w\\in C(q_i, D)} \\cos (\\varvec{q_j}, \\varvec{w}) \\times \\mathbbm {1}_{(\\cos (\\mathbf {q_j}, \\mathbf {w}) > \\theta )}, \\end{aligned}$$\\end{document}where term vectors denote their embeddings in a continues space, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\cos (.)$$\\end{document} function computes the cosine similarity between two vectors. Accordingly, the dissimilarity between query terms is computed as:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathrm {dis}(q_i, q_j) = 2 - \\cos (\\varvec{q_i}, \\varvec{q_j}). \\end{aligned}$$\\end{document}the cosine similarity gives a value in the range of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[-1, 1]$$\\end{document} meaning that in case of perfect similarity (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\cos (\\varvec{q_i}, \\varvec{q_j}) =1$$\\end{document}) the dissimilarity is minimum \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {dis}(q_i, q_j) = 1$$\\end{document} (note that because we use Eq. 7 in Eq. 5, the minimum value of dissimilarity should not be 0) and when \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\cos (\\varvec{q_i}, \\varvec{q_j}) = -1$$\\end{document} dissimilarity is maximum i.e., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {dis}(q_i, q_j) = 3$$\\end{document}.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "Document Relevance Score. Obtaining the relevance score of each local context of query terms in a document, we then need to score the document with respect to the query based on the scores of its local contexts. We start by estimating the relevance score of a document with respect to each query term. Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\zeta (q_i, D)$$\\end{document} denote the set of all local contexts of query term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} in document D,8\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\zeta (q_i, D) = \\{C_{1}(q_i, D), C_{2}(q_i, D), \\dots , C_{k}(q_i, D) \\}, \\end{aligned}$$\\end{document}where k equals to frequency of term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} in the document, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {TF}(q_i, D)$$\\end{document}.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "The relevance scores of local contexts in the set \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\zeta (q_i, D)$$\\end{document} should be aggregated to estimate the relevance score of document D with respect to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document}, denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_L(q_i, D)$$\\end{document}. Following the aggregation principles introduced by Kong et al. [38], we consider two different aggregation function. The first disjunctive relevance decision principle indicates that a document is relevant if any one of its local contexts is relevant to the query. Accordingly, we estimate \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_L(q_i, D)$$\\end{document} using9\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} S_L(q_i, D) = \\max _{C \\in \\zeta (q_i,D)}S(Q, C(q_i,D)). \\end{aligned}$$\\end{document}Therefore, a document is scored according to its most relevant part to the query.",
            "cite_spans": [
                {
                    "start": 1096,
                    "end": 1098,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "The second aggregate relevance  principle states that a document with more pieces relevant to the query should get a higher relevance score. Following this relevance principle, we compute the relevance score of a document as10\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} S'_L(q_i, D) = \\sum _{C \\in \\zeta (q_i,D)}S(Q, C(q_i,D)). \\end{aligned}$$\\end{document}The \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\max $$\\end{document} aggregation function conforms to the TREC definition of document relevance2; a document is relevant if any part of the document is relevant, regardless of how small that part is compared to the entire document. We also observed better retrieval performance using this aggregation. Therefore, we adopt the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\max $$\\end{document} aggregation function for our proposed model and show the effectiveness of using the other function in one experiment.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "Normalizing Local Relevance Scores. Relevance scores of documents with respect to query terms in Eq. (9) are not theoretically bonded above, because we do not consider any normalization over semantic similarity of query term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_j$$\\end{document} with respect to the query-centric context in Eq. 4.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "A transformation function f(.) to normalize local relevance scores of documents needs to satisfy three constraints: (1) vanishes at 0, (2) upper bounded to 1, (3) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f'(x) > 0$$\\end{document} to make sure that as the value of x increases, the output of function also increases. The simple yet effective function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f(x) = \\frac{x}{x + \\sigma }$$\\end{document} used in multiple information retrieval models [27] satisfies the three mentioned constraints. Therefore, to normalize the local relevance scores, we use this function as follows:11\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} S_N(q_i, D) = \\frac{S_L(q_i,D)}{S_L(q_i,D) + \\sigma }, \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma > 0$$\\end{document} is a free parameter in this function.",
            "cite_spans": [
                {
                    "start": 939,
                    "end": 941,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "Final Document Scores. Having the relevance score of a document with respect to each query term, the final score of the document can be calculated. For this purpose, we use weighted sum of scores of each query term to consider the importance of each query term in ranking. Therefore, the final score of a document is estimated as follow:12\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} score(Q,D) = \\sum _{q_i\\in Q} S_N(q_i, D) \\times \\mathcal {W}(q_i,D), \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {W}(qi,D)$$\\end{document} is the importance of query term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document} given document D.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "Query Term Importance. Importance of each query term has two sides: (1) global importance which is mainly computed by the inverse document frequency (idf) of the term in the collection. (2) local importance which can be the weight of the query term in the document. Existing retrieval models provide document ranking based on these two factors. Therefore, we can use any basic retrieval function to weigh query terms. In the experiments section, we show the results of using BM25, language modeling framework, and Log-logistic models for weighting query terms.",
            "cite_spans": [],
            "section": "The Proposed Semantic Model",
            "ref_spans": []
        },
        {
            "text": "Computational Time: To compute the semantic similarity between query terms and document, we use the query-centric contexts. To do that, we first extract the positions of query terms in the document by Indri. Then, we find the neighbors of the query terms based on their positions in the document. By finding the neighbors of the query terms, we compute the semantic similarity between query terms and the query-centric contexts (i.e., the neighbors of the query terms). In other words, we do not consider all terms in a document to compute the semantic similarity. In contrast, the generalized language model [13] take into account all terms in the document to find the semantic similarity between the query and document which makes their approach so expensive. Note that our approach still is more effective since is more compatible with the human judgment process. Our approach is also more efficient compared to the topic modeling language model (LDA) [35], since they also use all terms in a document to model term associations. Also, the running time for each of the Gibbs sampling in LDA increases linearly with the number of documents N and the number of topics K i.e., O(NK) which makes their approach even more expensive compared to the generalized language model.",
            "cite_spans": [
                {
                    "start": 610,
                    "end": 612,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 956,
                    "end": 958,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Zamani and Croft [39] proposed an embedding query expansion named EQE1 to estimate query language model. They used word embedding similarities to find terms in the entire vocabulary that are semantically related to the query terms. This method is much faster than the previous ones (i.e., LDA model [35] and generalized language model [13]) but is not optimal since it needs to compute similarity scores between query terms and all terms in the vocabulary.",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 20,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 300,
                    "end": 302,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 336,
                    "end": 338,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Properties of the Proposed Method: Fang et al. [11], proposed seven constraints for IR models and showed that it is necessary to satisfy them to get good performance. To compute the semantic similarity score between a query-centric context and query terms, it is also necessary to satisfy these constraints. For example, if we have a query with two terms, and two query-centric contexts, the context that can interpret more distinct query words should be assigned a higher score. Clinchant et al. [6] showed that Log-Logistic model satisfies all constraints in the PRF framework. Therefore, we modify this model in our approach to compute semantic similarity between query-centric context and query terms. In other words, by modifying Log-Logistic model and considering query-centric context as a small document, we make sure that our model satisfies all constraints proposed by Fang et al. [11].",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 50,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 498,
                    "end": 499,
                    "mention": "6",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 892,
                    "end": 894,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Tao and Zhai [32], proposed proximity based constraints for IR models. There are similar constraints for pseudo relevance feedback [25]. They showed that by comparing two documents that match the same number of query words, it is more desirable to rank the document in which all query terms are close to each other above another one. We argue that this assumption also is valid in semantic space. By using the query-centric window, we implicitly promote documents that have query terms close in semantic or exact matching. Therefore, our method can capture important IR characteristics i.e. exact/semantic matching signals, proximity heuristics, and query term importance.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 16,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 132,
                    "end": 134,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "In this section, without loss of generality, we use three simple but effective retrieval functions including LM, BM25, and Log-Logistic in our model to weight query terms in Eq. (12), which aims to answer RQ1. The results of this experiment are reported in Table 1. According to this table, semantic matching in the local contexts of documents improves retrieval effectiveness in all cases. The improvements are statistically significant in most cases. This shows the effectiveness of our approach in integration of semantic matching into retrieval models. One can also observe that using Log-Logistic function for weighting the importance of query terms in our model outperforms using BM25 or the language model framework in most cases. This observation demonstrates that relevance scores of local contexts are more compatible with the scores of the Log-logistic model. Therefore, in the next experiments, we use this retrieval model as the weighing function in our model.\n\n",
            "cite_spans": [],
            "section": "Effectiveness of Different Weighting Functions ::: Experiments",
            "ref_spans": [
                {
                    "start": 263,
                    "end": 264,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "In this section, we compare our model with the baselines, which aims to address RQ2. The results of this experiment are reported in Table 2. According to this table, the proposed method (i.e., LCD-Logistic) outperforms SDM. This shows that our model in addition to using the proximity of query terms in a document improves the retrieval performance by exploiting the semantic similarity of terms. Unlike SDM, the EQE1 and LDA baseline models consider semantic similarity and topic modeling in document ranking, respectively. According to the results in this table, LCD-Logistic outperforms EQE1 and LDA-based retrieval models. We only report the results of the LDA model on Robust due to the prohibit training time of LDA on the other two collections. These comparisons show the importance of capturing semantic similarity in the local context for information retrieval. We also report GMAP to evaluate our method in confrontation with hard queries. We also report the recall metric in this table. According to the results, LCD-Logistic improves recall in Robust and Gov2 collections substantially.",
            "cite_spans": [],
            "section": "Performance of the Proposed Model ::: Experiments",
            "ref_spans": [
                {
                    "start": 138,
                    "end": 139,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "This section aims to answer RQ3. The results of this experiment are shown in Table 34. Comparing LCD and LCA variants of our model together shows that the LCD variant outperforms LCA in all cases. This means that using the most relevant local context of a document to score it, following the disjunctive relevance principle, has better performance.\n",
            "cite_spans": [],
            "section": "Different Principle Functions For combining Relevance Evidences ::: Experiments",
            "ref_spans": [
                {
                    "start": 83,
                    "end": 84,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Parameter Sensitivity Analysis: Fig. 1a shows the sensitivity of the proposed method to the similarity threshold in Eq. 4. According to this figure, the best value for the similarity threshold is 0.5 in all datasets. It is worth noting that by setting the value of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta $$\\end{document} to 1, we just consider exact matching in the local context of query terms. Figure 1b shows sensitivity of LCD-Logistic to the normalization parameter \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma $$\\end{document} in Eq. 11. According to this figure, the best value for this parameter in all collections is 10.",
            "cite_spans": [],
            "section": "Different Principle Functions For combining Relevance Evidences ::: Experiments",
            "ref_spans": [
                {
                    "start": 37,
                    "end": 38,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 640,
                    "end": 641,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We propose a new model for semantic matching in information retrieval. Our model is designed based on simulating human judgment process to find high-quality similarity scores between query and document. The proposed method is designed to be able to capture important IR heuristics, e.g, proximity of query terms in documents, semantic matching between query and document terms, and importance of query terms. We showed that our model can be integrated into any retrieval models and improve their performance significantly.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Performance of the proposed method with different retrieval models as the weighing function in Eq. (12). \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\blacktriangle $$\\end{document} indicates that the improvements over corresponding retrieval model are statistically significant.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Performance of proposed method and baselines. The superscript \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\blacktriangle $$\\end{document} indicates that the improvements over all other baselines are statistically significant.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Comparing different principle function for combining local context relevance scores on Gov2 only.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Sensitivity of the proposed method (LCD-Logistic) to the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta $$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma $$\\end{document}",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Diagnostic evaluation of information retrieval models",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "ACM Trans. Inf. Syst.",
            "volume": "29",
            "issn": "2",
            "pages": "7:1-7:42",
            "other_ids": {
                "DOI": [
                    "10.1145/1961209.1961210"
                ]
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Deep neural networks for query expansion using word embeddings",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Imani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vakili",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Montazer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shakery",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "203-210",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Automatic keyword classification for information retrieval",
            "authors": [
                {
                    "first": "KS",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "Libr. Q.",
            "volume": "41",
            "issn": "4",
            "pages": "338-340",
            "other_ids": {
                "DOI": [
                    "10.1086/619985"
                ]
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Iterative estimation of document relevance score for pseudo-relevance feedback",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ariannezhad",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Montazeralghaem",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zamani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shakery",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "676-683",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Latent semantic indexing (LSI) fails for TREC collections",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Atreya",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Elkan",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "SIGKDD Explor. Newsl.",
            "volume": "12",
            "issn": "2",
            "pages": "5-10",
            "other_ids": {
                "DOI": [
                    "10.1145/1964897.1964900"
                ]
            }
        },
        "BIBREF23": {
            "title": "Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval",
            "authors": [
                {
                    "first": "SE",
                    "middle": [],
                    "last": "Robertson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Walker",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "SIGIR 1994",
            "volume": "",
            "issn": "",
            "pages": "232-241",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Salton",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "The SMART Retrieval System-Experiments in Automatic Document Processing",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "Query expansion using lexical-semantic relations",
            "authors": [
                {
                    "first": "EM",
                    "middle": [],
                    "last": "Voorhees",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "SIGIR 1994",
            "volume": "",
            "issn": "",
            "pages": "61-69",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "A retrospective study of a hybrid document-context based retrieval model",
            "authors": [
                {
                    "first": "HC",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "RW",
                    "middle": [],
                    "last": "Luk",
                    "suffix": ""
                },
                {
                    "first": "KF",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kwok",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Inf. Process. Manag.",
            "volume": "43",
            "issn": "5",
            "pages": "1308-1331",
            "other_ids": {
                "DOI": [
                    "10.1016/j.ipm.2006.10.009"
                ]
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "Latent Dirichlet allocation",
            "authors": [
                {
                    "first": "DM",
                    "middle": [],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "AY",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "MI",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res.",
            "volume": "3",
            "issn": "",
            "pages": "993-1022",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "Indexing by latent semantic analysis",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Deerwester",
                    "suffix": ""
                },
                {
                    "first": "ST",
                    "middle": [],
                    "last": "Dumais",
                    "suffix": ""
                },
                {
                    "first": "GW",
                    "middle": [],
                    "last": "Furnas",
                    "suffix": ""
                },
                {
                    "first": "TK",
                    "middle": [],
                    "last": "Landauer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Harshman",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "J. Am. Soc. Inf. Sci.",
            "volume": "41",
            "issn": "6",
            "pages": "391-407",
            "other_ids": {
                "DOI": [
                    "10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9"
                ]
            }
        },
        "BIBREF40": {
            "title": "Indexing by latent semantic analysis",
            "authors": [
                {
                    "first": "SC",
                    "middle": [],
                    "last": "Deerwester",
                    "suffix": ""
                },
                {
                    "first": "ST",
                    "middle": [],
                    "last": "Dumais",
                    "suffix": ""
                },
                {
                    "first": "TK",
                    "middle": [],
                    "last": "Landauer",
                    "suffix": ""
                },
                {
                    "first": "GW",
                    "middle": [],
                    "last": "Furnas",
                    "suffix": ""
                },
                {
                    "first": "RA",
                    "middle": [],
                    "last": "Harshman",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "J. Am. Soc. Inf. Sci. Technol.",
            "volume": "41",
            "issn": "",
            "pages": "391-407",
            "other_ids": {
                "DOI": [
                    "10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9"
                ]
            }
        }
    }
}