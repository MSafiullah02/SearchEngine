{
    "paper_id": "158fa09fbea0154dbe707e4bf507117862bb4157",
    "metadata": {
        "title": "Deep Cost-Sensitive Kernel Machine for Binary Software Vulnerability Detection",
        "authors": [
            {
                "first": "Tuan",
                "middle": [],
                "last": "Nguyen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Melbourne",
                        "country": "Australia"
                    }
                },
                "email": "tuan.nguyen@monash.edu"
            },
            {
                "first": "Trung",
                "middle": [],
                "last": "Le",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Melbourne",
                        "country": "Australia"
                    }
                },
                "email": "trunglm@monash.edu"
            },
            {
                "first": "Khanh",
                "middle": [],
                "last": "Nguyen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AI Research Lab, Trusting Social",
                    "institution": "",
                    "location": {
                        "settlement": "Melbourne",
                        "country": "Australia"
                    }
                },
                "email": "khanh.nguyen@trustingsocial.com"
            },
            {
                "first": "Olivier",
                "middle": [],
                "last": "De Vel",
                "suffix": "",
                "affiliation": {},
                "email": "olivier.devel@dst.defence.gov.au"
            },
            {
                "first": "Paul",
                "middle": [],
                "last": "Montague",
                "suffix": "",
                "affiliation": {},
                "email": "paul.montague@dst.defence.gov.au"
            },
            {
                "first": "John",
                "middle": [],
                "last": "Grundy",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Melbourne",
                        "country": "Australia"
                    }
                },
                "email": "john.grundy@monash.edu"
            },
            {
                "first": "Dinh",
                "middle": [],
                "last": "Phung",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Monash University",
                    "location": {
                        "settlement": "Melbourne",
                        "country": "Australia"
                    }
                },
                "email": "dinh.phung@monash.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Owing to the sharp rise in the severity of the threats imposed by software vulnerabilities, software vulnerability detection has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security. Software vulnerability detection can be carried out at the source code or binary level. However, the latter is more impactful and practical since when using commercial software, we usually only possess binary software. In this paper, we leverage deep learning and kernel methods to propose the Deep Cost-sensitive Kernel Machine, a method that inherits the advantages of deep learning methods in efficiently tackling structural data and kernel methods in learning the characteristic of vulnerable binary examples with high generalization capacity. We conduct experiments on two real-world binary datasets. The experimental results have shown a convincing outperformance of our proposed method over the baselines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Software vulnerabilities are specific flaws or oversights in a piece of software that can potentially allow attackers exploit the code to perform malicious acts including exposing or altering sensitive information, disrupting or destroying a system, or taking control of a computer system or program. Because of the ubiquity of computer software and the growth and the diversity in its development process, a great deal of computer software potentially possesses software vulnerabilities. This makes the problem of software vulnerability detection an important concern in the software industry and in the field of computer security.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Software vulnerability detection consists of source code and binary code vulnerability detection. Due to a large loss of the syntactic and semantic information provided by high-level programming languages during the compilation process, binary code vulnerability detection is significantly more difficult than source code vulnerability detection. In addition, in practice, binary vulnerability detection is more applicable and impactful than source code vulnerability detection. The reason is that when using a commercial application, we only possess its binary and usually do not possess its source code. The ability to detect the presence or absence of vulnerabilities in binary code, without getting access to source code, is therefore of major importance in the context of computer security. Some work has been proposed to detect vulnerabilities at the binary code level when source code is not available, notably work based on fuzzing, symbolic execution [1] , or techniques using handcrafted features extracted from dynamic analysis [4] . Recently, the work of [10] has pioneered learning automatic features for binary software vulnerability detection. In particular, this work was based on a Variational Auto-encoder [7] to work out representations of binary software so that representations of vulnerable and non-vulnerable binaries are encouraged to be maximally different for vulnerability detection purposes, while still preserving crucial information inherent in the original binaries.",
            "cite_spans": [
                {
                    "start": 960,
                    "end": 963,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1039,
                    "end": 1042,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1067,
                    "end": 1071,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1224,
                    "end": 1227,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "By nature, datasets for binary software vulnerability detection are typically imbalanced in the sense that the number of vulnerabilities is very small compared to the volume of non-vulnerable binary software. Another important trait of binary software vulnerability detection is that misclassifying vulnerable code as non-vulnerable is much more severe than many other misclassification decisions. In the literature, kernel methods in conjunction with the max-margin principle have shown their advantages in tackling imbalanced datasets in the context of anomaly and novelty detection [13, 18, 21] . The underlying idea is to employ the max-margin principle to learn the domain of normality, which is decomposed into a set of contours enclosing normal data that helps distinguish normality against abnormality. However, kernel methods are not able to efficiently handle sequential machine instructions in binary software. In contrast, deep recursive networks (e.g., recurrent neural networks or bidirectional recurrent neural networks) are very efficient and effective in tackling and exploiting temporal information in sequential data like sequential machine instructions in binary software.",
            "cite_spans": [
                {
                    "start": 585,
                    "end": 589,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 590,
                    "end": 593,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 594,
                    "end": 597,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To cope with the difference in the severity level of the kinds of misclassification, cost-sensitive loss has been leveraged with kernel methods in some previous works, notably [2, 5, 12] . However, these works either used non-decomposable losses or were solved in the dual form, which makes them less applicable to leverage with deep learning methods in which stochastic gradient descent method is employed to solve the corresponding optimization problem.",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 179,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 180,
                    "end": 182,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 183,
                    "end": 186,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To smoothly enable the incorporation of kernel methods, cost-sensitive loss, and deep learning in the context of binary code vulnerability detection, we propose a novel Cost-sensitive Kernel Machine (CKM) which is developed based on the max-margin principle to find two optimal parallel hyperplanes and employs cost sensitive loss to find the best decision hyperplane. In particular, our CKM first aims to learn two parallel hyperplanes that can separate vulnerability and non-vulnerability, while the margin which is defined as the distance between the two parallel hyperplanes is maximized. The optimal decision hyperplane of CKM is sought in the strip formed by the two parallel hyperplanes. To take into account the difference in importance level of two kinds of misclassification, we employ a cost-sensitive loss, where the misclassification of vulnerability as non-vulnerability is assigned a higher cost. We conduct experiments over two datasets, the NDSS18 binary dataset whose source code was collected and compiled to binaries in [10, 15] and binaries compiled from 6 open-source projects, which is a new dataset created by us. We strengthen and extend the tool developed in [10] to allow it to be able to handle more errors for compiling the source code in the six open-source projects into binaries. Our experimental results over these two binary datasets show that our proposed DCKM outperforms the baselines by a wide margin.",
            "cite_spans": [
                {
                    "start": 1040,
                    "end": 1044,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1045,
                    "end": 1048,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The major contributions of our work are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We upgrade the tool developed in [10] to create a new real-world binary dataset. -We propose a novel Cost-sensitive Kernel Machine that takes into account the difference in incurred costs of different kinds of misclassification and imbalanced data nature in binary software vulnerability detection. This CKM can be plugged neatly into a deep learning model and be trained using backpropagation. -We leverage deep learning, kernel methods, and a cost-sensitive based approach to build a novel Deep Cost-sensitive Kernel Machine that outperforms state-of-the-art baselines on our experimental datasets by a wide margin.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 38,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "By incorporating deep learning and kernel methods, we propose a Deep Costsensitive Kernel Machine (DCKM) for binary software vulnerability detection. In particular, we use a bidirectional recurrent neural network (BRNN) to summarize a sequence of machine instructions in binary software into a representation vector. This vector is then mapped into a Fourier random feature space via a finitedimensional random feature map [9, 11, 14, 17, 19] . Our proposed Cost-sensitive Kernel Machine (CKM) is invoked in the random feature space to detect vulnerable binary software. Note that the Fourier random feature map which is used in conjunction with our CKM and BRNN enables our DCKM to be trained nicely via back-propagation. Figure 1 presents an overview of the code data processing steps required to obtain the core parts of machine instructions from source code. From the source code repository, we identify the code functions and then fix any syntax errors using our automatic tool. The tool also invokes the gcc compiler to compile compilable functions into binaries. Subsequently, utilizing the objdump 1 tool, we disassemble the binaries into assembly code. Each function corresponds to an assembly code file. We then process the assembly code files to obtain a collection of machine instructions and eventually use the Capstone 2 framework to extract their core parts. Each core part in a machine instruction consists of two components: the opcode and the operands, called the instruction information (a sequence of bytes in hexadecimal format, i.e., memory location, registers, etc.). The opcode indicates the type of operation, whilst the operands contain the necessary information for the corresponding operation. Since both opcode and operands are important, we embed both the opcode and instruction information into vectors and then concatenate them. To embed the opcode, we undertake some preliminary analysis and find that there were a few hundred opcodes in our dataset. We then build a vocabulary of the opcodes, and after that embed them using one-hot vectors to obtain the opcode embedding e op .",
            "cite_spans": [
                {
                    "start": 423,
                    "end": 426,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 427,
                    "end": 430,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 431,
                    "end": 434,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 435,
                    "end": 438,
                    "text": "17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 439,
                    "end": 442,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 723,
                    "end": 731,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Our Approach: Deep Cost-Sensitive Kernel Machine"
        },
        {
            "text": "To embed the instruction information, we first compute the frequency vector as follows. We consider the operands as a sequence of hexadecimal bytes (i.e., 00, 01 to F F ) and count the frequencies of the hexadecimal bytes to obtain a frequency vector with 256 dimensions. The frequency vector is then multiplied by the embedding matrix to obtain the instruction information embedding e ii . More specifically, the output embedding is e = e op e ii where e op = one-hot(op) \u00d7 W op and e ii = freq (ii) \u00d7 W ii with the opcode op, the instruction information ii, one-hot vector one-hot(op), frequency vector freq (ii), and the embedding matrices W op \u2208 R V \u00d7d and W ii \u2208 R 256\u00d7d , where V is the vocabulary size of the opcodes and d is the embedding dimension. The process of embedding machine instructions is presented in Fig. 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 820,
                    "end": 826,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Data Processing and Embedding"
        },
        {
            "text": "We now present the general framework for our proposed Deep Cost-sensitive Kernel Machine. As shown in Fig. 3 , given a binary x, we first embed its machine instructions into vectors (see Sect. 2.1); the resulting vectors are then fed to a Bidirectional RNN with the sequence lenght of L to work out the representation",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 102,
                    "end": 108,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "General Framework of Deep Cost-Sensitive Kernel Machine"
        },
        {
            "text": "L-th hidden states (the left and right last hidden states) of the Bidirectional RNN, respectively. Finally, the vector representation h is mapped to a random feature space via a random feature map\u03a6 (\u00b7) [19] where we recruit a cost-sensitive kernel machine (see Sect. 2.3) to classify vulnerable and non-vulnerable binary software. Note that the formulation for\u03a6 is as follows:",
            "cite_spans": [
                {
                    "start": 202,
                    "end": 206,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "the left and right"
        },
        {
            "text": "where \u03c9 1 , . . . , \u03c9 D are the Fourier random elements as in [19] and the dimension of random feature space is hence 2D. We note that the use of a random feature map in conjunction with costsensitive kernel machine and bi-directional RNN allows us to easily do backpropagation when training our Deep Cost-sensitive Kernel Machine. In addition, let us denote the training set of binaries and their labels by",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 66,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "the left and right"
        },
        {
            "text": "where x i is a binary including many machine instructions and y i \u2208 {\u22121; 1} where the label \u22121 stands for vulnerable binary and the label 1 stands for nonvulnerable binary. Assume that after feeding the binaries x 1 , . . . , x N into the corresponding BRNN as described above, we obtain the representations h 1 , . . . , h N . We then map these representations to the random feature space via the random feature map\u03a6 (\u00b7) as defined in Eq. (1). We finally construct a cost-sensitive kernel machine (see Sect. 2.3) in the random feature space to help us distinguish vulnerability against non-vulnerability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "the left and right"
        },
        {
            "text": "General Idea of Cost-Sensitive Kernel Machine. We first find two parallel hyperplanes H \u22121 and H 1 in such a way that H \u22121 separates the non-vulnerable and vulnerable classes, H 1 separates the vulnerable and non-vulnerable classes, and the margin, which is the distance between the two parallel hyperplanes H \u22121 and H 1 , is maximized. We then find the optimal decision hyperplane H d by searching in the strip formed by H \u22121 and H 1 (see Fig. 4 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 440,
                    "end": 446,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Cost-Sensitive Kernel Machine"
        },
        {
            "text": "Let us denote the equations of H \u22121 and H 1 by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "We arrive at the optimization problem:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "by a factor k > 0 as (kw, kb \u22121 , kb 1 ). Therefore, we can safely assume that b 1 \u2212 b \u22121 = 1, and hence the following optimization problem: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "are non-negative slack variables and \u03bb > 0 is the regularization parameter.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "The primal form of the soft model optimization problem is hence of the following form:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "Finding the Optimal Decision Hyperplane. After solving the optimization problem in Eq. (2), we obtain the optimal solution w * , b * \u22121 , b * 1 where b * \u22121 = a * and b * 1 = 1 + a * for the two parallel hyperplanes. Let us denote the strip S formed by the two parallel hyperplanes and the set of training examples I in this strip as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "where u, v lie in the random feature space R 2D . Fig. 4 . Cost-sensitive kernel machine in the random feature space. We first find two optimal parallel hyperplanes H1 and H\u22121 with maximal margin and then search for the optimal decision hyperplane in the strip S formed by H1 and H\u22121 to balance between precision and recall for minimizing the cost-sensitive loss and obtaining a good F1 score.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 50,
                    "end": 56,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "As shown in Fig. 4 , when sliding a hyperplane from H 1 to H \u22121 , the recall is increased, but the precision is decreased. In contrast, when sliding a hyperplane from H \u22121 to H 1 , the precision is increased, but the recall is decreased. We hence desire to find out the optimal decision hyperplane to balance between precision and recall for minimizing the cost-sensitive loss and obtaining good F1 scores. We also conduct intensive experiments on real datasets to empirically demonstrate this intuition in Sect. 3.4. Inspired by this observation, we seek the optimal decision hyperplane H d by minimizing the cost-sensitive loss for the training examples inside the strip S, where we treat the two kinds of misclassification unequally. In particular, the cost of misclassifying a non-vulnerability as a vulnerability is 1, while misclassifying a vulnerability as a non-vulnerability is \u03b8. The value of \u03b8, the relative cost between two kinds of misclassification, is set depending on specific applications. In this application, we set \u03b8 = #non-vul : #vul >> 1, which makes sense because, in binary software vulnerability detection, the cost suffered by classifying vulnerable binary code as non-vulnerable is, in general, much more severe than the converse.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "Let |I| = M where |\u00b7| specifies the cardinality of a set and arrange the elements of I according to their distances to H \u22121 as I = {i 1 ",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 135,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": ". We now define the costsensitive loss for a given decision hyperplane:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "and the optimal decision hyperplane (w * )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "where the indicator function I S returns 1 if S is true and 0 if otherwise. It is worth noting if #non-vul \u2248 #vul (i.e., \u03b8 \u2248 1), we obtain a Support Vector Machine [3] and if #non-vul >> #vul (i.e., \u03b8 \u2248 0), we obtain a Oneclass Support Vector Machine [21] . We present Algorithm 1 to efficiently find the optimal decision hyperplane. The general idea is to sequentially process the M (yi t == 1)?loss = loss + \u03b8 : loss = loss \u2212 1; 7:",
            "cite_spans": [
                {
                    "start": 164,
                    "end": 167,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 251,
                    "end": 255,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "if loss < minLoss then 8: minLoss = loss; m * = t; 9: end if 10: t = t + 1; 11: until t > M + 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulations of the Hard and Soft Models."
        },
        {
            "text": "Creating labeled binary datasets for binary code vulnerability detection is one of the main contributions of our work. We first collected the source code from two datasets on GitHub: NDSS18 3 and six open-source projects 4 collected in [16] and then processed to create 2 labeled binary datasets. The NDSS18 binary dataset was created in previous work [10] -the functions were extracted from the original source code and then compiled successfully to obtain 8, 991 binaries using an automated tool. However, the source code in the NDSS18 dataset involves the code weaknesses CWE119 and CWE399, resulting in short source code chunks used to demonstrate the vulnerable examples, hence not perfectly reflecting real-world source code, while the source code files collected from the six open-source projects, namely FFmpeg, LibTIFF, LibPNG, VLC, Pidgin and Asterisk are all real-world examples. The statistics of our binary datasets are given in Table 1 .",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 191,
                    "text": "3",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 236,
                    "end": 240,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 352,
                    "end": 356,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [
                {
                    "start": 942,
                    "end": 949,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experimental Datasets"
        },
        {
            "text": "We compared our proposed DCKM with various baselines:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "-BRNN-C, BRNN-D: A vanilla Bidirectional RNN with a linear classifier and two dense layers on the top. -Para2Vec: The paragraph-to-vector distributional similarity model proposed in [8] which allows us to embed paragraphs into a vector space which are further classified using a neural network. -VDiscover: An approach proposed in [4] that utilizes lightweight static features to \"approximate\" a code structure to seek similarities between program slices. -VulDeePecker: An approach proposed in [15] for source code vulnerability detection. -BRNN-SVM: The Support Vector Machine using linear kernel, but leveraging our proposed feature extraction method. -Att-BGRU: An approach developed by [22] for sequence classification using the attention mechanism. -Text CNN: An approach proposed in [6] using a Convolutional Neural Network (CNN) to classify text.",
            "cite_spans": [
                {
                    "start": 182,
                    "end": 185,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 331,
                    "end": 334,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 495,
                    "end": 499,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 691,
                    "end": 695,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 790,
                    "end": 793,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "-MDSAE: A method called Maximal Divergence Sequential Auto-Encoder in [10] for binary software vulnerability detection. -OC-DeepSVDD: The One-class Deep Support Vector Data Description method proposed in [20] .",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 74,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 204,
                    "end": 208,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "The implementation of our model and the binary datasets for reproducing the experimental results can be found online at https://github.com/tuanrpt/ DCKM. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baselines"
        },
        {
            "text": "For our datasets, we split the data into 80% for training, 10% for validation, and the remaining 10% for testing. For the NDSS18 binary dataset, since it is used for the purpose of demonstrating the presence of vulnerabilities, each vulnerable source code is associated with its fixed version, hence this dataset is quite balanced.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter Setting"
        },
        {
            "text": "To mimic a real-world scenario, we made this dataset imbalanced by randomly removing vulnerable source code to keep the ratio #vul : #non-vul = 1 : 50. For the dataset from six open-source projects, we did not modify the datasets since they are real-world datasets. We employed a dynamic BRNN to tackle the variation in the number of machine instructions of the functions. For the BRNN baselines and our models, the size of the hidden unit was set to 128 for the six open-source projects's binary dataset and 256 for the NDSS18 dataset. For our model, we used Fourier random kernel with the number of random features 2D = 512 to approximate the RBF kernel, defined as K (x, x ) = exp \u2212\u03b3 x \u2212 x 2 , wherein the width of the kernel \u03b3 was searched in {2 \u221215 , 2 \u22123 } for the dataset from 6 open-source projects and NDSS18 dataset, respectively. The regularization parameter \u03bb was 0.01. We set the relative cost \u03b8 = #non-vul : #vul. We used the Adam optimizer with an initial learning rate equal to 0.0005. The minibatch size was set to 64 and results became promising after 100 training epochs. We implemented our proposed method in Python using Tensorflow, an open-source software library for Machine Intelligence developed by the Google Brain Team. We ran our experiments on a computer with an Intel Xeon Processor E5-1660 which had 8 cores at 3.0 GHz and 128 GB of RAM. For each dataset and method, we ran the experiment five times and reported the average predictive performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter Setting"
        },
        {
            "text": "Experimental Results on the Binary Datasets. We conducted a variety of experiments on our two binary datasets. We split each dataset into three parts: the subset of Windows binaries, the subset of Linux binaries, and the whole set of binaries to compare our methods with the baselines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "In the field of computer security, besides the AUC and F1 score which takes into account both precision and recall, the cost-sensitive loss, wherein we consider the fact that the misclassification of a vulnerability as a non-vulnerability is more severe than the converse, is also very important. The experimental results on the two datasets are shown in Table 2 and 3. It can be seen that our proposed method outperforms the baselines in all performance measures of interest including the cost-sensitive loss, F1 score, and AUC. Especially, our method significantly surpasses the baselines on the AUC score, one of the most important measures of success for anomaly detection. In addition, although our proposed DCKM aims to directly minimize the cost-sensitive loss, it can balance between precision and recall to maintain very good F1 and AUC scores. In what follows, we further explain this claim. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 355,
                    "end": 362,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Discovering the trend of scores and number of data points in the strip during the training process Fig. 5 shows the predictive scores and the number of data examples in the parallel strip on training and valid sets for the binary dataset from six open-source projects across the training process. It can be observed that the model gradually improves during the training process with an increase in the predictive scores, and a reduction in the amount of data in the strip from around 1,700 to 50.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 99,
                    "end": 105,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Inspection of Model Behaviors"
        },
        {
            "text": "The tendency of predictive scores when sliding the decision hyperplane in the strip formed by H \u22121 and H 1 By minimizing the cost-sensitive loss, we aim to find the optimal hyperplane which balances precision and recall, while at the same time maintaining good F1 and AUC scores. Figure 6 shows the tendency of scores and cost-sensitive loss when sliding the decision hyperplane in the strip formed by H \u22121 and H 1 . We especially focus on four milestone hyperplanes, namely H \u22121 , H 1 , the hyperplane that leads to the optimal F1 score, and the hyperplane that leads to the optimal cost-sensitive loss (i.e., our optimal decision hyperplane). As shown in Fig. 6 , our optimal decision hyperplane marked with the red stars can achieve the minimal cost-sensitive loss, while maintaining comparable F1 and AUC scores compared with the optimal-F1 hyperplane marked with the purple stars.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 280,
                    "end": 288,
                    "text": "Figure 6",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 657,
                    "end": 663,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Inspection of Model Behaviors"
        },
        {
            "text": "Binary software vulnerability detection has emerged as an important and crucial problem in the software industry, such as the embedded systems industry, and in the field of computer security. In this paper, we have leveraged deep learning and kernel methods to propose the Deep Cost-sensitive Kernel Machine for tackling binary software vulnerability detection. Our proposed method inherits the advantages of deep learning methods in efficiently tackling structural data and kernel methods in learning the characteristic of vulnerable binary examples with high generalization capacity. We conducted experiments on two binary datasets. The experimental results have shown a convincing outperformance of our proposed method compared to the state-of-the-art baselines.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Symbolic execution for software testing: three decades later",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cadar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sen",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Commun. ACM",
            "volume": "56",
            "issn": "2",
            "pages": "82--90",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "An optimized cost-sensitive SVM for imbalanced data learning",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Zaiane",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pei",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "S"
                    ],
                    "last": "Tseng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Motoda",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "PAKDD 2013",
            "volume": "7819",
            "issn": "",
            "pages": "280--292",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-37456-2_24"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Support-vector networks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cortes",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vapnik",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Mach. Learn",
            "volume": "20",
            "issn": "3",
            "pages": "273--297",
            "other_ids": {
                "DOI": [
                    "10.1007/BF00994018"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Toward large-scale vulnerability discovery using machine learning",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Grieco",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "L"
                    ],
                    "last": "Grinblat",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Uzal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rawat",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feist",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mounier",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy",
            "volume": "",
            "issn": "",
            "pages": "85--96",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Robust cost sensitive support vector machine",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Katsumata",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Takeda",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "AIS-TATS. JMLR Workshop and Conference Proceedings",
            "volume": "38",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Convolutional neural networks for sentence classification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
            "volume": "2014",
            "issn": "",
            "pages": "1746--1751",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Auto-encoding variational Bayes",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1312.6114"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Distributed representations of sentences and documents",
            "authors": [
                {
                    "first": "Q",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International on Machine Learning 2014. JMLR Workshop and Conference Proceedings",
            "volume": "32",
            "issn": "",
            "pages": "1188--1196",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Gogp: fast online regression with Gaussian processes",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Phung",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Data Mining",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Maximal divergence sequential autoencoder for binary software vulnerability detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Dual space gradient descent for online learning",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Phung",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in Neural Information Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Robust support vector machine",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Duong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Joint Conference on Neural Networks",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A unified model for support vector machine and support vector data description",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IJCNN",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Gogp: scalable geometricbased Gaussian process for online regression",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Phung",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Knowl. Inf. Syst. (KAIS) J",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "VulDeePecker: a deep learning-based system for vulnerability detection",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Cross-project transfer representation learning for vulnerable function discovery",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Transactions on Industrial Informatics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Large-scale online kernel learning with random feature reparameterization",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "D"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Bui",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Phung",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Kernel-based semi-supervised learning for novelty detection",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dinh",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "H"
                    ],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "2014 International Joint Conference on Neural Networks (IJCNN)",
            "volume": "",
            "issn": "",
            "pages": "4129--4136",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Random features for large-scale kernel machines",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rahimi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Recht",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1177--1184",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Deep one-class classification",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ruff",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research",
            "volume": "80",
            "issn": "",
            "pages": "10--15",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Estimating the support of a high-dimensional distribution",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Platt",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shawe-Taylor",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "C"
                    ],
                    "last": "Williamson",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Neural Comput",
            "volume": "13",
            "issn": "7",
            "pages": "1443--1471",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Attention-based bidirectional long short-term memory networks for relation classification",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An overview of the data processing and embedding process.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Machine instruction embedding process with examples. The opcode embedding eop is concatenated with instruction information embedding eii to obtain the output embedding e, a 2ddimensional vector.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "General framework of Deep Cost-sensitive Kernel Machine.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "t. :y i w T\u03a6 (h i ) \u2212 a \u2265 0, \u2200i = 1, . . . , N y i w T\u03a6 (h i ) \u2212 1 \u2212 a \u2265 0, \u2200i = 1, . . . , N where b \u22121 = a and b 1 = 1 + a.Invoking slack variables, we obtain the soft model: min w,a",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "+1 possible hyperplanes: (w * ) T\u03a6 (h)\u2212b m d = 0, \u2200i = 1, . . . , M +1 and compute the cost-sensitive loss cumulatively. The computational cost of this algorithm includes: i) the cost to determine S, which is O (2DN ), ii) the cost to sort the elements in S according to their distances to H \u22121 , which is O (M log M ), and iii) the cost to process the possible hyperplanes, which is O (M + 1). Algorithm 1. Pseudo-code for seeking the optimal decision hyperplane. Input: D = {(x1, y1) , . . . , (xN , yN )}, w * , b * \u22121 , b * 1 Output: m * , b * d 1: Determine S and I 2: Sort the elements in I as (w * ) T\u03a6 (hi 1 ) \u2264 (w * ) T\u03a6 (hi 2 ) \u2264 \u00b7 \u00b7 \u00b7 \u2264 (w * ) T\u03a6 (hi M ) 3: loss = |{i \u2208 I | yi = \u22121}| ; //all in S are classified as + 4: m * = 1; b * d = b 1 d ; minLoss = loss; t = 1; 5: repeat 6:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "86.588.9 94.3 0.06 92.9 92.9 92.9 96.4 0.03 87.1 85.7 84.4 92.1 0.58",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Predictive scores and the number of data examples in the S strip after 100 epochs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "The variation of predictive scores when sliding the hyperplane in the strip formed by H\u22121 and H1 on the NDSS18 (left) and the dataset from six open-source projects (right). The red line illustrates the tendency of the cost-sensitive loss, while the purple star and the red star represent the optimal F1 and the optimal cost-sensitive loss values, respectively. (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The statistics of the two binary datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The experimental results (%) except for the column CS of the proposed method compared with the baselines on the binary dataset from the six open-source projects. Pre, Rec, and CS are shorthand for the performance measures precision, recall, and cost-sensitive loss, respectively.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This research was supported under the Defence Science and Technology Group's Next Generation Technologies Program.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement."
        }
    ]
}