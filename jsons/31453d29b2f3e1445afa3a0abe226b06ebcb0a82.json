{
    "paper_id": "31453d29b2f3e1445afa3a0abe226b06ebcb0a82",
    "metadata": {
        "title": "Principle-to-Program: Neural Methods for Similar Question Retrieval in Online Communities",
        "authors": [
            {
                "first": "Muthusamy",
                "middle": [],
                "last": "Chelliah",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Manish",
                "middle": [],
                "last": "Shrivastava",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IIIT Hyderabad",
                    "location": {
                        "settlement": "Hyderabad",
                        "country": "India"
                    }
                },
                "email": ""
            },
            {
                "first": "Jaidam",
                "middle": [
                    "Ram"
                ],
                "last": "Tej",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Similar question retrieval is a challenge due to lexical gap between query and candidates in archive and is very different from traditional IR methods for duplicate detection, paraphrase identification and semantic equivalence. This tutorial covers recent deep learning techniques which overcome feature engineering issues with existing approaches based on translation models and latent topics. Hands-on proposal thus will introduce each concept from end user (e.g., questionanswer pairs) and technique (e.g., attention) perspectives, present state of the art methods and a walkthrough of programs executed on Jupyter notebook using real-world datasets demonstrating principles introduced.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Time lag between user posting a question and receiving its answer could be reduced by retrieving similar, historic questions from community question answering (cQA) archives. Two seemingly different questions may refer implicitly to a common problem with the same answer. Identifying semantic equivalence is thus critical for retrieving similar questions and to automate reusing answers available for such previous questions. This is a difficult task because different users may formulate the same question in a variety of ways, using different vocabulary (e.g., watersports vs. snorkeling) and structure. Similar questions hence vary in style, length and content quality (e.g., blood pressure vs. hypertension).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Lexical gap -different but related words between queried (e.g., knot) and existing (e.g., tangle) questions -rules out traditional IR models (e.g., BM25) as a solution. Text fragments in questions (e.g., disk full) could lead to correlated content (e.g., format) in answers. Similarity -different from relatedness (e.g., synonymy, antonymy) -has been addressed with strategies like machine translation, knowledge graphs and topic models though. Treating question-answer pairs as parallel text, relationships can be established through translation probability (e.g., word-to-word) -asymmetry being a handicap. Latent topics aligned across question-answer pairs is another option -heterogeneity being an issue.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Title/body of a question is concatenated into problem definition and IR task is to decide if 2 such text fragments are similar. Detecting (almost) exact copies of the same document in corpora from Web crawling/search systems is not enough as equivalent questions may have very little (or no) overlap. Considering only surface form of a question without factoring in semantics makes it hard to identify duplicates. Keyword-based retrieval methods (e.g., language/vector-space models) hence predict questions with same meaning as different. Traditional similarity measures based on word overlap -even those on a graded scale from 0 to 5 -have thus proven to be inadequate for capturing semantic equivalence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Background/Motivation"
        },
        {
            "text": "Paraphrase identification which helps determine if 2 sentences have the same meaning is not sufficient as well for similar question retrieval. Polysemy and word order are other challenges which similar question retrieval has to tackle like any other NLP tasks. Also, submitted questions have extraneous detailswhich obscures key information buried in the noise -in the body irrelevant to main question being asked. Title alone on the other hand lacks crucial detail present in question body. Building a large amount of training data with similar questions is expensive and careful feature engineering is time consuming. Deep learning techniques recently have been effective for sentence-level analysis of short texts in a variety of IR tasks. Size of a question in an online community however varies from a single sentence to detailed problem description with many sentences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Background/Motivation"
        },
        {
            "text": "CNNs transform first words into embeddings with unlabeled data and then build distributed, vector representations for pairs of original and related questions. Questions are scored next with a metric (e.g., cosine similarity) and those pairs above a threshold based on a held-out set are considered equivalent. During training, CNN is induced to produce similar vector representations for equivalent questions. We discuss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Networks (CNNs)/Question-Question Pairs"
        },
        {
            "text": "- [1] evaluates in-domain word embeddings vs. one trained with Wikipedia, estimates impact of training set size and evaluates aspects of domain adaptation, - [2] combines bag of words (BoW) to retrieve equivalent questions while learning to rank them according to similarity with a loss function,",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 5,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 158,
                    "end": 161,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Convolutional Neural Networks (CNNs)/Question-Question Pairs"
        },
        {
            "text": "- [4] integrates sentence modeling and semantic matching into a single framework without syntactic analysis and prior knowledge (e.g., wordnet). Word tokens are converted into vectors by a lookup layer and useful information is captured with convolutional/pooling layers; finally, matching metric is learnt -better than traditional ones (e.g., inner-product, Euclidean distance)between question/answer capturing their interaction with a tensor layer.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 5,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Convolutional Neural Networks (CNNs)/Question-Question Pairs"
        },
        {
            "text": "Available annotations on similar questions however are noisy and fragmented. An encoder maps title/body combination of questions -treated as word sequencesinto vector representation with a recurrent model. Complementary decoder is trained to reproduce title from noisy question body. We discuss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Recurrent Neural Networks (RNNs)/Similarity Features"
        },
        {
            "text": "- [6] incorporates adaptive gating in non-consecutive CNNs to focus temporal averaging on key pieces of questions. Training paradigm utilizes entire corpus of unannotated questions in a semi-supervised manner and fine tunes learning model discriminatively with limited annotations, - [14, 15] applies LSTM with attention to select entire sentences and subparts (word/chunk) from shallow syntactic trees towards question retrieval and tree kernels to filtered text representations exploiting implicit features of subtree space for learning question reranking.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 5,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 284,
                    "end": 288,
                    "text": "[14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 289,
                    "end": 292,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Recurrent Neural Networks (RNNs)/Similarity Features"
        },
        {
            "text": "User asking a question in cQA sites is required to choose a label from a predefined hierarchy of categories. This meta-data encodes attributes/properties of words from which similar words can be grouped according to categories. Language models represent words and question categories in a vector space and calculate question-question similarity with linear combinations of dot products of vectorsthus being heuristic on data or difficult to scale up. Each question is thus defined as a distribution which generates each word (embedding) independently and subsequently a kernel is used to assess question similarities. This design will require representation of words that belong to the same category to be close to each other thus benefiting embedding learning. We discuss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Latent Space/Meta-data"
        },
        {
            "text": "- [3] learns variable-length word embeddings with category information and aggregates them into fixed-size vectors, - [8] optimizes an objective which in turn applies a non-linear transformation considering only local-relatedness of words (i.e., category and small window in a question/associated answers), - [12] outperforms text-based methods in misflagged duplicate detection with features like user authority, question quality and relational data between questions.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 5,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 118,
                    "end": 121,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 309,
                    "end": 313,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Latent Space/Meta-data"
        },
        {
            "text": "Due to relatively short text, question-question pairs have insignificant information to determine their relationship. To combat scarcity of similar question pairs for training, question-answer pairs from archives can be leveraged in a weakly supervised fashion without manual labeling. An added advantage of this approach is mapping simple terms used by novice askers (e.g., short sighted) to technical terms (e.g., myopia) and concepts (e.g., lasik/laser surgery, contact lens) used by expert answerers. We discuss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Representation Learning/Question-Answer Pairs"
        },
        {
            "text": "- [5] learns shared parameters and similarity metric minimizing contrastive-loss energy function connecting twin networks, - [7] preserves local neighborhood structure of and mirrors semantic similarity among question and answer spaces, - [9] represents hierarchical structures of word and concept information with layer-by-layer composition and pooling leading to question embedding that captures semantics/syntax.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 5,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 125,
                    "end": 128,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 239,
                    "end": 242,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Representation Learning/Question-Answer Pairs"
        },
        {
            "text": "Essential constituents (e.g., destination) are those -name and value -important to meaning of the question (e.g., route). Units in a semantic parse can be leveraged to alleviate defining/labeling them in open domain. We discuss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention/Constituent Matching"
        },
        {
            "text": "- [11] combines FrameNet with neural networks through ensemble and embedding approaches for question retrieval with constituent matching, - [13] integrates shallow lexical mismatching information with initial rank by an external search engine to generate deep question representation with attention autoencoder, - [10] leverages semantic information in paired answers while alleviating noise caused by adding answers with three heterogeneous attention mechanisms for modeling temporal interaction in a long sentence, capturing relevance between questions and relevance between answers and extracting knowledge from answers.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 6,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 140,
                    "end": 144,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 314,
                    "end": 318,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Attention/Constituent Matching"
        },
        {
            "text": "Distributed representations help tackle lexical gap in question retrieval as features based on word embeddings that enable similarity calculation through neural networks; gated convolutions map key question information from lengthy detail to semantic representations and LSTM with attention weights alleviates noise in syntactic structure selecting most significant parse tree fragments from question text. Simultaneously embedding categories of questions into vector space helps model local relatedness of words in learning. Misflagging duplicate detection through user authority and question quality is more indicative of behavior problems (e.g., posting questions). Local linear embedding is leveraged to use collective corpus-level information for embedding historical question-answer pairs in a latent space without lexical correlation and separate topic/translation models. Attention encoders contain context information with focus on current word of input sequence thus avoiding bias towards sentence end.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Incorporating user ratings/reputation still remains unexplored. Semantic parsing techniques like abstract meaning representation is a future direction for essential constituent matching.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Detecting semantically equivalent questions in online user forums",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bogdanova",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Santos",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Barbosa",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zadrozny",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning",
            "volume": "",
            "issn": "",
            "pages": "123--131",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Learning hybrid representations to retrieve semantically equivalent questions",
            "authors": [
                {
                    "first": "Dos",
                    "middle": [],
                    "last": "Santos",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Barbosa",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bogdanova",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zadrozny",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "694--699",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Learning continuous word embedding with metadata for question retrieval in community question answering",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
            "volume": "1",
            "issn": "",
            "pages": "250--259",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Convolutional neural tensor network architecture for community-based question answering",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Qiu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Twenty-Fourth IJCAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Together we stand: siamese networks for similar question retrieval",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yenala",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chinnakotla",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shrivastava",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
            "volume": "1",
            "issn": "",
            "pages": "378--387",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Semi-supervised question retrieval with gated convolutions",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lei",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1512.05726"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Latent space embedding for retrieval in questionanswer archives",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Deepak",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Garg",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shevade",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "855--865",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Learning distributed representations of data in community question answering for question retrieval",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Ninth ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "533--542",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Concept embedded convolutional semantic model for question retrieval",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "WSDM 2017",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Adaptive multi-attention network incorporating answer information for duplicate question detection",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Duplicate question identification by integrating FrameNet with neural networks. In: 32nd AAAI Conference on Artificial Intelligence",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Detecting misflagged duplicate questions in community question-answering archives",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hoogeveen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bennett",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Verspoor",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Baldwin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Twelfth International AAAI Conference on Web and Social Media",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "An unsupervised model with attention autoencoders for question retrieval",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Neural attention for learning to rank questions in community question answering",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Romeo",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1734--1745",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Selecting sentences versus selecting tree constituents for automatic question ranking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Barr\u00f3rn-Cedeno",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Da San Martino",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Romeo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Moschitti",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "2515--2525",
            "other_ids": {}
        }
    },
    "ref_entries": {},
    "back_matter": []
}