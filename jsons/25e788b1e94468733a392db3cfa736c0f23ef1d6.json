{
    "paper_id": "25e788b1e94468733a392db3cfa736c0f23ef1d6",
    "metadata": {
        "title": "Online Algorithms for Multiclass Classification Using Partial Labels",
        "authors": [
            {
                "first": "Rajarshi",
                "middle": [],
                "last": "Bhattacharjee",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IIT Madras",
                    "location": {
                        "settlement": "Chennai",
                        "country": "India"
                    }
                },
                "email": "brajarshi91@gmail.com"
            },
            {
                "first": "(",
                "middle": [
                    "B"
                ],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Naresh",
                "middle": [],
                "last": "Manwani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IIIT Hyderabad",
                    "location": {
                        "settlement": "Hyderabad",
                        "country": "India"
                    }
                },
                "email": "naresh.manwani@iiit.ac.in"
            }
        ]
    },
    "abstract": [
        {
            "text": "In this paper, we propose online algorithms for multiclass classification using partial labels. We propose two variants of Perceptron called Avg Perceptron and Max Perceptron to deal with the partially labeled data. We also propose Avg Pegasos and Max Pegasos, which are extensions of the Pegasos algorithm. We also provide mistake bounds for Avg Perceptron and regret bound for Avg Pegasos. We show the effectiveness of the proposed approaches by experimenting on various datasets and comparing them with the standard Perceptron and Pegasos.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The online version of this chapter (https://",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Multiclass classification is a well-studied problem in machine learning. However, we assume that we know the true label for every example in the training data. In many applications, we don't have access to the true class label as labeling data is an expensive and time-consuming process. Instead, we get a set of candidate labels for every example. This setting is called multiclass learning with partial labels. The true or ground-truth label is assumed to be one of the instances in the partial label set. Partially labeled data is relatively easier to obtain and thus provides a cheap alternative to learning with exact labels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Learning with partial labels is referred to as superset label learning [13] , ambiguous label learning [2] , and by other names in different papers. Many proposed models try to disambiguate the correct labels from the incorrect ones. One popular approach is to treat the unknown correct label in the candidate set as a latent variable and then use an Expectation-Maximization type algorithm to estimate the correct label as well the model parameters iteratively [2, 9, 11, 13, 18] .",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 103,
                    "end": 106,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 462,
                    "end": 465,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 466,
                    "end": 468,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 469,
                    "end": 472,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 473,
                    "end": 476,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 477,
                    "end": 480,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Other approaches to label disambiguation include using a maximum margin formulation [20] which alternates between ground truth identification and maximizing the margin from the ground-truth label to all other labels. Regularization based approaches [8] for partial label learning have also been proposed. Another model assumes that the ground truth label is the one to which the maximum score is assigned in the candidate label set by the model [14] . Then the margin between this ground-truth label and all other labels not in the candidate set is maximized.",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 88,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 249,
                    "end": 252,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 445,
                    "end": 449,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Some approaches try to predict the label of an unseen instance by averaging the candidate labeling information of its nearest neighbors in the training set [10, 21] . Some formulations combine the partial label learning framework with other frameworks like multi-label learning [19] . There are also specific approaches that do not try to disambiguate the label set directly. For example, Zhang et al. [22] introduced an algorithm that works to utilize the entire candidate label set using a method involving error-correcting codes.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 160,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 161,
                    "end": 164,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 278,
                    "end": 282,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 402,
                    "end": 406,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A general risk minimization framework for learning with partial labels is discussed in Cour et al. [3, 4] . In this framework, any standard convex loss function can be modified to be used in the partial label setting. For a single instance, since the ground-truth label is not available, an average over the scores in the candidate label set is taken as a proxy to calculate the loss. Nguyen and Caruana [14] propose a risk minimization approach based on a non-convex max-margin loss for a partial label setting.",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 102,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 103,
                    "end": 105,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 404,
                    "end": 408,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we propose online algorithms for multiclass classification using partially labeled data. Perceptron [15] algorithm is one of the earliest online learning algorithms. Perceptron for multiclass classification is proposed in [7] . A unified framework for designing online update rules for multiclass classification was provided in [5] . An online variant of the support vector machine [17] called Pegasos is proposed in [16] . This algorithm is shown to achieve O(log T ) regret (where T is the number of rounds). Once again, all these online approaches assume that we know the true label for each example.",
            "cite_spans": [
                {
                    "start": 115,
                    "end": 119,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 237,
                    "end": 240,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 343,
                    "end": 346,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 397,
                    "end": 401,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 432,
                    "end": 436,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Online multiclass learning with partial labels remained an unaddressed problem. In this paper, we propose several online multiclass algorithms using partial labels. Our key contributions in this paper are as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "1. We propose Avg Perceptron and Max Perceptron, which extensions of Perceptron to handle the partial labels. Similarly, we propose Avg Pagasos and Max Pegasos, which are extensions of the Pegasos algorithm. 2. We derive mistake bounds for Avg Perceptron in both separable and general cases. Similarly, we provide log(T ) regret bound for Avg Pegasos. 3. We also provide thorough experimental validation of our algorithms using datasets of different dimensions and compare the performance of the proposed algorithms with standard multiclass Perceptron and Pegasos.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We now formally discuss the problem of multiclass classification given partially labeled training set. Let X \u2286 R d be the feature space from which the instances are drawn and let Y = {1, . . . , K} be the output label space. Every instance x \u2208 X is associated with a candidate label set Y \u2286 Y. The set of labels not present in the candidate label set is denoted by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multiclass Classification Using Partially Labeled Data"
        },
        {
            "text": "The ground-truth label associated with x is denoted by lowercase y. It is assumed that the actual label lies within the set Y (i.e., y \u2208 Y ). The goal is to learn a classifier h : X \u2192 Y. Let us assume that h(x) is a linear classifier. Thus, h(x) is parameterized by a matrix of weights W \u2208 R d\u00d7K and is defined as h(x) = arg max i\u2208 [K] w i .x where w i (ith column vector of W ) denotes the parameter vector corresponding to the i th class. Discrepancy between the true label and the predicted label is captured using 0-1 loss as",
            "cite_spans": [
                {
                    "start": 332,
                    "end": 335,
                    "text": "[K]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Multiclass Classification Using Partially Labeled Data"
        },
        {
            "text": "Here, I is the 0-1 indicator function, which evaluates to true when the condition mentioned is true and 0 otherwise. However, in the case of partial labels, we use partial (ambiguous) 0-1 loss [3] as follows.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 196,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Multiclass Classification Using Partially Labeled Data"
        },
        {
            "text": "Minimizing L A is difficult as it is not continuous. Thus, we use continuous surrogates for L A . A convex surrogate of L A is the average prediction hinge loss (APH) [3] which is defined as follows.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 170,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Multiclass Classification Using Partially Labeled Data"
        },
        {
            "text": "where |Y | is the size of the candidate label set and [a] + = max(a, 0). L AP H is shown to be a convex surrogate of L A in [4] . There is another non-convex surrogate loss function called the max prediction hinge loss (MPH) [14] that can be used for partial labels which is defined as follows:",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 127,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 225,
                    "end": 229,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Multiclass Classification Using Partially Labeled Data"
        },
        {
            "text": "In this paper, we present online algorithms based on stochastic gradient descent on L AP H and L MP H .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multiclass Classification Using Partially Labeled Data"
        },
        {
            "text": "In this section, we propose two variants of multiclass Perceptron using partial labels. Let the instance observed at time t be x t and its corresponding label set be Y t . The weight matrix at time t is W t and the ith column of W t is denoted by w t i . To update the weights, we propose two different schemes: (a) Avg Perceptron (using stochastic gradient descent on L AP H ) and (b) Max Perceptron (using stochastic gradient descent on L MP H ). We use following sub-gradients of the L AP H and L MP H .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multiclass Perceptron Using Partial Labels"
        },
        {
            "text": "We initialize the weight matrix as a matrix of zeros. At trial t, the update rule for w i can be written as: (4) and (5). The complete description of Avg Perceptron and Max Perceptron is provided in Algorithm 1 and 2 respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multiclass Perceptron Using Partial Labels"
        },
        {
            "text": "In the partial label setting, we say that mistake happens when the predicted class label for an example does not belong to its partial label set. We first define two variants of linear separability in a partial label setting as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mistake Bound Analysis"
        },
        {
            "text": "Let {(x 1 , Y 1 ), . . . , (x T , Y T )} be the training set for multiclass classification with partial labels. We say that the data is average linearly separable if there exist w 1 , . . . , w K \u2208 R d such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 (Average Linear Separability in Partial Label Setting)."
        },
        {
            "text": "Thus, average linear separability implies that {(x 1 , Y 1 ) , . . . , (x T , Y T )} be the training set for multiclass classification with partial labels. We say that the data is max linearly separable if there exist w 1 , . . . , w K \u2208 R d such that",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 47,
                    "end": 60,
                    "text": "{(x 1 , Y 1 )",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Definition 1 (Average Linear Separability in Partial Label Setting)."
        },
        {
            "text": "Thus, max linear separability implies that L MP H (h(x t ), Y t ) = 0, \u2200t \u2208 [T ].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 2 (Max Linear Separability in Partial Label Setting). Let"
        },
        {
            "text": "We bound the number of mistakes made by Avg Perceptron (Algorithm 1) as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. Avg Perceptron"
        },
        {
            "text": ". Then we get the following mistake bound for Avg Perceptron Algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1 (Mistake Bound for Avg Perceptron Under Average Linear Separability). Let"
        },
        {
            "text": "where c = min t |Y t |, R = max t ||x t || and \u03b3 \u2265 0 is the margin of separation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1 (Mistake Bound for Avg Perceptron Under Average Linear Separability). Let"
        },
        {
            "text": "The proof is given in Appendix A of [1] . We first notice that the bound is inversely proportional to the minimum label set size. This is intuitively obvious as the smaller the candidate label set size, the larger the chance of having a nonzero loss. When c = 1, the number of updates reduces to the normal multiclass Perceptron mistake bound for linearly separable data as given in [5] . Also, the number of mistakes is inversely proportional to \u03b3 2 . Linear separability (Definition 1) may not always hold for the training data. Thus, it is important to see how does the algorithm Avg Perceptron performs in such cases. We now bound the number of updates in T rounds for partially labeled data, which is linearly non-separable under L AP H .",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 39,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 383,
                    "end": 386,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Theorem 1 (Mistake Bound for Avg Perceptron Under Average Linear Separability). Let"
        },
        {
            "text": "Let (x 1 , Y 1 ), . . . , (x T , Y T ) be an input sequence presented to Avg Perceptron. Let W ( W = 1) be weight matrix corresponding to a multiclass",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 2 (Mistake Bound for Avg Perceptron in Non-Separable Case)."
        },
        {
            "text": "Then, mistakes bound for Avg Perceptron is as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 2. Max Perceptron"
        },
        {
            "text": "The proof is provided in the Appendix B of [1] .",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 46,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Algorithm 2. Max Perceptron"
        },
        {
            "text": "Pegasos [16] is an online algorithm originally proposed for an exact label setting. In Pegasos, L 2 regularizer of the weights is minimized along with the hinge loss, making the overall objective function strongly convex. The strong convexity enables the algorithm to achieve a O(log T ) regret in T trials. The objective function of the Pegasos at trial t is the following.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Online Multiclass Pegasos Using Partial Labels"
        },
        {
            "text": "Here, \u03bb is a regularization constant and ||W || is Frobenius norm of the weight matrix. Let W t be the weight matrix at the beginning of trial t. Then, W t+1 is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Online Multiclass Pegasos Using Partial Labels"
        },
        {
            "text": ", \u03b7 t is the step size at trial t and \u03a0 B is a projection operation onto the set B which is defined as B = {W : ||W || \u2264 1 \u221a \u03bb }. Thus, \u03a0 B (W ) = min{1, 1 (\u03bb||W ||) }W . We now propose extension of Pegasos [16] for online multiclass learning using partially labeled data. We again propose two variants of Pegasos: (a) Avg Pegasos (using average prediction hinge loss (Eq. 2)) and (b) Max Pegasos (using max prediction hinge loss (Eq. (3) ). We first note that \u2207 t can be written as:",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 430,
                    "end": 438,
                    "text": "(Eq. (3)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Online Multiclass Pegasos Using Partial Labels"
        },
        {
            "text": "where \u2207 W t L is given by Eq. (4) (for L AP H ) and Eq. (5) (for L MP H ). Complete description of Avg Pegasos and Max Pegasos are given in Algorithm 3 and Algorithm 4 respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Online Multiclass Pegasos Using Partial Labels"
        },
        {
            "text": "Input: \u03bb, T Initialize: W1 s.t. ||W 1 || \u2264 1 \u221a \u03bb for t = 1 to T do Get x t , Y t Set \u03b7t = 1 \u03bbt Calculate loss LAP H (h t (x t ), Y t ) using Eq. (2) if LAP H > 0 then W t+ 1 2 = (1 \u2212 \u03b7t\u03bb)W t \u2212 \u03b7t\u2207W LAP H where \u2207W LAP H is given by Eq. (4) W t+1 = min{1, 1/ \u221a \u03bb ||W t+ 1 2 || }W t+ 1 2 else W t+1 = W t end if end for Output: W T",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 3. Avg Pegasos"
        },
        {
            "text": "We now derive the regret bound for Avg Pegasos.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regret Bound Analysis of Avg Pegasos"
        },
        {
            "text": "Then the regret of Avg Pegasos is given as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Regret Bound Analysis of Avg Pegasos"
        },
        {
            "text": "The proof is given in Appendix C of [1] . We again see the regret is inversely proportional to the size of the minimum candidate label set. ",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 39,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Regret Bound Analysis of Avg Pegasos"
        },
        {
            "text": "Input: \u03bb, T Initialize: W1 s.t. ||W 1 || \u2264 1 \u221a \u03bb for t = 1 to T do Get x t , Y t Set \u03b7t = 1 \u03bbt Calculate loss LMPH(h t (x t ), Y t ) using Eq. (3) if LAP H > 0 then W t+ 1 2 = (1 \u2212 \u03b7t\u03bb)W t \u2212 \u03b7t\u2207W LMPH where \u2207W LMPH is given by Eq. (5) W t+1 = min{1, 1/ \u221a \u03bb ||W t+ 1 2 || }W t+ 1 2 else W t+1 = W t end if end for Output: W T",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 4. Max Pegasos"
        },
        {
            "text": "We now describe the experimental results. We perform experiments on Ecoli, Satimage, Dermatology, and USPS datasets (available on UCI repository [6] ) and MNIST dataset [12] . We perform experiments using the proposed algorithms Avg Perceptron, Max Perceptron, Avg Pegasos, and Max Pegasos. For benchmarking, we use Perceptron and Pegasos based on exact labels.",
            "cite_spans": [
                {
                    "start": 145,
                    "end": 148,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 169,
                    "end": 173,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "For all the datasets, the candidate or partial label set for each instance contains the true label and some labels selected uniformly at random from the remaining labels. After every trial, we find the average mis-classification rate (average of L 0\u22121 loss over examples seen till that trial) is calculated with respect to the true label. This sets a hard evaluation criteria for the algorithms. The number of rounds for each dataset is selected by observing when the error curves start to converge. For every dataset, we repeat the process of generating partial label sets and plotting the error curves 100 times and average the instantaneous error rates across the 100 runs. The final plots for each dataset have the average instantaneous error rate on the Y-axis and the number of rounds on the X-axis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "For every dataset, we plot the error rate curves for all the algorithms for different candidate label set sizes. This helps us in understanding how the online algorithms behave as the candidate label set size increases. For the Dermatology dataset, which contains six classes, we take candidate labels sets of sizes 2 and 4, respectively, as shown in Fig. 1 . We see that the average prediction loss based algorithms perform the better in both cases. The results for the Ecoli dataset for candidate label sets of size 2, 4 and 6 are shown in Fig. 2 . Here, we find that the Max Pegasos algorithm performs comparably to the algorithms based on the Average Prediction Loss for candidate labels set sizes 2 and 4. But for candidate label set size 8, the Max Prediction Loss performs significantly worse than the Average Prediction Loss based algorithm. The results for Satimage and USPS datasets are shown in Fig. 3 and 4 respectively. For Satimage, the Max Pegasos performs the best for label set of size 2. But for label set size 4, the Average Prediction Loss based algorithms perform much better. For USPS, we see that though for candidate labels set sizes 2 and 4, the Max Perceptron and Max Pegasos perform better than our algorithms, for label set sizes 6 and 8, the Average Prediction Loss based algorithms perform much better. The results for MNIST are provided in Fig. 5 . Here we observe the Max Perceptron and Max Pegasos performs much better than the other algorithms for label set sizes 2 and 4. However, for label set sizes 6 and 8, the Average Pegasos performs best. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 351,
                    "end": 357,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 542,
                    "end": 548,
                    "text": "Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 906,
                    "end": 918,
                    "text": "Fig. 3 and 4",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1371,
                    "end": 1377,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Fig. 2. Ecoli dataset results"
        },
        {
            "text": "Overall, we see that for smaller labels set sizes, the Max Prediction Loss performs quite well. However, the Average Prediction Loss shows the best for larger candidate label set sizes. Studying the convergence and theoretical properties of the non-convex Max Prediction Loss can be an exciting future direction for exploration. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 4. USPS dataset results"
        },
        {
            "text": "In this paper, we proposed online algorithms for classifying partially labeled data. This is very useful in real-life scenarios when multiple annotators give different labels for the same instance. We presented algorithms based on the Perceptron and Pegasos. We also provide mistake bounds for the Perceptron based algorithm and the regret bound for the Pegasos based algorithm. We also provide an experimental comparison of all the algorithms on various datasets. The results show that though the Average Prediction Loss is convex, the nonconvex Max Prediction Loss can also be useful for small labels set sizes. Providing a theoretical analysis for the Max Prediction Loss can be a useful endeavor in the future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Online algorithms for multiclass classification using partial labels",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bhattacharjee",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Manwani",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1912.11367"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Ambiguously labeled learning using dictionaries",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "M"
                    ],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chellappa",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Phillips",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Inf. Forensics Secur",
            "volume": "9",
            "issn": "12",
            "pages": "2076--2088",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Learning from partial labels",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cour",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sapp",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Taskar",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Mach. Learn. Res",
            "volume": "12",
            "issn": "",
            "pages": "1501--1536",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Learning from ambiguously labeled images",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cour",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sapp",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Taskar",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the IEEE Computer Society Conference on Computing Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "919--926",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Ultraconservative online algorithms for multiclass problems",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Crammer",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res",
            "volume": "3",
            "issn": "",
            "pages": "951--991",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "UCI machine learning repository",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dua",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Graff",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Pattern Classification and Scene Analysis",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Duda",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hart",
                    "suffix": ""
                }
            ],
            "year": 1973,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Partial label learning with self-guided retraining",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "An",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 33rd AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "3542--3549",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Learning from partial labels with minimum entropy",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Grandvalet",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Learning from ambiguously labeled examples",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "H\u00fcllermeier",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Beringer",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Intell. Data Anal",
            "volume": "10",
            "issn": "5",
            "pages": "419--439",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Learning with multiple labels",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ghahramani",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "921--928",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Gradient-based learning applied to document recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lecun",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Haffner",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proc. IEEE",
            "volume": "86",
            "issn": "11",
            "pages": "2278--2324",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A conditional multinomial mixture model for superset label learning",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dietterich",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bartlett",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "C N"
                    ],
                    "last": "Pereira",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J C"
                    ],
                    "last": "Burges",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "557--565",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Classification with partial labels",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Caruana",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery Data Mining",
            "volume": "",
            "issn": "",
            "pages": "551--559",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "The perceptron: a probabilistic model for information storage and organization in the brain",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Rosenblatt",
                    "suffix": ""
                }
            ],
            "year": 1958,
            "venue": "Psychol. Rev",
            "volume": "65",
            "issn": "",
            "pages": "386--407",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Pegasos: primal estimated sub-gradient solver for SVM",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shalev-Shwartz",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Srebro",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the International Conference on Machine Learning (ICML)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A tutorial on support vector regression",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Stat. Comput",
            "volume": "14",
            "issn": "3",
            "pages": "199--222",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Partially supervised learning by a Credal EM approach",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Vannoorenberghe",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Smets",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "ECSQARU 2005",
            "volume": "3571",
            "issn": "",
            "pages": "956--967",
            "other_ids": {
                "DOI": [
                    "10.1007/11518655_80"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Partial multi-label learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "K"
                    ],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Maximum margin partial label learning",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "M.-L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Mach. Learn",
            "volume": "106",
            "issn": "4",
            "pages": "573--593",
            "other_ids": {
                "DOI": [
                    "10.1007/s10994-016-5606-4"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Solving the partial label learning problem: an instance-based approach",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th International Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "4048--4054",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Disambiguation-free partial label learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "Z"
                    ],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "29",
            "issn": "10",
            "pages": "2155--2167",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Dermatology dataset results",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Satimage dataset results",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "MNIST dataset results",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}