{
    "paper_id": "25c128e9ab8e12f9fbbac669c961cd64d8a5940d",
    "metadata": {
        "title": "6GCVAE: Gated Convolutional Variational Autoencoder for IPv6 Target Generation",
        "authors": [
            {
                "first": "Tianyu",
                "middle": [],
                "last": "Cui",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "cuitianyu@iie.ac.cn"
            },
            {
                "first": "Gaopeng",
                "middle": [],
                "last": "Gou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "gougaopeng@iie.ac.cn"
            },
            {
                "first": "Gang",
                "middle": [],
                "last": "Xiong",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "xionggang@iie.ac.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "IPv6 scanning has always been a challenge for researchers in the field of network measurement. Due to the considerable IPv6 address space, while recent network speed and computational power have been improved, using a brute-force approach to probe the entire network space of IPv6 is almost impossible. Systems are required an algorithmic approach to generate more possible active target candidate sets to probe. In this paper, we first try to use deep learning to design such IPv6 target generation algorithms. The model effectively learns the address structure by stacking the gated convolutional layer to construct Variational Autoencoder (VAE). We also introduce two address classification methods to improve the model effect of the target generation. Experiments indicate that our approach 6GCVAE outperformed the conventional VAE models and the state of the art target generation algorithm in two active address datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In the network measurement task, in order to discover the active hosts in the network and judge their active state, the researchers usually use the network scanning method to actively detect all the hosts existing in the network space. Systems confirm that the host is active by sending the request packets and waiting until receiving the response packets from the host. However, IPv6 [4] contains a considerable address space. The current scanner [5] cannot complete the entire IPv6 network space scanning.",
            "cite_spans": [
                {
                    "start": 385,
                    "end": 388,
                    "text": "[4]",
                    "ref_id": null
                },
                {
                    "start": 448,
                    "end": 451,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The state of the art approach to solving this problem is using IPv6 target generation technology [6, 10, 16] . The technology requires a set of active IPv6 seed addresses as the input and learns the structure of the seed addresses to generate possible active IPv6 target candidate sets. Due to the semantics of the IPv6 address is opaque, it is difficult to infer the IPv6 address structure of a real host or perform effective analysis of the addressing schemes.",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 100,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 101,
                    "end": 104,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 105,
                    "end": 108,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The representative algorithms of IPv6 target generation technology include Entropy/IP [6] which trained the Bayesian network to generate active candidate sets. However, the approach requires assuming that address segments exist dependency. The confirmed model determined by experience and assumption may be influenced in various datasets, thus leading to quite different effects [6] . In addition, because of the characteristics of such algorithms, they will consume a long time under a large dataset.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 89,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 379,
                    "end": 382,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Deep neural network architectures are used for the batch processing of big data tasks. Models are able to automatically adapt to seed datasets by training, thus usually performing well in a variety of large datasets. Variational Autoencoder (VAE) [9] is a typical generative model in deep neural networks. The model samples the latent vector and finally reconstructs the text or image that is similar to the original. The encoding idea may contribute to deeply mine the potential relationship between addresses and active hosts. The gated convolutional network was proposed by Dauphin et al. [3] The convolution and gating mechanism of the model effectively learn the text structure while understanding the relevance of the text, which can help models learn the key features of IPv6 addresses.",
            "cite_spans": [
                {
                    "start": 247,
                    "end": 250,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 592,
                    "end": 595,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we use a deep neural network architecture for the first time to accomplish the IPv6 target generation task. Our contribution can be summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We first propose using deep learning architecture to achieve IPv6 target generation. Our work achieves a new model 6GCVAE that stacks the gated convolutional layer to construct VAE model. -We use two methods of seed classification, which contributes to explore the IPv6 addressing schemes to effectively improve the effect of the model. -Our model demonstrates better results on both two datasets than conventional VAE models (FNN VAE, Convolutional VAE, RNN VAE, LSTM VAE, and GRU VAE) and the state of the art target generation technology Entropy/IP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The organizational structure for the rest of the paper is as follows. Section 2 introduces the related work of IPv6 target generation. Section 3 introduces the background and considerations of this task. 6GCVAE architecture and seed classification methods are shown in Sect. 4. Section 5 evaluates our work and Sect. 6 summarizes the paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In previous work, researchers have found there are certain patterns in active IPv6 address sets. Planka and Berger [13] first explored the potential patterns of IPv6 active addresses in time and space. They used Multi-Resolution Aggregate plots to quantify the correlation of each portion of an address to grouping addresses together into dense address space regions. Czyz et al. [2] found 80% of the routes and 22% of the server addresses have only non-zero addresses in the lowest 16 bits of the address. Gasser et al. [7] used entropy clustering to classify the hitlist into different addressing schemes. We adopt their methods by performing seed classification to help neural networks improve model performance. Ullrich et al. [16] used a recursive algorithm for the first attempt to address generation. They iteratively searched for the largest match between each bit of the address and the current address range until the undetermined bits were left, which is used to generate a range of addresses to be scanned. Murdock et al. [10] introduced 6Gen, which generates the densest address range cluster by combining the closest Hamming distance addresses in each iteration. Foremski et al. [6] used Entropy/IP for efficient address generation. The algorithm models the entropy of address bits in the seed set and divides the bits into segments according to the entropy values. Then they used a Bayesian network to model the statistical dependence between the values of different segments. This learned statistical model can then generate target addresses for scanning. Different from these work, we use the neural network to construct the generated model and mainly compare it with Entropy/IP.",
            "cite_spans": [
                {
                    "start": 115,
                    "end": 119,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 380,
                    "end": 383,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 521,
                    "end": 524,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 731,
                    "end": 735,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1193,
                    "end": 1196,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Researchers have extensively studied the VAE models in many fields, including text generation [15] and image generation [14] . Recently, gated convolutional networks have made outstanding progress on many Natural Language Processing (NLP) tasks due to their parallel computing advantages. Dauphin et al. [3] first proposed the model and called its key modules Gated Linear Units (GLU). Their approach achieves state-of-the-art performance on the WikiText-103 benchmark. Gehring et al. [8] simplified the gradient propagation using GLU and made a breakthrough on the WMT'14 English-German and WMT'14 English-French translation. To the best of our knowledge, we are using a gated convolutional network for the first time to construct a VAE model and to overcome the challenge of the IPv6 target generation task.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 98,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 120,
                    "end": 124,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 304,
                    "end": 307,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 485,
                    "end": 488,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this section, we provide a brief description of IPv6 addressing background and our consideration of target generation tasks. We refer the reader to RFC 2460 [4] for a detailed description of the protocol.",
            "cite_spans": [
                {
                    "start": 160,
                    "end": 163,
                    "text": "[4]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "IPv6 Target Generation"
        },
        {
            "text": "An IPv6 address consists of a global network identifier (network prefix, e.g./32 prefix), subnet prefix, and an interface identifier (IID) [1] . It is composed of 128bit binary digits, which are usually represented in human-readable text format, using 8 groups of 4 hexadecimal digits and separating them by colons, as shown in Fig. 1 . Each of the hexadecimal digits is called a nybble. Since IPv6 addresses usually use \"::\" to replace groups of consecutive zero values and omit the first zero value in each group, a commonly used address format representation for IPv6 is also shown in Fig. 1 .",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 142,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 328,
                    "end": 334,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 588,
                    "end": 594,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "IPv6 Addressing Background"
        },
        {
            "text": "The overall architecture of 6GCVAE. The model requires seed sets removed the colon as input and learns the address structure distribution through the encoder. The decoder reconstructs the latent vector after sampling. After training, the generator produces considerable candidates sets waited for probing by a scanner, which can finally discover the active targets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 2."
        },
        {
            "text": "There are many IPv6 addressing schemes and network operators are reminded to treat interface identifiers as semantically opaque [1] . Administrators have the option to use various standards to customize the address types. In addition, some IPv6 addresses have SLAAC [12] address format that the 64-bit IID usually embeds the MAC address according to the EUI-64 standard [12] or is set completely pseudo-random [11] . Consider the sample addresses in Fig. 1 . In increasing order of complexity, these addresses appear to be:",
            "cite_spans": [
                {
                    "start": 128,
                    "end": 131,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 266,
                    "end": 270,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 370,
                    "end": 374,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 410,
                    "end": 414,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 450,
                    "end": 456,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Fig. 2."
        },
        {
            "text": "-an address with fixed IID value (::321).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 2."
        },
        {
            "text": "-an address with a structured value in the low 64 bits (perhaps a subnet distinguished by ::20). -a SLAAC address with EUI-64 Ethernet-MAC-based IID (ff:fe flag).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 2."
        },
        {
            "text": "-a SLAAC privacy address with a pseudorandom IID.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 2."
        },
        {
            "text": "Due to the semantic opacity of IPv6 addresses and the hybridization of multiple addressing schemes, the deep learning model may have difficulty in effectively training when learning the address structure. An address of the SLAAC format also has a highly randomized address structure, which is bound to pose a challenge to the generation task. However, to ensure that each addressing scheme can be included in the generation set, the selected seed set must contain all address patterns. Therefore, the target generation task requires the model to be able to effectively extract the underlying semantic information of IPv6 addresses. In addition, since the mixture of multiple structures, certain classification work on the seed set will alleviate the pressure on the model. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Considerations"
        },
        {
            "text": "In this section, we will introduce our approach and two seed classification methods for IPv6 target generation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approach"
        },
        {
            "text": "6GCVAE relies on stacked gated convolutional layers to form a Variational Autoencoder. The detailed model architecture is shown in Fig. 2 . We remove the colon in each address and leave the 32-bit hexadecimal as a sample input (e.g., 20010db8002000030000000000000301). Since each nybble may be one of 0-f characters, the alphabet size is 16 and we can arrive at a final input representation with a dimension of 32 \u00d7 16 after input embedding.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 131,
                    "end": 137,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "Approach"
        },
        {
            "text": "Our training model expects the generated address to be constantly approaching the input address to produce a new possible active target. To achieve the goal, the model is required to learn the distribution of the input by an encoder, sample latent vector and reconstruct the new generation by a decoder.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approach"
        },
        {
            "text": "The gated convolutional network enables to complete sequence tasks by adding a gating mechanism to the convolution. The structure is shown in Fig. 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 142,
                    "end": 148,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Gated Convolution Layer"
        },
        {
            "text": "We define the input embedding as E = [D 0 , ..., D i , ..., D 31 ], where D i represents the vector representation of the i-th nybble of the address. We use 32 3\u00d716 convolution kernels to convolve the input E to obtain a 32 \u00d7 32-dimensional output vector which is divided equally to vector A and vector B. Finally, we take the sigmoid function as the gate for the vector B in the second half to control the output of the vector A. The approach to compute the hidden layers H i can be summarized as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gated Convolution Layer"
        },
        {
            "text": "where \u03c3 is the sigmoid function and \u2297 is the element-wise product between matrices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gated Convolution Layer"
        },
        {
            "text": "Why Gated Convolution Layer. Using the gating method can effectively help us monitor the importance of each nybble of an IPv6 address. The convolution method also improves the sensitivity of the model to the relationship between each nybble of the address. This allows our model to be able to focus on address importance flags (e.g., the 23rd-26th nybbles of the EUI-64 address are always fffe) while discovering potential relationships between address nybbles (e.g., the fixed IID address typically has a contiguous 0).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gated Convolution Layer"
        },
        {
            "text": "In VAE models, a deterministic internal representation z (provided by the encoder) of an input x is usually replaced with a posterior distribution q(z|x). Inputs are then reconstructed by sampling z from this posterior and passing them through a decoder. After training, the model will mass-produce text or images by a generator. In this section, we will introduce the encoder, decoder, and generator structure in our approach.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Autoencoder"
        },
        {
            "text": "Encoder. In our model, we use two gated convolutional layers and an average pooling layer stack as the encoder for the model. In order to maintain the memory of the original input, we used a residual connection between each gated convolutional layer. According to the principle of VAE, we use two fully connected layers to train the mean \u03bc and the log variance log\u03c3 2 to learn the distribution of the input x.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Autoencoder"
        },
        {
            "text": "Decoder. To ensure that we can sample from any point of the latent space and still generate valid and diverse outputs, the posterior q(z|x) is regularized with its KL divergence from a prior distribution p(z). The prior is typically chosen to be a Gaussian with zero mean and unit variance. Then the latent vector z can be computed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Autoencoder"
        },
        {
            "text": "where is sampled from the prior.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Autoencoder"
        },
        {
            "text": "The decoder consists of a gated convolutional layer, fully connected layers, and a softmax activation. After sampling the latent vector z. We use the fully connected layer and adjust it to 32 \u00d7 16 dimensions as the input to the gated convolutional layer. Finally, the reconstructed address vector can be obtained through the fully connected layer and softmax activation function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Autoencoder"
        },
        {
            "text": "Our model loss consists of two parts, including the cross-entropy loss J xent and the KL divergence KL(q(z|x)||p(z)). The cross-entropy loss expects the smallest reconstruction error between the reconstructed vector y and the input seed x. The KL divergence constraint model samples from the standard normal distribution:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Autoencoder"
        },
        {
            "text": "Generator. After training, we use the trained decoder as a generator for batch generation of addresses. By sampling the 16-dimensional vector as a sample input in a standard normal distribution, the final generator outputs our ideal scan candidate. We set the sampling time N to control the number of targets we expect to generate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Autoencoder"
        },
        {
            "text": "Since IPv6 addresses include multiple addressing schemes, they are often intermixed in the seed set. Early classification of seeds with different structural patterns can help to improve the learning effect of the model on each structure of the address. The model then can generate addresses closer to the real structural pattern, which has greater possible activity. In this section, we will introduce two methods of seed classification that we used, including manual classification and unsupervised clustering.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seed Classification"
        },
        {
            "text": "Manual Classification. In Sect. 3.1, we discussed the possible structural composition of the address. In this paper, we divide the address into four categories in Fig. 1 , including fixed IID, low 64-bit subnet, SLAAC EUI-64, and SLAAC Privacy. We perform feature matching on the active seed set to estimate the address category to which the seed belongs:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 163,
                    "end": 169,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Seed Classification"
        },
        {
            "text": "-Fixed IID. The last 16 nybbles have a unique consecutive 0 in the address. It is speculated that the last 16 nybbles may consist of the fixed IID. -Low 64-bit subnet. The last 16 nybbles of the address have two or more consecutive 0 segments. It is speculated that it may consist of a subnet identifier and an IID. -SLAAC EUI-64. The 23rd-26th nybbles of the address are fffe.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seed Classification"
        },
        {
            "text": "-SLAAC privacy. After the statistics, the character appearance randomness of the last 16 nybbles is calculated, it is presumed to be a pseudo-random IID if the address has a high entropy value. We consider an address as SLAAC privacy if it has a greater entropy value than 0.8 (the highest is 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seed Classification"
        },
        {
            "text": "We perform an entropy clustering method on the seed set, which was proposed by Gasser et al. [7] . We applied the idea to the target generation algorithm for the first time.",
            "cite_spans": [
                {
                    "start": 93,
                    "end": 96,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Unsupervised Clustering."
        },
        {
            "text": "In an address set S, we define the probability P (x i ) for the character x i of the i-th nybble in an address, where x \u2208 \u03a9 = {0, 1, . .., f }. Then by calculating the entropy value H(X i ) for each nybble, we can get a fingerprint F a b of the address set S:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 124,
                    "end": 134,
                    "text": "= {0, 1, .",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Unsupervised Clustering."
        },
        {
            "text": "where a and b are the first and the last considered nybble, respectively. Since /32 prefix is a large prefix that administrators usually use, which containing enough active addresses, we extract F 9 32 for each /32 prefix network address set (all addresses have the same first 8 nybbles in each network address set) and use the k-means algorithm to cluster each network fingerprint to find similar entropy fingerprint categories.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised Clustering."
        },
        {
            "text": "In this section, we evaluate 6GCVAE effects. We will introduce the datasets used in the paper, the evaluation method, and our comparative experiment results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation"
        },
        {
            "text": "Our experimental datasets are mainly from two parts, a daily updated public dataset IPv6 Hitlist and a measurement dataset CERN IPv6 2018. Table 1 summarizes the datasets used in this paper. The public dataset IPv6 Hitlist is from the data scanning the IPv6 public list for daily active addresses, which is provided by Gasser et al. [7] . In addition, we passively collected address sets under the China Education and Research Network from March to July 2018. We continued to scan and track the IPs that are still active until October 14, 2019 as our measurement dataset. ",
            "cite_spans": [
                {
                    "start": 333,
                    "end": 336,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 139,
                    "end": 146,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "Scanning Method. To evaluate the activity of the generated address, we use the Zmapv6 tool [7] to perform ICMPv6, TCP/80, TCP/443, UDP/53, UDP/443 scans on the generated address. When the query sent by any scanning method gets a response, we will determine the address as active. Due to the difference in activity between hosts at different times, we maintain continuous scanning of the host for 3 days to ensure the accuracy of our method.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 94,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Method"
        },
        {
            "text": "Evaluation Metric. Since IPv6 target generation is different from text generation tasks, we need to define a new evaluation metric for the address generative model. In the case of a given seed set, N candidate represents the number of the generated candidate set, N hit represents the number of generated active addresses, N new represents the generated address that is active and not in the seed set. Then the active hit rate r hit and active generation rate r gen of the model can be computed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Method"
        },
        {
            "text": "We consider that r hit can represent the learning ability to learn from the seed set. r gen highlights the generation ability to generate new active addresses.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Method"
        },
        {
            "text": "First, we summarize our seed classification. After manual classification, the seed will be classified into four categories. Table 2 shows the classification details on the IPv6 Hitlist dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 124,
                    "end": 131,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Result of Seed Classification"
        },
        {
            "text": "For the unsupervised clustering, we use the elbow method to find the number of clusters, k, plotting the sum of squared errors (SSE) for k = {1, ..., 20}. We selected the value k = 6 for the point where increasing k does not yield a relatively large reduction in SSE. Figure 4 shows the results of the clustering. It is worth noting that there is a certain relationship between the seed classification results. We know that the closer the H(X i ) is to 0, the more likely the nybble is to be constant. The closer the H(X i ) is to 1, the more random the nybble is. Therefore, in Fig. 4 , Cluster 1-3 may be fixed IID or a low 64-bit subnet addresses. Cluster 5 is likely to be SLAAC EUI-64 addresses with the fffe flag. Cluster 4 and Cluster 6 are likely to be SLAAC privacy addresses because of the high entropy value of most of the nybbles.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 268,
                    "end": 276,
                    "text": "Figure 4",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 579,
                    "end": 585,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Result of Seed Classification"
        },
        {
            "text": "After seed classification, we trained 6GCVAE with each category of seed sets. Table 3 shows the effect of the model without seed classification, with manual classification and with unsupervised clustering on the IPv6 Hitlist dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 78,
                    "end": 85,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Result of Seed Classification"
        },
        {
            "text": "The model is trained by using the dataset or each category of seeds as a seed set and uses the generator to generate candidate targets after 1,000,000 samplings. We remove duplicate candidate targets and ultimately get a valid candidate set. The results show that seed classification can actually improve Table 3 . Model effect with 3 types of seed processing, including none of the seed classification, manual classification, and unsupervised clustering. the performance of the model. Among them, the most generated addresses are manually classified Fixed IID and unsupervised clustered Cluster 2 respectively in the two methods. However, Low 64-bit subnet, SLAAC EUI-64, and Cluster 3-6 show a lower r gen due to the complex address structure or lack of training samples. In addition, the model has a characteristic on the generation of SLAAC privacy addresses. All generated hits are new active targets. Because of the high randomness of this kind of address, the model may learn a high random structure, resulting in the generated addresses which are without duplicates. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 305,
                    "end": 312,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Result of Seed Classification"
        },
        {
            "text": "In order to verify the superiority of 6GCVAE, we built the baseline of the conventional VAE models by replacing the key components gated convolutional layer of 6GCVAE and compared them with our model. We also use the generator for 1,000,000 samples after training the model with the IPv6 Hitlist dataset. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparing with Conventional VAE Models"
        },
        {
            "text": "Entropy/IP [6] is the current state of the art address generation tool that can also efficiently generate active IPv6 targets. We compare the effects of 6GCVAE with Entropy/IP by training model and sampling 1,000,000 times for target generation as usual. As shown in Table 4 , the experimental results show that our model outperformed Entropy/IP under the IPv6 Hitlist dataset. Although the r hit of Entropy/IP is higher, its lower r gen indicates that it generates more addresses that are duplicated in the dataset. For representing the final effect of each seed classification method, we control the generation ratio of each type of address through r gen to maximum the generation of new active target N new . The ratio can be represented as (r gen1 : r gen2 : ... : r geni ), where i represents the category id of a seed classification method. Finally, we set the total sampling number N in each round of experiments and control the generation number of each category of seed set through the ratio. We then reached the best experimental results in Table 4 . 6GCVAE has been greatly improved with seed classification.",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 14,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 267,
                    "end": 274,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1051,
                    "end": 1058,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Comparing with Entropy/IP"
        },
        {
            "text": "In addition, in Fig. 5 , we evaluated N new and r gen by changing the sampling times N on the two datasets, which can prove the general generation ability of the models. Results indicate that our approach reaches a better performance than Entropy/IP. 6GCVAE found 1.60-1.79 times more hits than Entropy/IP. Under manual classification (MC) and unsupervised clustering (UC), the N new of 6GCVAE has been improved 1.52-1.85 and 2.50-3.67 times respectively. The seed classification methods have a higher r gen than all other approaches. Unsupervised clustering reached the best performance in our experiments.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 16,
                    "end": 22,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Comparing with Entropy/IP"
        },
        {
            "text": "In this paper, we explored the challenges of IPv6 target generation tasks. Our work achieved a new model 6GCVAE by constructing a gated convolutional Variational Autoencoder. In addition, we introduce two kinds of seed classification techniques, which effectively improve the address generation performance of the deep learning model. The results show that 6GCVAE is superior to the previous conventional VAE models. The address generation quality of 6GCVAE is better than the state of the art target generation algorithm Entropy/IP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Significance of IPv6 interface identifiers",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Carpenter",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Internet Engineering Task Force",
            "volume": "",
            "issn": "",
            "pages": "1--10",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Don't forget to lock the back door! a characterization of IPv6 network security policy",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Czyz",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Luckie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Allman",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bailey",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Language modeling with gated convolutional networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [
                        "N"
                    ],
                    "last": "Dauphin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Auli",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Grangier",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 34th International Conference on Machine Learning",
            "volume": "70",
            "issn": "",
            "pages": "933--941",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "ZMAP: fast internet-wide scanning and its security applications",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Durumeric",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Wustrow",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Halderman",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Presented as part of the 22nd {USENIX} security symposium ({USENIX} security 13)",
            "volume": "",
            "issn": "",
            "pages": "605--620",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Entropy/IP: uncovering structure in ipv6 addresses",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Foremski",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Plonka",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Berger",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 Internet Measurement Conference",
            "volume": "",
            "issn": "",
            "pages": "167--181",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Clusters in the expanse: understanding and unbiasing IPv6 hitlists",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Gasser",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Internet Measurement Conference",
            "volume": "",
            "issn": "",
            "pages": "364--378",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Convolutional sequence to sequence learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gehring",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Auli",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Grangier",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yarats",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "N"
                    ],
                    "last": "Dauphin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 34th International Conference on Machine Learning",
            "volume": "70",
            "issn": "",
            "pages": "1243--1252",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Auto-encoding variational Bayes",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1312.6114"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Target generation for internet-wide IPv6 scanning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Murdock",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bramsen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Durumeric",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Paxson",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 Internet Measurement Conference",
            "volume": "",
            "issn": "",
            "pages": "242--253",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Rfc3041: Privacy extensions for stateless address autoconfiguration in IPv6",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Narten",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "RFC",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "IPv6 stateless address autoconfiguration",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Narten",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Thomson",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Jinmei",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Temporal and spatial classification of active ipv6 addresses",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Plonka",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Berger",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 Internet Measurement Conference",
            "volume": "",
            "issn": "",
            "pages": "509--522",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Variational autoencoder for deep learning of images, labels and captions",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Pu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2352--2360",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A hybrid convolutional variational autoencoder for text generation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Semeniuta",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Severyn",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Barth",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1702.02390"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "On reconnaissance with IPv6: a pattern-based scanning approach",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ullrich",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kieseberg",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Krombholz",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Weippl",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "10th International Conference on Availability, Reliability and Security",
            "volume": "",
            "issn": "",
            "pages": "186--192",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Sample IPv6 addresses in presentation format with the low 64 bits shown bold.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Structure of the gated convolutional layer for IPv6 target generation. After convolution, the output of the vector A is controlled by the sigmoid value of vector B, which is used as an output gate to select the address vector.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The detail of unsupervised entropy clustering on the IPv6 Hitlist dataset. We obtained 6 clusters of all /32 prefix networks and the nybble distribution of each cluster.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The comparative experiments result by comparing with Entropy/IP on the two datasets. Nnew and rgen are evaluated under the different sampling times N .",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The detail of the two active address datasets we used in the paper.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The detail of manual classification on the IPv6 Hitlist dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "The comparative experiments result by comparing with 5 conventional VAE models and Entropy/IP. Results show that unsupervised clustering reached the best performance in our experiments.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "summarizes the results of the comparative experiments. The results show that due to the inability of feedforward neural networks to well capture semantic information, the FNN VAE displays a difficulty to complete the IPv6 target generation task. RNN VAE and Convolutional VAE only focus on sequence relationships or structure information, thus causing lower hits. By promoting the simple RNN layer to LSTM or GRU, the VAE model gets better performance than RNN VAE. Finally, 6GCVAE performs best under this task because of learning both the key segment structure and segment relationship information of an address.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}