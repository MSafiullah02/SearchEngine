{
    "paper_id": "a8d27aef2e0bdd4a25183382efe4c6cb23fbb6cb",
    "metadata": {
        "title": "Learning Undirected Graphs in Financial Markets",
        "authors": [
            {
                "first": "Jos\u00e9",
                "middle": [],
                "last": "Vin\u00edcius",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Electronic and Computer Engineering Department of Industrial Engineering and Decision Analytics The Hong Kong University of Science and Technology Clear Water Bay",
                    "institution": "The Hong Kong University of Science and Technology Clear Water Bay",
                    "location": {
                        "settlement": "Hong Kong, Hong Kong"
                    }
                },
                "email": ""
            },
            {
                "first": "Miranda",
                "middle": [],
                "last": "Cardoso",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Electronic and Computer Engineering Department of Industrial Engineering and Decision Analytics The Hong Kong University of Science and Technology Clear Water Bay",
                    "institution": "The Hong Kong University of Science and Technology Clear Water Bay",
                    "location": {
                        "settlement": "Hong Kong, Hong Kong"
                    }
                },
                "email": ""
            },
            {
                "first": "Daniel",
                "middle": [
                    "P"
                ],
                "last": "Palomar",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Electronic and Computer Engineering Department of Industrial Engineering and Decision Analytics The Hong Kong University of Science and Technology Clear Water Bay",
                    "institution": "The Hong Kong University of Science and Technology Clear Water Bay",
                    "location": {
                        "settlement": "Hong Kong, Hong Kong"
                    }
                },
                "email": "palomar@ust.hk"
            }
        ]
    },
    "abstract": [
        {
            "text": "We investigate the problem of learning undirected graphical models under Laplacian structural constraints from the point of view of financial market data. We show that Laplacian constraints have meaningful physical interpretations related to the market index factor and to conditional correlations between stocks. Those interpretations lead to a set of guidelines that users should be aware of when estimating graphs in financial markets. In addition, we propose algorithms to learn undirected graphs that account for stylized facts and tasks intrinsic to financial data such as non-stationarity and stock clustering.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Learning the structure of general graphical models is an NPhard task [1] whose importance is critical towards visualizing, understanding, and leveraging the full potential contained in the data that live in such structures. Learning graphs from data is a fundamental problem in the statistical graph learning and signal processing fields [2] - [4] , having a direct impact on applications such as unsupervised learning, clustering, and applied finance [5] - [7] .",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 72,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 338,
                    "end": 341,
                    "text": "[2]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 344,
                    "end": 347,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 452,
                    "end": 455,
                    "text": "[5]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 458,
                    "end": 461,
                    "text": "[7]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Nonetheless, most existing techniques for learning graphs are often unable to impose a particular graph structure due to their inability to incorporate prior information in the learning process. Moreover, most graph learning frameworks are designed towards static networks [3] , [4] , [8] , [9] which inherently neglect dynamic time-domain variations in real data. As a consequence, they usually lack practicality especially in non-stationary data regimes, which is often the case in data from financial stock markets.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 276,
                    "text": "[3]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 279,
                    "end": 282,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 285,
                    "end": 288,
                    "text": "[8]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 291,
                    "end": 294,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Motivated by practical applications such as clustering of stocks and understanding their time-domain variations, we investigate the problem of estimating graph matrices whose structure follow those of Laplacian matrices of undirected weighted graphs in a financial context both for static and dynamic graphs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The main contributions of our paper are as follows: 1) We for the first time provide natural interpretations for the Laplacian constraints of graphs estimated from stock market data. This leads to meaningful and intuitive guidelines on the data processing required prior to learning graphs. 2) We show that rank constraints alone, a practice often used by state-of-the-art methods, are not sufficient to learn k-component graphs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "3 ) We propose novel formulations to learn: i) k-component graphs and ii) time-varying graphs. 4) We develop simple trading strategies as a result of the estimated time-varying graphs.",
            "cite_spans": [
                {
                    "start": 2,
                    "end": 3,
                    "text": ")",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "A graph is denoted as a triple G = (V, E, W ), where V = {1, 2, . . . , p} is the vertex (or node) set, E \u2286 {{u, v} : u, v \u2208 V} is the edge set, that is, a subset of the set of all possible unordered pairs of p nodes such that {u, v} \u2208 E if and only if nodes u and v are connected. We denote the number of elements in E by |E|. W \u2208 R p\u00d7p + is the symmetric weighted adjacency matrix that satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "An attractive improper Gaussian Markov Random Field (GMRF) [10] is denoted as a p-dimensional, real-valued, Gaussian random variable x with mean vector \u00b5 and low-rank precision (inverse covariance) matrix \u039e. The data generating process is assumed to be a zero-mean, attractive improper GMRF x \u2208 R p , such that x i is the random variable generating a signal measured at node i, whose low-rank precision matrix is modeled as a graph Laplacian matrix. Assume we are given n observations from x, i.e., X \u2208 R n\u00d7p , X = [x 1 , x 2 , . . . , x p ], x i \u2208 R n . The Penalized Maximum Likelihood Estimator of the precision matrix of x, on the basis of the observed data X, is",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "text": "[10]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "where S is a similarity matrix, such as sample covariance (or correlation) matrix S \u221d X X, gdet(L) is the pseudo determinant of L, i.e., the product of its positive eigenvalues [11] , and h \u03b1 (L) is a regularization function, with hyperparameter vector \u03b1, to promote certain properties on L such as sparsity. Problem (1) is a fundamental problem in the graph signal processing field that has served as a cornerstone for many extensions, primarily those involving the inclusion of structure onto L [3], [6] . Even though Problem (1) is convex, assuming h \u03b1 (\u00b7) is convex, it is not adequate to be solved by disciplined convex programming languages, such as cvxpy [12] , due to scalability issues related to the computation of log gdet(L). Indeed, recently a few works have proposed algorithms based on Majorization-Minimization (MM) [13] , and Alternating Direction Method of Multipliers (ADMM) [14] to solve Problem (1) in an efficient and scalable fashion [3] , [4] .",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 502,
                    "end": 505,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 662,
                    "end": 666,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 832,
                    "end": 836,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 894,
                    "end": 898,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 957,
                    "end": 960,
                    "text": "[3]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 963,
                    "end": 966,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "On the other hand, due to nuisances involved in dealing with the term log gdet(L), several works focused on the assumption that the underlying signals in a graph are smooth [8] , [9] . In its simplest form, learning a smooth graph from a data matrix X \u2208 R n\u00d7p is tantamount to finding an adjacency matrix W that minimizes the Dirichlet energy:",
            "cite_spans": [
                {
                    "start": 173,
                    "end": 176,
                    "text": "[8]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 179,
                    "end": 182,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "In order for Problem (2) to be well-defined, several constraints have been proposed in the literature. For instance, Kalofolias et al. [9] proposed a convex formulation as follows (3) is convex and can be solved via primal-dual, ADMM-like algorithms [9] . It can be seen that the objective function in Problem (3) is actually an approximation to that of Problem (1) where the gdet(L) term has been upper bounded by p i=1 L ii . Therefore, Problem (3) can be thought of as an approximation of the penalized maximum likelihood estimator.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 24,
                    "text": "(2)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 135,
                    "end": 138,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 250,
                    "end": 253,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "A formulation to estimate k-component graphs based on the smooth signal approach was proposed in [5] . They proposed a two-stage algorithm where it first estimates a connected graph using, e.g., Problem (3) and then it projects the graph onto the set of Laplacian matrices of dimension p with rank p \u2212 k, where k is the given number of graph components.",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 100,
                    "text": "[5]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "Spectral constraints on the Laplacian matrix are an intuitive way to recover k-component graphs as the multiplicity of its zero eigenvalue, i.e., the nullity of L, dictates the number of components of a graph. The first proposed framework to impose structures on the estimated Laplacian matrix, under the attractive improper GMRF model, was done by Kumar et al. [6] , [15] , through the use of spectral constraints, as follows",
            "cite_spans": [
                {
                    "start": 362,
                    "end": 365,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 368,
                    "end": 372,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "(4) Note that Problem (4) learns a k-component graph without the need for a two-stage algorithm. However, a clear shortcoming of this formulation is that it does not control the degrees of the nodes in the graph, which may result in a trivial solution that contains isolated nodes, turning out not to be useful for clustering tasks especially when applied to noisy data sets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. BACKGROUND AND RELATED WORK"
        },
        {
            "text": "FOR STOCK SIGNALS Graphical representations of data are increasingly important tools in financial signal processing to uncover hidden relation-ships between variables [7] . In stock markets, one is generally interested in learning about conditional dependencies among stocks and how to leverage this information into practical scenarios such as portfolio design and crisis forecasting.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 170,
                    "text": "[7]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "Mathematically, we would like to estimate a precision matrix L that enjoys the following two key properties",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "The first property states that the Laplacian matrix L is singular and its null space contains the 1 vector. That means that any signal sampled from L, say x, is constrained to a subspace of rank p \u2212 1 satisfying x 1 = 0. In practice, (P1) implies that signals living in a graph G have zero graph-mean.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "Property (P2) together with (P1) implies that L is positive semidefinite. The fact that the off diagonal entries are nonpositive means that the Laplacian matrix only represents non-negative conditional dependencies 1 . This assumption is often met for stock data as assets are typically positively dependent [16] .",
            "cite_spans": [
                {
                    "start": 308,
                    "end": 312,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "These two properties along with efficient learning frameworks make the Laplacian-based graphical model a natural candidate for learning graphs of stock data. As a consequence of using the Laplacian model, we propose the following guidelines when estimating Laplacian matrices with stock market data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "\u2022 Correlation vs Covariance: Both the GMRF and smooth signal approaches rely on the term tr(SL) \u221d tr(W Z), where S is the sample covariance matrix. From the definition of Z (3), we observe that two perfectly correlated stocks but with large Euclidean distances would appear very different on the graph. Hence, we should use the sample correlation matrix S = Diag(S) \u22121/2 SDiag(S) \u22121/2 in case we want two highly correlated stocks to have a strong graph connection regardless of their variances. \u2022 Removing the market trend: A widely used and tested model for the returns of the stocks includes explicitly the dependency on the market factor:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "where \u03b2 denotes the market loadings, x mkt,t denotes the market index, and t is the residual idiosyncratic component with covariance matrix \u03a8. Since all the stocks are heavily dominated by the market index x mkt,t , it may be convenient to remove that component if we seek to explore the structure of the residual cross-dependency among the stocks, t . Thus, an alternative to using the full covariance matrix \u03a3 is to use the covariance matrix \u03a8 of the idiosyncratic component. However, since \u03b2 \u2248 1, it turns out that the market factor is automatically removed in the squared distance matrix Z:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "Even more interestingly, if one first normalizes each stock, whose variances are V(x (i) ) \u2248 \u03b2 2 i , we have 1 The correlation between any two pair of nodes conditioned on the rest of the graph is given as \u2212",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 110,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "x = 1x mkt +\u00af t , which implies that the market factor is automatically removed in the squared distance matrix. \u2022 Degree control: Enforcing a rank smaller than p \u2212 1 for the Laplacian matrix will generate a k-component graph, which is one desired goal. However, one may get the undesired result of having isolated nodes. The way to avoid isolated nodes is by controlling the degrees of the nodes. The GMRF formulation has the natural penalty term log gdet(L) in the objective, but that does not help in controlling the degrees of the nodes. Instead, some of the graph learning formulations from smooth signals include degree control via the constraint W 1 = 1, which fixes the degrees of all the nodes to 1. The regularization term 1 log(W 1) also avoids the trivial solution of any degree equals 0. Hence, any graph learning formulation that enforces a k-component graph (or low-rank Laplacian matrix) should also control the degrees of the nodes to avoid a trivial solution with isolated nodes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. GRAPH LAPLACIAN CONSTRAINTS INTERPRETATION"
        },
        {
            "text": "In this section, we propose graph learning formulations to account for: 1) k-component structures and 2) non-stationarity of financial stock market data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. PROPOSED FORMULATIONS"
        },
        {
            "text": "We propose the following formulation to learn a kcomponent graph:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. k-component graphs: GMRF formulation"
        },
        {
            "text": "Problem (6) is highly non-convex and non-differentiable due to the constraint rank(L) = p \u2212 k, which is equivalently to saying that the sum of the k smallest eigenvalues of L is equal to zero, i.e., k i=1 \u03bb i (L) = 0 [5] (assuming eigenvalues in increasing order). By Fan's theorem [17] ,",
            "cite_spans": [
                {
                    "start": 217,
                    "end": 220,
                    "text": "[5]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 282,
                    "end": 286,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "A. k-component graphs: GMRF formulation"
        },
        {
            "text": "Thus, a relaxed version of Problem (6) becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. k-component graphs: GMRF formulation"
        },
        {
            "text": "Even though still non-convex, but now differentiable, Problem (8) can be solved in an alternating fashion. More precisely, for a given L, say L l , we have the following subproblem for V : minimize",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. k-component graphs: GMRF formulation"
        },
        {
            "text": "whose solution is given by the k eigenvectors associated with the k smallest eigenvalues of L l [18] .",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 100,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "A. k-component graphs: GMRF formulation"
        },
        {
            "text": "For a fixed value of V , say V l , we have the following subproblem for L:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. k-component graphs: GMRF formulation"
        },
        {
            "text": "Problem (10) is convex and can be solved efficiently by, e.g., the methods developed in [3] , [4] , [6] . Algorithm 1 summarizes the implementation to solve Problem (8). ",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 91,
                    "text": "[3]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 94,
                    "end": 97,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 100,
                    "end": 103,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "A. k-component graphs: GMRF formulation"
        },
        {
            "text": "Most graph learning frameworks are designed towards static graphs, which inherently neglect time-domain variations in real data. As a result, they usually lack practicality for finance especially during nonstationary regimes, e.g., an economic crisis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Time-varying graphs"
        },
        {
            "text": "A time-varying graph is defined as a sequence of graphs stacked over time, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Time-varying graphs"
        },
        {
            "text": "We assume the node set of each graph to be the same, i.e., V t = {1, 2, . . . , p} , \u2200 t = 1, 2, . . . , T .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Time-varying graphs"
        },
        {
            "text": "The data generating process is as follows. Assume that for every graph G t we associate an attractive improper GMRF",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Time-varying graphs"
        },
        {
            "text": "where L t is the precision matrix of the tth GMRF that is assumed to have graph Laplacian structure. Further, suppose we are given n t observations from x t , i.e., X t \u2208 R nt\u00d7p , then we propose the following optimization program to learn the Laplacian matrices {L t } T t=1 on the basis",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Time-varying graphs"
        },
        {
            "text": "t=1 is a sequence of similarity matrices, d : R p\u00d7p \u00d7 R p\u00d7p \u2192 R + is a distance function that measures the similarity between L t and L t\u22121 in order to impose time consistency, e.g., d(L t , L t\u22121 ) L t \u2212 L t\u22121 2 F , and \u03b4 \u2208 R ++ is its corresponding hyperparameter. The solution to Problem (11) is obtained on a rolling window basis, which is tantamount to a causal estimatorL t|t , t = 1, . . . , T . In other words, to estimate, e.g., L 3 , we only use information up to and including time t = 3, i.e., {S t } 3 t=1 . The time-varying graph learning formulations proposed in [19] and [20] are not adequate to our particular scenario because those formulations are based on the smooth signal approach, rather than the statistical GMRF model, and solve a dynamic graph conditioned on all the T chunks of observations, i.e.,L t|T , t = 1, . . . , T , which inevitably introduces look-ahead biases.",
            "cite_spans": [
                {
                    "start": 291,
                    "end": 295,
                    "text": "(11)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 578,
                    "end": 582,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 587,
                    "end": 591,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "B. Time-varying graphs"
        },
        {
            "text": "Algorithm 2 summarizes the implementation to solve Problem (11) . Remark: for practical programming language implementation of the proposed algorithms, one can take into account that the Laplacian matrix L is symmetric and its diagonal elements are mappings of the off-diagonal ones. Therefore, only elements above (or below) the diagonal of L need to be updated. This approach is similar to that used by Kalofolias et al. [9] and Kumar et al. [6] .",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "text": "(11)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 423,
                    "end": 426,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 444,
                    "end": 447,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "B. Time-varying graphs"
        },
        {
            "text": "In the experiments that follow, we use log-returns timeseries data from stocks belonging to three sectors of the S&P500 index, namely Industrials, Consumer Staples, and Energy. We collect price data of 130 stocks from Jan. 1st 2016 to Jan. 1st 2019 from Yahoo! Finance, which represents 753 days worth of data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. NUMERICAL EXPERIMENTS"
        },
        {
            "text": "In order to measure the effects of the market factor and the data scaling, we learn four graphs using the two-stage algorithm proposed by [5] . Fig. 1a shows that removing the market and using the covariance matrix as input leads to a graph with many possibly spurious connections (grey edges). Fig. 1b shows that using the sample correlation matrix introduces an improvement, but there exist still many spurious dependencies. Fig. 1c shows that not removing the market presents an improvement, but the use of the covariance matrix leads to many possibly fake connections. Fig. 1d combines the correlation matrix as input and not removing the market, which clearly shows a meaningful representation of a graph from stocks belonging to three distinct sectors.",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 141,
                    "text": "[5]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 144,
                    "end": 151,
                    "text": "Fig. 1a",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 295,
                    "end": 302,
                    "text": "Fig. 1b",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 427,
                    "end": 434,
                    "text": "Fig. 1c",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 573,
                    "end": 580,
                    "text": "Fig. 1d",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Effects of market factor and data scaling"
        },
        {
            "text": "To illustrate the importance of controlling the nodes degrees while learning k-component graphs, we conduct a comparison between the algorithm proposed in [6] and Algorithm 1 on the basis of the sample correlation matrix. Fig. 2 shows the estimated financial stock networks with k = 3. It is clear that SGL [6] (benchmark) is unable to account for the trivial solution with isolated nodes, whereas the proposed algorithm returns a graph with a meaningful representation. shows a meaningful estimated structure. We fixed the \u03b2 = 10 for both methods.",
            "cite_spans": [
                {
                    "start": 155,
                    "end": 158,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 307,
                    "end": 310,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 222,
                    "end": 228,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "B. Effect of degree control"
        },
        {
            "text": "We consider an experiment with data from FAAMUNG companies (Facebook, Apple, Amazon, Microsoft, Uber, Netflix, and Google) from June 1st 2019 to May 1st 2020, totalling 230 days worth of data, which includes the most recent economic crisis due to the pandemic associated with COVID-19. For each period of 30 days we estimate graphs on a rolling window fashion, shifting the window one day at a time, on the basis of the sample correlation matricesS 1 , . . . ,S 200 . At the end, we estimate 200 graphs, i.e.,L 1 , . . . ,L 200 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Time-varying Experiment"
        },
        {
            "text": "Based on this 200 estimated graphs we compute the algebraic connectivity, i.e., the second smallest eigenvalue of L t , as an indicator of variation of the graph. We use this indicator to acquire insights on possible trends of the stock market. Note that other indicators could be used in practice such as the spectral radius, \u03bb max (L t ), and the time consistency, L t \u2212L t\u22121 2 F . Fig. 3a shows the S&P500 log-price, where the impact of the COVID-19 pandemic is clear around March 2020. Fig. 3b depicts the algebraic connectivity indicator computed from each estimated graph along the time axis. Fig. 3c shows the evolution of the graph network at certain dates. It is clear both from the indicator (Fig. 3b ) and the network visualization (Fig. 3c ) that around September 2019 the market has changed significantly. That is consistent with news involving the impeachment inquiry of US President Donald J. Trump. From the middle of March 2020 to the beginning of May the market saw its largest drop since the financial crisis in 2008. This can also be noticed through the indicator and the network visualization. . Panel (c) shows the estimated networks with \u03b4 = 100 for several dates. It can be noticed that there is an increase in conditional correlation among the stocks during times of economic crisis.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 384,
                    "end": 391,
                    "text": "Fig. 3a",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 490,
                    "end": 497,
                    "text": "Fig. 3b",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 599,
                    "end": 606,
                    "text": "Fig. 3c",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 702,
                    "end": 710,
                    "text": "(Fig. 3b",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 743,
                    "end": 751,
                    "text": "(Fig. 3c",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "C. Time-varying Experiment"
        },
        {
            "text": "By leveraging the proposed time-varying graph learning algorithm, we perform an experiment comparing two simple trading strategies: (S1) uniformly invest a unit of budget during the whole period; (S2) uniformly invest a unit of budget according to whether of not the algebraic connectivity of the graph falls below a fixed threshold of \u03c4 = 1.0. Fig. 4 shows the cumulative sum of the profits and losses (PnL) over time for (S1) and (S2). It can be observed that (S2) outperforms (S1) by smartly entering/exiting the market based on the algebraic connectivity of the estimated graphs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 345,
                    "end": 351,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "D. Trading Application"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Highdimensional Gaussian graphical model selection: Walk summability and local separation criterion",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anandkumar",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "Y F"
                    ],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Willsky",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Journal of Machine Learning Research",
            "volume": "13",
            "issn": "1",
            "pages": "2293--2337",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "The cumulative PnL of strategy (S2), which enters/exits the market according to the knowledge of algebraic connectivity of the estimated graph, outperforms that of strategy (S1)",
            "authors": [],
            "year": null,
            "venue": "Uniform investment strategies on FAAMUNG companies",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Discovering structure by learning sparse graph",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "M"
                    ],
                    "last": "Lake",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Tenenbaum",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 33rd Annual Cognitive Science Conference",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Graph learning from data under Laplacian and structural constraints",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "E"
                    ],
                    "last": "Egilmez",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Pavez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ortega",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Journal of Selected Topics in Signal Processing",
            "volume": "11",
            "issn": "6",
            "pages": "825--841",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Optimization algorithms for graph laplacian estimation via admm and mm",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Palomar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Signal Processing",
            "volume": "67",
            "issn": "16",
            "pages": "4231--4244",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "The constrained Laplacian rank algorithm for graph-based clustering",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, ser. AAAI'16",
            "volume": "",
            "issn": "",
            "pages": "1969--1976",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A unified framework for structured graph learning via spectral constraints",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "V"
                    ],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cardoso",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Palomar",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Machine Learning Research",
            "volume": "21",
            "issn": "",
            "pages": "1--60",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "A review of two decades of correlations, hierarchies, networks and clustering in financial markets",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Marti",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Nielsen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bikowski",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Donnat",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1703.00485"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Learning Laplacian matrix in smooth graph signal representations",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Thanou",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Frossard",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Vandergheynst",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Transactions on Signal Processing",
            "volume": "64",
            "issn": "23",
            "pages": "6160--6173",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "How to learn a graph from smooth signals",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kalofolias",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning Research",
            "volume": "51",
            "issn": "",
            "pages": "920--929",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Gaussian Markov Random Fields: Theory And Applications (Monographs on Statistics and Applied Probability). Chapman & Hall/CRC",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rue",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Held",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Cauchybinet for pseudo-determinants",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Linear Algebra and its Applications",
            "volume": "459",
            "issn": "",
            "pages": "522--547",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "CVXPY: A Python-embedded modeling language for convex optimization",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Diamond",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Boyd",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Journal of Machine Learning Research",
            "volume": "17",
            "issn": "83",
            "pages": "1--5",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Majorization-minimization algorithms in signal processing, communications, and machine learning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Babu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Palomar",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Transactions on Signal Processing",
            "volume": "65",
            "issn": "3",
            "pages": "794--816",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Boyd",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parikh",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Peleato",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Eckstein",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Foundations and Trends in Machine Learning",
            "volume": "3",
            "issn": "1",
            "pages": "1--122",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Structured graph learning via laplacian spectral constraints",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "V"
                    ],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cardoso",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Palomar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Covariance Matrix Estimation under Total Positivity for Portfolio Selection",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Roy",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Uhler",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "On a theorem of Weyl concerning eigenvalues of linear transformations i",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                }
            ],
            "year": 1949,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "35",
            "issn": "11",
            "pages": "652--655",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Optimization Algorithms on Matrix Manifolds",
            "authors": [
                {
                    "first": "P.-A",
                    "middle": [],
                    "last": "Absil",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Mahony",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sepulchre",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Learning time varying graphs",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kalofolias",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Loukas",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Thanou",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Frossard",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
            "volume": "",
            "issn": "",
            "pages": "2826--2830",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Time-varying graph learning with constraints on graph temporal variation",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "O"
                    ],
                    "last": "Yamada",
                    "suffix": ""
                },
                {
                    "first": "Yuichi",
                    "middle": [],
                    "last": "Tanaka",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2001.03346"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "GMRF k-component graph learning Data: Similarity matrix S, initial estimate L 0 , rank constraint hyperparameter \u03b2 > 0. Result: Laplacian estimation L 1 while not converged do 2 update V l+1 by solving Problem (9) fixing L at L l 3 update L l+1 by solving Problem (10) fixing V at V l+1 4 end",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Time-varying graph learning Data: Similarity matrices S 1 , . . . , S T , number of observations {n t } T t=1 Result: Causal Graph Laplacians estimates {L t|t }",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Graphs estimated with different data preprocessing. Panel (d) gives the best results with scaled data and no market removed.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Effect of lack of degree control on the optimization formulation. Benchmark algorithm SGL, panel (a), fails to obtain a meaningful solution, showing that spectral constraints alone are not sufficient to recover a non-trivial k-component graph. The proposed algorithm in panel (b)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "indicator with rolling window and time consistency (\u03b4 = 100)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Estimated time-varying indicators and network visualization of FAAMUNG companies during 2019-2020. Panel (a) shows the log-price of the S&P500 index. Panel (b) show the algebraic connectivity indicator for \u03b4 = 100",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}