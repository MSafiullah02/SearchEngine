{
    "paper_id": "126fb7df6bcab2b70000dfe5b940ada63ae1ba6a",
    "metadata": {
        "title": "COVID-TWITTER-BERT: A NATURAL LANGUAGE PROCESSING MODEL TO ANALYSE COVID-19 CONTENT ON TWITTER",
        "authors": [
            {
                "first": "Martin",
                "middle": [],
                "last": "M\u00fcller",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Digital Epidemiology Lab EPFL Geneva",
                    "institution": "",
                    "location": {
                        "settlement": "Geneva",
                        "country": "Switzerland, Switzerland, Spain"
                    }
                },
                "email": "martin.muller@epfl.ch"
            },
            {
                "first": "Marcel",
                "middle": [],
                "last": "Salath\u00e9",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Digital Epidemiology Lab EPFL Geneva",
                    "institution": "",
                    "location": {
                        "settlement": "Geneva",
                        "country": "Switzerland, Switzerland, Spain"
                    }
                },
                "email": "marcel.salathe@epfl.ch"
            },
            {
                "first": "Per",
                "middle": [
                    "E"
                ],
                "last": "Kummervold",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Digital Epidemiology Lab EPFL Geneva",
                    "institution": "",
                    "location": {
                        "settlement": "Geneva",
                        "country": "Switzerland, Switzerland, Spain"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10-30% marginal improvement compared to its base model, BERT-LARGE, on five different classification datasets. The largest improvements are on the target domain. Pretrained transformer models, such as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural language processing tasks, including classification, question-answering and chatbots. CT-BERT is optimised to be used on COVID-19 content, in particular from social media.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Twitter has been a valuable source of news and a public medium for expression during the COVID-19 pandemic. However, manually classifying, filtering and summarising the large amount of information available on COVID-19 on Twitter is impossible and has also been a challenging task to solve with tools from the field of machine learning and natural language processing (NLP). To improve our understanding of Twitter messages related to COVID-19 content as well as the analysis of this content, we have therefore developed a model called COVID-Twitter-BERT (CT-BERT) 1 .",
            "cite_spans": [
                {
                    "start": 565,
                    "end": 566,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Transformer-based models have changed the landscape of NLP. Models such as BERT, RoBERTa and AL-BERT are all based on the same principle -training bi-directional transformer models on huge unlabelled text corpuses [1, 2, 3, 4] . This process is done using methods such as mask language modelling (MLM), next sentence prediction (NSP) and sentence order prediction (SOP). Different models vary slightly in how these methods are applied, but in general, all training is done in a fully unsupervised manner. This process generates a general language model that is then used as input for a supervised finetuning for specific language processing tasks, such as classification, question-answering models, and chatbots.",
            "cite_spans": [
                {
                    "start": 214,
                    "end": 217,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 218,
                    "end": 220,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 221,
                    "end": 223,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 224,
                    "end": 226,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our model is based on the BERT-LARGE (English, uncased, whole word masking) model. BERT-LARGE is trained mainly on raw text data from Wikipedia (3.5B words) and a free book corpus (0.8B words) [2] . Whilst this is an impressive amount of text, it still contains little information about any specific subdomain. To im-prove performance in subdomains, we have seen numerous transformer-based models trained on specialised corpuses. Some of the most popular ones are BIOBERT [5] and SCIBERT [6] . These models are trained using the exact same unsupervised training techniques as the main models (MLM/NSP/SOP). They can be trained from scratch, but this requires a very large corpus, so a more common approach is to start with the trained weights from a general model. In this study, this process is called domain-specific pretraining. When trained, such models can be used as replacements for general language models and be trained for downstream tasks.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 196,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 472,
                    "end": 475,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 488,
                    "end": 491,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The CT-BERT model is trained on a corpus of 160M tweets about the coronavirus collected through the Crowdbreaks platform [7] during the period from January 12 to April 16, 2020. Crowdbreaks uses the Twitter filter stream API to listen to a set of COVID-19-related keywords 2 in the English language. Prior to training, the original corpus was cleaned for retweet tags. Each tweet was pseudonymised by replacing all Twitter usernames with a common text token. A similar procedure was performed on all URLs to web pages. We also replaced all unicode emoticons with textual ASCII representations (e.g. :smile: for ) using the Python emoji library 3 . In the end, all retweets, duplicates and close duplicates were removed from the dataset, resulting in a final corpus of 22.5M tweets that comprise a total of 0.6B words. The domain-specific pretraining dataset therefore consists of 1/7th the size of what is used for training the main base model. Tweets were treated as individual documents and segmented into sentences using the spaCy library [8] .",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 124,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1042,
                    "end": 1045,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "All input sequences to the BERT models are converted to a set of tokens from a 30 000-word vocabulary. As all Twitter messages are limited to 280 characters, this allows us to reduce the sequence length to 96 tokens, thereby increasing the training batch sizes to 1024 examples. We use a dupe factor of 10 on the dataset, resulting in 285M training examples and 2.5M validation examples. A constant learning rate of 2e-5, as recommended on the official BERT GitHub 4 when doing domain-specific pretraining.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "Loss and accuracy was calculated through the pretraining procedure. For every 100 000 training steps, we therefore save a checkpoint and finetune this towards a variety of downstream classification tasks. Distributed training was performed using Tensorflow 2.2 on a TPU v3-8 (128GB of RAM) for 120 h.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "To assess the performance of our model on downstream classification tasks, we selected five independent training sets. Three of them are publicly available datasets, and two are from internal projects not yet published. All datasets consist of Twitter-related data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation"
        },
        {
            "text": "This dataset is a subsample of the data used for training CT-BERT, specifically for the period between January 12 and February 24, 2020. Annotators on Amazon Turk (MTurk) were asked to categorise a given tweet text into either being a personal narrative (33.3%) or news (66.7%). The annotation was performed using the Crowdbreaks platform [7] .",
            "cite_spans": [
                {
                    "start": 339,
                    "end": 342,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "COVID-19 Category (CC)"
        },
        {
            "text": "This dataset contains a collection of measles-and vaccination-related US-geolocated tweets collected between March 2, 2011 and October 9, 2016. The dataset was first used by Pananos et al. [9] , but a modified version from M\u00fcller et al. [7] was used here. The dataset contains three classes: positive (towards vaccinations) (51.9%), negative (7.1%) and neutral/others (41.0%). The neutral category was used for tweets which are either irrelevant or ambiguous. Annotation was performed on MTurk.",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 192,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 237,
                    "end": 240,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Vaccine Sentiment (VS)"
        },
        {
            "text": "The dataset is from a so far unpublished project related to the stance towards the use of maternal vaccines. Experts in the field annotated the data into four categories: neutral (41.0%), discouraging (25.3%), promotional (43.9%) and ambiguous (14.3%). Each tweet was annotated threefold, and disagreement amongst the experts was resolved in each case by using a common scoring criterion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maternal Vaccine Stance (MVS)"
        },
        {
            "text": "This is an open dataset from SemEval-2016 Task 4: Sentiment Analysis in Twitter [10] . In particular, we used the dataset for subtask A, a dataset annotated fivefold into three categories: negative (15.7%), neutral (45.9%) and positive (38.4%). We make a small adjustment to this dataset by fully anonymising links and usernames.",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 84,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Twitter Sentiment SemEval (SE)"
        },
        {
            "text": "SST-2 is a public dataset consisting of binary sentiment labels, negative (44.3%) and positive (55.7%), within sentences [11] . Sentences were extracted from a dataset of movie reviews [12] and did not originate from Twitter, making SST-2 our only non-Twitter dataset.",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 125,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 185,
                    "end": 189,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Stanford Sentiment Treebank 2 (SST-2)"
        },
        {
            "text": "The dataset split size is predefined for the SST-2 and SE datasets. For the SST-2 dataset, the test dataset is not released. For the other datasets, we aimed at a split of around 50%-30% between the training and development sets, leaving a test set of 20% which was not used in this work. Our intention was not to optimise the finetuned models but to thoroughly evaluate the performance of the domain-specific CT-BERT-model. We experimented with different numbers of epochs for each training dataset for BERT-LARGE (i.e. checkpoint 0 of CT-BERT) and selected the optimal one. We then used this number in sub-sequent experiments on the respective dataset. We ended with three epochs for SST-2, CC and SE, five epochs for VC and 10 epochs for MVC, all with a learning rate of 2e-05. The number of epochs was dependent on both the size and balance of the categories. Larger and unbalanced sets require more epochs. Figure 1 shows the progress of pretraining CT-BERT at intervals of 25k training steps and the evaluation of 1k steps on a held-out validation dataset. All metrics considered improve throughout the training process. The improvement on the MLM loss task is most notable and yields a final value of 1.48. The NSP task improves only marginally, as it already performs very well initially. Training was stopped at 500 000, an equivalent of 512M training examples, which we consider as our final model. This corresponds to roughly 1.8 training epochs. All metrics for the MLM and NLM tasks improve steadily throughout training. However, using loss/metrics for these tasks to evaluate the correct time to stop training is difficult.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 912,
                    "end": 920,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Stanford Sentiment Treebank 2 (SST-2)"
        },
        {
            "text": "To assess the performance of our model properly, we compared the mean F1 score of CT initial performance varies widely across datasets, we compute the relative improvement in marginal performance (\u2206MP) for each dataset. \u2206MP is calculated as follows:",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 84,
                    "text": "CT",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Evaluation on classification datasets"
        },
        {
            "text": "From this metric, we can observe the largest improvement of our model on the COVID-19-specific dataset (CC), with a \u2206MP value of 25.88%. The marginal improvement is also high on the Twitter datasets related to vaccine sentiment (MVS). Our model likewise shows some improvements on the SST-2 and SemEval datasets, but to a smaller extent. We also note that for the COVID-19-related dataset, most of the marginal improvement occurred after 100k pretraining steps. SST-2, the only non-Twitter dataset, improves much more slowly and reaches its final performance only after 200k pretraining steps.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation on classification datasets"
        },
        {
            "text": "Amongst runs on the same model and dataset, some degree of variance in performance was observed. This variance is mostly driven by runs with a particularly low performance. We observe that the variance is dataset dependent, but it does not increase throughout different pretraining checkpoints and is comparable to the variance observed on BERT-LARGE (pretraining step zero). The most stable training seems to be on the SemEval training set, and the least stable one is on SST-2, but most of this difference is within the error margins.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation on intermediary pretraining checkpoints"
        },
        {
            "text": "The most accurate way to evaluate the performance of a domain-specific model is to apply it on specific downstream tasks. CT-BERT is evaluated on five different Twitter-based datasets. Compared to BERT-LARGE, it improves significantly on all datasets. However, the improvement is largest in datasets related to health, particularly in datasets related to COVID-19. We therefore expect CT-BERT to perform similarly well on other classification problems on COVID-19-related data sources, but particularly on text derived from social media platforms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Whilst it is expected that the benefit of using CT-BERT instead of BERT-LARGE is greatest when working with Twitter COVID-19 text, it is reasonable to expect some per-formance gains even when working with general Twitter messages (SemEval dataset) or with a non-Twitter dataset (SST-2).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Our results show that the MLM and NSP metrics during the pretraining align to some degree with downstream performance on classification tasks. However, compared with COVID-19 or health-related content, out-of-domain text might require longer pretraining to achieve a similar performance boost.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Whilst we have observed an improvement in performance on classification tasks, we did not test our model on other natural language understanding tasks. Furthermore, at the time of this paper\u00e2\u0202\u0179s writing, we only had access to one COVID-19-related dataset. The general performance The best way to evaluate pretrained transformer models is to finetune them on downstream tasks. Finetuning a classifier on a pre-trained model is considered computationally cheap. The training time is usually done in an hour or two on a GPU. Using this method for evaluation is more expensive, as it requires evaluating multiple checkpoints to monitor improvement and on several varied datasets to show robustness. As finetuning results vary between each run, each experiment must be performed multiple times when the goal is to study the pretrained model. In this case, we repeated the training for six checkpoints, 10 runs for each checkpoint on all the five datasets. A total of 300 evaluation runs were performed. The computational cost for evaluation is therefore on par with the pretraining. Large and reliable training and validation sets make this task easier, as the number of repetitions can be reduced.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "All the tests are done on categorisation tasks, as this task is easier in terms of both data access and evaluation. However, transformer-based models can be used for a wide range of tasks, such as named entity recognition and question answering. It is expected that CT-BERT can also be used for these kinds of tasks within our target domain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Our primary goal in this work was to obtain stable results on the finetuning in order to evaluate the pre-trained model, not to necessarily optimise the finetuning. The number of finetuning epochs and the learning rate are, for instance, have been optimised for BERT-LARGE, not for CT-BERT. This means that there is still great room for optimisation on the downstream task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "The model, code and public datasets are available in our GitHub repository: https://github.com/ digitalepidemiologylab/covid-twitter-bert.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Availability"
        },
        {
            "text": "PK received funding from the European Commission for the call H2020-MSCA-IF-2017 and the funding scheme MSCA-IF-EF-ST for the VACMA project (grant agreement ID: 797876).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Funding"
        },
        {
            "text": "MM and MS received funding through the Versatile Emerging infectious disease Observatory grant as a part of the European Commission\u00e2\u0202\u0179s Horizon 2020 framework programme (grant agreement ID: 874735).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Funding"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Ashish",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "Llion",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "Aidan",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "\u0141ukasz",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [
                        "Toutanova"
                    ],
                    "last": "Bert",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.04805"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A robustly optimized bert pretraining approach",
            "authors": [
                {
                    "first": "Yinhan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Myle",
                    "middle": [],
                    "last": "Ott",
                    "suffix": ""
                },
                {
                    "first": "Naman",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Jingfei",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Mandar",
                    "middle": [],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "Danqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                },
                {
                    "first": "Veselin",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Roberta",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1907.11692"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "authors": [
                {
                    "first": "Zhenzhong",
                    "middle": [],
                    "last": "Lan",
                    "suffix": ""
                },
                {
                    "first": "Mingda",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Goodman",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Gimpel",
                    "suffix": ""
                },
                {
                    "first": "Piyush",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "Radu",
                    "middle": [],
                    "last": "Soricut",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1909.11942"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "authors": [
                {
                    "first": "Jinhyuk",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Wonjin",
                    "middle": [],
                    "last": "Yoon",
                    "suffix": ""
                },
                {
                    "first": "Sungdong",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Donghyeon",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Sunkyu",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "Chan",
                    "middle": [],
                    "last": "Ho So",
                    "suffix": ""
                },
                {
                    "first": "Jaewoo",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Bioinformatics",
            "volume": "36",
            "issn": "4",
            "pages": "1234--1240",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Scibert: Pretrained contextualized embeddings for scientific text",
            "authors": [
                {
                    "first": "Iz",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "Arman",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1903.10676"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Crowdbreaks: Tracking health trends using public social media data and crowdsourcing",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Martin",
                    "suffix": ""
                },
                {
                    "first": "Marcel",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Salath\u00e9",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Frontiers in public health",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing",
            "authors": [
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Honnibal",
                    "suffix": ""
                },
                {
                    "first": "Ines",
                    "middle": [],
                    "last": "Montani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Critical dynamics in population vaccinating behavior",
            "authors": [
                {
                    "first": "Demetri",
                    "middle": [],
                    "last": "Pananos",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                },
                {
                    "first": "Clara",
                    "middle": [],
                    "last": "Bury",
                    "suffix": ""
                },
                {
                    "first": "Justin",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Schonfeld",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sharada",
                    "suffix": ""
                },
                {
                    "first": "Brendan",
                    "middle": [],
                    "last": "Mohanty",
                    "suffix": ""
                },
                {
                    "first": "Marcel",
                    "middle": [],
                    "last": "Nyhan",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [
                        "T"
                    ],
                    "last": "Salath\u00e9",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bauch",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "114",
            "issn": "52",
            "pages": "13762--13767",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Semeval-2016 task 4: Sentiment analysis in twitter",
            "authors": [
                {
                    "first": "Preslav",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "Alan",
                    "middle": [],
                    "last": "Ritter",
                    "suffix": ""
                },
                {
                    "first": "Sara",
                    "middle": [],
                    "last": "Rosenthal",
                    "suffix": ""
                },
                {
                    "first": "Fabrizio",
                    "middle": [],
                    "last": "Sebastiani",
                    "suffix": ""
                },
                {
                    "first": "Veselin",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1912.01973"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "authors": [
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Perelygin",
                    "suffix": ""
                },
                {
                    "first": "Jean",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Jason",
                    "middle": [],
                    "last": "Chuang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Christopher",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Andrew",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Potts",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 2013 conference on empirical methods in natural language processing",
            "volume": "",
            "issn": "",
            "pages": "1631--1642",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "authors": [
                {
                    "first": "Bo",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "Lillian",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics",
            "volume": "",
            "issn": "",
            "pages": "115--124",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Evaluation metrics for the domain-specific pretraining of CT-BERT. Shown are the loss and accuracy of masked language modelling (MLM) and next sentence prediction (NSP) tasks.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "So far, we have seen improvements in the final CT-BERT model on all evaluated datasets. To understand whether the observed decrease in loss during pretraining linearly translates into performance on downstream classification tasks, we evaluated CT-BERT on five intermediary versions (checkpoints) of the model and on the zero checkpoint, which corresponds to the original BERT-LARGE model. At each intermediary checkpoint, 10 repeated training runs (finetunings) for each of the five datasets were performed, and the mean F1 score was recorded.Figure 2 shows the marginal performance increase (\u2206MP) at specific pretraining steps. Our experiments show that downstream performance increases fast up to step 200k in the pretraining and only demonstrates marginal improve-ment afterwards. The loss curve, on the other hand, shows a gradual increase even after step 200k.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Marginal performance increase in the F1 score (\u2206MP) on finetuning on various classification tasks at increasing steps of pretraining. Zero on the x-axis corresponds to the base model, which is BERT-LARGE in this case. Our model improves on all evaluated datasets, with the biggest relative improvement being in the COVID-19 category dataset. The bands show the standard error of the mean (SEM) out of 10 repeats. of our model might be improved further by considering pretraining under different hyperparameters, particularly modifications to the learning rate schedules, training batch sizes and optimisers. Future work might include evaluation on other datasets and the inclusion of more recent training data.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Overview of the evaluation datasets. All five evaluation datasets are multi-class datasets with sometimes strong label imbalance, visualised by the proportional bar width in the label column. N and Neg stand for negative; Disc and A stand for discouraging and ambiguous, respectively.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The research was supported with Cloud TPUs from Google's TensorFlow Research Cloud and Google Cloud credits in the context of COVID-19-related research.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        },
        {
            "text": "The authors have no conflicts of interest to declare.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflicts of Interest"
        }
    ]
}