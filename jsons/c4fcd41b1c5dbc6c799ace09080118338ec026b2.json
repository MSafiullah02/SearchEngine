{
    "paper_id": "c4fcd41b1c5dbc6c799ace09080118338ec026b2",
    "metadata": {
        "title": "An Early Study on Intelligent Analysis of Speech under COVID-19: Severity, Sleep Quality, Fatigue, and Anxiety",
        "authors": [
            {
                "first": "Jing",
                "middle": [],
                "last": "Han",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Augsburg",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Kun",
                "middle": [],
                "last": "Qian",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Educational Physiology Laboratory",
                    "institution": "University of Tokyo",
                    "location": {
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Meishu",
                "middle": [],
                "last": "Song",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Augsburg",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Zijiang",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Augsburg",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Zhao",
                "middle": [],
                "last": "Ren",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Augsburg",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Shuo",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Augsburg",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            },
            {
                "first": "Juan",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Huazhong University of Science and Technology",
                    "location": {
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Huaiyuan",
                "middle": [],
                "last": "Zheng",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Huazhong University of Science and Technology",
                    "location": {
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Wei",
                "middle": [],
                "last": "Ji",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Huazhong University of Science and Technology",
                    "location": {
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Tomoya",
                "middle": [],
                "last": "Koike",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Educational Physiology Laboratory",
                    "institution": "University of Tokyo",
                    "location": {
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Xiao",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Children's Hospital of Chongqing Medical University",
                    "location": {
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Zixing",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Yoshiharu",
                "middle": [],
                "last": "Yamamoto",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Educational Physiology Laboratory",
                    "institution": "University of Tokyo",
                    "location": {
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Bj\u00f6rn",
                "middle": [
                    "W"
                ],
                "last": "Schuller",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Augsburg",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "The COVID-19 outbreak was announced as a global pandemic by the World Health Organisation in March 2020 and has affected a growing number of people in the past few weeks. In this context, advanced artificial intelligence techniques are brought to the fore in responding to fight against and reduce the impact of this global health crisis. In this study, we focus on developing some potential use-cases of intelligent speech analysis for COVID-19 diagnosed patients. In particular, by analysing speech recordings from these patients, we construct audio-onlybased models to automatically categorise the health state of patients from four aspects, including the severity of illness, sleep quality, fatigue, and anxiety. For this purpose, two established acoustic feature sets and support vector machines are utilised. Our experiments show that an average accuracy of .69 obtained estimating the severity of illness, which is derived from the number of days in hospitalisation. We hope that this study can foster an extremely fast, low-cost, and convenient way to automatically detect the COVID-19 disease.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The emergence and spread of the novel coronavirus and the related COVID-19 disease is deemed as a major public health threat for almost all countries around the world. Moreover, explicitly or implicitly, the coronavirus pandemic has brought an unprecedented impact on people across the world. To combat the COVID-19 pandemic and its consequences, clinicians, nurses, and other care providers are battling in the front-line. Apart from that, scientists and researchers from a bench of research domains are also stepping up in response to the challenges raised by this pandemic. For instance, several different kinds of drugs and vaccines are being developed and trialled, to treat the virus or to protect against it [1, 2, 3, 4, 5] , and meanwhile methods and technologies are designed and investigated to accelerate the diagnostic testing speed [6, 7] .",
            "cite_spans": [
                {
                    "start": 715,
                    "end": 718,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 719,
                    "end": 721,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 722,
                    "end": 724,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 725,
                    "end": 727,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 728,
                    "end": 730,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 845,
                    "end": 848,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 849,
                    "end": 851,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Particularly, considering the community of data science, massive efforts have been and are still being made to mine information data-driven. In particular, a number of works have been proposed to promote automatic screening by analysing chest CT images [8, 9, 10, 11] . For instance, in [8] , the deep model COV-Net was developed to extract visual features to detect COVID-19. However, no research work has yet been done to explore sound-based COVID-19 assessment.",
            "cite_spans": [
                {
                    "start": 253,
                    "end": 256,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 257,
                    "end": 259,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 260,
                    "end": 263,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 264,
                    "end": 267,
                    "text": "11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 287,
                    "end": 290,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the perspective of sound analysis, as coronavirus is a respiratory illness, abnormal breathing patterns from patients intuitively might be a potential indicator for diagnosis [12] . According to the latest clinical research, the severity of the COVID-19 disease can be categorised into three levels, namely, mild, moderate, and severe illness [13] . For each level, various typical respiratory symptoms can be observed, from dry cough presented in mild illness, to shortness of breath in moderate illness, and further to severe dyspnea, respiratory distress, or tachypnea (respiratory frequency > 30 breaths/min) in severe illness [13] . Meanwhile, all these breathing disorders lead to abnormal variations of articulation. Consequently, it can be of great interest to use automatic speech and voice analysis to aid COVID-19 diagnosis, which is non-invasive and low-cost.",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 346,
                    "end": 350,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 634,
                    "end": 638,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In addition, there could be many meaningful and powerful audio-based tools and applications, which are so far underestimated, and hence underutilised. Pretty recently, scientists elaborated several potential use-cases in the fight against COVID-19 spread via exploiting intelligent speech and sound analysis [14] . Specifically, these envisioned directions are grouped into three categories, i. e., audio-based risk assessment, audio-based diagnosis, and audio-based monitors such as for monitoring of spread, social distancing, treatment and recovery, and patient wellbeing [14] .",
            "cite_spans": [
                {
                    "start": 308,
                    "end": 312,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 575,
                    "end": 579,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Albeit the importance of the work by analysing voice or speech signals to battle this virus pandemic, no empirical research work has been done to date. To fill this gap, we present an early study on the intelligent analysis of speech under COVID-19. To the best of our knowledge, this is the first work towards this direction. Particularly, we take a data-driven approach to automatically detect the patients' symptom severity, as well as their physical and mental states. It is our hope that this step can help develop a rapid, cheap, and easy way to diagnose the COVID-19 disease, and assist medical doctors. 3. Today is the twelfth day since I stayed in the hospital.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "4. I wish I could rehabilitate and leave hospital soon.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Moreover, three self-report questions were answered by each patient, regarding her (or his) sleep quality, fatigue, and anxiety. Specifically, participants rated their sleep quality/ fatigue/ anxiety by choosing from three different levels (i. e., low, mid, and high). Furthermore, regarding demographic information, another four characteristics of the patients were collected, including age, gender, height, and weight. Note that, the height and weight information from 13 patients were not provided. A statistical overview of the data can be seen in Table 1 . Furthermore, a distribution of the self-reported sleep quality, fatigue, and anxiety, grouped by gender, is illustrated in Fig. 1 Figure 1 : Distribution of 51 COVID-19 patients' self-reported questionnaires regarding to their health states in three categories, namely, sleep quality, fatigue, and anxiety. For each category, the patient is asked to select one of three degrees (i. e., low, mid, and high).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 552,
                    "end": 559,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 685,
                    "end": 691,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 692,
                    "end": 700,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "The weather today is sunny."
        },
        {
            "text": "Once the COVID-19 audio data were collected, a series of data preprocessing processes were implemented. Specifically, we did the following four processes: data cleansing, handannotating of voice activities, speaker diarisation, and speech transcription. Details are provided below. Data Cleansing: as the recordings were collected in the wild, there were few unsuccessful recordings where the patient failed to provide any speech rather than noisy background. In such cases, the recordings were discarded for further analysis. As a result, recordings from one female patient were discarded, resulting in data from only 51 subjects for further processing. Voice Activity Detection: for each recording, then, the presence or absence of human voice was detected manually in Audacity 3 . This is because, for some recordings, there were silence periods for the first few seconds and/or the last few seconds. Note that, we only removed the beginning and the end of unvoiced parts from each recording where no audible breathing took place. Hence, only voiced segments (e. g., speech, breathing, and coughing) from the recordings were maintained. Speaker Diarisation: among the remaining voiced segments, there is speech from other individuals than the targeted patient. For this reason, we manually checked and annotated the speaker identities for each voiced segment, indicating if the voice was generated by the patient, or by anyone else. Speech transcription: after annotating the speaker identities, voiced segments from the targeted diagnosed patients were converted into text transcriptions. Note that, while the collection was in the wild, beyond the aforementioned five sentences, some spontaneous recordings were spoken by the patients but with impromptu and unscripted content.",
            "cite_spans": [
                {
                    "start": 780,
                    "end": 781,
                    "text": "3",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "After data preprocessing, we obtained in total of 378 segments. For this preliminary study, we focus only on the scripted segments from patients, leading to 260 pieces of recordings for further analysis. A statistic of the distribution of the five sentences is provided in Table 2 . It can be seen that the distribution is imbalanced. The reason is that, some patients recorded the same content more than once, while some patients failed to supply all five recordings.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 273,
                    "end": 280,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Data Preprocessing"
        },
        {
            "text": "These 260 audio segments from 51 COVID-19 infected patients, were then converted to mono signals with a sampling rate of 16 kHz for further analysis. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "In this section, we detail out the experiments that were exectued to verify the feasibility of audio-only-based COVID-19 patient state assessment. More specifically, we first describe the experimental setups including the applied acoustic feature sets as well as related evaluation strategies. Afterwards, we elaborate on the experiment performance for COVID-19 severity estimation, as well as prediction performance of three COVID-19 patient selfreported status attributes, namely, sleep quality, fatigue, and anxiety degrees. Last, we discuss the limitation of the current study, and provide future work plans and directions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "Two established acoustic feature sets were considered in this study, namely, the Computational Paralinguistics Challenge (COMPARE) set and the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS). Specifically, these feature sets were extracted with the openSMILE toolkit [15] . The COMPARE feature set is a large-scale brute-force set utilised in the series of INTERSPEECH Computational Paralinguistics Challenges since 2013 [16, 17] . It contains 6 373 static features by computing various statistical functionals over 65 low-level descriptor (LLD) contours [16] . These LLDs consist of spectral (relative spectra auditory bands 1-26, spectral energy, spectral slope, spectral sharpness, spectral centroid, etc.), cepstral (Mel frequency cepstral coefficient 1-14), prosodic (loudness, root mean square energy, zero-crossing rate, F0 via subharmonic summation, etc.), and voice quality features (probability of voicing, jitter, shimmer and harmonics-to-noise ratio). For more details, the reader is referred to [16] .",
            "cite_spans": [
                {
                    "start": 281,
                    "end": 285,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 435,
                    "end": 439,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 440,
                    "end": 443,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 569,
                    "end": 573,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Feature Extraction"
        },
        {
            "text": "Different from the large-scale COMPARE set, the other feature set applied in this work, eGeMAPS, is considerably smaller. It consists of only 88 features derived from 25 LLDs. Particularly, these features were chosen concerning their capabilities to describe affective physiological changes in voice production. For more details about these features, please refer to [18] .",
            "cite_spans": [
                {
                    "start": 367,
                    "end": 371,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Feature Extraction"
        },
        {
            "text": "In this work, we carried out four audio-based classification tasks. First, we performed COVID-19 severity estimation based on the number of days of the hospitalisation. The hypothesis is that, a COVID-19 patient is generally very sick at the early stage of the hospitalisation, and then recovers step by step. As a consequence, the patients were approximately grouped into three categories, i. e., the high-severity stage for the first 25 days, the mid-severity stage between 25 and 50 days, and the low-severity stage after 50 days. Besides, further three classification tasks considered in this study are to predict the self-reported sleep quality, fatigue, and anxiety levels of COVID-19 patients, the potential of which has been spotted in [14] .",
            "cite_spans": [
                {
                    "start": 744,
                    "end": 748,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Strategy"
        },
        {
            "text": "For these classification tasks, we implemented Support Vector Machines (SVMs) with a linear kernel function as the classifiers for all experiments, due to its widespread usage and appealing performance achieved in intelligent speech analy- sis [19, 20] . Specifically, a series of complexity constants C were evaluated in [10 \u22127 , 10 \u22126 , \u00b7 \u00b7 \u00b7 , 10 \u22121 , 10 0 ]. Further, to deal with the imbalanced data during training, a class weighting strategy was employed to automatically adjust the C values in proportion to the importance of each class. The SVMs were implemented in Python based on the scikit-learn library. Moreover, for all experiments in this study, Leave-One-Subject-Out (LOSO) cross-validation evaluations were carried out to satisfy the speaker independence evaluation constraint. In this context, all the 260 instances were divided into 51 speakerindependent folds, with each fold containing only instances from one patient. With the LOSO evaluation scheme, one of the 51 folds was used as the test set and the other folds were put together to form a training set to train an SVM model. Then, this process was repeated 51 times until all folds were utilised as the test set. Note that, for each folder, an on-line standardisation was applied to the test set by using the means and variations of the respective training partition.",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 248,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 249,
                    "end": 252,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Strategy"
        },
        {
            "text": "Then, the average performance was computed over the predictions of all instances. In this work, we utilise three most frequently-used measures, i. e., Unweighted Average Recall (UAR), the overall accuracy (also known as Weighted Average Recall or WAR), and the F1 Score (also known as F-score or F-measure) that is the harmonic mean of precision and recall.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Strategy"
        },
        {
            "text": "In Table 3 , we report the performance of the best SVM models for the two selected feature sets, respectively. In particular, the best model was chosen from varied SVMs with different C values based on UAR. It can be seen that, the large feature set, ComPARE, performs slightly better than eGeMAPS for the severity estimation, achieving .68 UAR, .69 accuracy, and .66 F1-score. Moreover, we further inspect the audio recordings from patients with varied severity levels. An illustration is given in Fig. 2 . In particular, three recordings were taken from three different patients, who were asked to say the same content. The first patient failed to produce the sentence due to his severe symptoms. The second sample is from a female patient. She successfully spoke the whole content following the given template, however, had to pause several times to take a heavy breath before carrying on with the remaining content. In contrast, the third patient managed to generate the same recording more clearly and fluently.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 499,
                    "end": 505,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "Severity Estimation"
        },
        {
            "text": "Considering audio-based sleep quality, fatigue, and anxiety estimation, we further trained SVM models for each task separately. Corresponding results are shown in Table 4 , where the performance of the best models for each task and each feature set are provided. Similarly, the best performance was taken where the highest UAR was obtained, and performance in terms Figure 2 : Illustration of the spectrograms from recordings by COVID-19 diagnosed patients with high (top), mid (middle), and low (bottom) severity degrees. All were requested to speak the same content, i. e., the second sentence in the template (cf. Section 2).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 163,
                    "end": 170,
                    "text": "Table 4",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 366,
                    "end": 374,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Sleep Quality, Fatigue, and Anxiety Prediction"
        },
        {
            "text": "of accuracy and F1-score is given. When comparing the three tasks, the best performance is achieved for sleep quality classification, reaching up to .61 UAR. Then, for anxiety prediction, a UAR of .56 is attained. When it comes to fatigue prediction, the best performance of UAR is only .46, which is, however, above chance level (.33 for three-class classification).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sleep Quality, Fatigue, and Anxiety Prediction"
        },
        {
            "text": "Further, when comparing two selected feature sets, the compact eGeMAPS set consistently outperforms the large-scale ComPARE feature set. On the one hand, these results reveal the effectiveness of the eGeMAPS set for audio-based sleep quality, fatigue, and anxiety detection. On the other hand, the inferior performance based on ComPARE might be due to the low number of training samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sleep Quality, Fatigue, and Anxiety Prediction"
        },
        {
            "text": "In this preliminary study, experiments were carried out based on speech recordings from COVID-19 infected and hospitalised patients. The results have demonstrated the feasibility and effectiveness of audio-only-based COVID-19 analysis, specifically in estimating the severity level of the disease, and in predicting the health and wellbeing status of patients including sleep quality, fatigue, and anxiety. Nonetheless, there are still many ways to extend the present study for further development.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "First, due to time limitation, the collected data set is relatively small, and lacks control group data from both healthy subjects and patients with other respiratory diseases. These data collections are still in progress for more comprehensive analysis in the future. In addition, AI techniques can be considered to tackle the data scarcity issue, such as data augmentation via generative adversarial networks [21, 22, 23] . Given more data, the performance of our models is expected to be further improved and more robust.",
            "cite_spans": [
                {
                    "start": 411,
                    "end": 415,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 416,
                    "end": 419,
                    "text": "22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 420,
                    "end": 423,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Second, only functional features computed over whole segments were investigated. However, abnormal respiratory symptoms might be instantaneous and occur only in a short period of time. In this context, analysing low-level features in successive frames with sequential modelling might bring further performance improvement. Moreover, in addition to conventional handcrafted features, deep representation learning algorithms might be explored to learn representative and salient data-driven features for COVID-19 related tasks. These include deep latent representation learning [24] , self-supervised learning [25] , and transfer learning [26] to name but a few.",
            "cite_spans": [
                {
                    "start": 576,
                    "end": 580,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 608,
                    "end": 612,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 637,
                    "end": 641,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Further, in this study, the severity estimation based on days in hospitalisation is in a rough fashion, as we are in lack of other [27, 28, 29] , more objective and accurate labels could be attained. Last but not least, in this paper, SVMs were separately trained to estimate four tasks. Considering the potential correlation between the severity of the disease, and the patient's sleep quality (or mood), a multi-task learning model might help effectively exploit the mutually dependent information from these tasks [30, 31] .",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 135,
                    "text": "[27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 136,
                    "end": 139,
                    "text": "28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 140,
                    "end": 143,
                    "text": "29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 517,
                    "end": 521,
                    "text": "[30,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 522,
                    "end": 525,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "At the time of writing this paper, the world has reported a total of 3 020 117 confirmed COVID-19 cases and 209 799 fatalities, according to a dashboard developed and maintained by the Johns Hopkins University 4 . To leverage the potential of computer audition to fight against this global health crisis, for the first time, experiments have been performed based on the speech of 51 COVID-19 infected and hospitalised patients from China. In particular, audio-based models have been constructed and assessed to predict the severity of the disease, as well as health and wellbeing-relevant mental states of the patients including sleep quality, fatigue, and anxiety. Experimental results have shown the great potential of exploiting audio analysis in the fight against COVID-19 spread.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "In the future, we will continue the data collection process as well as collecting relevant clinical reports for a comprehensive understanding of the patient state. In addition, we attempt to introduce interpretable models and techniques to make the predictions more traceable, transparent, and trustworthy [32] .",
            "cite_spans": [
                {
                    "start": 306,
                    "end": 310,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We express our deepest sorrow for those who left us due to COVID-19; they are lives, not numbers. We further express our highest gratitude and respect to the clinicians and scientists, and anyone else these days helping to fight against COVID-19, and at the same time help us maintain our daily lives. This work was partially supported by the Zhejiang Lab's International Talent Fund for Young Professionals (Project HANAMI), P. R. China, the JSPS Postdoctoral Fellowship for Research in Japan (ID No. P19081) from the Japan Society for the Promotion of Science (JSPS), Japan, and the Grants-in-Aid for Scientific Research (No. 19F19081 and No. 17H00878) from the Ministry of Education, Culture, Sports, Science and Technology (MEXT), Japan.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Research and development on therapeutic agents and vaccines for COVID-19 and related human coronavirus diseases",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "V"
                    ],
                    "last": "Garner",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Watkins",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Carter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Smoot",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Gregg",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "D"
                    ],
                    "last": "Daniels",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jervey",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Albaiu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ACS Central Science",
            "volume": "6",
            "issn": "3",
            "pages": "315--331",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Traditional chinese medicine for COVID-19 treatment",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "A.-H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X.-J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Pharmacological research",
            "volume": "155",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "COVID-19: Immunopathology and its implications for therapy",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Nature Reviews Immunology",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "News feature: Avoiding pitfalls in the pursuit of a COVID-19 vaccine",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Peeples",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the National Academy of Sciences",
            "volume": "117",
            "issn": "15",
            "pages": "8218--8221",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Developing Covid-19 vaccines at pandemic speed",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Lurie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Saville",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hatchett",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Halton",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "New England Journal of Medicine",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Fast and simple highthroughput testing of COVID 19",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Durner",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Burggraf",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Czibere",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Fleige",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Madejska",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "C"
                    ],
                    "last": "Watts",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Krieg-Schneider",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Becker",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Dental Materials",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Innovative screening tests for COVID-19 in South Korea",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "S.-I",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "B"
                    ],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Clinical and Experimental Emergency Medicine",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "16",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "COVID-CAPS: A capsule network-based framework for identification of COVID-19 cases from X-ray images",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Heidarian",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Naderkhani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oikonomou",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "N"
                    ],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2004.02696"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "COVID-Net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest radiography images",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.09871"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Covid-resnet: A deep learning framework for screening of COVID19 from radiographs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Farooq",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hafeez",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.14395"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Abnormal respiratory patterns classifier may contribute to large-scale screening of people infected with covid-19 in an accurate and unobtrusive manner",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X.-P",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.05534"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Features, evaluation and treatment coronavirus (COVID-19),\" in Statpearls",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cascella",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rajnik",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cuomo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Dulebohn",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "Di"
                    ],
                    "last": "Napoli",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Covid-19 and computer audition: An overview on what speech & sound analysis could contribute in the sars-cov-2 corona crisis",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "W"
                    ],
                    "last": "Schuller",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Schuller",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.11117"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "openSMILE -the Munich versatile and fast open-source audio feature extractor",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Eyben",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "W\u00f6llmer",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proc. ACM International Conference on Multimedia (MM)",
            "volume": "",
            "issn": "",
            "pages": "1459--1462",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Steidl",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Batliner",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)",
            "volume": "",
            "issn": "",
            "pages": "148--152",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Baby Sounds & Orca Activity",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Batliner",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bergler",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness",
            "volume": "",
            "issn": "",
            "pages": "2378--2382",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "The geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Eyben",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Scherer",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sundberg",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Andr\u00e9",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Busso",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Devillers",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Epps",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Laukka",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Narayanan",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Truong",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Transactions on Affective Computing",
            "volume": "7",
            "issn": "2",
            "pages": "190--202",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "At the border of acoustics and linguistics: Bag-of-audio-words for the recognition of emotions in speech",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schmitt",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ringeval",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)",
            "volume": "",
            "issn": "",
            "pages": "495--499",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Prediction-based learning for continuous emotion recognition in speech",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ringeval",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "volume": "",
            "issn": "",
            "pages": "5005--5009",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Generative adversarial nets",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pouget-Abadie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mirza",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Warde-Farley",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ozair",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. the 27th Annual Conference on Neural Information Processing Systems (NIPS)",
            "volume": "",
            "issn": "",
            "pages": "2672--2680",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Snore-GANs: Improving Automatic Snore Sound Classification with Synthesized Data",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Janott",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Journal of Biomedical and Health Informatics",
            "volume": "24",
            "issn": "1",
            "pages": "300--310",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Prospectives",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Cummins",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Special Issue on Computational Intelligence for Affective Computing and Sentiment Analysis",
            "volume": "14",
            "issn": "",
            "pages": "68--81",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Emotion recognition in speech with latent discriminative representations learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Keren",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Acta Acustica united with Acustica",
            "volume": "104",
            "issn": "5",
            "pages": "737--740",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Learning problem-agnostic speech representations from multiple self-supervised tasks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pascual",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ravanelli",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Serr\u00e0",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bonafonte",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)",
            "volume": "",
            "issn": "",
            "pages": "161--165",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Learning Image-based Representations for Heart Sound Classification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Cummins",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Pandit",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. 8th International Conference on Digital Health (DH)",
            "volume": "",
            "issn": "",
            "pages": "143--147",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Chest CT findings in coronavirus disease-19 (COVID-19): relationship to duration of infection",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bernheim",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Mei",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "19",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Time course of lung changes on chest CT during recovery from 2019 novel coronavirus (COVID-19) pneumonia",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "15",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Relation between chest CT findings and clinical conditions of coronavirus disease (COVID-19) pneumonia: a multicenter study",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "American Journal of Roentgenology",
            "volume": "214",
            "issn": "5",
            "pages": "1072--1077",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Cross-corpus acoustic emotion recognition from singing and speaking: A multi-task learning approach",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "M"
                    ],
                    "last": "Provost",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Essl",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "volume": "",
            "issn": "",
            "pages": "5805--5809",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Parthasarathy",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Busso",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)",
            "volume": "",
            "issn": "",
            "pages": "1103--1107",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Adadi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Berrada",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Access",
            "volume": "6",
            "issn": "",
            "pages": "52--138",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "TABREF0": {
            "text": "Statistics of the COVID-19 audio data. from two hospitals in Wuhan, China. Data were collected between 20 to 26 March 2020. For each patient, five sentences were recorded one after another via the Wechat App, when doctors and nurses were making their daily rounding to check the patients. As a result, five recordings per patient were acquired during data collection. Note that, these five sentences all have neutral meaning and one example (relating to the date and days as variables) is given below:",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": ".",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Distribution of the five targeted sentences collected from 51 COVID-19 diagnosed patients.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Performance in terms of unweighted average recall (UAR), accuracy (WAR), and F1-score for three-class COVID-19 severity classification. The complexities Cs that achieved the best performance are reported as well. Note: chance level is .33 for UAR.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Performance in terms of unweighted average recall (UAR), accuracy (WAR), and F1-score for three three-class COVID-19 patient state tasks, i. e., sleep (quality), fatigue, and anxiety predictions. The SVM complexities C that achieved the best performance are further given. Note: chance level is .33 for UAR.information regarding the patients' health states. Similarly, the self-reported questionnaires about the patients' conditions are rather subjective, as they could have different principles and perception. If clinical examinations and reports of the patients are provided such as CT scans of their lungs",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}