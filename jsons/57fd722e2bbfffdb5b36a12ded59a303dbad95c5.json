{
    "paper_id": "57fd722e2bbfffdb5b36a12ded59a303dbad95c5",
    "metadata": {
        "title": "On the Generation of Medical Dialogues for COVID-19",
        "authors": [
            {
                "first": "Wenmian",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Guangtao",
                "middle": [],
                "last": "Zeng",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Bowen",
                "middle": [],
                "last": "Tan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Zeqian",
                "middle": [],
                "last": "Ju",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "\u2020",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Subrato",
                "middle": [],
                "last": "Chakravorty",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Xuehai",
                "middle": [],
                "last": "He",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Shu",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Xingyi",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Qingyang",
                "middle": [],
                "last": "Wu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Zhou",
                "middle": [],
                "last": "Yu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Eric",
                "middle": [],
                "last": "Xing",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Pengtao",
                "middle": [],
                "last": "Xie",
                "suffix": "",
                "affiliation": {},
                "email": "pengtaoxie2008@gmail.com"
            },
            {
                "first": "U",
                "middle": [
                    "C"
                ],
                "last": "San Diego",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "",
                "middle": [],
                "last": "Cmu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "U",
                "middle": [
                    "C"
                ],
                "last": "Davis",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Under the pandemic of COVID-19, people experiencing COVID19-related symptoms or exposed to risk factors have a pressing need to consult doctors. Due to hospital closure, a lot of consulting services have been moved online. Because of the shortage of medical professionals, many people cannot receive online consultations timely. To address this problem, we aim to develop a medical dialogue system that can provide COVID19-related consultations. We collected two dialogue datasets -CovidDialog -(in English and Chinese respectively) containing conversations between doctors and patients about COVID-19. On these two datasets, we train several dialogue generation models based on Transformer, GPT, and BERT-GPT. Since the two COVID-19 dialogue datasets are small in size, which bear high risk of overfitting, we leverage transfer learning to mitigate data deficiency. Specifically, we take the pretrained models of Transformer, GPT, and BERT-GPT on dialog datasets and other large-scale texts, then finetune them on our CovidDialog datasets. Experiments demonstrate that these approaches are promising in generating meaningful medical dialogues about COVID-19. But more advanced approaches are needed to build a fully useful dialogue system that can offer accurate COVID-related consultations. The data and code are available at https://github.com/UCSD-AI4H/COVID-Dialogue",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "As of May 8th in 2020, the COVID-19 pandemic has killed 272,778 people out of 3,910,738 infected cases. People who are experiencing symptoms (e.g., fever, cough) similar to those of COVID-19 or were exposed to risk factors such as close contact with infected cases have a pressing need to consult doctors, largely because of the panic over this unknown new disease. However, under the pandemic situation, coming to hospitals is dangerous and has high risk of suffering cross-infection. Cross-infection refers to the fact that many people visiting hospitals at the same time and infected individuals will spread coronavirus to healthy ones. To prevent spreading of the coronavirus, many non-urgent clinics and hospitals have been closed physically and encourage people to consult doctors through telemedicine services (e.g., phone calls, video conferencing). However, medical professionals are highly occupied by taking care of the infected patients and have very thin bandwidth to deal with the surging requests of consultations related to COVID-19. As a result, many people could not receive timely advice for effectively dealing with their medical conditions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To address the large imbalance between the surging need of consultations from citizens and the severe shortage of medical professionals available to provide online consultation services, it is highly valuable to develop intelligent dialogue systems which act as virtual doctors to provide COVID-related consultations to people. These virtual doctors can greatly ease the burden of human doctors and timely address the concerns of the public.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To facilitate the research and development of COVID19-targeted dialogue systems, we build two medical dialogue datasets that contain conversations between doctors and patients, about COVID-19 and other pneumonia: (1) an English dataset containing 603 consultations, 1232 utterances, and 90664 tokens (English words); (2) a Chinese dataset containing 1088 consultations, 9494 utterances, and 406550 tokens (Chinese characters).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "On these two datasets, we train several dialogue generation models based on Transformer (Vaswani et al., 2017) , GPT Zhang et al., 2019) , and BERT-GPT (Wu et al., 2019; . Transformer is an encoder and decoder architecture which takes the conversation history as inputs and generates the response. Selfattention is used to capture the long-range dependency among tokens. GPT is a language model based on the Transformer decoder. When generating a response, GPT predicts the next token using its context including the already decoded tokens in this response and the conversation history. BERT-GPT is an encoder-decoder architecture as well where the pretrained BERT (Devlin et al., 2018) is used to encode the conversation history and GPT is used to decode the response. The small size of CovidDialog datasets incurs high risk of overfitting, if directly training the large-sized neural models on CovidDialog. To alleviate this risk, we take the pretrained weights of these models on large-scale dialogue dataset and other corpus and finetune the weights on CovidDialog. Experiments demonstrate that the models trained on CovidDialog datasets are promising in generating clinically meaningful consultations about COVID-19. The datasets and code are publicly available at https://github.com/UCSD-AI4H/COVID-Dialogue",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 110,
                    "text": "(Vaswani et al., 2017)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 117,
                    "end": 136,
                    "text": "Zhang et al., 2019)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 152,
                    "end": 169,
                    "text": "(Wu et al., 2019;",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 665,
                    "end": 686,
                    "text": "(Devlin et al., 2018)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest of the papers are organized as follows. Section 2 and 3 present the datasets and methods. Section 4 gives experimental results. Section 5 reviews related works and Section 6 concludes the paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we present two collected datasets -CovidDialog-English and CovidDialog-Chinese -which contain medical conversations between patients and doctors about COVID-19 and other related pneumonia. The statistics of these two datasets are summarized in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 261,
                    "end": 268,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "The CovidDialog-English dataset contains 603 English consultations about COVID-19 and other related pneumonia, having 1,232 utterances. The number of tokens (English words) is 90,664. The average, maximum, and minimum number of utterances in a conversation is 2.0, 17, and 2 respectively. The average, maximum, and minimum number of tokens in an utterance is 49.8, 339, and 2 respectively. Each consultation starts with a short description of the medical conditions of a patient, followed by the conversation between the patient Description of medical condition I have a little fever with no history of foreign travel or contact. What is the chance of Covid-19?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The English Dataset"
        },
        {
            "text": "Patient: Hello doctor, I am suffering from coughing, throat infection from last week. At that time fever did not persist and also did not felt any chest pain. Two days later, I consulted with a doctor. He prescribed Cavidur 625, Montek LC, Ambrolite syrup and Betaline gargle solution. Since then throat infection improved and frequent cough also coming out. Coughing also improved remarkably though not completely. From yesterday onwards fever is occuring (maximum 100-degree Celcius). I have not come in touch with any foreign returned person nor went outside. In our state, there is no incidence of Covid-19. Please suggest what to do? Doctor: Hello, I can understand your concern. In my opinion, you should get done a chest x-ray and CBC (complete blood count). If both these are normal then no need to worry much. I hope this helps. Patient: Thank you doctor. After doing all these I can upload all for further query. Doctor: Hi, yes, upload in this query only. I will see and revert to you. Patient: Thank you doctor. As per your guidelines, I have made one test. Due to city shutdown, I could not able to make an x-ray of chest. Fever is coming every 12 hours. But cough does not persist currently. The only problem is a fever. I am uploading the blood examination report. Please advice. Doctor: Hi, I can understand your concern. I have gone through the report you have attached. (attachment removed to protect patient identity). Your total count is on the higher side of normal along with low hemoglobin. So start Azithromycin 500 mg 1-0-0, Tablet Dolo 650 1-0-1. Once your fever and cough subside, start an Iron tablet for low hemoglobin. I hope this helps. Patient: Thank you doctor. How many tablets of Azithromycin to be taken? Is it in the morning? I want to make you know that I have already taken Azithromycin five days before. Should she start it? And also can I start an Iron tablet today itself? Doctor: Hi, Yes, you can take iron tablet from today onwards. And no need for Azithromycin now as you have already taken it. Drink plenty of fluids orally and keep yourself hydrated. Do warm water gargles and steam inhalation four to five times a day. and a doctor. Figure 1 shows an example. The original dialogues are crawled from online healthcare forums, including icliniq.com 1 , healthcaremagic.com 2 , and healthtap.com 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 2181,
                    "end": 2189,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Dialogue"
        },
        {
            "text": "The CovidDialog-Chinese dataset contains 1,088 Chinese consultations about COVID-19 and other related pneumonia, having 9,494 utterances. In this work, we develop models directly on Chinese characters without performing word segmentation. Each Chinese character in the text is treated as a token. The total number of tokens in the dataset is 406,550. The average, maximum, and minimum number of utterances in a conversation is 8.7, 116, and 2 respectively. The average, maximum, and minimum number of tokens in an utterance is 42.8, 2001, and 1 respectively. Each consultation consists of three parts:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Chinese Dataset"
        },
        {
            "text": "(1) description of patient's medical condition and history;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Chinese Dataset"
        },
        {
            "text": "(2) conversation between patient and doctor; (3) (optional) diagnosis and treatment suggestions given by the doctor. In the description of the patient's medical condition and history, the following fields are included: present disease, detailed description of present disease, what help is needed from the doctor, how long the disease has been, medications, allergies, and past diseases. This description is used as the first utterance from the patient. Figure 2 shows an exemplar consultation. The data is crawled from haodf.com 4 , which is an online platform of healthcare services, including medical consultation, scheduling appointments with doctors, etc. Duplicated and incomplete dialogues were removed.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 454,
                    "end": 462,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "The Chinese Dataset"
        },
        {
            "text": "In this section, we present several well-established and state-of-the-art methods for dialogue generation. Given a dialogue containing a sequence of alternating utterances between patient and doctor, we process it into a set of pairs {(s i , t i )} where the target t i is a response from the doctor and the source s i is the concatenation of all utterances (from both patient and doctor) before t i . A dialogue generation model takes s as input and generates t. The size of the CovidDialog datasets is small. Directly training neural models on these small datasets would result in poor generalization on unseen data. To solve this problem, we utilize transfer learning, which pretrains the neural models on large corpus, then finetunes the pretrained models on the CovidDialog datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Generating response t from the conversation history s is a typical sequence-to-sequence (seq2seq) (Sutskever et al., 2014) modeling problem. Transformer (Vaswani et al., 2017) is an encoder-decoder architecture for sequence-to-sequence (seq2seq) modeling. Different from seq2seq models (Sutskever et al., 2014) that are based on recurrent neural networks (e.g., LSTM (Hochreiter and Schmidhuber, 1997) , GRU (Chung et al., 2014) ) which model a sequence of tokens via a recurrent manner and hence is computationally inefficient. Transformer eschews recurrent computation and instead uses self-attention which not only can capture the dependency between tokens but also is amenable for parallel computation with high efficiency. Self-attention calculates the correlation among every pair of tokens and uses these correlation scores to create \"attentive\" representations by taking weighted summation of tokens' embeddings. Transformer is composed of a stack of building blocks, each consisting of a self-attention layer and a position-wise feed-forward layer. Residual connection (He et al., 2016) is applied around each of the two sub-layers, followed by layer normalization (Ba et al., 2016) . Given the input sequence, an encoder, which is a stack of such building blocks, is applied to obtain a representation for each token. Then the decoder takes these representations as inputs and decodes the sequence of output tokens. To decode the i-th token, the decoder first uses self-attention to encode the already decoded sequence y 1 , \u00b7 \u00b7 \u00b7 , y i\u22121 , then performs input-output attention between the encodings of y 1 , \u00b7 \u00b7 \u00b7 , y i\u22121 and those of the input sequence. The \"attentive\" representations are then fed into a feedforward layer. The three steps are repeated for multiple times. Finally, the representation is fed into a linear layer to predict the next token. The weight parameters in Transformer is learned by maximizing the conditional likelihood of output sequences conditioned on the corresponding input sequences.",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 122,
                    "text": "(Sutskever et al., 2014)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 153,
                    "end": 175,
                    "text": "(Vaswani et al., 2017)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 286,
                    "end": 310,
                    "text": "(Sutskever et al., 2014)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 367,
                    "end": 401,
                    "text": "(Hochreiter and Schmidhuber, 1997)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 408,
                    "end": 428,
                    "text": "(Chung et al., 2014)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1078,
                    "end": 1095,
                    "text": "(He et al., 2016)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1174,
                    "end": 1191,
                    "text": "(Ba et al., 2016)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Transformer"
        },
        {
            "text": "The GPT model (Radford et al., a ) is a language model (LM) based on Transformer. Different from Transformer which defines a conditional probability on an output sequence given an input sequence, GPT defines a marginal probability on a single sequence. Given a sequence of tokens x 1 , \u00b7 \u00b7 \u00b7 , x n , an LM defines a probability on the sequence:",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 32,
                    "text": "(Radford et al., a",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "GPT"
        },
        {
            "text": "which basically predicts the next token based on the historical sequence. In GPT, p(x i |x 1 , \u00b7 \u00b7 \u00b7 , x i\u22121 ) is defined using the Transformer decoder, which first uses a stack of self-attention and feed-forward layers (each followed by layer normalization) to encode x 1 , \u00b7 \u00b7 \u00b7 , x i\u22121 , then predicts x i from the encodings of x 1 , \u00b7 \u00b7 \u00b7 , x i\u22121 . The weight parameters are learned by maximizing the likelihood on the sequence of tokens. GPT-2 (Radford et al., b) is an extension of GPT, which modifies GPT by moving layer normalization to the input of each sub-block and adding an additional layer normalization after the final self-attention block. Byte pair encoding (BPE) (Sennrich et al., 2015) is used to represent the input sequence of tokens.",
            "cite_spans": [
                {
                    "start": 449,
                    "end": 468,
                    "text": "(Radford et al., b)",
                    "ref_id": null
                },
                {
                    "start": 681,
                    "end": 704,
                    "text": "(Sennrich et al., 2015)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "GPT"
        },
        {
            "text": "Pretrained GPT models for dialogue generation DialoGPT (Zhang et al., 2019) is a GPT-2 model pretrained on English Reddit dialogues. The dataset is extracted from comment chains in Reddit from 2005 till 2017, comprising 147,116,725 dialogue instances with 1.8 billion tokens. Given a dialogue history S and a ground-truth response T = x 1 , \u00b7 \u00b7 \u00b7 , x n , DialoGPT is trained to maximize the following probability",
            "cite_spans": [
                {
                    "start": 55,
                    "end": 75,
                    "text": "(Zhang et al., 2019)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "GPT"
        },
        {
            "text": "where the conditional probabilities are defined by the Transformer decoder. A maximum mutual information (MMI) (Li et al., 2015) scoring function is used to penalize generated responses that are bland. We finetune DialoGPT on our CovidDialog-English dataset for generating English COVID-19 dialogues. GPT2-chitchat 5 is a GPT-2 model pretrained on Chinese Chatbot Corpus 6 which contains about 14M dialogues and 500k-Chinese-Dialog 7 which contains 500K Chinese dialogues. The training strategy of GPT2-chitchat is the same as that of DialoGPT. We finetune GPT2-chitchat on our CovidDialog-Chinese dataset for generating Chinese COVID-19 dialogues.",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 128,
                    "text": "(Li et al., 2015)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "GPT"
        },
        {
            "text": "BERT-GPT (Wu et al., 2019) is a model used for dialogue generation where pretrained BERT is used to encode the conversation history and GPT is used to generate the responses. While GPT focuses on learning a Transformer decoder for text generation purposes, BERT (Devlin et al., 2018) aims to learn a Transformer encoder for representing texts. BERTs model architecture is a multi-layer bidirectional Transformer encoder. In BERT, the Transformer uses bidirectional self-attention, whereas in GPT every token can only attend to context to its left. To train the encoder, BERT masks some percentage of the input tokens at random, and then predict those masked tokens by feeding the final hidden vectors (produced by the encoder) corresponding to the mask tokens into an output softmax over the vocabulary. Since BERT leverages context to both the left and the right for representing a token, it presumably has better representation power than GPT which only leverages context to the left. In dialogue generation, for the given conversation history, instead of using GPT for obtaining the representation, we can use a more powerful pretrained BERT to encode it. The BERT encoding of the conversation history is fed into GPT to generate the response. In BERT-GPT, the pretraining of the BERT encoder and the GPT decoder is conducted separately, which may lead to inferior performance. Auto-Regressive Transformers (BART) ) has a similar architecture as BERT-GPT, but trains the BERT encoder and GPT decoder jointly. To pretrain the BART weights, the input text is corrupted randomly, such as token masking, token deletion, text infilling, etc., then the network 5. https://github.com/yangjianxin1/GPT2-chitchat 6. https://github.com/codemayq/chinese_chatbot_corpus 7. https://drive.google.com/file/d/1nEuew_KNpTMbyy7BO4c8bXMXN351RCPp/view is learned to reconstruct the original text. BART is pretrained on the data used in , consisting of 160Gb of news, books, stories, and web texts.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 26,
                    "text": "(Wu et al., 2019)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 262,
                    "end": 283,
                    "text": "(Devlin et al., 2018)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "BERT-GPT"
        },
        {
            "text": "Pretrained BERT-GPT models for dialogue generation BERT-GPT-Chinese (Wu et al., 2019 ) is a BERT-GPT model pretrained on Chinese corpus. For the BERT encoder in BERT-GPT-Chinese, it is set to the Chinese BERT (Cui et al., 2019) , which is a large-scale pretrained BERT language model on Chinese texts. For the GPT decoder in BERT-GPT, it has the same architecture as BERT but applies lower-triangular mask for autoregressive text generation. The decoder is initialized with Chinese BERTs weights. Then the decoder is pretrained with a maximum likelihood estimation (MLE) objective on a large-scale multidomain Chinese corpus. The resulting model consists of a bidirectional Transformer as the encoder, a unidirectional Transformer as the decoder, and an attention mechanism to connect them. The Chinese corpus used for pretraining is collected from the Large Scale Chinese Corpus for NLP 8 , including the following datasets: Chinese Wikipedia which contains 104M articles, News which contains 2.5 million news articles from 63,000 sources, Baike QA which is a wiki question answering (QA) dataset with 1.5 million QA pairs from 493 different domains, and Community QA which contains 4.1 million comments and 28 thousand topics. The total size of these datasets is 15.4 GB. We finetune BERT-GPT-Chinese on the CovidDialog-Chinese dataset for Chinese COVID-19 dialogue generation. For English COVID-19 dialogue generation, we finetune the pretrained BART model on the CovidDialog-English dataset.",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 84,
                    "text": "(Wu et al., 2019",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 209,
                    "end": 227,
                    "text": "(Cui et al., 2019)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "BERT-GPT"
        },
        {
            "text": "For the English dataset, we split it into a training, a validation, and a test set based on dialogues, with a ratio of 8:1:1. Table 2 shows the statistics of the data split. The size of the vocabulary (number of unique English words) was set to x. The hyperparameters were tuned on the validation dataset. For all methods, we used the Adam (Kingma and Ba, 2014) optimizer with linear learning rate scheduling, setting the initial learning rate as 4e-5 and the batch size as 4. The objective is the cross entropy loss with label smoothing where the factor was set to 0.1. For pretrained models, we finetune them on the CovidDialog-English dataset for 5 epochs, while for the un-pretrained Transformer, we train it for 50 epochs. We set a checkpoint at the end of every epoch and finally take the one with the lowest perplexity on validation set as the final model. In response generation, for all models, we use beam search with beam width of 10 as our decoding strategy. For DialoGPT (Zhang et al., 2019) , we used three variants with different sizes: DialoGPT-small, DialoGPT-medium, DialoGPT-large, with 117M, 345M and 762M weight parameters respectively. Maximum mutual information was not used.",
            "cite_spans": [
                {
                    "start": 352,
                    "end": 361,
                    "text": "Ba, 2014)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 984,
                    "end": 1004,
                    "text": "(Zhang et al., 2019)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [
                {
                    "start": 126,
                    "end": 133,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Experimental Settings"
        },
        {
            "text": "We performed automatic evaluation, using metrics including perplexity, NIST-n (Doddington, 2002) (where n = 4), BLEU-n (Papineni et al., 2002) (where n = 2 and 4), METEOR (Lavie and Agarwal, 2007) , Entropy-n (Zhang et al., 2018) Dist-n (Li et al., 2015) (where n = 1 and 2). BLEU, METEOR, and NIST are common metrics for evaluating machine translation. They compare the similarity between generated responses and the ground-truth by matching n-grams. NIST is a variant of BLEU, which weights n-gram matches using information gain to penalize uninformative n-grams. Perplexity is used to measure the quality and smoothness of generated responses. Entropy and Dist are used to measure the lexical diversity of generated responses. For perplexity, the lower, the better. For other metrics, the higher, the better.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 142,
                    "text": "(Papineni et al., 2002)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 171,
                    "end": 196,
                    "text": "(Lavie and Agarwal, 2007)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 209,
                    "end": 229,
                    "text": "(Zhang et al., 2018)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 237,
                    "end": 254,
                    "text": "(Li et al., 2015)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "Table 3 summarizes the results achieved by different methods. From this table, we make the following observations. First, pretrained models including DialoGPT and BART in general perform better than un-pretrained Transformer. This demonstrates the effectiveness of transfer learning, which leverages external large-scale data to learn powerful representations of texts. Second, BART achieves lower perplexity than DialoGPT models. This is probably because BART is pretrained on a much larger and more diverse corpus than DialoGPT, which enables BART to better model the language. Third, DialoGPT-large performs better than BART on machine translation metrics including NIST, BLEU, and METEOR. This is probably because DialoGPT-large is pretrained on dialogue data and therefore tends to generate n-grams that are more related to dialogues. Fourth, on diversity-related metrics including Entropy and Dist, BART are on par with DialoGPT models. Note that the comparison between different architectures is not totally fair since they are pretrained on different corpus. Due to the lack of computing resources, we are not able to make a fair comparison by training these architectures on the same corpus. We will leave such a study",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Patient: i have all the symptoms except fever, i went to medicross and dr said i can get tested if i want to i'm not sure if i should. she gave me antibiotics klacid xl 500mg, she said i can take it if i feel worse i'm worried it will make immune system bad?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conversation history"
        },
        {
            "text": "Groundtruth response in brief: antibiotic i don't recommend antibiotics for a simple viral upper respiratory tract infection unless examination revealed signs of acute bronchitis or sinusitis. they are not effective for viral infections like covid 19 with no bacterial lung involvement either. if you've been exposed to someone with covid 19 or or if you or someone you were exposed to travelled to a region where it was endemic, get tested would you like to video or text chat with me? to the future. The average length of the generated responses by different methods is close to that of the ground-truth, which is around 50. Figure 3 shows an example of generating a doctor's response given the utterance of a patient. As can be seen, the response generated by BART is more relevant, informative, and human-like, compared with those generated by other baselines. BART's response suggests the patient to get tested for COVID-19 since the patient stated that \"I have all the symptoms except fever\". This response gives correct and informative medical advice: \"get tested if you have fever, cough, or shortness of breath\", \"if you are a smoker or have been in contact with someone with covid, get tested\". The response is human-like, with correct grammar and semantics. It begins with a welcome opening, then provides medical advice, and finally offers to further discuss via video. In contrast, the response generated by DialoGPT-large is not informative. It does not provide any useful medical advice. The response generated by DialoGPT-medium is informative, but not very relevant. The patient has no fever, but this response focuses on talking about the causes of fever. Similar to DialoGPT-large, the responses generated by DialoGPT-small and Transformer are uninformative. Based on dialogues, we split the Chinese dataset into a training set, validation set, and test set, with a ratio of 8:1:1. Table 4 shows the statistics of the data split. The vocabulary size (number of unique Chinese characters) was set to 13317. The hyperparameters were tuned on the validation set. We stop the training procedure when the validation loss stops to decrease. For DialoGPT, we used the DialoGPT-small architecture where the number of layers in the Transformer was set to 10. The context size was set to 300. The embedding size was set to 768. The number of heads in multi-head self-attention was set to 12. The epsilon parameter in layer normalization was set to 1e-5. Network weights were optimized with Adam, with an initial learning rate of 1.5e-4 and a batch size of 8. The Noam learning rate scheduler with 2000 warm-up steps was used. In the finetuning of BERT-GPT, the max length of the source sequence and target sequence was set to 400. The encoder and decoder structures are similar to those in BERT, which is a Transformer with 12 layers and the size of the hidden states is 768. The network weights are optimized with stochastic gradient descent with a learning rate of 1e-4. For Transformer, we used the HuggingFace implementation 9 and followed their default hyperparameter settings. During decoding for all methods, beam search with k = 50 was used. We evaluated the models using perplexity, NIST-4, BLEU-2, 4, METEOR, Entropy-4, and Dist-1, 2. Table 5 summarizes the results. From this table, we make the following observations. First, pretrained models including DialoGPT and BERT-GPT achieve lower perplexity than Transformer. This further demonstrates the effectiveness of transfer learning. Second, DialoGPT-MMI achieves better scores on machine translation metrics, which is consistent with the results on the CovidDialog-English dataset. Third, BERT-GPT achieves much better Dist scores than other methods. We manually checked the generated responses by BERT-GPT. Indeed, they are more diverse than others. Fourth, maximum mutual Figure 4 shows an example of generating a doctor's response given the utterance of a patient. The response generated by BERT-GPT matches with the ground-truth, both of which indicate that the patient has low risk of being infected. The responses generated by other methods are not understandable.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 627,
                    "end": 635,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1901,
                    "end": 1908,
                    "text": "Table 4",
                    "ref_id": "TABREF7"
                },
                {
                    "start": 3254,
                    "end": 3261,
                    "text": "Table 5",
                    "ref_id": "TABREF9"
                },
                {
                    "start": 3846,
                    "end": 3854,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Conversation history"
        },
        {
            "text": "Many works have been devoted to developing medical dialogue systems. Please refer to (Laranjo et al., 2018) for a comprehensive review. Some methods (Lucas et al., 2017; Philip et al., 2017; Tanaka et al., 2017) predefine a sequence of steps or states which are used to guide the conversation. Other methods (Rhee et al., 2014; Ireland et al., 2016; Fitzpatrick et al., 2017) use predetermined templates to extract information from the conversation history and use rules to generate responses from the filled slots in the templates. These methods rely heavily on knowledge engineering and are difficult to be quickly adapted to a new and time-sensitive task such as COVID-19 dialogue generation.",
            "cite_spans": [
                {
                    "start": 85,
                    "end": 107,
                    "text": "(Laranjo et al., 2018)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 149,
                    "end": 169,
                    "text": "(Lucas et al., 2017;",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 170,
                    "end": 190,
                    "text": "Philip et al., 2017;",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 191,
                    "end": 211,
                    "text": "Tanaka et al., 2017)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 308,
                    "end": 327,
                    "text": "(Rhee et al., 2014;",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 328,
                    "end": 349,
                    "text": "Ireland et al., 2016;",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 350,
                    "end": 375,
                    "text": "Fitzpatrick et al., 2017)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "Data-driven medical dialogue generation based on neural networks has been investigated in several works. proposed a task-oriented dialogue system to make medical diagnosis automatically based on reinforcement learning. The system converses with patients to collect additional symptoms beyond their self-reports. Xu et al. (Xu et al., 2019) proposed a knowledge-routed relational dialogue system that incorporates medical knowledge graph into topic transition in dialogue management. Xia et al. (Xia et al.) developed a reinforcement learning (RL) based dialogue system for automatic diagnosis. They proposed a policy gradient framework based on the generative adversarial network to optimize the RL model. In these works, the neural models are trained from scratch on small-sized medical dialogue datasets, which are prone to overfitting.",
            "cite_spans": [
                {
                    "start": 322,
                    "end": 339,
                    "text": "(Xu et al., 2019)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 494,
                    "end": 506,
                    "text": "(Xia et al.)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "In this work, we make the first attempt to develop dialogue systems that can provide medical consultations about COVID-19. To achieve this goal, we first collected two datasets -CovidDialog -which contain medical conversations between patients and doctors about COVID-19. Then on these datasets, we train dialogue generation models based on pretrained Transformer, DialoGPT, and BERT-GPT on large-scale dialogue datasets and other corpus. Experimental results show that these trained models are promising in generating clinically meaningful and linguistically high-quality consultations for COVID-19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "authors": [
                {
                    "first": "Junyoung",
                    "middle": [],
                    "last": "Chung",
                    "suffix": ""
                },
                {
                    "first": "Caglar",
                    "middle": [],
                    "last": "Gulcehre",
                    "suffix": ""
                },
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.3555"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Pre-training with whole word masking for chinese bert",
            "authors": [
                {
                    "first": "Yiming",
                    "middle": [],
                    "last": "Cui",
                    "suffix": ""
                },
                {
                    "first": "Wanxiang",
                    "middle": [],
                    "last": "Che",
                    "suffix": ""
                },
                {
                    "first": "Ting",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Bing",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Ziqing",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Shijin",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Guoping",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.08101"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Pretraining of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [
                        "Toutanova"
                    ],
                    "last": "Bert",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.04805"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics",
            "authors": [
                {
                    "first": "George",
                    "middle": [],
                    "last": "Doddington",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Proceedings of the second international conference on Human Language Technology Research",
            "volume": "",
            "issn": "",
            "pages": "138--145",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (woebot): a randomized controlled trial",
            "authors": [
                {
                    "first": "Kathleen",
                    "middle": [
                        "Kara"
                    ],
                    "last": "Fitzpatrick",
                    "suffix": ""
                },
                {
                    "first": "Alison",
                    "middle": [],
                    "last": "Darcy",
                    "suffix": ""
                },
                {
                    "first": "Molly",
                    "middle": [],
                    "last": "Vierhile",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "JMIR mental health",
            "volume": "4",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Xiangyu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shaoqing",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "Sepp",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J\u00fcrgen",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural computation",
            "volume": "9",
            "issn": "8",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Hello harlie: enabling speech monitoring through chat-bot conversations",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Ireland",
                    "suffix": ""
                },
                {
                    "first": "Christina",
                    "middle": [],
                    "last": "Atay",
                    "suffix": ""
                },
                {
                    "first": "Jacki",
                    "middle": [],
                    "last": "Liddle",
                    "suffix": ""
                },
                {
                    "first": "Dana",
                    "middle": [],
                    "last": "Bradford",
                    "suffix": ""
                },
                {
                    "first": "Helen",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Olivia",
                    "middle": [],
                    "last": "Rushin",
                    "suffix": ""
                },
                {
                    "first": "Thomas",
                    "middle": [],
                    "last": "Mullins",
                    "suffix": ""
                },
                {
                    "first": "Dan",
                    "middle": [],
                    "last": "Angus",
                    "suffix": ""
                },
                {
                    "first": "Janet",
                    "middle": [],
                    "last": "Wiles",
                    "suffix": ""
                },
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Mcbride",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Digital Health Innovation for Consumers, Clinicians, Connectivity and Community-Selected Papers from the 24th Australian National Health Informatics Conference",
            "volume": "227",
            "issn": "",
            "pages": "55--60",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Diederik",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.6980"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Conversational agents in healthcare: a systematic review",
            "authors": [
                {
                    "first": "Liliana",
                    "middle": [],
                    "last": "Laranjo",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                },
                {
                    "first": "Huong",
                    "middle": [
                        "Ly"
                    ],
                    "last": "Dunn",
                    "suffix": ""
                },
                {
                    "first": "Ahmet",
                    "middle": [
                        "Baki"
                    ],
                    "last": "Tong",
                    "suffix": ""
                },
                {
                    "first": "Jessica",
                    "middle": [],
                    "last": "Kocaballi",
                    "suffix": ""
                },
                {
                    "first": "Rabia",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Didi",
                    "middle": [],
                    "last": "Bashir",
                    "suffix": ""
                },
                {
                    "first": "Blanca",
                    "middle": [],
                    "last": "Surian",
                    "suffix": ""
                },
                {
                    "first": "Farah",
                    "middle": [],
                    "last": "Gallego",
                    "suffix": ""
                },
                {
                    "first": "Annie",
                    "middle": [
                        "Ys"
                    ],
                    "last": "Magrabi",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lau",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of the American Medical Informatics Association",
            "volume": "25",
            "issn": "9",
            "pages": "1248--1258",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments",
            "authors": [
                {
                    "first": "Alon",
                    "middle": [],
                    "last": "Lavie",
                    "suffix": ""
                },
                {
                    "first": "Abhaya",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the second workshop on statistical machine translation",
            "volume": "",
            "issn": "",
            "pages": "228--231",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "authors": [
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "Yinhan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Naman",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Marjan",
                    "middle": [],
                    "last": "Ghazvininejad",
                    "suffix": ""
                },
                {
                    "first": "Abdelrahman",
                    "middle": [],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Ves",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1910.13461"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A diversity-promoting objective function for neural conversation models",
            "authors": [
                {
                    "first": "Jiwei",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Michel",
                    "middle": [],
                    "last": "Galley",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Brockett",
                    "suffix": ""
                },
                {
                    "first": "Jianfeng",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Bill",
                    "middle": [],
                    "last": "Dolan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1510.03055"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A robustly optimized bert pretraining approach",
            "authors": [
                {
                    "first": "Yinhan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Myle",
                    "middle": [],
                    "last": "Ott",
                    "suffix": ""
                },
                {
                    "first": "Naman",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "Jingfei",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "Mandar",
                    "middle": [],
                    "last": "Joshi",
                    "suffix": ""
                },
                {
                    "first": "Danqi",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Omer",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Lewis",
                    "suffix": ""
                },
                {
                    "first": "Luke",
                    "middle": [],
                    "last": "Zettlemoyer",
                    "suffix": ""
                },
                {
                    "first": "Veselin",
                    "middle": [],
                    "last": "Stoyanov",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Roberta",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1907.11692"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Reporting mental health symptoms: breaking down barriers to care with virtual human interviewers",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gale",
                    "suffix": ""
                },
                {
                    "first": "Albert",
                    "middle": [],
                    "last": "Lucas",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [],
                    "last": "Rizzo",
                    "suffix": ""
                },
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Gratch",
                    "suffix": ""
                },
                {
                    "first": "Giota",
                    "middle": [],
                    "last": "Scherer",
                    "suffix": ""
                },
                {
                    "first": "Jill",
                    "middle": [],
                    "last": "Stratou",
                    "suffix": ""
                },
                {
                    "first": "Louis-Philippe",
                    "middle": [],
                    "last": "Boberg",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Morency",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Frontiers in Robotics and AI",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "authors": [
                {
                    "first": "Kishore",
                    "middle": [],
                    "last": "Papineni",
                    "suffix": ""
                },
                {
                    "first": "Salim",
                    "middle": [],
                    "last": "Roukos",
                    "suffix": ""
                },
                {
                    "first": "Todd",
                    "middle": [],
                    "last": "Ward",
                    "suffix": ""
                },
                {
                    "first": "Wei-Jing",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Proceedings of the 40th annual meeting on association for computational linguistics",
            "volume": "",
            "issn": "",
            "pages": "311--318",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Virtual human as a new diagnostic tool, a proof of concept study in the field of major depressive disorders",
            "authors": [
                {
                    "first": "Pierre",
                    "middle": [],
                    "last": "Philip",
                    "suffix": ""
                },
                {
                    "first": "Jean-Arthur",
                    "middle": [],
                    "last": "Micoulaud-Franchi",
                    "suffix": ""
                },
                {
                    "first": "Patricia",
                    "middle": [],
                    "last": "Sagaspe",
                    "suffix": ""
                },
                {
                    "first": "Etienne",
                    "middle": [
                        "De"
                    ],
                    "last": "Sevin",
                    "suffix": ""
                },
                {
                    "first": "J\u00e9r\u00f4me",
                    "middle": [],
                    "last": "Olive",
                    "suffix": ""
                },
                {
                    "first": "St\u00e9phanie",
                    "middle": [],
                    "last": "Bioulac",
                    "suffix": ""
                },
                {
                    "first": "Alain",
                    "middle": [],
                    "last": "Sauteraud",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Scientific reports",
            "volume": "7",
            "issn": "1",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Karthik",
                    "middle": [],
                    "last": "Narasimhan",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Language models are unsupervised multitask learners",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Rewon",
                    "middle": [],
                    "last": "Child",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Luan",
                    "suffix": ""
                },
                {
                    "first": "Dario",
                    "middle": [],
                    "last": "Amodei",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Mobile phone-based asthma self-management aid for adolescents (masmaa): a feasibility study. Patient preference and adherence",
            "authors": [
                {
                    "first": "Hyekyun",
                    "middle": [],
                    "last": "Rhee",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Allen",
                    "suffix": ""
                },
                {
                    "first": "Jennifer",
                    "middle": [],
                    "last": "Mammen",
                    "suffix": ""
                },
                {
                    "first": "Mary",
                    "middle": [],
                    "last": "Swift",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Neural machine translation of rare words with subword units",
            "authors": [
                {
                    "first": "Rico",
                    "middle": [],
                    "last": "Sennrich",
                    "suffix": ""
                },
                {
                    "first": "Barry",
                    "middle": [],
                    "last": "Haddow",
                    "suffix": ""
                },
                {
                    "first": "Alexandra",
                    "middle": [],
                    "last": "Birch",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1508.07909"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Sequence to sequence learning with neural networks",
            "authors": [
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Oriol",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "Quoc V",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3104--3112",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders",
            "authors": [
                {
                    "first": "Hiroki",
                    "middle": [],
                    "last": "Tanaka",
                    "suffix": ""
                },
                {
                    "first": "Hideki",
                    "middle": [],
                    "last": "Negoro",
                    "suffix": ""
                },
                {
                    "first": "Hidemi",
                    "middle": [],
                    "last": "Iwasaka",
                    "suffix": ""
                },
                {
                    "first": "Satoshi",
                    "middle": [],
                    "last": "Nakamura",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "PloS one",
            "volume": "12",
            "issn": "8",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Ashish",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "Llion",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "Aidan",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "Lukasz",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Task-oriented dialogue system for automatic diagnosis",
            "authors": [
                {
                    "first": "Zhongyu",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Qianlong",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Baolin",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Huaixiao",
                    "middle": [],
                    "last": "Tou",
                    "suffix": ""
                },
                {
                    "first": "Ting",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Xuan-Jing",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Kam-Fai",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                },
                {
                    "first": "Xiang",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
            "volume": "2",
            "issn": "",
            "pages": "201--207",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Importance-aware learning for neural headline editing",
            "authors": [
                {
                    "first": "Qingyang",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Lei",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Hao",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Ying",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "Zhou",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1912.01114"
                ]
            }
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Generative adversarial regularized mutual information policy gradient framework for automatic diagnosis",
            "authors": [
                {
                    "first": "Yuan",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "Jingbo",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Zhenhui",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Chao",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Haifeng",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "End-to-end knowledge-routed relational dialogue system for automatic diagnosis",
            "authors": [
                {
                    "first": "Lin",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Qixian",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Ke",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "Xiaodan",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "Jianheng",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Liang",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "33",
            "issn": "",
            "pages": "7346--7353",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Generating informative and diverse conversational responses via adversarial information maximization",
            "authors": [
                {
                    "first": "Yizhe",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Michel",
                    "middle": [],
                    "last": "Galley",
                    "suffix": ""
                },
                {
                    "first": "Jianfeng",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Zhe",
                    "middle": [],
                    "last": "Gan",
                    "suffix": ""
                },
                {
                    "first": "Xiujun",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Brockett",
                    "suffix": ""
                },
                {
                    "first": "Bill",
                    "middle": [],
                    "last": "Dolan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1810--1820",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
            "authors": [
                {
                    "first": "Yizhe",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Siqi",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Michel",
                    "middle": [],
                    "last": "Galley",
                    "suffix": ""
                },
                {
                    "first": "Yen-Chun",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Brockett",
                    "suffix": ""
                },
                {
                    "first": "Xiang",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Jianfeng",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Jingjing",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Bill",
                    "middle": [],
                    "last": "Dolan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1911.00536"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An exemplar consultation in the CovidDialog-English dataset. It consists of a brief description of the patient's medical conditions and the conversation between the patient and a doctor.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "\u75c5\u60c5\u6458\u8981\u53ca\u521d\u6b65\u5370\u8c61: \u8fc7\u654f\u6027\u54b3\u55fd\uff0c\u7126\u8651\u6291\u90c1\u72b6\u6001 (Summary of the condition and initial impressions: Allergic cough, anxiety and depression.) \u603b\u7ed3\u5efa\u8bae: \u897f\u66ff\u5229\u55ea\u7247\u53e3\u670d\uff0c\u5fc5\u8981\u65f6\u5fc3\u7406\u79d1\u5c31\u8bca\u3002 (Summary of recommendations: Take cetirizine tablets orally, if necessary, see a psychologist.) An exemplar consultation in the CovidDialog-Chinese dataset. It consists of (1) description of medical conditions and history of the patient, (2) dialogue between doctor and patient, and (3) diagnosis and treatment suggestions given by the doctor.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "An example of generated English responses.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "An example of generated Chinese responses. information (MMI) does not have a clear efficacy in improving the quality of generated responses.",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "(where n = 4), and 8. https://github.com/brightmart/nlp_chinese_corpus",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "English dataset split statistics",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Performance on the CovidDialog-English test set.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "DialoGPT-large: do you have access to a telephonic consult, or do you need to go in person? would you like to video or text chat with me? i can understand your concern. i have reviewed your query and here is my advice. yes, telephonically would be best. would you be happy to chat with you? i have answered your query. let me know if i can assist you further.DialoGPT-small: in brief: yes. if you feel worse you should go to a doctor. would you like to video or text chat with me? let me know if i can assist you with any further questions.Transformer: hello and welcome to 'ask a doctor' service. i have reviewed your query and here is my advice. i have reviewed your query. let me know if i can assist you further.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Chinese dataset split statistics",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "9. https://github.com/huggingface/transformers Transformer DialoGPT, no MMI DialoGPT, with MMI BERT-GPT",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "Performance on the CovidDialog-Chinese test set. Patient: \u53bb\u533b\u9662\uff0c\u88ab\u68c0\u67e5\u7684\u5de5\u4f5c\u4eba\u5458\u6234\u4e00\u6b21\u6027\u624b\u5957\u78b0\u4e86\u4e0b\u624b\u8155\uff0c\u505c\u597d\u8f66\u5c31\u53bb\u6d17\u4e86\u624b\u3002\u6211\u8fd9 \u4f1a\u6709\u88ab\u611f\u67d3\u65b0\u51a0\u75c5\u6bd2\u5417? (In the hospital, my waist was touched by a nurse wearing gloves. I washed my hands after parking. Did I have the risk of being infected by coronavirus?) Groundtruth: \u4e0d\u4f1a\u7684\uff0c\u4e0d\u7528\u62c5\u5fc3\u3002(Not likely. Do not worry.) BERT-GPT: \u611f\u67d3\u53ef\u80fd\u6027\u975e\u5e38\u5c0f\u3002(The likelihood of being infected is very small) DialoGPT, MMI: \u8fd8\u9700\u8981\u662f\u5c31\u4e5f\u5c31\u611f\u67d3\u6ca1\u6709\u5176\u4ed6 37.\u5728\u53d1\u6765\uff0c\u5e72\u54b3\u3002\u8fd8\u6709\u70b9\u4e4f\u529b\u4e00\u822c\u5728\u5bb6\u5403 \u611f\u89c9\u3002\u6ca1\u6709\u7684\uff0c\u5403\u611f\u5192\u4e86\uff0c\u5c31\u597d\u3002\u6211\u8fd9\u79cd\u6e05\u761f\u80f6\u56ca DialoGPT, no MMI: \u6211\u5c31\u662f\u75ab\u60c5\u51b5\u4e0d\u4f1a\u4e0d\u4f1a\u6709\u6ca1\u6709 Transformer: \u770b\u4e0d\u5efa\u8bae\u4f60\u7d27\u5f20\u4e5f\u6ca1\u6709\u3002\u6ce8\u610f\u7387\u5f88\u591a\u559d\u70b9\u3002",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}