{
    "paper_id": "PMC7148230",
    "metadata": {
        "title": "Reinforced Rewards Framework for Text Style Transfer",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Abhilasha",
                "middle": [],
                "last": "Sancheti",
                "suffix": "",
                "email": "sancheti@cs.umd.edu",
                "affiliation": {}
            },
            {
                "first": "Kundan",
                "middle": [],
                "last": "Krishna",
                "suffix": "",
                "email": "kundank@andrew.cmu.edu",
                "affiliation": {}
            },
            {
                "first": "Balaji",
                "middle": [
                    "Vasan"
                ],
                "last": "Srinivasan",
                "suffix": "",
                "email": "balsrini@adobe.com",
                "affiliation": {}
            },
            {
                "first": "Anandhavelu",
                "middle": [],
                "last": "Natarajan",
                "suffix": "",
                "email": "anandvn@adobe.com",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Text style transfer deals with transforming a given piece of text in such a way that the stylistic properties change to that of the target text while preserving the core content of the given text. This is an active area of research because of its wide applicability in the field of content creation including news rewriting, generating messages with a particular style to maintain the personality of a brand, etc. The stylistic properties may denote various linguistic phenomenon, from syntactic changes [7, 23] to sentiment modifications [4, 10, 18] or extent of formality in a sentence [16].",
            "cite_spans": [
                {
                    "start": 505,
                    "end": 506,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 508,
                    "end": 510,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 540,
                    "end": 541,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 543,
                    "end": 545,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 547,
                    "end": 549,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 589,
                    "end": 591,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Most of the existing works in this area either use copy-enriched sequence-to-sequence models [7] or employ an adversarial [4, 15, 18] or much simpler generative approaches [10] based on the disentanglement of style and content in text. On the other hand, more recent works like [19] and [3] perform the task of style transfer without disentangling style and content, as practically this condition cannot always be met. However, all of these works use word-level objective function (eg. cross-entropy) while training which is inconsistent with the desired metrics (content preservation and transfer strength) to be optimized in style transfer tasks. These metrics are generally calculated at a sentence-level and use of word level objective functions is not sufficient. Moreover, discreteness of these metrics makes it even harder to directly optimize the model over these metrics.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 95,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 123,
                    "end": 124,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 126,
                    "end": 128,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 130,
                    "end": 132,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 173,
                    "end": 175,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 279,
                    "end": 281,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 288,
                    "end": 289,
                    "mention": "3",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Recent advancements in Reinforcement Learning and its effectiveness in various NLP tasks like sequence modelling [8], abstractive summarization [14], and a related one machine translation [21] have motivated us to leverage reinforcement learning approaches in style transfer tasks.",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 115,
                    "mention": "8",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 145,
                    "end": 147,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 189,
                    "end": 191,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we propose a reinforcement learning (RL) based framework which adopts to optimize sequence-level objectives to perform text style transfer. Our reinforced rewards framework is based on a sequence-to-sequence model with attention [1, 12] and copy-mechanism [7] to perform the task of text style transfer. The sentence generated by this model along with the ground truth sentence is passed to a content module and a style classifier which calculates the metric scores to finally obtain the reward values. These rewards are then propagated back to the sequence-to-sequence model in the form of loss terms.",
            "cite_spans": [
                {
                    "start": 245,
                    "end": 246,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 248,
                    "end": 250,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 272,
                    "end": 273,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The rest of our paper is organized as follows: we discuss related work on text style transfer in Sect. 2. The proposed reinforced rewards framework is introduced in Sect. 3. We evaluate our framework and report the results on formality transfer task in Sect. 4, on affective dimension like excitement in Sect. 5 and on Shakespearean-Modern English corpus in Sect. 6. In Sect. 7, we discuss few qualitative sample outputs. Finally, we conclude the paper in Sect. 8.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Style transfer approaches can be broadly categorized as style transfer with parallel corpus and style transfer with non-parallel corpus.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Parallel corpus consists of input-output sentence pairs with mapping. Since such corpora are not readily available and difficult to curate, efforts here are limited. [23] introduced a parallel corpus of 30K sentence pairs to transfer Shakespearean English to modern English and benchmark various phrase-based machine translation methods for this task. [7] use a copy-enriched sequence-to-sequence approach for Shakespearizing modern English and show that it outperforms the previous benchmarks by [23]. Recently, [16] introduced a parallel corpus of formal and informal sentences and benchmark various neural frameworks to transfer sentences across different formality levels. Our approach contributes in this field of parallel style transfer and extends the work by [7] by directly optimizing the metrics used for evaluating the style transfer tasks.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 169,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 353,
                    "end": 354,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 498,
                    "end": 500,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 514,
                    "end": 516,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 768,
                    "end": 769,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Another class of explorations are in the area of non-parallel text style transfer [4, 10, 15, 18] which does not require mapping between the input and output sentences. [4] compose a non-parallel dataset for paper-news titles and propose models to learn separate representations for style and content using adversarial frameworks. [18] assume a shared latent content distribution across a given corpora and propose a method that leverages refined alignment of latent representations to perform style transfer. [10] define style in terms of attributes (such as, sentiment) localized to parts of the sentence and learn to disentangle style from content in an unsupervised setting. Although these approaches perform well on the transfer task, content preservation is generally observed to be low due to the non-parallel nature of the data. Along this line, parallel style transfer approaches have shown better performance in benchmarks despite the data curation challenges [16].",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 84,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 86,
                    "end": 88,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 90,
                    "end": 92,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 94,
                    "end": 96,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 170,
                    "end": 171,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 332,
                    "end": 334,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 511,
                    "end": 513,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 971,
                    "end": 973,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Style transfer models are primarily evaluated on content preservation and transfer strength. But the existing approaches do not optimize on these metrics and rather teach the model to generate sentences to match the ground truth. This is partly because of the reliance on a differentiable training objective and discreteness of these metrics makes it challenging to differentiate the objective. Leveraging recent advancements in reinforcement learning approaches, we propose a reinforcement learning based text style transfer framework which directly optimizes the model on the desired evaluation metrics. Though there exists some prior work on reinforcement learning for machine translation [21], sequence modelling [8] and abstractive summarization [14] dealing model optimization for qualitative metrics like Rouge [11], they do not consider style aspects which is one of the main requirements of style transfer tasks. More recently, efforts [5, 22] have been made to incorporate RL in style transfer tasks in a non-parallel setup. However, our work is in the field of parallel text style transfer which is not much explored.",
            "cite_spans": [
                {
                    "start": 693,
                    "end": 695,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 718,
                    "end": 719,
                    "mention": "8",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 752,
                    "end": 754,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 819,
                    "end": 821,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 946,
                    "end": 947,
                    "mention": "5",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 949,
                    "end": 951,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Our work is different from these related works in the sense that we take care of content preservation and transfer strength with the use of a content module (to ensure content preservation) and cooperative style discriminator (style classifier) without explicitly separating content and style. We illustrate the improvement in the performance of the framework on the task of transferring text between different levels of formality [16]. Furthermore, we present the generalizability of the proposed approach by evaluating it on a self-curated excitement corpus as well as modern English to Shakespearean corpus [7].",
            "cite_spans": [
                {
                    "start": 432,
                    "end": 434,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 611,
                    "end": 612,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "To preserve the content while transferring the style, we leverage Self-Critic Sequence Training (SCST) [17] approach and optimize the framework with BLEU scores as the reward. SCST is a policy gradient method for reinforcement learning and is used to train end-to-end models directly on non-differentiable metrics. We use BLEU score as reward for content preservation because it measures the overlap between the ground truth and the generated sentences. Teaching the network to favor this would result in high overlap with the ground truth and subsequently preserve the content of the source sentence since ground truth ensures this preservation.",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 106,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Content Module: Rewarding Content Preservation ::: Reinforced Rewards Framework",
            "ref_spans": []
        },
        {
            "text": "We produce two output sentences \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^s$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{\\prime }$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^s$$\\end{document} is sampled from the distribution \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p(y_{t}^{s}|y_{1:t-1}^{s},x)$$\\end{document} at each decoding time step and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{\\prime }$$\\end{document} (baseline output) is obtained by greedily maximizing the output distribution at each time step. The BLEU score between the sampled and greedy sequences is computed as the reward and the corresponding content-preservation loss is given by,\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{cp} = (r(y^{\\prime })-r(y^{s}))\\sum \\nolimits _{t=1}^{m}{\\log (p(y_{t}^{s}| y_{1:t-1}^s,x))}, \\end{aligned}$$\\end{document}where the log term is the log likelihood on sampled sequence and the difference term is the difference between the reward (BLEU score) for the greedily sampled \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{\\prime }$$\\end{document} and multinomially sampled \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{s}$$\\end{document} sentences. Note that our formulation is flexible and does not require the metric to be differentiable because rewards are used as weights to the log-likelihood loss. Minimizing \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{cp}$$\\end{document} is equivalent to encouraging the model to generate sentences which have higher reward as compared to the baseline \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{\\prime }$$\\end{document} and thus increasing the reward expectation of the model. The framework can now be trained end to end by using this loss function along with the cross entropy loss to preserve the content of the source sentence in the transferred sentence.",
            "cite_spans": [],
            "section": "Content Module: Rewarding Content Preservation ::: Reinforced Rewards Framework",
            "ref_spans": []
        },
        {
            "text": "To optimize the model to generate sentences which belong to the target style, it is possible to use a similar loss function as above and use it with the SCST framework [17]. However, that will require a formal measure for the target style aspect. Here, we present an alternate framework where such a formal measure is not readily available. We train a convolutional neural network based style classifier as proposed by [9] on the training dataset. This style classifier predicts the likelihood that an input sentence is in the target style, and the likelihood is taken as a proxy to the reward for style of a sentence and appended to a discriminator-based loss function extended from [6]. Based on the transfer direction, we add the following term to the cross-entropy loss,\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{ts}= {\\left\\{ \\begin{array}{ll} - \\log (1-s(y^{\\prime })),&{} \\text {high to low level} \\\\ - \\log (s(y^{\\prime })),&{} \\text {low to high level} \\end{array}\\right. } \\end{aligned}$$\\end{document}In this formulation, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{\\prime }$$\\end{document} is the greedily generated output from the decoder and s(\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{\\prime }$$\\end{document}) is the likelihood score predicted by the classifier for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y^{\\prime }$$\\end{document}. When transfer is done from high to low level of style, minimization of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ts}$$\\end{document} will encourage generation of sentences such that the classifier score is as low as possible. When the sentences are transferred from low to high level of style then the formulation ensures that the generated sentences have a score as high as possible. The framework is trained end-to-end using this loss function to generate the sentences which belong to the target style.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 171,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 420,
                    "end": 421,
                    "mention": "9",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 685,
                    "end": 686,
                    "mention": "6",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Style Classifier: Rewarding Transfer Strength ::: Reinforced Rewards Framework",
            "ref_spans": []
        },
        {
            "text": "The overall loss function thus can be written as a combination of the 3 loss functions,\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Loss= \\alpha L_{ml}+ \\beta L_{cp}+ \\gamma L_{ts} $$\\end{document}We train various models using this loss function and different training methodologies (setting \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha =1.0$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta =0.125$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma =1.0$$\\end{document} after hyper-parameter tuning) as described in the next section. During the inference phase, the model predicts a probability distribution over the vocabulary based on the sentence generated so far and the word having the highest probability is chosen as the next word till the maximum length of the output sentence is reached. Note that unlike training phase in which case both the input and ground truth transferred sentences are available to the model, only the input sentence is made available to the model.",
            "cite_spans": [],
            "section": "Training and Inference ::: Reinforced Rewards Framework",
            "ref_spans": []
        },
        {
            "text": "We evaluate the proposed approach on the GYAFC [16] dataset which is a parallel corpus for formal-informal text. We present the transfer task results in both the directions - formal to informal and vice-versa. This dataset (from Entertainment and Music domain) consists of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}56K informal-formal sentence pairs: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}52K in train, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}1.5K in test and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}2.5K in validation split.",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 50,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "We use both human and automatic evaluation measures for content preservation and transfer strength to illustrate the performance of the proposed approach.",
            "cite_spans": [],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "Content preservation measures the degree to which the target style model outputs have the same meaning as the input style sentence. Following [16], we measure preservation of content using BLEU [13] score between the ground truth and the generated sentence since the ground truth ensures that content of the source style sentence is preserved in it. For human evaluation, we presented 50 randomly selected model outputs to the Mechanical turk annotators and requested them to rate the outputs on a Likert [2] scale of 6 as described in [16].",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 145,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 195,
                    "end": 197,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 506,
                    "end": 507,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 537,
                    "end": 539,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "Transfer strength measures the degree to which style transfer was carried out. We reuse the classifiers that we built to provide rewards to the generated sentences (Sect. 3.2). A score above 0.5 from the classifier represents that the generated sentence belongs to the target style and to the source style otherwise. We define accuracy as the fraction of generated sentences which are classified to be in the target style. The higher the accuracy, higher is the transfer strength. For human evaluation, we ask the Mechanical turk annotators to rate the generated sentence on a Likert scale of 5 as described in [16].",
            "cite_spans": [
                {
                    "start": 612,
                    "end": 614,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "Following [4] who illustrate the trade-off between the two metrics - content preservation and transfer strength, we combine the two evaluation measures and present an overall score for the transfer task since both the measures are central to different aspects of text style transfer task. The trade-off arises because the best content preservation can be achieved by simply copying the source sentence. However, the transfer strength in such scenario will be the worst. We compute overall score in the following way\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathrm {Overall} = \\frac{\\text {BLEU} \\times \\text {Accuracy}}{\\text {BLEU} + \\text {Accuracy}} \\end{aligned}$$\\end{document}which is similar to F1-score since content preservation can be considered as measuring recall of the amount of source content retained in the target style sentence and transfer strength acts as a measure of precision with which the transfer task is carried out. In the above formulation, both BLEU and accuracy scores are normalized to be between 0 and 1.\n",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 12,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "We first ran an ablation study to demonstrate the improvement in performance of the model with introduction of the two loss terms in the various settings differing in the way training is being carried out. Below we provide details about each of the settings.",
            "cite_spans": [],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "\nCopyNMT: Trained with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ml}$$\\end{document}TS: Trained with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ml}$$\\end{document} followed by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha L_{ml}+ \\gamma L_{ts}$$\\end{document}CP: Trained with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ml}$$\\end{document} followed by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha L_{ml}+ \\beta L_{cp}$$\\end{document}TS+CP: Trained with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ml}$$\\end{document} followed by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha L_{ml}+\\beta L_{cp}+\\gamma L_{ts}$$\\end{document}TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP: Trained with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ml}$$\\end{document} followed by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha L_{ml}+ \\gamma L_{ts}$$\\end{document} and finally with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha L_{ml}+ \\beta L_{cp}$$\\end{document}CP\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}TS: Trained with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ml}$$\\end{document} followed by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha L_{ml}+ \\beta L_{cp}$$\\end{document} and finally with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha L_{ml}+ \\gamma L_{ts}$$\\end{document}\n",
            "cite_spans": [],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "Training with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ml}$$\\end{document} alone in all the above settings is done for 10 epochs with all the hyper-parameters set as default in the off-the-shelf implementation of [7]. Each of the iterative model training is done using the model with the best performance on validation set for 5 more epochs. We can observe from Table 1 that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{ts}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{cp}$$\\end{document} helps in improving the accuracy which measures transfer strength (TS) and BLEU score which measures content preservation (CP) respectively as compared to CopyNMT. When all the three loss terms are used simultaneously (TS+CP) the resulting performance lies between TS and CP, indicating that there is a trade-off between the two metrics and improvement in one metric is at the cost of another as observed by [4]. This phenomenon is evident from the results of TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP and CP\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}TS where the network gets a bit biased towards the latter optimization. Moreover, improvement in CP\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}TS and TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP as compared to TS and CP respectively suggests that incremental training better helps in teaching the framework. Since the performance on both transfer strength and content preservation metrics plays an important role in text style transfer task, we chose TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP, which has the maximum overall score, over the other models for further analysis.\n",
            "cite_spans": [
                {
                    "start": 445,
                    "end": 446,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1602,
                    "end": 1603,
                    "mention": "4",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": [
                {
                    "start": 599,
                    "end": 600,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Baselines: We compare the proposed approach TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP against the state-of-the-art cross-aligned autoencoder style transfer approach (Cross-Aligned) by [18]1, parallel style transfer approach (CopyNMT) by [7]2 and neural encoder-decoder based transformer model [20]3.",
            "cite_spans": [
                {
                    "start": 445,
                    "end": 447,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 498,
                    "end": 499,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 554,
                    "end": 556,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": []
        },
        {
            "text": "Results: It can be seen from Table 2 that even though the transformer model has the best accuracy, it fails in preserving the content. Closer look at the outputs (formal to informal transfer task in Table 4) reveal that it generates sentences in target style but the sentences do not preserve the meaning of the input and sometimes are out of context (discussed in the Sect. 7). Cross-Aligned performs the worst in informal to formal transfer task among all the other approaches because it is generating a lot of unknowns and is not able to preserve content. TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP, on the other hand, has the highest overall score and performs the best in preserving the content. We also observed that the dataset had many sentences containing proper nouns like name of the songs, person or artists. In such cases, copy mechanism helps in retaining the proper nouns whereas other models are not able to do so. This is evident from the higher BLEU scores for our proposed model. Table 3 presents the human evaluation results aggregated over three annotators per sample. It can be seen that in at least 70% of the cases, annotators rated model outputs from TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP as better than the three baselines on both the evaluated metrics except for the content preservation as compared to CopyNMT in formal to informal task wherein, both the models perform equally good. One reason behind this is that both the models use copy-mechanism.\n",
            "cite_spans": [],
            "section": "Experiments: Reinforcing Formality (GYAFC Dataset)",
            "ref_spans": [
                {
                    "start": 35,
                    "end": 36,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 205,
                    "end": 206,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1264,
                    "end": 1265,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "In order to demonstrate the generalizability of our approach on an affective style dimension like excitement (the feeling of enthusiasm and eagerness), we curated our own dataset using reviews from Yelp dataset4 which is a subset of Yelp\u2019s businesses, reviews, and user data. We request human annotators to provide rewrites for given exciting sentences such that they sound as non-exciting/boring as possible. Reviews with rating greater than or equal to 3 were filtered out and considered as exciting to get the non-exciting/boring rewrites. We also asked the annotators to rate the given and transferred sentences on a Likert scale of 1 (No Excitement at all) to 5 (Very high Excitement). The dataset thus curated was split into train (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}36K), test (1K) and validation (2K) sets. We evaluate the transfer quality on content preservation and transfer strength metrics as defined in Sect. 4.",
            "cite_spans": [],
            "section": "Experiments: Beyond Formality (Excitement Dataset)",
            "ref_spans": []
        },
        {
            "text": "For measuring the transfer strength we train a classifier as described in Sect. 3.2. We use the annotations provided by the human annotators on these sentences to get the labels for the two styles. Sentences with a rating greater than or equal to 3 were considered as exciting and non-exciting otherwise.",
            "cite_spans": [],
            "section": "Experiments: Beyond Formality (Excitement Dataset)",
            "ref_spans": []
        },
        {
            "text": "Results: The transfer task in this case is to convert the input sentence with high excitement (exciting) to a sentence with low excitement (non-exciting) and vice-versa. We can observe from Table 2 that model performance in the case of excitement transfer task is similar to what we observed in the formality transfer task. However, CopyNMT performs the best in transferring style in case of non-exciting to exciting transfer task because the model has picked up on expressive words (\u2018awesome\u2019, \u2018great\u2019, and \u2018amazing\u2019) which helps in boosting the transfer strength. TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP (with highest overall score) consistently outperforms Cross-Aligned in all the metrics and both the directions. Table 3 presents the human evaluation results on this transfer task. We notice that humans preferred outputs from our proposed model at least 60% of the times on both the measures as compared to the other three baselines. This provides an evidence that the proposed RL-based framework indeed helps in improving generation of more content preserving sentences which align with the target style.",
            "cite_spans": [],
            "section": "Experiments: Beyond Formality (Excitement Dataset)",
            "ref_spans": [
                {
                    "start": 196,
                    "end": 197,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 986,
                    "end": 987,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Besides affective style dimensions, our approach can also be extended to other style transfer tasks like converting modern English to Shakespearean English. To illustrate the performance of our model on this task we experimented with the corpus used in [7]. The dataset consists of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}21K modern-Shakespearean English sentence pairs with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}18K in train, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}1.5K in test and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}1.2K in validation split. We use the same evaluation measures as in the previous two tasks for illustrating the model performance and generalizability of the approach. For this task we present only the automatic evaluation results because manual evaluation of this task is not easy since it requires an understanding of Shakespearean english and finding such population is a difficult task due to limited availability.",
            "cite_spans": [
                {
                    "start": 254,
                    "end": 255,
                    "mention": "7",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Experiments: Beyond Affective Elements (English Dataset)",
            "ref_spans": []
        },
        {
            "text": "Results: We can observe from Table 2 that model performance in the case of this transfer task is also similar to what we have observed in the earlier two transfer tasks. Although Cross-Aligned has better accuracy than TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP, it fails to preserve the content (sample 3 of Table 6). Similar is the case with transformer which outperforms others in accuracy but is not able to retain the content (sample 1 of Table 6). TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP outperforms the three baselines in preserving the content with the highest overall score. This establishes the viability of our approach to various types of text style transfer tasks.",
            "cite_spans": [],
            "section": "Experiments: Beyond Affective Elements (English Dataset)",
            "ref_spans": [
                {
                    "start": 35,
                    "end": 36,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 573,
                    "end": 574,
                    "mention": "6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 708,
                    "end": 709,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "These experiments further indicate that our proposed reinforcement learning framework improves the transfer strength and content preservation of parallel style transfer frameworks and is also generalizable across various stylistic expression.\n",
            "cite_spans": [],
            "section": "Experiments: Beyond Affective Elements (English Dataset)",
            "ref_spans": []
        },
        {
            "text": "\n\n\n",
            "cite_spans": [],
            "section": "Experiments: Beyond Affective Elements (English Dataset)",
            "ref_spans": []
        },
        {
            "text": "In this section, we provide few qualitative samples from the baselines and the proposed reinforcement learning based model. We can observe from the transformer model output for Input 1 and 2 in formal to informal column of Table 4 that it generates sentences with correct target style but does not preserve the content. It either adds random content or deletes the required content (\u2018band\u2019 instead of \u2018better\u2019 in 1 and \u2018hot\u2019 instead of \u2018talented\u2019 in 2). As mentioned earlier, in sample output 3 of Table 4, Cross-Aligned is unable to retain the content and tend to generate unknown tokens. CopyNMT, even though is able to preserve content, tend to generate repeated token like \u2018please\u2019 in sample input 2 (informal to formal task) which results in lower BLEU score than our proposed approach. Transformer model outputs for exciting to non-exciting task in samples 1 and 2 of Table 5, miss specific content words like \u2018environment\u2019 and \u2018alisha\u2019 respectively. However, it is able to generate the sentences in target style. Similary, Cross-Aligned and CopyNMT are also not able to retain the name of the server in sample 2 of Table 5. Sample 2 of Shakespearean to Modern English and 1 of Modern to Shakespearean English task in Table 6 provide evidence for high accuracy and lower BLEU scores for transformer model. From sample 2 of Shakespearean to modern English transfer task, we can observe that Cross-Aligned although can generate the sentence in the target style is not able to preserve the entities like \u2018father\u2019 and \u2018child\u2019. On the other hand, TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP can not only generate the sentences in the target style but is also able to retain the entities. There are few cases when CopyNMT is better in preserving the content as compared to other models, for instance, sample 1 of formal to informal transfer task and sample 3 of non-exciting to exciting transfer task since it leverages copy-mechanism.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": [
                {
                    "start": 229,
                    "end": 230,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 504,
                    "end": 505,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 880,
                    "end": 881,
                    "mention": "5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 1128,
                    "end": 1129,
                    "mention": "5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 1230,
                    "end": 1231,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "Another point to notice is the lexical level changes made to reflect the target style. For example, the use of \u2018would\u2019, \u2018don\u2019t\u2019 and \u2018inform\u2019 instead of \u2018want\u2019, \u2018dono\u2019 and \u2018let me know\u2019 respectively for transforming informal sentences into formal ones. Use of colloquial words like \u2018u\u2019, \u2018gonna\u2019 and \u2018mama\u2019 for converting the formal sentences to informal can be observed from the sample outputs. Not only lexical level changes but structural transformations can also be observed as in \u2018Please inform me if you find out\u2019. In case of excitement transfer task, use of strong expressive words like \u2018amazing\u2019 and \u2018great\u2019 makes the sentence sound more exciting while less expressive words such as \u2018okay\u2019 and \u2018good\u2019 makes the sentence less exciting. Use of \u2018thou\u2019 for you and \u2018hither\u2019 for here are more frequently used in Shakespearean English than in modern English. These sample outputs indeed provide an evidence that our model is able to learn these lexical or structural level differences in various transfer tasks, be it formality, beyond formality or beyond affective dimensions.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "The primary contribution of this work is a reinforce rewards based sequence-to-sequence model which explicitly optimizes over content preservation and transfer strength metrics for style transfer with parallel corpus. Initial results are promising and generalize to other stylistic characteristics as illustrated in our experimental sections. Leveraging this approach for simultaneously changing multiple stylistic properties (for e.g. high excitement and low formality) is a subject of further research.",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Ablation study to demonstrate the improvement of the addition of the loss terms on formality transfer task.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Comparison of TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP with the baselines on the three transfer tasks in both the directions. All the scores are normalized to be between 0 and 1.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Human evaluation results of 50 randomly selected model outputs. The values represent the % of times annotators rated model outputs from TS\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow $$\\end{document}CP (R) as better than the baseline CopyNMT (C), Transformer (T) and Cross-Aligned (S) over the metrics. I-F (E-NE) refers to informal to formal (exciting to non-exciting) task.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Sample model outputs and target style reference for Informal to Formal and Formal to Informal style transfer task. The first line is the source style sentence (input), second line is the reference output and the following lines correspond to the outputs from the baselines and the RL-based model.\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Sample model outputs and target style reference for Exciting to Non-exciting and Non-exciting to Exciting style transfer task. The first line is the source style sentence (input), second line is the reference output and the following lines correspond to the outputs from the baselines and RL-based model.\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6.: Sample model outputs and target style reference for Modern to Shakespearean English and Shakespearean to Modern English transfer task. The first line is the source style sentence (input), second line is the reference output and the following lines correspond to the outputs from the baselines and the RL-based model.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Model overview",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}