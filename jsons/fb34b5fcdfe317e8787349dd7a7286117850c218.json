{
    "paper_id": "fb34b5fcdfe317e8787349dd7a7286117850c218",
    "metadata": {
        "title": "MiniSeg: An Extremely Minimum Network for Efficient COVID-19 Segmentation",
        "authors": [
            {
                "first": "Yu",
                "middle": [],
                "last": "Qiu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Yun",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jing",
                "middle": [],
                "last": "Xu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "The rapid spread of the new pandemic, coronavirus disease 2019 , has seriously threatened global health. The gold standard for COVID-19 diagnosis is the tried-andtrue polymerase chain reaction (PCR), but PCR is a laborious, time-consuming and complicated manual process that is in short supply. Deep learning based computer-aided screening, e.g., infection segmentation, is thus viewed as an alternative due to its great successes in medical imaging. However, the publicly available COVID-19 training data are limited, which would easily cause overfitting of traditional deep learning methods that are usually data-hungry with millions of parameters. On the other hand, fast training/testing and low computational cost are also important for quick deployment and development of computer-aided COVID-19 screening systems, but traditional deep learning methods, especially for image segmentation, are usually computationally intensive. To address the above problems, we propose MiniSeg, a lightweight deep learning model for efficient COVID-19 segmentation. Compared with traditional segmentation methods, MiniSeg has several significant strengths: i) it only has 472K parameters and is thus not easy to overfit; ii) it has high computational efficiency and is thus convenient for practical deployment; iii) it can be fast retrained by other users using their private COVID-19 data for further improving performance. In addition, we build a comprehensive COVID-19 segmentation benchmark for comparing MiniSeg with traditional methods. Code and models will be released to promote the research and practical deployment for computer-aided COVID-19 screening.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "A S one of the most serious pandemics in human history, coronavirus disease 2019 (COVID- 19) continues to threaten global health with thousands of newly infected patients every day. Since the end of 2019, COVID-19 has infected more than 2.0M people and caused 130K deaths. COVID-19 is caused by the infection of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) which can be spread by breathing, coughing, sneezing, or other means of excreting infectious viruses. Effective screening of infected patients is of high importance to the fight against COVID-19 because i) early diagnosis of COVID-19 can help mitigate the spread of viruses by isolating the infected patients early; ii) imposing treatment early greatly improves the chances Y. Qiu is with College of Artificial Intelligence, Nankai University, Tianjin 300350, China (e-mail: yqiu@mail.nankai.edu.cn).",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 92,
                    "text": "19)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Y. Liu is with College of Computer Science, Nankai University, Tianjin 300350, China (e-mail: nk12csly@mail.nankai.edu.cn).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "J. Xu is with College of Artificial Intelligence, Nankai University, Tianjin 300350, China (e-mail: xujing@nankai.edu.cn).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Joint first author: Y. Qiu and Y. Liu; joint corresponding author: Y. Liu and J. Xu of survival; iii) confirmed patients should also be repeatedly diagnosed in order to confirm whether the virus is cleared. The gold standard for COVID-19 diagnosis is the tried-and-true polymerase chain reaction (PCR) [1] which detects nucleic acid (e.g., RNA) of SARS-CoV-2 from respiratory specimens (e.g., nasopharyngeal swabs, oropharyngeal swabs, nasal midturbinate swabs, and anterior nares swabs) in laboratories of Biosafety Level 2 (BSL-2). As a laborious, time-consuming and complicated manual process, PCR testing is in short supply, although many countries are racing to increase supplies.",
            "cite_spans": [
                {
                    "start": 302,
                    "end": 305,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Another COVID-19 screening method is radiography examination by analyzing chest radiography images, e.g., X-ray and computed tomography (CT). As found in recent studies [2] , [3] , there exist characteristic abnormalities in the chest radiography images of infected patients. Some researchers suggest that chest CT should be considered as a primary tool for COVID-19 screening in epidemic areas [4] . Since chest radiography imaging can be easily conducted in modern hospitals, radiography examination is faster and cheaper than PCR testing, in some cases, even showing higher sensitivity [5] . However, the bottleneck is that radiography examination needs expert radiologists to interpret radiography images, while human eyes are not sensitive enough to subtle visual indicators especially in the exhausted state of overwork [6] , [7] . Therefore, computer-aided systems are expected for automatic and accurate radiography interpretation. Computer-aided systems could learn to capture subtle details and will never feel tired like human beings.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 172,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 175,
                    "end": 178,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 395,
                    "end": 398,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 589,
                    "end": 592,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 826,
                    "end": 829,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 832,
                    "end": 835,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "When it comes to computer-aided COVID-19 screening, deep learning based technology is a good choice due to its uncountable successful stories in computer vision and medical imaging. In some cases, deep learning can even outperform human beings [6] , [8] - [10] . However, directly applying traditional deep learning models for COVID-19 screening is suboptimal. On one hand, these models usually have millions of parameters and thus require a large amount of labeled data for training. The problem is that the publicly available COVID-19 data are limited and thus easy to cause overfitting of traditional data-hungry models. On the other hand, traditional deep learning methods, especially the ones for image segmentation, are usually computationally intensive. Considering the current severe pandemic situation, fast training/testing and low computational load are essential for quick deployment and development of computer-aided COVID-19 screening systems.",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 247,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 250,
                    "end": 253,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 256,
                    "end": 260,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "It is a widely accepted concept that overfitting is easier to happen when a model has more parameters and less training data. To solve the above problems of COVID-19 segmentation, we observe that lightweight networks are not only uneasy to arXiv:2004.09750v1 [cs.CV] 21 Apr 2020 overfit owing to their small number of parameters but also likely to be efficient, making them suitable for computer-aided COVID-19 screening systems. Therefore, we think lightweight COVID-19 segmentation should be the technical solution of this paper. The key is how to achieve accurate segmentation under the constraints of the number of network parameters and high efficiency. Although replacing the vanilla convolution with the combination of depthwise separable convolution (DSConv) and pointwise convolution [11] , [12] can reduce the number of network parameters, the accuracy usually decreases as the network shrinks [11] - [15] . To achieve our goal, we find the accuracy of image segmentation can be improved with better multi-scale learning. Many multi-scale learning strategies have significantly pushed forward the state of the arts of segmentation [16] - [24] . Hence a proper multi-scale learning strategy has the potential to ensure the segmentation accuracy of lightweight networks.",
            "cite_spans": [
                {
                    "start": 793,
                    "end": 797,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 800,
                    "end": 804,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 904,
                    "end": 908,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 911,
                    "end": 915,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1148,
                    "end": 1152,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "With the above analyses, our effort starts with the design of an Attentive Hierarchical Spatial Pyramid (AHSP) module for effective lightweight multi-scale learning. AHSP first builds a spatial pyramid of dilated depthwise separable convolutions and feature pooling for learning multi-scale semantic features. Then, the learned multi-scale features are fused hierarchically to enhance the capacity of multi-scale representation. Finally, the multi-scale features are merged under the guidance of the attention mechanism which learns to highlight essential information and filter out noisy information in radiography images. With the AHSP module incorporated, we propose an extremely minimum network for efficient segmentation of COVID-19 infected areas in chest CT images. Our method, namely MiniSeg, only has 472K parameters, two orders of magnitude less than traditional image segmentation methods, so that current limited COVID-19 data can be adopted for training MiniSeg. We build a comprehensive COVID-19 segmentation benchmark, including the well-known methods for both medical image segmentation and semantic image segmentation, for extensively comparing MiniSeg with previous stateof-the-art methods. Experiments demonstrate that MiniSeg performs favorably against previous state-of-the-art segmentation methods with high efficiency and limited COVID-19 training data. Code and models will be released to promote the future research and deployment of computer-aided COVID-19 screening.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "In summary, our contributions are threefold:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2022 We propose an Attentive Hierarchical Spatial Pyramid (AHSP) module for effective lightweight multi-scale learning which plays an essential role in image segmentation. \u2022 With the AHSP module incorporated, we present an extremely minimum network, MiniSeg, for accurate and efficient COVID-19 segmentation with limited training data. \u2022 For extensive comparison of MiniSeg with previous stateof-the-art segmentation methods, we build a comprehensive COVID-19 segmentation benchmark where MiniSeg performs favorably against previous competitors with high efficiency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "In this section, we briefly review recent development in image segmentation and the techniques for designing efficient networks. We also discuss some recent studies for computeraided COVID-19 screening.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Image segmentation is a hot topic due to its wide range of applications. Since the invention of fully convolutional networks (FCNs) [25] , FCNs based methods have dominated this field. Objects in images usually exhibit very large scale changes, so multi-scale learning plays an essential role in image segmentation. Hence most of the current state-of-theart methods aim at designing FCNs to learn effective multiscale representations from input images. For example, Ronneberger et al. [26] proposed the well-known U-Net architecture that is actually an encoder-decoder network for fusing the deep features from the top to bottom layers. DeconvNet [27] and SegNet [28] make careful designs to improve the U-Net architecture. U-Net++ [29] improves U-Net by introducing a series of nested, dense skip connections between the encoder and decoder sub-networks. Attention U-Net [30] improves U-Net by using the attention mechanism to learn to focus on target structures. Some studies [31] - [34] also aggregate multiscale deep features from multi-level layers for final dense prediction. DeepLab [16] and its variants [17] , [18] , [35] design ASPP modules using dilated convolutions with different dilation rates to learn multi-scale features. Based on ASPP, DenseASPP [19] connects a set of dilated convolutional layers densely, such that it generates multi-scale features that cover a larger scale range densely.",
            "cite_spans": [
                {
                    "start": 132,
                    "end": 136,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 485,
                    "end": 489,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 647,
                    "end": 651,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 663,
                    "end": 667,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 732,
                    "end": 736,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 872,
                    "end": 876,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 978,
                    "end": 982,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 985,
                    "end": 989,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1112,
                    "end": 1116,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Besides the multi-scale learning, some studies focus on exploiting the global context information through pyramid pooling [20] , context encoding [36] , or non-local operations [37] , [38] . Moreover, DFN [24] introduces a smooth network to handle the intra-class inconsistency problem and a border network to make the bilateral features of boundary distinguishable. Wu et al. [39] tried to find a good compromise between network depth and width to improve segmentation accuracy. Some methods [35] , [40] , [41] use conditional random fields (CRF) or Markov random fields (MRF) to model the spatial relationship in semantic segmentation. The above models aim at improving the segmentation accuracy without consideration of model size and inference speed, so they are impractical for COVID-19 segmentation which only has limited training data and requires high efficiency. In the experiment section, we will show that the limited COVID-19 training data cannot optimize these large models well.",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 146,
                    "end": 150,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 177,
                    "end": 181,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 184,
                    "end": 188,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 205,
                    "end": 209,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 377,
                    "end": 381,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 493,
                    "end": 497,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 500,
                    "end": 504,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 507,
                    "end": 511,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Lightweight networks aim at reducing the parameters and improving the efficiency of deep networks. Convolutional factorization is an intuitive way to reduce the computational complexity of convolution operations. Specifically, many wellknown network architectures decompose the standard convolution into multiple steps to reduce the computational complexity, including Flattened Model [42] , Inception networks [43] - [45] , Xception [11] , ResNeXt [46] , and MobileNets [12] , [13] . Among them, Xception [11] and MobileNets [12] , [13] factorize a convolution operation into a pointwise convolution and a DSConv. The pointwise convolution is actually a 1 \u00d7 1 convolution, which is used for interaction among channels. The depthwise convolution is a grouped convolution with the number of groups equaling to the number of output channels, so that it can process each feature channel separably. Shuf-fleNets [14] , [15] further factorize a pointwise convolution into a channel shuffle operation and a grouped pointwise convolution for reducing the parameters and complexity. A standard 2D convolution can also be factorized into two asymmetric 1D convolutions [43] - [45] , [47] . There are some studies focusing on designing efficient semantic segmentation networks [21] , [22] , [43] , [48] - [50] . ESPNet [21] decomposes the standard convolution into a pointwise convolution and a spatial pyramid of dilated convolutions. ESPNetv2 [22] extends ESPNet [21] using grouped pointwise and dilated DSConv. Considering COVID-19 segmentation, our proposed MiniSeg should have a small number of parameters for training with limited data. Our observation of the essential role of multi-scale learning in image segmentation helps MiniSeg achieve higher accuracy while running at a fast speed.",
            "cite_spans": [
                {
                    "start": 385,
                    "end": 389,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 411,
                    "end": 415,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 418,
                    "end": 422,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 434,
                    "end": 438,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 449,
                    "end": 453,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 471,
                    "end": 475,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 478,
                    "end": 482,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 506,
                    "end": 510,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 526,
                    "end": 530,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 533,
                    "end": 537,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 908,
                    "end": 912,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 915,
                    "end": 919,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1167,
                    "end": 1171,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1274,
                    "end": 1278,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1309,
                    "end": 1313,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1435,
                    "end": 1439,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1455,
                    "end": 1459,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Another way to build efficient networks is network compression. Previous studies have adopted various techniques, such as shrinking [51] , parameter quantization [52] , pruning [53] and hashing [54] , to compress networks. Some research [55] - [57] also quantizes the network weights into low bits to reduce the model size and computational complexity. In this paper, we must avoid the training of large networks owing to the shortage of COVID-19 data. Therefore, these methods are unsuitable for our goal, because they aim at compressing pretrained large networks rather than directly train a lightweight model.",
            "cite_spans": [
                {
                    "start": 132,
                    "end": 136,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 162,
                    "end": 166,
                    "text": "[52]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 177,
                    "end": 181,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 194,
                    "end": 198,
                    "text": "[54]",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 237,
                    "end": 241,
                    "text": "[55]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 244,
                    "end": 248,
                    "text": "[57]",
                    "ref_id": "BIBREF56"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "Computer-aided COVID-19 screening has already attracted the attention of medical imaging researchers to alleviate the shortage of PCR supply. Some studies [58] - [60] design deep neural networks to classify chest CT images for COVID-19 screening, but their code is not released. Inspired by the open-source efforts by the research community, a chest X-ray classification network [7] is proposed for COVID-19 screening and its code has been available. There are also some other X-ray classification based COVID-19 screening networks [61] , [62] . In this paper, we focus on segmenting COVID-19 infected areas from chest CT images, because segmentation can provide more useful information than image classification and chest CT has been demonstrated to be a very useful tool for COVID-19 screening [4] , [5] .",
            "cite_spans": [
                {
                    "start": 155,
                    "end": 159,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 162,
                    "end": 166,
                    "text": "[60]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 379,
                    "end": 382,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 532,
                    "end": 536,
                    "text": "[61]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 539,
                    "end": 543,
                    "text": "[62]",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 796,
                    "end": 799,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 802,
                    "end": 805,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "II. RELATED WORK"
        },
        {
            "text": "In this section, we first introduce our Attentive Hierarchical Spatial Pyramid (AHSP) module for effective and lightweight multi-scale learning. Then, we present the network architecture of MiniSeg for the segmentation of COVID-19 infected lung areas. At last, we provide the training strategies of MiniSeg.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. METHODOLOGY"
        },
        {
            "text": "Although the factorization of a convolution operation into a pointwise convolution and a depthwise separable convolution (DSConv) can significantly reduce the number of network parameters and computational complexity, it usually comes with the decrease of accuracy [11] - [15] . Inspired by the fact that effective multi-scale learning plays an essential role in improving segmentation accuracy [16] - [24] , we propose the AHSP module for effective multi-scale learning in a lightweight and efficient setting. Besides some common convolution operations such as vanilla convolution, pointwise convolution, and DSConv, we introduce the dilated DSConv convolution which adopts a dilated convolution kernel for each input channel. Suppose F k\u00d7k r denotes a vanilla convolution, where k \u00d7 k is the size of convolution kernel and r is the dilation rate. SupposeF k\u00d7k r denotes a depthwise separable convolution, where k \u00d7 k and r have the same meaning as F k\u00d7k r . The subscript r will be omitted without ambiguity if we have a dilation rate of 1, i.e., r = 1. For example, F 1\u00d71 represents a pointwise convolution (i.e., 1 \u00d7 1 convolution). F 3\u00d73 2 represents a dilated DSConv with a dilation rate of 2.",
            "cite_spans": [
                {
                    "start": 265,
                    "end": 269,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 272,
                    "end": 276,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 395,
                    "end": 399,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 402,
                    "end": 406,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "With the above definitions of basic operations, we continue by introducing the proposed AHSP module which is illustrated in Fig. 1 . Let X \u2208 R C\u00d7H\u00d7W be the input feature map, so that the output feature map is H(X) \u2208 R C \u00d7H \u00d7W , where H denotes the transformation function of AHSP for its input. C, H, and W are the number of channels, height, and width of input feature map X, respectively; similar definitions hold for C , H , and W . The input feature map X is first processed by a pointwise convolution to shrink the number of channels into C /K, in which K is the number of parallel dilated branches which will be described later. This operation can be written as",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 124,
                    "end": 130,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "Then, the generated feature map S is fed into K parallel dilated DSConv, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "where the dilation rate is increased exponentially for enlarging the receptive field. Equation (2) is the basis for multi-scale learning with large dilation rates capturing large-scale information and small dilation rates capturing local information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "We also add an average pooling operation for S to enrich the multi-scale information, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "where AvgP ool 3\u00d73 represents the average pooling with kernel size of 3 \u00d7 3. Note that we have F k \u2208 R C K \u00d7H \u00d7W for k = 0, 1, \u00b7 \u00b7 \u00b7 , K. If we have H = H or W = W , the convolution and pooling operations in (2) and (3) will have a stride of 2 to downsample the feature map by a scale of 2; otherwise, the stride will be 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "These multi-scale feature maps produced by (2) and (3) are merged in an attentive hierarchical manner. We first add them up hierarchically as\u1e1e",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "where feature maps are gradually fused from small scales to large scales to enhance the representation capability of multiscale learning. We further adopt a spatial attention mechanism to make the AHSP module automatically learn to focus on target structures of varying scales. On the other hand, the attention mechanism can also learn to suppress irrelevant information at some feature scales and emphasize essential information at other scales. Such self-attention make each scale speak for itself to decide how important it is in the multiscale learning process. The transformation of\u1e1e by spatial attention can be formulated as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "in which \u03c3 is a sigmoid activation function and \u2297 indicates element-wise multiplication. The pointwise convolution in (5) outputs a single-channel feature map which is then transformed to a spatial attention map by the sigmoid function. This attention map is replicated to the same size as\u1e1e k , i.e., C K \u00d7 H \u00d7 W , before element-wise multiplication. Considering the efficiency, we can compute the attention map for all K branches together, like",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "where Concat(\u00b7) means to concatenate a series of feature maps along the channel dimension. The pointwise convolution in (6) is a K-grouped convolution with K output channels, so we have A \u2208 R K\u00d7H \u00d7W . Hence we can rewrite (5) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "in which A[k] means the k-th channel of A. Note that (5) is equivalent to (6) and (7). Finally, we merge and fuse the above hierarchical feature maps asF = Concat(F 1 ,F 2 , \u00b7 \u00b7 \u00b7 ,F K ),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "where BatchNorm(\u00b7) denotes the batch normalization [63] and PReLU(\u00b7) indicates PReLU (i.e., Parametric ReLU) activation function [9] . The pointwise convolution in (8) is a K-grouped convolution with C output channels, so that this pointwise convolution aims at fusingF k (k = 1, 2, \u00b7 \u00b7 \u00b7 , K) separately, i.e., adding connection to channels for depthwise convolutions in (2) . The fusion among hierarchical scales is conducted using the first pointwise convolution in the next AHSP module of MiniSeg, which means (1) also serves to fuse features of various scales in the previous AHSP module. Such a design can reduce the number of convolution parameters in (8) by K times when compared with that using a vanilla pointwise convolution, i.e., C 2 /K vs. C 2 .",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 55,
                    "text": "[63]",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 129,
                    "end": 132,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 372,
                    "end": 375,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "Given an input feature map X \u2208 R C\u00d7H\u00d7W , we can compute the output feature map H(X) \u2208 R C \u00d7H \u00d7W of an AHSP module using (1)- (8) . We can easily find that increasing K will reduce the number of AHSP parameters. Considering the balance between segmentation accuracy and efficiency, we set K = 4 in our experiments. The proposed AHSP module not only significantly reduces the number of parameters but also enables to learn effective multi-scale features, so that we can adopt the limited COVID-19 data to train a high-quality segmenter.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 128,
                    "text": "(8)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "A. Attentive Hierarchical Spatial Pyramid Module"
        },
        {
            "text": "Here, we describe in detail the network architecture of the proposed lightweight COVID-19 segmenter, i.e., MiniSeg. MiniSeg has an encoder-decoder structure where the encoder sub-network focuses on learning effective multi-scale representations for the input image, while the decoder sub-network gradually aggregates the representations at different levels of the encoder to predict COVID-19 infected areas. The network architecture of MiniSeg is displayed in Fig. 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 460,
                    "end": 466,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "B. Network Architecture"
        },
        {
            "text": "The encoder sub-network uses AHSP as the basic module, consisting of two paths that are connected through a series of nested skip pathways. Suppose I \u2208 R 3\u00d7H\u00d7W denotes an input chest CT image, where a grayscale CT image is replicated three times to make its number of channels the same as color images. The input I is downsampled four times in the encoder sub-network, resulting in four scales of 1 we downsample until the 1/16 scale for enlarging the receptive field and reducing the computational complexity. Suppose in the encoder sub-network we denote the output feature map of the i-th stage and the j-th block as E i j , w.r.t. i \u2208 {1, 2, 3, 4} and j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , N i }, where N i indicates the number of blocks in the i-th stage. Therefore, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": ", in which C i is the number of feature channels at the i-th stage. The abovementioned block refers to the proposed AHSP module except for the first stage whose basic block is the vanilla Convolution Block (CB). Since the number of feature channels at the first stage (i.e., C 1 ) is small, the vanilla convolution will not introduce too many parameters. Without ambiguity, let H i j (\u00b7) be the transformation function of the i-th stage and the j-th block without distinguishing whether this block is a vanilla convolution or an AHSP module. For the another path, we propose a Downsampler Block (DB) block. The transformation function of a DB block is denoted as H i (\u00b7) (i \u2208 {1, 2, 3, 4}):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "whereF 5\u00d75 (\u00b7) has a stride of 2 for downsampling. Suppose the output of H i (\u00b7) is E i . Therefore, for the first block of the first stage, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "For the first block of other stages, we compute the output feature map as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "Here, H i 1 (\u00b7) (i \u2208 {1, 2, 3, 4}) has a stride of 2 for feature downsampling by a scale of 2. For other blocks, the output feature map is computed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "where H i j (\u00b7) has a stride of 1 and a residual connection is included for better optimization. As shown in (12) , the output of another path H i (\u00b7) is connected to the main path. The computation of E i can be formulated as",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "(12)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "except for i = 1:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "Through (12) and (13), the two paths of the encoder subnetwork build nested skip connections. Such a design benefits the multi-scale learning of the encoder. Considering the balance among the number of network parameters, segmentation accuracy, and efficiency, we set C i to 16, 64, 128, and 256, and set N i to 3, 4, 9, and 7, for i = 1, 2, 3, 4, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "The decoder sub-network is simple for efficient multiscale feature decoding. Since the top feature map of the encoder has a scale of 1/16 of the original input, it is suboptimal to predict COVID-19 infected areas directly owing to the loss of fine details. We instead utilize a simple decoder sub-network to gradually upsample and fuse the learned feature map at each scale of the encoder sub-network. A Feature Fusion Module (FFM) is proposed for feature aggregation. Let H i (\u00b7) represent the function of FFM:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "in which H i (X) (i = 1, 2, 3) has C i channels and the pointwise convolution is utilized to adjust such number of channels. We denote the feature map in the decoder as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "We compute other D i (i = 3, 2, 1) as (Upsample(D i+1 , 2) ),",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 38,
                    "end": 58,
                    "text": "(Upsample(D i+1 , 2)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "B. Network Architecture"
        },
        {
            "text": "where Upsample(\u00b7, t) means to upsample a feature map by a scale of t using bilinear interpolation. In this way, the decoder sub-network enhance the high-level semantic features with low-level fine details, so that MiniSeg can make accurate predictions for COVID-19 infected areas.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "With D i (i = 1, 2, 3, 4) computed, we can make dense prediction using a pointwise convolution, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "where Softmax(\u00b7) is a standard softmax function and this pointwise convolution has two output channels representing two classes of background and COVID-19 infected areas, respectively. P i \u2208 R 1\u00d7H\u00d7W is the predicted class label map. We utilize P 1 as the final output prediction. In the training, we impose deep supervision [66] by replacing the softmax function in (17) with the well-known cross-entropy loss function, i.e.,",
            "cite_spans": [
                {
                    "start": 324,
                    "end": 328,
                    "text": "[66]",
                    "ref_id": "BIBREF65"
                }
            ],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "in which CEL(\u00b7) indicates the standard cross-entropy loss function and G is the ground-truth label map. The total loss is calculated as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "where \u03bb is a weight to balance the losses at different stages.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "In this paper, we follow previous studies [20] , [36] , [51] to empirically set \u03bb as 0.4.",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 46,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 49,
                    "end": 53,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 56,
                    "end": 60,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "B. Network Architecture"
        },
        {
            "text": "Implementation details. We implement the proposed MiniSeg network using the well-known PyTorch [67] framework. Adam optimization [68] is used for training with the weight decay of 1e-4. We adopt the learning policy of poly where the initial learning rate is 1e-3 and we train 100 epochs on the training set. A batch size of 5 is used. Note that we train all previous state-of-the-art segmentation methods using the same training settings as our MiniSeg for a fair comparison. All experiments are performed on a TITAN RTX GPU.",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 99,
                    "text": "[67]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 129,
                    "end": 133,
                    "text": "[68]",
                    "ref_id": "BIBREF67"
                }
            ],
            "ref_spans": [],
            "section": "A. Experimental Setup"
        },
        {
            "text": "We utilize an open-access COVID-19 CT segmentation dataset [69] to evaluate MiniSeg. Due to the constraints of personal privacy and government policy, this dataset is the only publicly available COVID-19 segmentation dataset. It consists of 100 axial CT images from \u223c60 patients with COVID-19, so that this is a small but diverse dataset with each patient contributing \u223c1.6 axial CT images. Each CT image is carefully annotated by a radiologist to provide the segmentation mask of COVID-19 infected areas. We randomly choose 60 CT images for training models, and another 40 CT images are used for performance evaluation. In the training, we resize the image into multiple scales, i.e., 0.5, 0.75, 1.0, 1.25, and 1.5. We also utilize the standard cropping and random flipping for data augmentation. Thanks to the high diversity of this dataset, the evaluation results of models on this dataset are representative.",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "text": "[69]",
                    "ref_id": "BIBREF68"
                }
            ],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "Evaluation metrics. In this paper, we evaluate the COVID-19 segmentation accuracy using five widely used evaluation metrics in medical imaging analysis, i.e., mean intersection over union (mIoU), sensitivity (SEN), specificity (SPC), Dice similarity coefficient (DSC), and the Hausdorff distance (HD). The metric of mIoU is a typical measure for semantic segmentation by computing the overlap rate between prediction and ground truth for each class and then averaging across all classes. Here, sensitivity represents the ability of the COVID-19 infected area of ground truth to be predicted as it is. Specificity represents the ability of the background region of ground truth to be predicted as background. Dice similarity coefficient is an overlap index that can represent the degree of similarity between predicted COVID-19 area and labeled COVID-19 area of ground truth. Hausdorff distance (HD) measures the structural differences among two given objects and is the minimum distance between the ground truth and segmented region. These metrics are defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "where TP, FP, TN, FN indicate the number of pixels in the true positive, false positive, true negative, and false negative regions, respectively. S m = {s m1 , s m2 , . . . , s mi } is the curve generated from ground truth of the COVID-19 infected area, and S a = {s a1 , s a2 , . . . , s ai } is the curve formed by segmentation methods. Suppose we have h(S m , S a ) = max sm\u2208Sm min sa\u2208Sa ||s m \u2212 s a || where || \u00b7 || is Euclidean distance; h(S a , S m ) can be similarly defined. Specifically, SEN, SPC, and DSC range between 0 and 1; The larger these values, the better the model. Note that the lower value of HD indicates better segmentation accuracy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "Before comparing with state-of-the-art competitors, we conduct ablation studies to demonstrate the effectiveness of our model components. The effect of the main components is summarized in Table I . We start with a single-branch module that only has the DSConv with a dilation rate of 1. Replacing all AHSP modules in MiniSeg with such singlebranch modules and removing the two-path design of the MiniSeg encoder, we achieve an mIoU of 78.10%. Then, we extend such a single-branch module to a multi-branch module using the spatial pyramid as in the AHSP module, such a multi-branch module improves the mIoU to 78.79%, which demonstrates the importance of multi-scale learning. Next, we add our attentive hierarchical fusion strategy to get our AHSP module, so we can improve the mIoU to 80.30%, which proves the superiority of the attentive hierarchical fusion. We continue by adding the two-path design to the encoder sub-network to recover MiniSeg, and we achieve an mIoU of 81.27%, which validates that such a two-path design can benefit the network optimization. At last, pretraining the MiniSeg encoder sub-network on the ImageNet dataset [70] further pushes the mIoU to 82.12. These ablation studies demonstrate that the main components in MiniSeg are all effective in COVID-19 segmentation.",
            "cite_spans": [
                {
                    "start": 1144,
                    "end": 1148,
                    "text": "[70]",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [
                {
                    "start": 189,
                    "end": 196,
                    "text": "Table I",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "B. Ablation Studies"
        },
        {
            "text": "Besides the above main components, we also conduct ablation studies for some design choices of MiniSeg. The results are shown in Table II . We first replace the PReLU activation function [9] with the ReLU function [71] . Then, we remove the decoder sub-network and change the stride of the last stage from 2 to 1, so we can directly make predictions at the scale of 1/8 and upsample to the original size, as in previous studies [16] , [19] - [21] , [23] , [24] , [36] - [38] , [47] - [49] , [64] , [65] . Next, we remove the deep supervision in the training, which means that only L 1 in (19) is used. Furthermore, we replace the Convolution Blocks (CB) in the first stage with the AHSP modules. Besides, we also replace the 5 \u00d7 5 DSConv in the Downsampler Blocks (DB) with 3 \u00d7 3 DSConv. At last, we replace the Feature Fusion Modules (FFM) in the decoder subnetwork with AHSP modules. We can see that the default setting achieves the best overall performance, especially in terms of the most important and best-known mIoU metric.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 190,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 214,
                    "end": 218,
                    "text": "[71]",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 428,
                    "end": 432,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 435,
                    "end": 439,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 442,
                    "end": 446,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 449,
                    "end": 453,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 456,
                    "end": 460,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 463,
                    "end": 467,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 470,
                    "end": 474,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 477,
                    "end": 481,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 484,
                    "end": 488,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 491,
                    "end": 495,
                    "text": "[64]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 498,
                    "end": 502,
                    "text": "[65]",
                    "ref_id": "BIBREF64"
                }
            ],
            "ref_spans": [
                {
                    "start": 129,
                    "end": 137,
                    "text": "Table II",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "B. Ablation Studies"
        },
        {
            "text": "To compare MiniSeg with previous state-of-the-art competitors and promote the research for COVID-19 segmentation, we build a comprehensive benchmark. This benchmark contain 26 previous state-of-the-art image segmentation methods, including U-Net [26] , FCN-8s [25] , FRRN [23] , SegNet [28] , PSPNet [20] , DeepLabv3 [17] , DeepLabv3+ [18] , UNet++ [29] , Attention U-Net [30] , BiSeNet [72] , DenseASPP [19] , DFN [24] , EncNet [36] , OCNet [73] , DANet [74] , MobileNet [12] , MobileNetv2 [13] , MobileNetv3 [75] , ShuffleNet [14] , ShuffleNetv2 [15] , ENet [64] , CGNet [48] , EDANet [49] , LEDNet [50] , ESPNet [21] , and ESPNetv2 [22] , Among them, MobileNet, MobileNetv2, MobileNetv3, ShuffleNet, and ShuffleNetv2 are designed for lightweight image classification, we view them as the encoder and add the decoder of MiniSeg to them, so that they are reformed as image segmentation models. Besides, ENet, CGNet, EDANet, LEDNet, ESPNet, and ESPNetv2, are well-known lightweight segmentation models. We not only evaluate these methods using five widely used metrics for medical imaging segmentation but also report their numbers of parameters, numbers of FLOPs, and speed, where FLOPs and speed are tested using a 512 \u00d7 512 input image and a TITAN RTX GPU. We think this benchmark would be useful for future research on COVID-19 segmentation.",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 250,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 260,
                    "end": 264,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 272,
                    "end": 276,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 286,
                    "end": 290,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 300,
                    "end": 304,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 317,
                    "end": 321,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 335,
                    "end": 339,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 349,
                    "end": 353,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 372,
                    "end": 376,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 387,
                    "end": 391,
                    "text": "[72]",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 404,
                    "end": 408,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 415,
                    "end": 419,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 429,
                    "end": 433,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 442,
                    "end": 446,
                    "text": "[73]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 455,
                    "end": 459,
                    "text": "[74]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 472,
                    "end": 476,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 491,
                    "end": 495,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 510,
                    "end": 514,
                    "text": "[75]",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 528,
                    "end": 532,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 548,
                    "end": 552,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 560,
                    "end": 564,
                    "text": "[64]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 573,
                    "end": 577,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 587,
                    "end": 591,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 601,
                    "end": 605,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 615,
                    "end": 619,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 635,
                    "end": 639,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "C. Comparison with State-of-the-art Methods"
        },
        {
            "text": "The evaluation results of MiniSeg and other competitors are displayed in Table III . We can see that lightweight models seem to outperform traditional segmentation networks, which proves our conjecture that networks with a small number of parameters are more suitable for COVID-19 segmentation due to the limited training data. Among traditional large networks, FCN-8s [25] achieves the best performance. We think it is because the simplest FCN-8s is easy to training than other carefully designed networks, which further demonstrates that previous state-of-the-art segmentation networks with too many parameters are not suitable for COVID-19 segmentation. Lightweight models, including Mobilenets [12] , [13] , [75] , ShuffleNets [14] , [15] , and lightweight segmentation models, achieve very competitive performance, but the proposed MiniSeg are more stable across various evaluation metrics. In terms of the best-known metric of mIoU, MiniSeg achieves the best performance when training with or without ImageNet pretraining [70] . This demonstrates the superiority of MiniSeg in COVID-19 infected area segmentation.",
            "cite_spans": [
                {
                    "start": 369,
                    "end": 373,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 698,
                    "end": 702,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 705,
                    "end": 709,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 712,
                    "end": 716,
                    "text": "[75]",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 731,
                    "end": 735,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 738,
                    "end": 742,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "text": "[70]",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [
                {
                    "start": 73,
                    "end": 82,
                    "text": "Table III",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "C. Comparison with State-of-the-art Methods"
        },
        {
            "text": "Compared with other methods, MiniSeg consistently achieves the best performance in terms of four metrics, including mIoU, SPE, DSC, and HD. For the metric of SEN, MiniSeg performs slightly worse than the best method. Furthermore, we note that no competitors can consistently perform well on all metrics. The fact that MiniSeg consistently outperforms other competitors demonstrates its effectiveness. MiniSeg also has a low computational load (i.e., fewer FLOPs) and fast speed, making it convenient for practical deployment which is of high importance in the current serious situation of COVID-19. Fig. 3 provide some examples for segmenting COVID-19 infected areas from chest CT images using MiniSeg. We can observe that the results of MiniSeg are very close to the ground truth segmentation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 599,
                    "end": 605,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "C. Comparison with State-of-the-art Methods"
        },
        {
            "text": "In this paper, we focus on segmenting COVID-19 infected areas from chest CT images. To address the lack of COVID-19 training data and meet the efficiency requirement for the deployment of computer-aided COVID-19 screening systems, we propose an extremely minimum network, i.e., MiniSeg, for accurate and efficient COVID-19 segmentation. MiniSeg adopts a novel multi-scale learning module, i.e., the Attentive Hierarchical Spatial Pyramid (AHSP) module, to ensure its accuracy under the constraints of the extremely minimum network size. To extensive compare MiniSeg with previous state-of-the-art image segmentation methods and promote the research on COVID-19 segmentation, we build a comprehensive benchmark that would be useful for future research. The comparison of MiniSeg with state-of-the-art image segmentation methods demonstrates that MiniSeg not only achieves the best performance but also has high efficiency. The code and models of this paper will be released.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Detection of SARS-CoV-2 in different types of clinical specimens",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "J. American Medical Association",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "The Lancet",
            "volume": "395",
            "issn": "10223",
            "pages": "497--506",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Imaging profile of the COVID-19 infection: Radiologic findings and literature review",
            "authors": [
                {
                    "first": "M.-Y",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Lui",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "S"
                    ],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Leung",
                    "suffix": ""
                },
                {
                    "first": "P.-L",
                    "middle": [],
                    "last": "Khong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology: Cardiothoracic Imaging",
            "volume": "2",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: A report of 1014 cases",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ai",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Sensitivity of chest CT for COVID-19: Comparison to RT-PCR",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Rethinking computer-aided tuberculosis diagnosis",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y.-H",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ban",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "COVID-Net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest radiography images",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.09871"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Deep learning face representation by joint identification-verification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Adv. Neural Inform. Process. Syst",
            "volume": "",
            "issn": "",
            "pages": "1988--1996",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "1026--1034",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Richer convolutional features for edge detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "J.-W",
                    "middle": [],
                    "last": "Bian",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "41",
            "issn": "8",
            "pages": "1939--1946",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Xception: Deep learning with depthwise separable convolutions",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chollet",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "1251--1258",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kalenichenko",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Weyand",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Andreetto",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1704.04861"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "MobileNetV2: Inverted residuals and linear bottlenecks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sandler",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zhmoginov",
                    "suffix": ""
                },
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "4510--4520",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "ShuffleNet: An extremely efficient convolutional neural network for mobile devices",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "6848--6856",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "ShuffleNet v2: Practical guidelines for efficient CNN architecture design",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H.-T",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "116--131",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs",
            "authors": [
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kokkinos",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "40",
            "issn": "4",
            "pages": "834--848",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Rethinking atrous convolution for semantic image segmentation",
            "authors": [
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schroff",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1706.05587"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
            "authors": [
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schroff",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "801--818",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "DenseASPP for semantic segmentation in street scenes",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "3684--3692",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Pyramid scene parsing network",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "2881--2890",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "ESPNet: Efficient spatial pyramid of dilated convolutions for semantic segmentation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mehta",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rastegari",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Caspi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shapiro",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hajishirzi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "552--568",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "ESPNetv2: A light-weight, power efficient, and general purpose convolutional neural network",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mehta",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rastegari",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shapiro",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hajishirzi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Full-resolution residual networks for semantic segmentation in street scenes",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pohlen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hermans",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mathias",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Leibe",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "4151--4160",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Learning a discriminative feature network for semantic segmentation",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "1857--1866",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Fully convolutional networks for semantic segmentation",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "39",
            "issn": "4",
            "pages": "640--651",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "U-Net: Convolutional networks for biomedical image segmentation",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Med. Image Comput. Comput. Assist. Interv. Soc",
            "volume": "",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Learning deconvolution network for semantic segmentation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Noh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "1520--1528",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "SegNet: A deep convolutional encoder-decoder architecture for image segmentation",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Badrinarayanan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kendall",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cipolla",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "39",
            "issn": "12",
            "pages": "2481--2495",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Unet++: A nested U-Net architecture for medical image segmentation",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M R"
                    ],
                    "last": "Siddiquee",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tajbakhsh",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support",
            "volume": "",
            "issn": "",
            "pages": "3--11",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Attention U-Net: Learning where to look for the pancreas",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Oktay",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schlemper",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "L"
                    ],
                    "last": "Folgoc",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Heinrich",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Misawa",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mori",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mcdonagh",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "Y"
                    ],
                    "last": "Hammerla",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kainz",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.03999"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Attention to scale: Scale-aware semantic image segmentation",
            "authors": [
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "3640--3649",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Hypercolumns for object segmentation and fine-grained localization",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hariharan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Arbel\u00e1ez",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "447--456",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "RefineNet: Multi-path refinement networks for high-resolution semantic segmentation",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Milan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "1925--1934",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "648--663",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs",
            "authors": [
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kokkinos",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Learn. Represent",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Context encoding for semantic segmentation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Dana",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tyagi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "7151--7160",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "CCNet: Criss-cross attention for semantic segmentation",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "603--612",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Asymmetric non-local neural networks for semantic segmentation",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "593--602",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Wider or deeper: Revisiting the resnet model for visual recognition",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "V D"
                    ],
                    "last": "Hengel",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Pattern Recogn",
            "volume": "90",
            "issn": "",
            "pages": "119--133",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Semantic image segmentation via deep parsing network",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Loy",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "1377--1385",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Conditional random fields as recurrent neural networks,\" in Int. Conf. Comput. Vis",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jayasumana",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Romera-Paredes",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vineet",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "H S"
                    ],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1529--1537",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Flattened convolutional neural networks for feedforward acceleration",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dundar",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Culurciello",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Learn. Represent",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Going deeper with convolutions",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sermanet",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Erhan",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rabinovich",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "1--9",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Rethinking the inception architecture for computer vision",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shlens",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "2818--2826",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Inception-v4, Inception-ResNet and the impact of residual connections on learning",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "AAAI Conf. Artif. Intell",
            "volume": "",
            "issn": "",
            "pages": "4278--4284",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Aggregated residual transformations for deep neural networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "1492--1500",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "ERFNet: Efficient residual factorized convnet for real-time semantic segmentation",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Romera",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Alvarez",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "M"
                    ],
                    "last": "Bergasa",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Arroyo",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Intell. Transp. Syst",
            "volume": "19",
            "issn": "1",
            "pages": "263--272",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "CGNet: A light-weight context guided network for semantic segmentation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1811.08201"
                ]
            }
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Efficient dense modules of asymmetric convolution for real-time semantic segmentation",
            "authors": [
                {
                    "first": "S.-Y",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "H.-M",
                    "middle": [],
                    "last": "Hang",
                    "suffix": ""
                },
                {
                    "first": "S.-W",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "J.-J",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM Multimedia Asia on ZZZ",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "LEDNet: A lightweight encoder-decoder network for real-time semantic segmentation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Latecki",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Int. Conf. Image Process",
            "volume": "",
            "issn": "",
            "pages": "1860--1864",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "ICNet for real-time semantic segmentation on high-resolution images",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "405--420",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Quantized convolutional neural networks for mobile devices",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Leng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "4820--4828",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Dally",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Int. Conf. Learn. Represent",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Compressing neural networks with the hashing trick",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Wilson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tyree",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "Q"
                    ],
                    "last": "Weinberger",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Mach. Learn",
            "volume": "",
            "issn": "",
            "pages": "2285--2294",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Quantized neural networks: Training neural networks with low precision weights and activations",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Hubara",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Courbariaux",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Soudry",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "El-Yaniv",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J. Mach. Learn. Res",
            "volume": "18",
            "issn": "1",
            "pages": "6869--6898",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "XNOR-Net: ImageNet classification using binary convolutional neural networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rastegari",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Ordonez",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Redmon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "525--542",
            "other_ids": {}
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Courbariaux",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Hubara",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Soudry",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "El-Yaniv",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1602.02830"
                ]
            }
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Rapid AI development cycle for the coronavirus (COVID-19) pandemic: Initial results for automated detection & patient monitoring using deep learning CT image analysis",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Gozes",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Frid-Adar",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Greenspan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "D"
                    ],
                    "last": "Browning",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bernheim",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Siegel",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.05037"
                ]
            }
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Deep learning system to screen coronavirus disease 2019 pneumonia",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.09334"
                ]
            }
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Automatic detection of coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Narin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kaya",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Pamuk",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.10849"
                ]
            }
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "COVID-19 screening on chest X-ray images using deep learning based anomaly detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.12338"
                ]
            }
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Mach. Learn",
            "volume": "",
            "issn": "",
            "pages": "448--456",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "ENet: A deep neural network architecture for real-time semantic segmentation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Paszke",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chaurasia",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Culurciello",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1606.02147"
                ]
            }
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "ContextNet: Exploring context and detail for semantic segmentation in real-time",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Poudel",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Bonde",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liwicki",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zach",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Brit. Mach. Vis. Conf",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Deeply-supervised nets",
            "authors": [
                {
                    "first": "C.-Y",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Gallagher",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Artif. Intell. Stat",
            "volume": "",
            "issn": "",
            "pages": "562--570",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Automatic differentiation in PyTorch",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Paszke",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chintala",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Chanan",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Devito",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Desmaison",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Antiga",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lerer",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Adv. Neural Inform. Process. Syst. Worksh",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Learn. Represent",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "COVID-19 CT segmentation dataset",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "Imagenet large scale visual recognition challenge",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Russakovsky",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Krause",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Satheesh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Karpathy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Khosla",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bernstein",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Comput. Vis",
            "volume": "115",
            "issn": "3",
            "pages": "211--252",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Rectified linear units improve restricted boltzmann machines",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nair",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Int. Conf. Mach. Learn",
            "volume": "",
            "issn": "",
            "pages": "807--814",
            "other_ids": {}
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "BiSeNet: Bilateral segmentation network for real-time semantic segmentation",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "325--341",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "OCNet: Object context network for scene parsing",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1809.00916"
                ]
            }
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "Dual attention network for scene segmentation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "Searching for mobilenetv3",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sandler",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vasudevan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "1314--1324",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Illustration of the proposed AHSP module.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "/2, 1/4, 1/8, and 1/16, with four stages processing such four scales, respectively. Downsampling happens in the first block of each stage. Previous semantic image segmentation models usually only downsample images into 1/8 scale[16],[19]-[21],[23],[24],[36]-[38],[47]-[49],[64],[65], however, in this paper,",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "OF MAIN COMPONENTS IN MINISEG. A METRIC MARKED BY \u2191 MEANS THAT A MODEL IS BETTER IF IT ACHIEVES HIGHER RESULTS IN TERMS OF THIS METRIC, WHILE \u2191 MEANS THE LOWER THE RESULTS, THE BETTER THE MODEL. NOTE THAT THE METRIC HD DOES NOT HAVE THE UNIT OF %. Single Branch Multiple Branches Attentive Hierarchy Two-path Encoder ImageNet Pretraining Metrics (%) mIoU \u2191 SEN \u2191 SPE \u2191 DSC \u2191 HD \u2193 TABLE II EFFECTS OF SOME DESIGN CHOICES IN MINISEG. EACH DESIGN CHOICE IS REPLACED WITH THE OPERATION IN THE TABLE OR DIRECTLY REMOVED (). A METRIC MARKED BY \u2191 MEANS THAT A MODEL IS BETTER IF IT ACHIEVES HIGHER RESULTS IN TERMS OF THIS METRIC, WHILE \u2191 MEANS THE LOWER THE RESULTS, THE BETTER THE MODEL. NOTE THAT THE METRIC HD DOES NOT HAVE THE UNIT OF %.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "OF MINISEG WITH PREVIOUS STATE-OF-THE-ART SEGMENTATION MODELS. A METRIC MARKED BY \u2191 MEANS THAT A MODEL IS BETTER IF IT ACHIEVES HIGHER RESULTS IN TERMS OF THIS METRIC, WHILE \u2191 MEANS THE LOWER THE RESULTS, THE BETTER THE MODEL. NOTE THAT THE METRIC HD DOES NOT HAVE THE UNIT OF %. FLOPS AND SPEED ARE TESTED USING A 512 \u00d7 512 INPUT IMAGE.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}