{
    "paper_id": "d565e47fb1fbec8f99d0b63476317caa72d62e04",
    "metadata": {
        "title": "Multi-level Memory Network with CRFs for Keyphrase Extraction",
        "authors": [
            {
                "first": "Tao",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Civil Aviation University of China",
                    "location": {
                        "settlement": "Tianjin",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Yuxiang",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Civil Aviation University of China",
                    "location": {
                        "settlement": "Tianjin",
                        "country": "China"
                    }
                },
                "email": "yxzhang@cauc.edu.cn"
            },
            {
                "first": "Haoxiang",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Civil Aviation University of China",
                    "location": {
                        "settlement": "Tianjin",
                        "country": "China"
                    }
                },
                "email": "zhu.hx@outlook.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "Keyphrase, that concisely describe the high-level topics discussed in a document, are very useful for a wide range of natural language processing (NLP) tasks. Current popular supervised methods for keyphrase extraction commonly cannot effectively utilize the longrange contextual information in text. In this paper, we focus on how to effectively exploit the long-range contextual information to improve the keyphrase extraction performance. Specifically, we propose a multilevel memory network with the conditional random fields (CRFs), which allows to have unrestricted access to the long-range and local contextual information in text. We first design the multi-level memory network with sentence level and document level to enhance the text representation. Then, we integrate the multi-level memory network with the CRFs, which has an advantage in modeling the local contextual information. Compared with the recent state-of-the-art methods, our model can achieve better results through experiments on two datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Automatic keyphrase extraction is to recommend a set of representative phrases that are related to the main topics discussed in a document. Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing tasks such as information extraction, text summarization and question answering [21] . However, the performance of existing methods is still far from being satisfactory [10] . The main reason is that it is very challenging to determine whether a phrase or sets of phrases can accurately capture main topics that are presented in the document.",
            "cite_spans": [
                {
                    "start": 363,
                    "end": 367,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 452,
                    "end": 456,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Existing methods for keyphrase extraction can be broadly divided into unsupervised and supervised methods. Specifically, unsupervised approaches directly treat keyphrase extraction as a ranking problem, scoring each word using various measures such as TF-IDF (term frequency-inverse document frequency) and graph-based ranking scores (e.g., degree centrality or PageRank score) [7, 8, 22, 27] .",
            "cite_spans": [
                {
                    "start": 378,
                    "end": 381,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 382,
                    "end": 384,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 385,
                    "end": 388,
                    "text": "22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 389,
                    "end": 392,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Supervised methods usually treat the keyphrase extraction as a binary classification task, in which a classifier is trained on the features of labeled keyphrases to determine whether a candidate phrase is a keyphrase [9, 10, 19] . Compared with unsupervised methods, supervised approaches can yield good results given sufficient training samples.",
            "cite_spans": [
                {
                    "start": 217,
                    "end": 220,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 221,
                    "end": 224,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 225,
                    "end": 228,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, EK-CRF [9] employed the CRFs to extract keyphrases from scientific research articles, which was trained on token-based features incorporating linguistic, document structure information, and expert knowledge. This work achieved better performance on keyphrase extraction task and was shown to be state-of-the-art in previous traditional supervised methods. However, if we can not consider the features used in EK-CRF, CRFs only capture local structural dependencies. In addition, EK-CRF mainly relies on manual feature engineering, which may require considerable effort and domain-specific knowledge.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 20,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we aim to capture the long-range contextual information hidden in the text sequence and remove the need of manual feature engineering to extract keyphrases form scientific papers. Specifically, we formulate the keyphrase extraction as a sequence labeling task. We first use the memory network [23] to capture the long-range contextual information hidden in text data. Note that although plain recurrent neural networks (RNNs) can encode the sequential text and their variants such as long short-term memory (LSTM) models can further capture non-local patterns, they still exhibit a significant local bias in practice [14] . In order to make full use of the effective information hidden in text sequence, we extend the input memory of the memory network with two different levels: sentence level and document level. Secondly, we use the CRF model to capture the dependencies between adjacent words in text sequence and determine whether a candidate phrase is a keyphrase. Finally, we conduct comprehensive experiments over two publicly available datasets (KDD and WWW) in Computer Science area. Experimental results show that the proposed approach outperforms several state-of-the-art supervised methods.",
            "cite_spans": [
                {
                    "start": 307,
                    "end": 311,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 631,
                    "end": 635,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The remainder of this paper is organized as follows. We firstly summarize related works on keyphrase extraction and memory networks in Sect. 2. Secondly, the proposed model for keyphrase extraction is described in Sect. 3. Then, the datasets, experimental results and discussions are illustrated in Sect. 4. Finally, we conclude this paper in Sect. 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we firstly review the related works on keyphrase extraction and then summarize the existing works on the memory network.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "As mentioned in Sect. 1, existing approaches for keyphrase extraction can be broadly divided into unsupervised and supervised methods. This work is mainly related to supervised methods which have been proven to be effective in the keyphrase extraction task. Research on supervised methods has focused on two issues: classifier selection and feature design. Current state-of-the-art classifiers typically include Na\u00efve Bayes [3, 24] , decision trees [19] , CRFs [9] , deep recurrent neural networks (RNN) [16] , etc. The features used to represent an instance can be broadly divided into three categories: statistical features, structural features and syntactic features [10] .",
            "cite_spans": [
                {
                    "start": 424,
                    "end": 427,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 428,
                    "end": 431,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 449,
                    "end": 453,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 461,
                    "end": 464,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 504,
                    "end": 508,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 670,
                    "end": 674,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Keyphrase Extraction"
        },
        {
            "text": "Zhang et al. [26] is the first to use the CRFs extracting keyphrases, which provides a way to explore the local contextual information in text sequence and traditional features, to identify each candidate word by sequence labeling. Bhaskar et al. [2] employed CRFs trained mainly on linguistic features such as part-of-speech (POS), chunking and named-entity tags for keyphrase extraction. Gollapalli et al. [9] also utilized CRFs to extract keyphrases from research papers, which was trained on token-based features incorporating linguistic, documentstructure information and expert knowledge. CopyRNN [16] is the first to employ the sequence-to-sequence (Seq2Seq) deep learning model to predict keyphrases for documents. Following CopyRNN, a few extensions have been proposed to help better generate keyphrases [4, 28] . In addition, Alzaidy et al. [1] integrated the CRF with the bidirectional long short term memory networks (LSTMs) to extract keyphrases from research papers. However, this method didn't capture the long-range contextual dependencies between words in text. We extend the memory network with stronger storage capacity to jointly maintain local structural information provided by RNNs with long-range dependencies in the long text.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 247,
                    "end": 250,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 408,
                    "end": 411,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 603,
                    "end": 607,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 813,
                    "end": 816,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 817,
                    "end": 820,
                    "text": "28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 851,
                    "end": 854,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Keyphrase Extraction"
        },
        {
            "text": "Despite of the success of the RNNs on various text modeling tasks, simple RNNs still exhibit a significant local bias in practice [25] . Memory network [23] enhances the long-term memory capability of deep network by augmenting the internal memory with a series of extra memory components, and provides a general approach for modeling long-range dependencies and making multi-hop reasoning, which has advanced many NLP tasks such as question answering [20] and reading documents [17] . Sukhbaatar et al. [20] proposed the end-to-end memory network, which can be trained end-to-end without any intervention. Kumar et al. [12] proposed the dynamic memory network, which uses a sentence-level attention mechanism to update its internal memory during multi-hop inference. Miller et al. [17] encoded prior knowledge by introducing a key memory structure which stores facts to address to the relevant memory value. Taking inspiration from these works, we design the multi-level memory network with CRFs to extract keyphrases from research papers.",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 134,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 152,
                    "end": 156,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 452,
                    "end": 456,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 479,
                    "end": 483,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 504,
                    "end": 508,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 620,
                    "end": 624,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 782,
                    "end": 786,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Memory Network"
        },
        {
            "text": "In this section, we first present the problem definition in Subsect. 3.1. We then explain the overview of our proposed model in Subsect. 3.2, followed by the details of each component of the model from Subsect. 3.3 to 3.4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Keyphrase extraction is formulated as a task of sequence labeling, predicting a label for each word in the input text sequence. More specifically, we denote the input source document as a sequence x = {x 1 , x 2 , ..., x l }, where x t represents t-th input word and l is the length of the sequence. The goal of the model is to predict a sequence of labels y = {y 1 , y 2 , ..., y l }, where each label y t corresponding to the input word x t represents whether x t is a keyphrase word or not keyphrase word. The memory layer can be divided into two parallel modules at different levels: sentence level memory and document level memory. Each module further includes three components similar to the works [15, 20] : (1) the input memory vector m t , which captures the information from the word embedding layer of the text sequence; (2) the current input embedding u t , which is the representation of the current word; and (3) the output vector c t , which is similar to the input memory m t .",
            "cite_spans": [
                {
                    "start": 704,
                    "end": 708,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 709,
                    "end": 712,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "In addition, the output memory representation of the memory layer o, summarizing the long-range semantic and structure information from the input text sequence without distance limitation, is calculated by a weighted sum over the output representations, in which the attention weights are determined by measuring the similarity between the input memory vector and the current input embedding. Finally, the output of the memory layer o is fed into the CRF layer to predict keyphrases for documents using sequence labelling model CRF. In the remainder of this section, we will present the MLM-CRF in detail.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Overview"
        },
        {
            "text": "Input Memory Representation. At first, the word embedding look-up table, trained by GloVe [18] , is applied to map each word x t in the text sequence into an embedding vector x t . Although we can directly use this embedding vector as the input memory representation in the context of the memory network, in order to tackle the drawback of insensitivity to temporal information between memory cells [15] , we obtain the input memory representation m t by adopting the bidirectional gate recurrent unit (GRU) [5] to encode the word embedding vectors ",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 94,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 399,
                    "end": 403,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 508,
                    "end": 511,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "where \u2212 \u2192 W m , \u2190 \u2212 W m and b m are three trainable parameters to adjust the input memory representation m t . Current Input Representation. In order to calculate the attention weight of each element in the input memory, we enforce the current input to be in the same space as the input memory. More specifically, we use the obtained m t to represent the current input representation u t , i.e., u t = m t . Note that as illustrated in Fig. 1 , the current input u t in the sentence level and the document level is set to the same. We will detail it in the subsection Extensions.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 436,
                    "end": 442,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Memory Layer"
        },
        {
            "text": "Attention Weight Calculation. We compute the attention weight of each element in the input memory by measuring the relevance between the current input u t and each element of the input memory m i with a softmax function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "where sof tmax(",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "Output Memory Representation. Similar to the input memory vector m t , the output vector c i is also the contextual representation vector which is used to capture the contextual semantic information of the text sequence. The output vector is also encoded by bidirectional GRU, but with different parameters in the GRUs function and tanh layers of Eq. (1), (2) and (3). The output vector is used to generate the final output memory of the memory layer o t , which is the weighted sum over the attention weight and the output vector, as:",
            "cite_spans": [
                {
                    "start": 356,
                    "end": 359,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "The output memory allows the model to have unrestricted access to elements in previous steps as opposed to a single hidden state in RNNs, which will helps the CRF fully utilize the long-range dependencies in the text sequence to better predict keyphrases for documents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "Extensions. Many existing works [6, 17] discussed the influence of the different lengths covered by the attention mechanism in the memory network. Inspired by these works, we explore the different length of the input memory at two different levels: the sentence level and document level.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 35,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 36,
                    "end": 39,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "In the sentence level, the attention mechanism covers just the sentence containing the current word, and the corresponding attention weight is calculated as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "where i \u2208 [1, n] and n is the length of the sentence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "In the document level, the whole document is covered by the attention mechanism and the attention weight as computed as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "where i \u2208 [1, l] and l is the length of the document. The current input u t used in both the sentence level and the document level is set to the same. Then we calculate the final output memory of the memory layer as:",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 16,
                    "text": "[1, l]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "where \u03bb is a hyper-parameter, which is set to adjust weight from the different levels. Thus we replace Eq. (4) and (5) with Eq. (6), (7) and (8). The output memory of the memory layer can capture the information from not only the local but also the long-distance context by using the two-level output vectors. We can further extend the model by stacking multiple memory hops for capturing multiple fact from the memory, which stacks hops between the current input u t and the k-th hop o k t to be the input to the (k + 1)-th hop:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "In our model, we simply limit the hop to only 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Memory Layer"
        },
        {
            "text": "In this section, we feed the output of the memory layer into the CRF layer for extracting keyphrases. CRF [13] has been proven to be effective for sequence labeling tasks. We use CRF jointly with the memory network to predict the sequence of labels for the keyphrase extraction task. Given the input text sequence x = {x 1 , x 2 , ..., x l } and the sequence of output labels y = {y 1 , y 2 , ..., y l }, the score is computed as:",
            "cite_spans": [
                {
                    "start": 106,
                    "end": 110,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "CRF Layer"
        },
        {
            "text": "where P is the linearly transformed matrix from the output matrix of memory network U such that P = U W . The size of U is l \u00d7 d, where d is the size of the output vector u. The size of W is the weight matrix with the size of d \u00d7 k, where k is the number of labels. Thus, the size of P is l \u00d7 k, where p i,j represents the score of the j-th label of the i-th word of the input sequence. In addition, A is the matrix of transition scores, in which a i,j represents the score of a transition from the label i to label j. The probability of the label sequence y can be calculated by the softmax function as follow:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CRF Layer"
        },
        {
            "text": "where Y x represents all possible label sequences given a input sequence x. In the training procedure, we maximize the log-probability of the correct label sequence:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CRF Layer"
        },
        {
            "text": "This objective function and its gradients can be efficiently computed by dynamic programming algorithm. In order to find the best sequence of labels during decoding, the Viterbi algorithm is employed to decode the label sequence efficiently by maximizing the score s(x,\u1ef9):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CRF Layer"
        },
        {
            "text": "To analyze the effectiveness of our model for keyphrase extraction, we conduct comparative experiments on two scientific publication datasets provided by Caragea et al. [3] , which are from two top-tier machine learning conferences: ACM Knowledge Discovery and Data Mining (KDD) and ACM World Wide Web (WWW). Each dataset consists of the research paper titles, abstracts and corresponding author manually labeled keyphrases (gold standard). A detail description of datasets is summarized in Table 1 , containing the total number of abstracts and keyphrases in the original dataset (#Abs/#KPs(All)), the number of abstracts for which at least one author-labeled keyphrase could be located and the total number of keyphrases located (#Abs/#KPs(Locatable)), the percentage of keyphrases not present in the abstracts (MissingKPs), the average number of keyphrase per paper (AvgKPs), and the number of keyphrases with one, two, three and more than three tokens found in these abstracts.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 172,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 491,
                    "end": 498,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "Almost all previous works on keyphrase extraction use precision (P ), recall (R) and F1-score (F 1) to evaluate the results. Hence, we also keep our evaluation metric consistent. P , R and F 1 are defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "where # c is the number of correctly extracted keyphrases predicted by model, # e is the total number of keyphrases predicted by model and # s is the total number of standard keyphrases labeled by author.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "In our experiments, we partition the dataset into three groups using tenfold cross-validation: Onefold is used as the testing data; Onefold is used as the validating data; the remaining eightfolds are used as the training data. We report the average results of tenfold cross-validation. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "In the experiment, we divide keyphrases into two categories: simple keyphrase (1-gram, referred to as SK) and complicated keyphrase (n-grams (n > 1), that is composed by two or more than two words, referred to as CK). Each word in dataset is labeled with non-keyphrase (O), simple keyphrase (SK) or complicated keyphrase (CK). For the complicated keyphrase, B-CK, M-CK and E-CK correspond to the beginning, middle and end word of CK, respectively. Thus, the number of labels is set to k = 5. In order to eliminate the negative effects of different text formats, we convert the input text to lowercase in data pre-processing and employ two binary lexical features: whether the word contains digits or punctuation, which is similar to the works [11, 15] , We use the 50-dimensional embeddings pre-trained by GloVe 1 [18] on two corpora: Wikipedia2014 2 and Gigaword5 3 . In addition, some parameters of model are empirically set as follows: the hyper-parameter in Eq. (8) \u03bb = 0.6, and the trainable parameters in Eq. (3) \u2212 \u2192 W m and \u2190 \u2212 W m \u2208 R 50\u00d750 , b m \u2208 R 50 . Dropout is applied to all GRU recurrent units on the input and output connections to avoid over-fitting, with a keep rate of 0.6 in the training procedure.",
            "cite_spans": [
                {
                    "start": 743,
                    "end": 747,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 748,
                    "end": 751,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 814,
                    "end": 818,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Implementation Details"
        },
        {
            "text": "To evaluate the performance of our method, we compare our method with four state-of-the-art keyphrase extraction methods, as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparative Methods"
        },
        {
            "text": "-KEA [24] , which employs a supervised Na\u00efve Bayes classifier to extract keyphrases using only two features: TF-IDF (i.e., term frequency-inverse document frequency) of a phrase and the distance of a phrase from the beginning of a document (i.e., its relative position). -CeKE [3] , which also uses a Na\u00efve Bayes classifier for extracting keyphrases from research papers embedded in citation networks. This work designs some novel features for keyphrase extraction based on citation context information and uses them in conjunction with traditional features which have been widely used in the previous supervised works of keyphrase extraction. -EK-CRF [9] , which is the state-of-the-art traditional supervised method, and uses the CRF algorithm based on sequence labeling to extract keyphrases from research papers. This method incorporates the expert-knowledge and domain-specific hints. -CopyRNN [16] , which is the first to employ sequence-to-sequence (Seq2Seq) framework with attention and copy mechanisms to generate keyphrases. This method is able to predict absent keyphrases that do not appear in the target document. -M-CRF s , which is the simplified model of our complete model MLM-CRF.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 277,
                    "end": 280,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 652,
                    "end": 655,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 899,
                    "end": 903,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Comparative Methods"
        },
        {
            "text": "M-CRF s only uses the attention at sentence level. That is, the coverage of attention mechanism in memory network only depends on the length of the sentence, which the current word belongs to. In this model, the output memory vector and attention weight are calculated by the Eq. (5) and (6) (7), respectively. Table 2 shows the comparison of results of our model with other state-of-the-art supervised approaches. From Table 2 , we can see that the overall results of KDD dataset is better than those of WWW, which is consistent with percentage of keyphrases not present in the abstracts (MissingKPs) in given research papers (MissingKPs = 51.12% on KDD, 56.39% on WWW), as given in Table 1 . The benefit is that the experiment can reflect real application environment. We first conduct experiments to compare the MLM-CRF with its two simplified models M-CRF s and M-CRF d . As the results given in Table 2 , MLM-CRF gets the best results in terms of performance measures, and M-CRF d achieves better results than M-CRF s . These results indicate that long-range and more contextual information is more conducive to keyphrase extraction. More specifically, the contextual information captured by M-CRF d is longer than by M-CRF s , and MLM-CRF can capture the contextual information in the sentence level and document level.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 311,
                    "end": 318,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 420,
                    "end": 427,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 684,
                    "end": 691,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 900,
                    "end": 907,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Comparative Methods"
        },
        {
            "text": "Secondly, we discuss the comparison of our model MLM-CRF with other comparative keyphrase prediction methods, including KEA, CeKE, EK-CRF and CopyRNN. As given in Table 2 , MLM-CRF outperforms all comparative methods on two datasets, and even the M-CRF d has a margin over the best performing extraction method CRF on the two test datasets. It is also worth mentioning that CeKE includes features based on the document-citation network, EK-CRF designs complex features integrating expert-knowledge and domainspecific hints during keyphrase extraction, and CopyRNN can generate absent keyphrases, whereas our model does not need to use extra knowledge, design complex features, and can not predict absent keyphrases.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 163,
                    "end": 170,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Comparison with Supervised Prediction Methods"
        },
        {
            "text": "In conclusion, the MLM-CRF can capture automatically much useful information from the source text for keyphrase extraction. Thus, our model gets the best results in terms of the performance measures, indicating that our method indeed outperforms the other approaches on all two datasets. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with Supervised Prediction Methods"
        },
        {
            "text": "We compare our different models in two different types of keyphrases: SK (including only single word, i.e., 1-gram) and CK (including several consecutive words,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison in Different Types of Keyphrases"
        },
        {
            "text": "i.e., n-gram, n \u2265 2). We first compare the MLM-CRF with its two simplified models M-CRF s and M-CRF d in SK and CK on both KDD and WWW datasets, respectively. As the results given in Table 3 , both in SK and in CK, MLM-CRF gets the best results in terms of the performance measures, and M-CRF d achieves better results than M-CRF s on two datasets. These results indicate that long-range and more contextual information is more conducive to keyphrase extraction. Secondly, we compare the growth performance from the simplified method M-CRF s to MLM-CRF in SK and CK. From Table 3 , we can see that the growth of F1-score is 0.0781 for SK and 0.0450 for CK on KDD, and is 0.0305 for SK and 0.0190 for CK on WWW. It is obvious enough that the growth of F1-score in SK is more than that in CK on both KDD and WWW datasets. We can obtain the similar growth trends of Precision and Recall in SK and CK on two datasets. These results show that the 1-gram keyphrases have a stronger long-distance dependencies in text sequence than the n-gram (n \u2265 2) keyphrases, which are more dependent on the local structural information. The experimental setup might be able to explain the main reason for these results. More specifically, for identifying keyphrases labeled by CK using the CRF model, different labels of CK are restricted, while for identifying keyphrases labeled by SK, the single SK label is totally unrestricted. For example, the B-CK label must be followed by the M-CK or E-CK label in the experiments.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 183,
                    "end": 190,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 572,
                    "end": 579,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Comparison in Different Types of Keyphrases"
        },
        {
            "text": "Finally, we discuss the performance of our models in different types of keyphrases SK and CK. As the results given in Table 3 , we can see that our models can obtain better performance in CK than that in SK. The main reason may be that the percentage of 1-gram keyphrases in all keyphrases is significantly less than the percentage of n-gram (n \u2265 2) keyphrases.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 118,
                    "end": 125,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Comparison in Different Types of Keyphrases"
        },
        {
            "text": "In this paper, we proposed a multi-level memory network with CRFs named MLM-CRF for extracting keyphrases from scientific research papers. In particular, we first extended the input memory of the memory network with two different levels (i.e., sentence level and document level) to capture the long-range contextual information hidden in text data. We then employed the CRF model to capture the structural dependencies between adjacent words in text sequence and determine whether a candidate phrase is a keyphrase. Our experimental results have shown that the proposed model MLM-CRF can significantly outperform the state-of-the-art supervised prediction approaches (including three extraction methods and one generation method) on both WWW and KDD datasets. In future, we plan to explore the more effective attention mechanism for taking much less computing costs in encoding the long document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Bi-LSTM-CRF sequence labeling for keyphrase extraction from scholarly documents",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Alzaidy",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Caragea",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Giles",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of WWW",
            "volume": "",
            "issn": "",
            "pages": "2551--2557",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Keyphrase extraction in scientific articles: a supervised approach",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bhaskar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nongmeikapam",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bandyopadhyay",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of COLING",
            "volume": "",
            "issn": "",
            "pages": "17--24",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Citation-enhanced keyphrase extraction from research papers: a supervised approach",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Caragea",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Bulgarov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Godea",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Das Gollapalli",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of EMNLP",
            "volume": "",
            "issn": "",
            "pages": "1435--1446",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Title-guided encoding for keyphrase generation",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Lyu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of AAAI",
            "volume": "",
            "issn": "",
            "pages": "6268--6275",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chung",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gulcehre",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of NIPS",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Transformer-XL: attentive language models beyond a fixed-length context",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of ACL",
            "volume": "",
            "issn": "",
            "pages": "2978--2988",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Positionrank: an unsupervised approach to keyphrase extraction from scholarly documents",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Florescu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Caragea",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of ACL",
            "volume": "",
            "issn": "",
            "pages": "1105--1115",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Extracting keyphrases from research papers using citation networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "D"
                    ],
                    "last": "Gollapalli",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Caragea",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of AAAI",
            "volume": "",
            "issn": "",
            "pages": "1629--1635",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Incorporating expert knowledge into keyphrase extraction",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "D"
                    ],
                    "last": "Gollapalli",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [
                        "L"
                    ],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of AAAI",
            "volume": "",
            "issn": "",
            "pages": "3180--3187",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Automatic keyphrase extraction: a survey of the state of the art",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "S"
                    ],
                    "last": "Hasan",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of ACL",
            "volume": "",
            "issn": "",
            "pages": "1262--1273",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Bidirectional LSTM-CRF models for sequence tagging",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1508.01991"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Ask me anything: dynamic memory networks for natural language processing",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of ICML",
            "volume": "",
            "issn": "",
            "pages": "1378--1387",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lafferty",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "C"
                    ],
                    "last": "Pereira",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Proceedings of ICML",
            "volume": "",
            "issn": "",
            "pages": "282--289",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Linzen",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Dupoux",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Goldberg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Transactions of ACL",
            "volume": "",
            "issn": "",
            "pages": "521--535",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Capturing long-range contextual dependencies with memory-enhanced conditional random fields",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Baldwin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Cohn",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of IJCNLP",
            "volume": "",
            "issn": "",
            "pages": "555--565",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Deep keyphrase generation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Brusilovsky",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of ACL",
            "volume": "",
            "issn": "",
            "pages": "582--592",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Key-value memory networks for directly reading documents",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fisch",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dodge",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Karimi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of EMNLP",
            "volume": "",
            "issn": "",
            "pages": "1400--1409",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Glove: global vectors for word representation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of EMNLP",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Supervised keyphrase extraction as positive unlabeled learning",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sterckx",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Caragea",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Demeester",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Develder",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of EMNLP",
            "volume": "",
            "issn": "",
            "pages": "1924--1929",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "End-to-end memory networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sukhbaatar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Szlam",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of NIPS",
            "volume": "",
            "issn": "",
            "pages": "2440--2448",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Qalink: enriching text documents with relevant Q&A site contents",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of CIKM",
            "volume": "",
            "issn": "",
            "pages": "1359--1368",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Exploiting description knowledge for keyphrase extraction",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of PRICAI",
            "volume": "",
            "issn": "",
            "pages": "130--142",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Memory networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chopra",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bordes",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Kea: practical automated keyphrase extraction",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "H"
                    ],
                    "last": "Witten",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "W"
                    ],
                    "last": "Paynter",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Frank",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gutwin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "G"
                    ],
                    "last": "Nevill-Manning",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Proceedings of ACM DL",
            "volume": "",
            "issn": "",
            "pages": "254--255",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Ensemble neural relation extraction with adaptive boosting",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of IJCAI",
            "volume": "",
            "issn": "",
            "pages": "4532--4538",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Automatic keyword extraction from documents using conditional random fields",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "J. Comput. Inf. Syst",
            "volume": "4",
            "issn": "3",
            "pages": "1169--1180",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Mike: keyphrase extraction by integrating multidimensional information",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "D"
                    ],
                    "last": "Gollapalli",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of CIKM",
            "volume": "",
            "issn": "",
            "pages": "1349--1358",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Incorporating linguistic constraints into keyphrase generation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of ACL",
            "volume": "",
            "issn": "",
            "pages": "5224--5233",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "illustrates the overview of the Multi-Level Memory network with CRFs (MLM-CRF) for keyphrase extraction. This model includes two main parts: the memory layer, capturing long-range dependencies using the deep memory network; and the CRF layer, capturing local dependencies and labeling each word of input text sequence with five different labels (detailed in Subsect. 4.3).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "An overview of the MLM-CRF model for keyphrase extraction. It is shown only as a single hop.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": ", respectively. -M-CRF d , which is the another simplified model of our complete model MLM-CRF. M-CRF d only calculates attention weight between the current word and the whole input document. In this model, the output memory vector and attention weight are computed by the Eq. (5) and",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Statistics of the two benchmark datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Comparison of the proposed models with other approaches Method KDD WWW Precision Recall F1-score Precision Recall F1-score",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Comparison of our different models in different type keyphrases.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This work was partially supported by grants from the National Natural Science Foundation of China (Nos. U1933114, 61573231) and Open Project Foundation of Intelligent Information Processing Key Laboratory of Shanxi Province (No. CICIP2018004).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}