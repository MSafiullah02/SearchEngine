{
    "paper_id": "PMC7206281",
    "metadata": {
        "title": "Bottom-Up and Top-Down Graph Pooling",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Jia-Qi",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "email": "yangjq@lamda.nju.edu.cn",
                "affiliation": {}
            },
            {
                "first": "De-Chuan",
                "middle": [],
                "last": "Zhan",
                "suffix": "",
                "email": "zhandc@nju.edu.cn",
                "affiliation": {}
            },
            {
                "first": "Xin-Chun",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "email": "lixc@lamda.nju.edu.cn",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "The revolution of deep learning [8] has profoundly affected the development of many application fields, such as computer vision [10], natural language processing [12], and audio signal processing [11]. The architecture of neural networks has evolved a lot in recent years, convolutional neural network (CNN) [10] remains the most successful model in applications that can exploit grid-like data structure.",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 34,
                    "mention": "8",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 129,
                    "end": 131,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 163,
                    "end": 165,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 197,
                    "end": 199,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 309,
                    "end": 311,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Some structured data, e.g., images, can be represented as grid-like graph structure, yet CNNs are not directly applicable to general graph data. The traditional approach for dealing with graph data is utilizing graph kernel [5]. These methods suffered from drawbacks of kernel methods such as high computational complexity and very shallow model, thus didn\u2019t perform well on relatively large datasets. Graph neural networks (GNN) [21] aims at building deep learning methods for graph data such as social networks, citation networks, and the world wide web, and its effectiveness has been shown in many real-world applications. The basic idea of most GNN models is to migrate successful deep learning building-blocks to graph models. Graph convolutional neural networks (GCN) [13] is a prominent GNN variant, which is an analogy of CNN.",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 226,
                    "mention": "5",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 431,
                    "end": 433,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 776,
                    "end": 778,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Pooling layer is a critical component of most deep neural networks [8]. State-of-the-art CNNs usually use successive convolutional layers with small kernel sizes followed by one or more pooling layers, which can increase the effective receptive field while keeping the efficiency and representation learning power of convolutional layers. Graph poolings play a similar role in GCNs, and pooling layers are also necessary for tasks such as graph classification, graph encoding, or sub-graph sampling. However, the most popular pooling methods such as spacial max pooling and average pooling can\u2019t be incorporated into GCNs easily. Some recent researches focus on providing pooling methods that applicable in GCNs, such as DiffPool [23] and SAGPool [14], and they have shown significant improvement on many graph classification tasks.",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 69,
                    "mention": "8",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 731,
                    "end": 733,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 748,
                    "end": 750,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Existing graph pooling methods [7, 24] rely on features extracted by graph convolution layers to calculate a score for each node. Graph convolution works by aggregating information from neighbor nodes defined by the adjacency matrix, so the feature produced by graph convolution is local feature like in common CNNs, where a convolution kernel considers only a small area. The local feature can only reflect local structure around a node itself, but can hardly reflect the importance of a node within a larger area since macroscopic information is never available in local features.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 33,
                    "mention": "7",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 35,
                    "end": 37,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In CNNs the local features are exploited in an intuitive way: 1) Only spacial nearby areas are compared during pooling, which is well-defined when considering local features 2) Pooling layers guarantee a fixed fraction of local features are retained (e.g. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{1}{4}$$\\end{document}). So even if the most important part is dropped, any one of its neighbor features will hopefully be fine enough because of the local similarity. On the other hand, existing graph pooling methods work in a very different way: 1) All nodes are compared based on their local features, including those nodes that are far apart from each other. 2) A fixed fraction of all nodes are retrained, so the global structure of the graph may change a lot if some critical parts are dropped completely.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To solve this problem, we propose a new graph pooling method called (BUTDPool). The core idea is to gather high-level information with one or more coarse pooling layers, then feed this information back to the low-level representation to help to learn a more fine-grained pooling function. Specifically, we propose to apply a bottom-up pooling layer to enlarge the receptive field of each node, then use a top-down unpooling layer to map the pooled graph back and add to the original graph. Finally, a fine-grained pooling layer is applied to the graph with global information. Experiments showed the advantage of our method compared to state-of-the-art graph pooling methods, especially on large graphs.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Generally, we make several noteworthy contributions as follows:We analyzed a drawback of existing graph pooling methods that have not been noticed before: they are all pooling over graph globally but only based on local features.We proposed a new graph pooling structure called to tackle that drawback, which is generally applicable and can be complementary with existing methods.Experiments on real-world network datasets are conducted. The experimental results demonstrate that the achieves better results than many existing methods.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We give a brief overview of some related works in this section.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Graph Convolution. (GC) is a graph representation learning method with a sound theoretical foundation and has been the backbone of many successful graph learning methods, which is also the workhorse of most graph pooling methods. The most widely used graph convolution [13] is an approximation of localized spectral convolution. There are variants of graph convolution, e.g. Graph-SAGE [9] use LSTM instead of mean as aggregation function in graph convolution, however, they preserves the localized property so the drawback discussed in Sect. 3.2 remains.",
            "cite_spans": [
                {
                    "start": 270,
                    "end": 272,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 387,
                    "end": 388,
                    "mention": "9",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Hierarchical and Global Graph Pooling. Global pooling aims at obtaining a global summary of a whole graph, which gives GCNs the ability to process graphs with different node numbers and graph structure. Typical global pooling methods including SortPool  [24] and Set2Set  [18]. However, global pooling methods lack the ability to learn hierarchical or high-level representations. Hierarchical pooling methods aim to provide a texture-level to object-level representations at different layers and wipe off useless information progressively by multiple pooling layers, which is very important in deep models. SAGPool  [14] is a recently proposed hierarchical pooling method that has state-of-the-art performance on many benchmark datasets. Since it\u2019s unlikely to learn the global structure of a large graph using a single global pooling layer, hierarchical pooling is more important in deep learning. So we only consider hierarchical pooling methods in the rest of our paper.",
            "cite_spans": [
                {
                    "start": 255,
                    "end": 257,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 273,
                    "end": 275,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 617,
                    "end": 619,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Score-Based Graph Max-Pooling. A nature idea considering graph pooling is to learn the importance of each node, then only keep the most important nodes and simply drop other unimportant nodes. This is the basic idea of a bunch of graph pooling methods we call as score-based methods, since they calculate a score as a metric of relative importance of each node. SAGPool [14] and gPool [7] are typical score-based pooling methods.",
            "cite_spans": [
                {
                    "start": 371,
                    "end": 373,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 386,
                    "end": 387,
                    "mention": "7",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Differentiable Graph Pooling. Score-based methods have a drawback that the top-k operation used in score-based methods is not differentiable. Fully differentiable models are usually easier to optimize than models with non-differentiable components, differentiable graph pooling methods are proposed to tackle this issue. DiffPool [23] is a representative differentiable pooling method where a large graph is downsampled to a small graph with pre-fixed size in a fully differentiable approach. Since the time and space complexity of DiffPool is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {O}(|\\mathcal {V}|^2)$$\\end{document} rather than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {O}(|\\mathcal {E}|)$$\\end{document} of sparse methods such as SAGPool, DiffPool can be very slow or even not applicable on large graphs.",
            "cite_spans": [
                {
                    "start": 331,
                    "end": 333,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Bottom-Up and Top-Down Visual Attention. Visual attention mechanisms have been widely used in image captioning and visual question answering (VQA) [1, 16], and similar attention mechanism has been proved to exist in human visual system [3].",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 149,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 151,
                    "end": 153,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 237,
                    "end": 238,
                    "mention": "3",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The combination of bottom-up and top-down attention was suggested by [1] in VQA task, where the bottom-up attention uses an object detection model to focus on concerned regions, then the top-down attention utilizes language feature to take attention on the image regions most related to the question. Although our method shares a similar name with the bottom-up and top-down attention in VQA, they are completely different in motivation, application, and implementation.",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 71,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "We define a graph as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {G}=(\\mathcal {V}, A)$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {V}$$\\end{document} is the vertex set that consisting of nodes \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\left\\{ v_{1}, \\ldots , v_{n}\\right\\} $$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A \\in \\mathbb {R}^{n \\times n}$$\\end{document} is the adjacency matrix (typically symmetric and sparse) where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_{ij}$$\\end{document} denotes the edge weight between nodes \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_j$$\\end{document}. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_{ij}=0$$\\end{document} denotes the edge does not exist. The degree matrix is defined as a diagonal matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D=\\mathrm {diag}\\left( d_{1}, \\ldots , d_{n}\\right) $$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_{i}=\\sum _{j} a_{i j}$$\\end{document}. In graph convolution, the adjacency matrix with self-connections \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{A} \\in \\mathbb {R}^{n \\times n}$$\\end{document} is usually used instead of A, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{A} = A + I_n$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{D} \\in \\mathbb {R}^{n \\times n}$$\\end{document} is the degree matrix of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{A}$$\\end{document}. In a deep neural network, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X^\\ell $$\\end{document} denotes the input of the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ell $$\\end{document}th layer, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X^0$$\\end{document} denote the original feature matrix of the input graph.",
            "cite_spans": [],
            "section": "Notations ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Each node \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i$$\\end{document} in a graph has a feature vector denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {x}_{i} \\in \\mathbb {R}^{d}$$\\end{document}. For a graph with n nodes, the feature matrix is denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X \\in \\mathbb {R}^{n \\times d}$$\\end{document}, where row i correspond to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {x}_i$$\\end{document}. In graph classification task, each graph belongs to one out of C classes, given a graph we want to predict it\u2019s class.",
            "cite_spans": [],
            "section": "Notations ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "We analyze a drawback of existing pooling methods in this section, which we attribute to the contradiction of local features and global pooling criterion.",
            "cite_spans": [],
            "section": "Local Features and Pooling Criterion ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "We define local features as the features that only contain information gathered from the very nearby area. For example, in CNNs, the features are typically calculated by a small kernel (e.g., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3\\times 3$$\\end{document}). The kernel size determines the receptive field of a convolutional layer, which is the largest area a feature can see, and that feature is innocent to the outside of this receptive field.",
            "cite_spans": [],
            "section": "Local Features and Pooling Criterion ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The features produced by graph convolutions are also local: typical graph convolutions only consider neighbor nodes, i.e., one-hop connection [13]. This is the expected behavior of GCNs by design to inherit merits of CNNs: parameter sharing at different local area, which can be regarded as a strong prior that local patterns are applicable everywhere [8].",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 145,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 353,
                    "end": 354,
                    "mention": "8",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Local Features and Pooling Criterion ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Typical pooling layers in CNNs, say max-pooling without loss of generality, select one feature out of a small feature set (e.g., 1 out of 4 in a max-pooling with size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2 \\times 2$$\\end{document}), then all selected features form a smaller feature map. These local features are comparable since they have information about each other, and even if the feature of most importance is dropped, the selected feature may still be representative of this small area because of local similarity in most nature data like image. The overall structure won\u2019t get disturbed after pooling even if the pooling is totally random, which makes poolings in CNNs robust and relatively easier to train.",
            "cite_spans": [],
            "section": "Local Features and Pooling Criterion ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "However, existing graph poolings utilize these local features in a very different and counterintuitive way compared to common poolings in CNNs. They use local features to calculate a score [7, 14] to define the importance of each node, then they simply select the nodes with the largest scores. The scores are also local features as we defined before, since they only rely on local features produced by GCNs. The scores of two far apart nodes can\u2019t reflect any information of their relationship or their relative importance in the whole graph, which harms the ability of poolings to retain global structure thus will harm performance in downstream tasks.",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 191,
                    "mention": "7",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 193,
                    "end": 195,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Local Features and Pooling Criterion ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The analysis of the drawback of existing methods also sheds light on the idea to resolve it: we need to make local features non-local, i.e., have a larger receptive field, in order to apply score-based pooling in a more fine-grained way.",
            "cite_spans": [],
            "section": "Local Features and Pooling Criterion ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Stacking GC layers can increase receptive field, however not efficient since a GC layer can only increase receptive field by 2 hops. And it\u2019s indeed hard to train a deep GCN [15, 20].",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 177,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 179,
                    "end": 181,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Bottom-Up Pooling ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "To gather a macroscopic view of the whole graph, we use a stack of base pooling layers to do a coarse pooling. A base pooling layer can be any pooling layer that can reduce node number by a fixed ratio r such as most score-based pooling methods. We define a base pooling layer (BPL) as1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\tilde{X}, \\tilde{A} = \\mathrm {BPL}_{\\varTheta }(X, A, r) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{X}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{A}$$\\end{document} is the feature matrix and adjacency matrix of the graph after pooling, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varTheta $$\\end{document} is the parameter of this base pooling layer.",
            "cite_spans": [],
            "section": "Bottom-Up Pooling ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Without loss of generality, we use SAGPool [14] as our base pooling layer. Given input X, A and r, the SAGPool calculates a score vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {y} = GC(X)$$\\end{document}. Based on this score, the k nodes with largest scores are selected and their indexes are denoted as idx. The output features is calculated by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{X}=tanh(y[idx])\\odot X[idx]$$\\end{document}, the output adjacent matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{A}=A[idx, idx]$$\\end{document}, where the [] operator selects elements based on row and column indexes. The GC is a graph convolution layer with output feature size 1 so that the output of this layer can be used as the score. When a retain-ratio r is given instead of k, we define \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=|\\mathcal {V}|*r$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 44,
                    "end": 46,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Bottom-Up Pooling ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "A bottom-up pooling layer (BUPL) can be defined as a stack of base pooling layers, for example, a bottom-up pooling layer with 2 base pooling layers can be defined by2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\tilde{X}_1^{\\ell }, \\tilde{A}_1^{\\ell }&= \\mathrm {BPL}_{\\varTheta _{bu1}}(X^{\\ell }, A^{\\ell }, r_{bu}) \\end{aligned}$$\\end{document}\n3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\tilde{X}^{\\ell }, \\tilde{A}^{\\ell }&= \\mathrm {BPL}_{\\varTheta _{bu2}}(\\tilde{X}_1^{\\ell }, \\tilde{A}_1^{\\ell }, r_{bu}) \\end{aligned}$$\\end{document}The corresponding bottom-up pooling layer is denoted by4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\tilde{X}^{\\ell }, \\tilde{A}^{\\ell }, \\text {idx}_{\\text {bu}}&= \\mathrm {BUPL}_{\\varTheta _{bu1};\\varTheta _{bu2}}(X^{\\ell }, A^{\\ell }, r_{bu}) \\end{aligned}$$\\end{document}Where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{X}^{\\ell }, \\tilde{A}^{\\ell }$$\\end{document} is the output of a bottom-up pooling layer. Different from SAGPool layer, we return an index \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {idx}_{\\text {bu}}$$\\end{document} in BUPL to memorize the map between input nodes and output nodes.",
            "cite_spans": [],
            "section": "Bottom-Up Pooling ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The bottom-up pooling layer can produce a much smaller graph, since the retain ratio is relatively small. This graph can be viewed as a rough summary of the original graph and the receptive field of each node is larger. Roughly, the receptive field of remaining nodes can still cover most nodes, thus this is a high-level graph.",
            "cite_spans": [],
            "section": "Bottom-Up Pooling ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Notice that DiffPool is not applicable as a bottom-up pooling layer since we prefer a sparse pooling method.",
            "cite_spans": [],
            "section": "Bottom-Up Pooling ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Now we have a high-level pooling result produced by the bottom-up pooling layer denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tilde{X}^{\\ell }, \\tilde{A}^{\\ell }$$\\end{document}.",
            "cite_spans": [],
            "section": "Top-Down Unpooling and Fine-Grained Pooling Layer ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "In order to feed high-level information back to the low-level graph, we should define a mapping from the downsampled small graph to the original large graph with more nodes, which is unpooling. A nature idea is to apply attention to the small graph and large graph. However, this attention operation will take \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {O}(|\\mathcal {V}_{small}|\\times |\\mathcal {V}_{large}|)$$\\end{document} time complexity, which loses the merit of the sparse property of GCN.",
            "cite_spans": [],
            "section": "Top-Down Unpooling and Fine-Grained Pooling Layer ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "To keep efficiency and simplicity, we save the index of selected nodes at every bottom-up pooling step so that we can recover the mapping of nodes easily. This is similar to the gUnpool layer proposed by [7]. However, they use zero value in the dropped nodes, which does not feed any information back to those nodes. To fix this, we take the mean of all retained nodes as their corresponding high-level features.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 206,
                    "mention": "7",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Top-Down Unpooling and Fine-Grained Pooling Layer ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The top-down unpooling can be denoted as follows:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\hat{X}^\\ell , \\hat{A}^\\ell&= \\mathrm {TDUPL}(\\tilde{X}^\\ell , \\tilde{A}^\\ell , \\text {idx}_{\\text {bu}}) \\end{aligned}$$\\end{document}Where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {TDUPL}$$\\end{document} is the top-down unpooling layer.",
            "cite_spans": [],
            "section": "Top-Down Unpooling and Fine-Grained Pooling Layer ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The retained nodes in unpooling result have information of their own receptive field, and other averaged nodes have information of the whole graph. When this graph is injected to low-level graph, each nodes will have both local and global information (an averaged node will have a retained neighbour with large probability, viceversa. Then one hop relation is considered in following graph convolution).",
            "cite_spans": [],
            "section": "Top-Down Unpooling and Fine-Grained Pooling Layer ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Then the high-level features are summed with low-level features as input of the fine-grained pooling layer. Which can be defined as6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Z^{\\ell }&= X^{\\ell } + \\hat{X}^\\ell \\end{aligned}$$\\end{document}\n7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} X^{\\ell +1}, A^{\\ell +1}&= \\mathrm {FGPL}_{\\varTheta _{fg}}(Z^{\\ell }, A^{\\ell }, r) \\end{aligned}$$\\end{document}Where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {FGPL}_{\\varTheta _{fg}}$$\\end{document} is the fine-grained pooling layer, which can be any score-based pooling layer. The r is the retain ratio, which is larger than the retain ratio \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_{bu}$$\\end{document} used in bottom-up pooling layers. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z^{\\ell }$$\\end{document} is the feature combined with local information \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X^{\\ell }$$\\end{document} and higher level information \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\hat{X}^{\\ell }$$\\end{document}, which gives FGPL the power to learn a more fine-grained pooling.",
            "cite_spans": [],
            "section": "Top-Down Unpooling and Fine-Grained Pooling Layer ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The proposed bottom-up and top-down pooling layer combines bottom-up layer and top-down layer defined before and can be denoted by8\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} X^{\\ell +1}, A^{\\ell +1} = BUTD_{\\varTheta _{fg};\\varTheta _{bu1}; \\varTheta _{bu2}}(X^{\\ell }, A^{\\ell }, r_{bu}, r) \\end{aligned}$$\\end{document}The procedure of a bottom-up and top-down pooling layer is summarized in Algorithm 1.\n",
            "cite_spans": [],
            "section": "Top-Down Unpooling and Fine-Grained Pooling Layer ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "We give a brief introduction of compared methods in the following sections.",
            "cite_spans": [],
            "section": "Compared Methods ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "gPool is a score-based method used in the Graph U-Nets [7]. gPool suppose that there is a direction defined by vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {p}^\\ell $$\\end{document} at the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ell $$\\end{document}th layer that the nodes \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i$$\\end{document} with feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {x}_i$$\\end{document} align with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {p}^\\ell $$\\end{document} best is the most relative nodes. So the score of node \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i$$\\end{document} is defined as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {x}_i^T \\mathbf {p}^\\ell {/}\\left\\| \\mathbf {p}^{\\ell }\\right\\| $$\\end{document}. The score (after a sigmoid function) is multiplied to the input of next layer to make \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {p}^\\ell $$\\end{document} optimizable.",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 57,
                    "mention": "7",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Compared Methods ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "SAGPool is a score-based method proposed by [14]. The authors of SAGPool argue that gPool does not consider topology relationship in graphs since all nodes are projected to the same \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {p}^\\ell $$\\end{document}. SAGPool uses graph convolution blocks to calculate score instead.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 47,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Compared Methods ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "DiffPool is a fully differentiable graph pooling method introduced in [23]. DiffPool uses GNN layers to learn a soft assignment of each node to a cluster, then pool a cluster into a node.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 73,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Compared Methods ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "As mentioned in [17], the data split is a crucial factor that affects evaluation a lot. So we generate 20 different random splits (80%train, 10%validate, 10%test) of every dataset at first, then evaluate each model on these 20 splits and take the average accuracy as our measurement. All methods are implemented using pytorch-geometric [6].",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 19,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 337,
                    "end": 338,
                    "mention": "6",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Experiment Protocol ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Model Architecture. We follow the model architecture proposed in [14] to make a fair comparison, with the graph pooling layers replaced by compared methods, Figure 2 depicts the model architecture.\n",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 68,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Experiment Protocol ::: Experiments",
            "ref_spans": [
                {
                    "start": 164,
                    "end": 165,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Datasets. We selected several graph classification benchmark datasets from real biological and chemical applications. The D&D dataset [4] and PROTEINS dataset [2, 4] are protein classification datasets. The NCI1 dataset and NCI109 dataset [19] represent two balanced subsets of datasets of chemical compounds classification. REDDIT-MULTI-5K [22] is a social network dataset with large graphs.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 136,
                    "mention": "4",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 160,
                    "end": 161,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 163,
                    "end": 164,
                    "mention": "4",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 240,
                    "end": 242,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 342,
                    "end": 344,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Experiment Protocol ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "The datasets are summarized in Table 1.\n",
            "cite_spans": [],
            "section": "Experiment Protocol ::: Experiments",
            "ref_spans": [
                {
                    "start": 37,
                    "end": 38,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Summary of Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "From Table 2, we can see that, BUTDPool achieves clear performance gain over the compared methods, especially on D&D and REDDIT dataset with large graphs.",
            "cite_spans": [],
            "section": "Summary of Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 11,
                    "end": 12,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "BUTDPool is a stack of existing graph pooling methods, the overhead of time complexity is determined by the number of base pooling layers. For a BUTDPool layer with 2 bottom-up layers, the running time will be roughly 3 times of a single base pooling layer (2 bottom up layer, 1 fine-grained layer). The space complexity is similar. In a typical deep neural network, the number of pooling layers is small compare to other convolutional layers, so this overhead is affordable.",
            "cite_spans": [],
            "section": "Complexity Analysis ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "On the other hand, the overhead of DiffPool is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$|\\mathcal {V}|$$\\end{document} times complexity of time and space compared to sparse methods, which limits its usability on even medium size graphs.",
            "cite_spans": [],
            "section": "Complexity Analysis ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We analyzed the contradiction of local feature and global pooling in existing graph pooling methods, then introduced a novel and easy-to-implement improvement BUTDPool over existing graph pooling methods to mitigate this problem. The large graph is pooled by a bottom-up pooling layer to produce a high-level overview, and then the high-level information is feedback to the low-level graph by a top-down unpooling layer. Finally, a fine-grained pooling criterion is learned. The proposed bottom-up and top-down architecture is generally applicable when we need to select a sub-graph from a large graph and the quality of the sub-graph matters. Experiments demonstrated the effectiveness of our approach on several graph classification tasks. The proposed BUTDPool can be an alternative building block in GNNs with the potential to make improvements in many existing models.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Summary of datasets\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Average accuracy and standard deviation of 20 random runs. *: About \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2\\%$$\\end{document} largest of graphs in D&D dataset are dropped because of being too large for efficiency when training and evaluating DiffPool. NA: We found DiffPool on REDDIT dataset is very slow or cause out-of-memory, since we focus on efficient pooling method we excluded this experiment.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: A bottom-up and top-down graph pooling layer. We use different background color to denote effective receptive field, the darker the background the larger the effective receptive field. Nodes with same color denote the same nodes at different level. This figure depicted a possible pooling procedure on a simple graph. (Color figure online)",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Model architecture",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Protein function prediction via graph kernels",
            "authors": [
                {
                    "first": "KM",
                    "middle": [],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "CS",
                    "middle": [],
                    "last": "Ong",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Schoenauer",
                    "suffix": ""
                },
                {
                    "first": "SVN",
                    "middle": [],
                    "last": "Vishwanathan",
                    "suffix": ""
                },
                {
                    "first": "AJ",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "HP",
                    "middle": [],
                    "last": "Kriegel",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Bioinformatics",
            "volume": "21",
            "issn": "Suppl 1",
            "pages": "i47-i56",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/bti1007"
                ]
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Control of goal-directed and stimulus-driven attention in the brain",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Corbetta",
                    "suffix": ""
                },
                {
                    "first": "GL",
                    "middle": [],
                    "last": "Shulman",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Nat. Rev. Neurosci.",
            "volume": "3",
            "issn": "3",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/nrn755"
                ]
            }
        },
        "BIBREF18": {
            "title": "Distinguishing enzyme structures from non-enzymes without alignments",
            "authors": [
                {
                    "first": "PD",
                    "middle": [],
                    "last": "Dobson",
                    "suffix": ""
                },
                {
                    "first": "AJ",
                    "middle": [],
                    "last": "Doig",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Mol. Biol.",
            "volume": "330",
            "issn": "4",
            "pages": "771-783",
            "other_ids": {
                "DOI": [
                    "10.1016/S0022-2836(03)00628-4"
                ]
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [
                {
                    "first": "IJ",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "AC",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Deep Learning",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}