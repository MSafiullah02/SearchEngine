{
    "paper_id": "f6f366ddbd485eca452b6234b4080552c43c9506",
    "metadata": {
        "title": "SRCEK: A Continuous Embedding of the Channel Selection Problem for WPLS Modeling",
        "authors": [
            {
                "first": "Steven",
                "middle": [
                    "E"
                ],
                "last": "Pav",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "INTRODUCTION",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "SRCEK, pronounced \"SIR check,\" is a technique for selecting useful channels for affine modeling of a response by PLS. The technique embeds the discrete channel selection problem into the continuous space of predictor preweighting, then employs a Quasi-Newton (or other) optimization algorithm to optimize the preweighting vector. Once the weighting vector has been optimized, the magnitudes of the weights indicate the relative importance of each channel. The relative importances are used to construct n different models, the k th consisting of the k most important channels. The different models are then compared by means of cross validation or an information criterion (e.g., BIC), allowing automatic selection of a good subset of the channels. The analytical Jacobian of the PLS regression vector with respect to the predictor weighting is derived to facilitate optimization of the latter. This formulation exploits the reduced rank of the predictor matrix to gain some speedup when the number of observations is fewer than the number of predictors (the usual case for e.g., IR spectroscopy). The method compares favourably with predictor selection techniques surveyed by Forina et al. [13] Let X be an m \u00d7 n matrix of observed \"predictors\", with each column of X a \"channel\" of the investigative instrument. Let y be an m-dimensional column vector of corresponding \"responses\" of each of the objects. Each row of X, with corresponding element of y, represents an observation of an \"object\". In the general context of chemometrics, the number of objects is generally far fewer than the number of channels: m \u226a n.",
            "cite_spans": [
                {
                    "start": 1191,
                    "end": 1195,
                    "text": "[13]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "An affine predictive model consists of an n-vector \u03b2 and scalar \"intercept\" b 0 such that y \u2248 X\u03b2 + b 0 1 m , where 1 m is the vector of m ones. When calibrated on a given collection of predictors and responses X and y, different algorithms produce different affine models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "A good model is marked by a residual with small norm. That is, i \u01eb i 2 is small, where \u01eb = y \u2212 X\u03b2 \u2212 b 0 1 m is the residual. However, a model which explains the observed data well may give poor predictive ability over as-yet-unobserved objects, due to \"overfitting.\"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Cross validation (CV) is used to address this deficiency. The idea is to divide the tested objects into two groups, one of which is used to build a model (\u03b2, b 0 ), the other is used to test the quality of the model. By leaving these validation or test objects out of the calibration, this technique simulates the predictive ability of the observed data on unobserved data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Because the division in two groups is arbitrary, the process is often repeated in a round robin fashion, with different subdivisions. The quality of prediction of each division is then averaged. In one extreme form, known as \"leave one out\" (LOO) or \"delete-1\", the model building and testing is performed m times, each time with m \u2212 1 objects in the calibration group, and the single remaining object in the test group. For comparing different subsets of the channels, Shao proved that delete-1 CV is asymptotically inconsistent, i.e., it has a nonzero probability of overfitting. [33, 34] Some terminology is now required. The norm (more specifically the 2-norm) of a vector v is its Euclidian length:",
            "cite_spans": [
                {
                    "start": 582,
                    "end": 586,
                    "text": "[33,",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 587,
                    "end": 590,
                    "text": "34]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "where X j and y j are the predictors and responses of the j th test group which contains m j objects, while \u03b2 j , b 0j is built from the j th calibration group. The mean squared error of p. 2/29",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "7. W (k+j) \u22a4 P (k) = 0, and P (k+j) \u22a4 P (k) = 0 for all j > 1, and k \u2265 1. (Note the strict inequality for j.)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Proof. First note that the update of X (k) is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "It can also be expressed as X (k+1) = X (k) \u2212 T (k) P (k) \u22a4 = X (k) I \u2212 W (k) P (k) \u22a4 = X (k) L (k) , p. 6/29 P (k) = X (k) \u22a4 \u0393T (k) /t k = X (0) \u2212 T (0) P (0) \u22a4 \u2212 . . . \u2212 T (k\u22121) P (k\u22121) \u22a4 \u22a4 \u0393T (k) /t k = X (0) \u22a4 \u2212 P (0) T (0) \u22a4 \u2212 . . . \u2212 P (k\u22121) T (k\u22121) \u22a4 \u0393T (k) /t k = X (0) \u22a4 \u0393T (k) /t k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "p. 8/29",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The modern chemometrician suffers from an embarrassment of riches: investigative instruments (e.g., NIR spectrometers) commonly allow measurements in more discrete \"channels\" than the relevant underlying degrees of freedom or the number of objects under investigation. While a wide array of channels may provide greater predictive power, some channels may confound prediction of the relevant response, essentially measuring only \"noise.\" Moreover, the principle of parsimony dictates that a predictive model must focus on as few of the channels as practical, or fewer.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The problem was mitigated by the development of principal component regression (PCR) and partial least squares (PLS) [6, 9, 32, 15, 20] , two algorithmic techniques which essentially project many variate predictors onto a small dimensional vector space before building * spav@alumni.cmu.edu This method of wavelength selection may be covered by patent. [29] This research was originally conducted during the author's tenure at Nellcor, a subsidiary of Tyco Healthcare, now known as 'Covidien'. The author wishes to thank M. Forina for providing data sets and guidance regarding prior work on the topic. Some of this research was conducted at the time the author was a juror in the court of Judge Donald S. Mitchell, Department #602, City and County of San Francisco, California: \"Everyone's here and we. are. ready.\" a predictive model. While these techniques are very good at capturing the largest or most relevant underlying variations in multivariate predictors, they are not impervious to disinformative or useless predictors. For example, it has been shown that the predictive ability of PLS is degraded by a term quadratic in the number of channels. [25] A number of clever and theoretically well grounded techniques exist for the rejection of useless wavelengths. [5, 18, 28, 2] Rather than discuss them in any depth here, I refer the reader to the excellent comparative study by Forina et al. [13] prediction (MSEP) of a model is the mean square error of the data used to build the model: MSEP = y \u2212 X\u03b2 \u2212 b 0 1 m 2 2 /m, where (\u03b2, b 0 ) is built using (all) the data X and y. The prefix \"root\" refers to square root, thus the root mean squared error of cross validation (RMSECV) is \u221a MSECV; similarly for RMSEP, etc. The goal of channel selection appears to be, given the observed data, the CV groups and a model building technique, select the subset of the n available columns of X which minimizes RMSECV when only those columns of X are used in the model building and testing. In this formulation the number of possible solutions is 2 n ; exhaustive search becomes impractical when n is larger than about 17. Subset selection heuristics such as Tabu search, Simulated Annealing (SA) and Genetic Algorithms (GA) which generate and test subsets of the n available channels can only hope to explore a small part of the 2 n sized discrete space of possible subsets, and are susceptible to backtracking and falling into local minima. [18, 28, 2] Even when restricted to subsets of no more than a given number of channels, say d, heuristic search can cover only a small part of the search space, which has size around n d .",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 120,
                    "text": "[6,",
                    "ref_id": null
                },
                {
                    "start": 121,
                    "end": 123,
                    "text": "9,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 124,
                    "end": 127,
                    "text": "32,",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 128,
                    "end": 131,
                    "text": "15,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 132,
                    "end": 135,
                    "text": "20]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 353,
                    "end": 357,
                    "text": "[29]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "text": "[25]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "text": "[5,",
                    "ref_id": null
                },
                {
                    "start": 1275,
                    "end": 1278,
                    "text": "18,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1279,
                    "end": 1282,
                    "text": "28,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 1283,
                    "end": 1285,
                    "text": "2]",
                    "ref_id": null
                },
                {
                    "start": 1401,
                    "end": 1405,
                    "text": "[13]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 2439,
                    "end": 2443,
                    "text": "[18,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 2444,
                    "end": 2447,
                    "text": "28,",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 2448,
                    "end": 2450,
                    "text": "2]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Considerable progress was made on the problem by the introduction of iterative predictor weighting (IPW) by Forina et al. [11] This channel selection technique reformulates the problem as one of selecting a vector in n dimensional Euclidian space, hereafter denoted R n , rather than on the discrete space of binary n-dimensional vectors. In terms of size of the search space there would seem to be no advantage to this reformulation. However, I argue that the continuous embedding allows the importance of each channel to be evaluated and changed simultaneously, in parallel, rather than serially. This will lead to a channel selection technique with runtime asymptotically linear in the number of channels.",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[11]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Predictive weighting can be viewed as a preprocessing step. Let \u03bb \u2208 R n be a vector of weights 1 . Let \u039b be the diagonal n \u00d7 n matrix whose diagonal is the vector \u03bb. Hereafter I will let diag (v) denote the diagonal matrix whose diagonal is v, so \u039b = diag (\u03bb). The \u03bb-weighted predictor matrix is the product X\u039b: the k th column of the weighted predictor matrix is the k th column of X times \u03bb k . Weighted predictors are then used in the cross validation study, both in the calibration and test sets. As such, the quality of the cross validation (RMSECV) can be viewed as a scalar function of the vector \u03bb, once the data and CV groups and model building method (and order) have been fixed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Note that when the regression model (\u03b2, b 0 ) is built by ordinary least squares (OLS), the quality of cross validation is constant with respect to \u03bb. This occurs because the weighted regression model output by OLS is constant with respect to nonzero predictor weighting, i.e., \u039b\u03b2 (\u03bb), is constant over all \u03bb with nonzero elements. PLS, however, does not share this property, and the quality of cross validation is affected by predictor weighting. When used as a preprocessing technique prior to the application of PLS, the usual strategy is to apply predictor weighting \u03bb where \u03bb k = 1/\u03c3 2 k , where\u03c3 2 k is the sample standard deviation of the k th channel of the predictors based on the observations in the entire sample X, a technique called \"autoscaling.\" There is no reason to believe a priori that this choice of \u03bb gives good cross validation. Rather an a priori choice of weighting should depend on knowledge of the test instrument or the tested objects. Alternatively, one can let the data inform the choice of predictor weighting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The method I propose is to minimize the RMSECV as a function of \u03bb. This can be achieved by any of a number of modern optimization algorithms, including BFGS [27, 23] , which I explore and advocate here. Once a local minimum has been found, the magnitude of the elements of the optimal \u03bb suggest the importance of the corresponding channels to the observed data. This ordering suggest n different models, the k th consisting of the first k channels by order of decreasing importance. These models can then be compared using any model selection technique, e.g., minimization of RMSECV or an information criterion. [33, 34, 1, 3, 4] I call this technique \"SRCEK\" (pronounced \"SIR check\"), an acronym for \"Selecting Regressors by Continuous Embedding in K-dimensions,\" but also taken from the Slove word sr\u010dek , meaning \"sweetheart.\"",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 161,
                    "text": "[27,",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 162,
                    "end": 165,
                    "text": "23]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 612,
                    "end": 616,
                    "text": "[33,",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 617,
                    "end": 620,
                    "text": "34,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 621,
                    "end": 623,
                    "text": "1,",
                    "ref_id": null
                },
                {
                    "start": 624,
                    "end": 626,
                    "text": "3,",
                    "ref_id": null
                },
                {
                    "start": 627,
                    "end": 629,
                    "text": "4]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The assumption underlying PLS regression is that predictor and response take the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(W)PLS Regression"
        },
        {
            "text": "where the vectorsT (k) are orthogonal, and the remainder terms, E and f , are random variables. The vectorT (0) is 1, the vector of all ones. It is also assumed that the response remainder term, f , is homoscedastic, i.e.,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(W)PLS Regression"
        },
        {
            "text": "When this assumption cannot be supported, weighted PLS (WPLS) regression is appropriate. [19, 14, 17] Let \u0393 be a symmetric positive definite matrix such that \u0393 = c E f f \u22a4 \u22121 , for some (perhaps unknown) scalar c. Weighted PLS assumes a decomposition of X and y as in equations 1 and 2, but with the property that the vectorsT (k) are \u0393-orthogonal:",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 93,
                    "text": "[19,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 94,
                    "end": 97,
                    "text": "14,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 98,
                    "end": 101,
                    "text": "17]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "(W)PLS Regression"
        },
        {
            "text": "WPLS regression with l factors computes m \u00d7 l matrix T, n \u00d7 l matrix P and l vector q such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(W)PLS Regression"
        },
        {
            "text": "where T (0) is the vector of all ones, and P (0) \u22a4 and q 0 are the (\u0393-weighted) means of X and y. The affine model constructed by WPLS takes the form \u03b2 = W P \u22a4 W \u22121 q, for some",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(W)PLS Regression"
        },
        {
            "text": "The use of the words \"weight\" or \"weighting\" in this context is traditional, and parallels the usage for ordinary least squares. It should not be confused with the predictor weighting applied to the predictors. To distinguish them, I will refer to \u0393 as the response weights. For the remainder of this paper, I will assume that \u0393 is a diagonal vector, \u0393 = diag (\u03b3) . This is not necessary for correctness of the algorithm, only for its fast runtime, for which a sufficiently sparse \u0393 would also suffice.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(W)PLS Regression"
        },
        {
            "text": "p. 4/29",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(W)PLS Regression"
        },
        {
            "text": "Given fixed data, I will presuppose the existence of a selected order, l. The selection of l should be informed by the underlying structure of the objects under investigation, or by an automatic technique. [13] In the chemometric context, the number of factors should be less (preferably far less) than the number of objects: l \u2264 m. Let \u03c6 (\u03bb) be the RMSECV for the CV when the CV groups are fixed, and the affine model is built by l-factor WPLS with \u03b3 given, and using predictor weights \u03bb. To employ quasi-Newton minimization, the gradient of \u03c6 with respect to \u03bb must be computed. While this can be approximated numerically with little extra programmer effort, the computational overhead can be prohibitive. Thus I develop the analytic formulation of the gradient. At this point, the reader may wish to consult the brief guide to vector calculus provided in Section A.",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 210,
                    "text": "[13]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "RMSECV and its Gradient"
        },
        {
            "text": "In the general formulation, there is an response weight associated with each observation. These weights should be used in both the model construction and error computation phases. Thus, I rewrite the RMSECV as a weighted RMSECV as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RMSECV and its Gradient"
        },
        {
            "text": "where \u01eb (j) is the residual of the j th test group, \u0393 (j) is the diagonal matrix of the response weights of the j th test group, and 1 (j) is the appropriate length vector of all ones. The gradient of this is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RMSECV and its Gradient"
        },
        {
            "text": "Each residual takes the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RMSECV and its Gradient"
        },
        {
            "text": "thus the Jacobian of the residual is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RMSECV and its Gradient"
        },
        {
            "text": "(Consult equation 7 and equation 8 in Section A for proofs.) Here and in the following, I use X cal , y cal to refer to the calibration objects, and X tst and y tst to refer to the test objects of a single cross validation partitioning. This reduces the problem to the computation of the Jacobian and gradient of the WPLS regression vector and intercept with respect to \u03bb.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RMSECV and its Gradient"
        },
        {
            "text": "A variant of the canonical WPLS computation is given in Algorithm 1 on the following page. This algorithm is different from the usual formulation in that the vectors W (k) , T (k) and P (k) are not normalized; it is simple to show, however, that the resultant vectors W (k) , T (k) and P (k) are identical to those produced by the canonical computation, up to scaling.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation"
        },
        {
            "text": "Algorithm 1 Algorithm to compute the WPLS regression vector. Input: m \u00d7 n matrix and m vector, number of factors, and a diagonal response weight matrix. Output: The regression vector and intercept. WPLS(X (0) , y, l, \u0393 = diag (\u03b3)) (1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "(Requires \u0398 (m) flops per loop.) (6) if y \u22a4 \u0393T (k) = 0 (7)",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 36,
                    "text": "(6)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "Maximum rank achieved; Let l = k and break the for loop.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "(Requires \u0398 (mn) flops per loop.) (12) Let W be the matrix with columns W (1,2,...,l) . Similarly define P, q.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "(Requires \u0398 (nl) flops, using back substitution.)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "The change in scaling does not affect the resultant regression vector, \u03b2, nor does it change the matrix X (k) . I prove some properties of Algorithm 1, nearly all of which hold for the canonical WPLS algorithm:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "for k > 0. Now the parts of the lemma: 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "2 First I prove X (k+1) W (k) = 0 as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "which follows because P (k) \u22a4 W (k) = 1 To prove for general j, we note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "lowing from the previous part. Similarly for P (k+j) \u22a4 W (k) . 4 Again I prove for j = 1 first:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "Similarly to above we can prove for j > 1 by writing X (k+j) = X (k+1) L.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "5 By the previous result,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "6 This is by simple definition:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "Thus by rearranging, P (k) is a linear combination of W (k) and W (k+1) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "7 From item 6:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "Since j > 1, by orthogonality of the W (item 3), the right hand side is zero, as needed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "For the P , first use item 3 to assert",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "and thus either P (k+j) \u22a4 P (k) = 0 as desired or T (k) \u22a4 \u0393y = 0. The algorithm detects this possibility and terminates if it holds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "Thus, as in canonical WPLS, the matrix P \u22a4 W is bidiagonal upper triangular; however, for this variant, the matrix has unit main diagonal. This variant of the algorithm is more amenable to a Jacobian computation, although conceivably it could be susceptible to numerical underflow or overflow.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "p. 5/29"
        },
        {
            "text": "I now present a different way of computing the same results as Algorithm 1, but by reusing old computations to compute seemingly unnecessary intermediate quantities which will be useful in the Jacobian computation. Moreover, the Jacobian computation will exploit the assumption that m \u226a n to gain an asymptotic reduction in runtime. This is achieved by performing the computations in the m-dimensional space, that is in the quantities related to y and T (k) , and avoiding the n-dimensional quantities W (k) and P (k) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "The variant algorithm introduces the \"preimage\" vectors V (k) and the preimage of the regression coefficient \u03b1. By preimage, I mean in X (0) \u22a4 \u0393, thus, as an invariant, these vectors will satisfy W (k) = X (0) \u22a4 \u0393V (k) and \u03b2 = X (0) \u22a4 \u0393\u03b1. The variant algorithm also computes the vectors",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "Note that any explicit updating of the matrix X (k) is absent from this version of the algorithm, rather the updating is performed implicitly. This will facilitate the computation of the Jacobian when X (0) is replaced in the sequel by X (0) \u039b. The following lemma confirms that this variant form of the algorithm produces the same results as Algorithm 1, that is the same vectors T and q, consistent vectors V , and produces the same affine model (\u03b2, b 0 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "Then the following hold:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "Proof. The parts of the lemma:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "1 Rewrite X (k) , then use the fact that the T are \u0393-orthogonal (Lemma 5.1 item 5):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "2 By definition, and \u0393-orthogonality of the T :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "To show w 0 = 0 it suffices to note that U (k) is chosen to be \u0393-orthogonal to T (0) :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "3 For k > 0, rewrite X (k+1) :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "For k = 0, since w 0 = 0, it suffices to show T (1) = U (1) . Rewriting X (1) :",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 59,
                    "text": "(1)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "4 First, for k > 0, restate item 6 of Lemma 5.1:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "Factoring to preimages using item 1 gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "The definition of q k then gives the result. For k = 0, rewrite X (1) :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "For concreteness, I present the WPLS algorithm via intermediate calculations as Algorithm 2 on the next page.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Rank Sensitive Implicit WPLS Algorithm"
        },
        {
            "text": "Now I amend Algorithm 2 with derivative computations to create an algorithm that computes the regression coefficient for input X (1) \u039b, and y (1) , and returns the preimage of the regression vector, \u03b1, as well as its Jacobian \u2202\u03b1 \u2202\u03bb , and the gradient of the intercept, \u2207 \u03bb b 0 .",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 145,
                    "text": "(1)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "p. 9/29",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "Algorithm 2 Algorithm to compute the WPLS regression vector, with X (k) implicit. Input: Matrix and vector, factors, and diagonal response weight matrix. Output: The regression vector and intercept.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "Full rank achieved; Let l = k and break the for loop.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "This is given as Algorithm 3 on the following page. In addition to the intermediate quantities used in Algorithm 2, this algorithm also computes some intermediate derivatives, some of which need to be stored until the end of the computation. The required derivatives are",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "\u2202\u03bb , \u2207 \u03bb q k and \u2207 \u03bb w k for k \u2265 1, and \u2207 \u03bb r k , \u2202U (k) \u2202\u03bb , \u2207 \u03bb t k , and \u2202T (k) \u2202\u03bb for the most recent k. Careful inspection and the vector calculus rules outlined in Section A are all that is required to verify that Algorithm 3 correctly computes the Jacobian of the model \u03b2. The only theoretical complication in the transformation of Algorithm 2 to Algorithm 3 is the explicit formulation of the back substitution to compute v = M \u22121 q. Given that M is upper triangular, bidiagonal with unit diagonal, inspection reveals that the back substitution in Algorithm 3 is computed correctly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "Inspection of Algorithm 1 reveals that WPLS computation requires \u0398 (mnl) floating point operations, where X (1) is m \u00d7 n, and l is the ultimate number of factors used. Thus a numerical approximation to the Jacobian using n evaluations of Algorithm 1 gives an algorithm with asymptotic runtime of \u0398 mn 2 l . Inspection of Algorithm 3 reveals that it computes the Jacobian exactly in \u0398 m 2 nl . The runtime limiting operation is the mul-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "\u2202\u03bb , with runtime of \u0398 m 2 n per loop.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "It would appear that one would incur a further cost of \u0398 mn 2 in the conversion of p. 10/29",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "Algorithm 3 Algorithm to compute the WPLS regression vector and Jacobian. Input: Predictor and response, factors, response weights and predictor weights.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "Output: The preimage of the regression vector and its Jacobian, and the intercept and its gradient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "Full rank achieved; Let l = k and break the for loop.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "p. 11/29 \u2202\u03b1 \u2202\u03bb to \u2202\u03b2 \u2202\u03bb , as it requires the multiplication \u039bX (1) \u22a4 \u0393 \u2202\u03b1 \u2202\u03bb . However, this can be avoided if the ultimate goal is computation of the Jacobian of the residual, rather than the Jacobian of the regression coefficients. Referring back to equation 3, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "Letting m t be the number of objects in the test group, the multiplication X tst \u039b 2 X \u22a4 cal requires O (mm t n) flops, and the multiplication X tst \u039b 2 X \u22a4 cal \u0393 \u2202\u03b1(\u03bb) \u2202\u03bb also requires O (mm t n) flops. Thus the computation of \u2202\u01eb \u2202\u03bb can be done with O mm t n + m 2 nl flops, which is linear in n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "For concreteness, the residual computation with analytic Jacobian was coded and compared for accuracy and speed against a \"slow\" analytic version (one which does not exploit the reduced rank in the Jacobian computation) and a numerical approximation to the Jacobian. Run times are compared in Figure 1 for varying number of channels; the difference in asymptotic behavior with respect to n is evident. For the case of 40 calibration objects and 10 test objects generated randomly with 2000 channels, the fast analytic computation of residual Jacobian took about 1.7 seconds, the slow analytic took about 44 seconds, and the numeric approximation took about 84 seconds on the platform tested (see Section 11 for details). Note that the \"slow\" analytic version is actually preferred in the case that m \u2265 n, as it runs in time \u0398 mn 2 l . However, in spectroscopy it is usually the case that m \u226a n.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 293,
                    "end": 301,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "WPLS Computation with Jacobian"
        },
        {
            "text": "The BFGS algorithm 2 is a quasi-Newton optimization algorithm. That is, the algorithm models a scalar function of many variables by a quadratic function with an approximate Hessian. The approximation to the Hessian is improved at each step by two rank one updates. The BFGS algorithm enjoys a number of properties which make it attractive to the numerical analyst: provable superlinear global convergence for some convex optimization problems; provable superlinear local convergence for some nonconvex problems; robustness and good performance in practice; deterministic formulation; relative simplicity of implementation; and, perhaps most importantly to the practical analyst, the algorithm has been implemented in a number of widely available libraries and packages, many of which accept the objective function as a blackbox. [27, 26] The BFGS algorithm is an iterative solver. That is, it starts with some initial estimate of a good \u03bb, say \u03bb (0) , and produces successive estimates, \u03bb (k) , which are supposed to converge to a local minimizer of the objective function. Each iteration consists of a computation of the gradient of the objective at \u03bb (k) . The algorithm constructs a search direction, call it \u03c1 (k) , by multiplying the inverse approximate Hessian by the negative gradient. Then a line search is performed to find an acceptable step in the search direction, that is to find the \u03b1 (k) used to construct \u03bb (k+1) = \u03bb (k) + \u03b1 (k) \u03c1 (k) . In the backtracking algorithm used to perform line search described by Nocedal and Wright, a number of prospective values of \u03b1 (k) may be tested; the objective function must be computed for each prospective value, but the gradient need not be computed. [27, Algorithm 3.1] A fast implementation of the BFGS algorithm should not query the blackbox function for gradients during the backtracking phase.",
            "cite_spans": [
                {
                    "start": 829,
                    "end": 833,
                    "text": "[27,",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 834,
                    "end": 837,
                    "text": "26]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "As mentioned above, the BFGS requires some initial estimate of the Hessian of the objective function. When a good initial estimate of the Hessian is impractical, the practical analyst punts, and resorts to the identity matrix. Under this choice, the first search direction is the negative gradient, i.e., the direction of steepest descent. The BFGS constructs better estimates of the Hessian by local measurement of the curvature of the objective function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "Depending on the implementation, the BFGS algorithm may have to store the approximate Hessian of the objective function or the inverse approximate Hessian. In either case, the storage requirement is \u2126 n 2 . To avoid this, one can use the limited memory BFGS algorithm, which approximates the Hessian by a fixed number of the previous iterative updates, which avoids the need for quadratic storage. This method evidently works as well as BFGS in practice for many problems. [23, 16, 26] p. 13/29 9 Selecting Wavelengths from an Optimal \u03bb Once a predictor weighting \u03bb has been found which gives a small RMSECV, one must use the \u03bb to select a subset of the channels. That is, one must reverse the embedding, finding a subset of the channels in the discrete space of all such subsets which somehow approximates the continuous solution given by \u03bb. Without loss of generality, one may assume that \u03bb has unit norm, i.e., \u03bb 2 = 1, since the effective WPLS regression vector is invariant under scaling, i.e., c\u039b\u03b2 (c\u03bb) is constant for all nonzero values of c. This latter fact is proved by considering the output of the canonical WPLS algorithm, which normalizes the vectors W (k) and T (k) . Moreover, I assume that the elements of \u03bb are nonnegative, again without loss of generality.",
            "cite_spans": [
                {
                    "start": 473,
                    "end": 477,
                    "text": "[23,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 478,
                    "end": 481,
                    "text": "16,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 482,
                    "end": 485,
                    "text": "26]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "Clearly, the weightings of the channels somehow signify their importance, and can be used in the selection of a subset of the channels. The ordering in significance indicated by \u03bb suggests n different possible choices of subsets 3 , the k th of which is the subset with the k most significant channels. If the acceptable number of channels is bounded by an external restriction, say an upper bound of n f , then one should select the subset of the n f most significant channels. Without any external restrictions, one should select the subset of channels (or \"model\") which minimizes some measure of predictive quality, such as RMSECV or an information criterion like Schwarz' Bayesian Information Criterion (BIC).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "The asymptotic (in m) consistency of model selection criteria was examined by Shao. [33, 34] A number of differences exist between the formulation studied by Shao and that presented here: our design matrix is assumed to be of reduced rank (i.e., equation 1 describes a reduced rank matrix) and non-deterministic 4 ; our affine model is built by PLS rather than OLS. However, absent any extant results for the reduced rank formulation, I follow Shao's work, which you may take with a grain of salt.",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 88,
                    "text": "[33,",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 89,
                    "end": 92,
                    "text": "34]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "I will focus on two model comparison criteria suggested by Shao: delete-d CV, and BIC. Delete-d CV is regular cross validation with d objects in the validation set. The model which minimizes RMSECV under the given grouping is selected. Because m d can be very large, only a number of the possible CV groupings are used. Shao's study suggests that Monte Carlo selection of the CV groups can be effective with only O (m) of the possible groupings used. Shao also proved that d/m \u2192 1 is a prerequisite for asymptotic consistency. In his simulation study, he used d \u2248 m \u2212 m 3/4 , and found that it outperformed delete-1 CV, especially in those tests where selecting overly large models is possible. [33] Shao also examines a class of model selection criteria which contains the General Information Criterion described by Rao and Wu, the minimization of which, under certain assumptions, is equivalent to minimizing BIC. [34, 31] For a subset of k channels, the reduced rank form of this criterion is",
            "cite_spans": [
                {
                    "start": 695,
                    "end": 699,
                    "text": "[33]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 916,
                    "end": 920,
                    "text": "[34,",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 921,
                    "end": 924,
                    "text": "31]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "where MSEP is based on the given set of k channels and l factor PLS. I use the denominator term m \u2212 l \u2212 1, rather than m \u2212 k as suggested by Shao for the OLS formulation, based on a simulation study. This allows meaningful comparison in situations where k > m, although in this case the expected value of MSEP is penalized by a term quadratic in k/m. [25] To continue the mongrelization of this criterion, I find it useful to replace MSEP by MSECV for appropriately chosen CV groups:",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 355,
                    "text": "[25]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "Minimization of this criterion favors parsimony more than minimization of RMSECV alone. Until the asymptotic consistency of the reduced rank/PLS model selection problem is addressed theoretically, I cannot recommend one of these criteria over the other. It is not obvious that the magnitudes of the elements of \u03bb are sufficient to establish an importance ordering on the channels. For instance, it might be appropriate to multiply the elements of \u03bb by the corresponding element of the regression vector \u03b2 chosen by WPLS on the entire data set, and use that Kronecker product vector as the importance ordering. It might be argued that that product should further be multiplied by the sample standard deviation of the channels. As there seems to be no general trend in comparing the two methods, I recommend implementing each of these techniques and allowing the information criterion to select whichever model is best, irrespective of which pragma produced the ordering.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The BFGS Algorithm"
        },
        {
            "text": "The ultimate goal is selection of a subset of the channels which minimizes delete-d RMSECV or one of the information criteria. This should guide the choice of the objective function which we numerically minimize in the continuous framework. The obvious choice is to minimize RMSECV, however the choice of the CV groups can lead to an asymptotically inconsistent selection procedure or long runtime. Moreover, the minimization of RMSECV may also select a \u03bb with a large number of nontrivial elements, which makes reversing the embedding difficult or noninformative.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crafting an Objective Function"
        },
        {
            "text": "Thus one may choose to minimize an objective function which approximates one of the information criteria, balancing quality and parsimony, rather than minimizing RMSECV. Recall, for example, aBIC = ln MSECV + (k log m)/(m \u2212 l \u2212 1). The continuous embedding of the MSECV term with respect to \u03bb is understood. To complete the embedding it only remains to estimate the subset size (number of channels retained) of the model indicated by a continuous predictor weighting \u03bb. My suggestion is to use a ratio of the p-norm to the q-norm:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crafting an Objective Function"
        },
        {
            "text": "for 0 < p < q < \u221e. I claim \u03ba p,q (\u03bb) is an appropriate choice of model size estimate. Note that \u03ba p,q is scale invariant, that is, \u03ba p,q (c\u03bb) is constant for each nonzero choice of the scalar c. Also note that if \u03bb consists of j ones and n \u2212 j zeros, then \u03ba p,q (\u03bb) = j. See Figure 2 for the behaviour of this function for various values of p, q. Using smaller values of p creates a stronger drive towards binary vectors by penalizing non binary vectors.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 275,
                    "end": 283,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Crafting an Objective Function"
        },
        {
            "text": "Thus to approximately minimize BIC, one could minimize \u03c8 (\u03bb) = df ln (MSECV (\u03bb)) + (ln (m) \u03ba p,q (\u03bb))/(m \u2212 l \u2212 1), Figure 2 : The function \u03ba p,q (\u03bb) is plotted in polar coordinates for the two dimensional vector \u03bb = cos \u03b8, sin \u03b8 , and various values of p, q in the quadrant 0 \u2264 \u03b8 \u2264 \u03c0/2. Note that each of the graphs passes through (0, 1), (\u03c0/4, 2) and (\u03c0/2, 1), as guaranteed by the fact that \u03ba p,q measures the number of nonzero elements in a scaled binary vector. The circles of radius 1 and 2 are also plotted.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 115,
                    "end": 123,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Crafting an Objective Function"
        },
        {
            "text": "the gradient of which is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Crafting an Objective Function"
        },
        {
            "text": "The method was implemented in the Matlab tm language. All results in this paper were performed in the GPL Matlab clone, GNU Octave (version 2.1.69) [8] , compiled with BLAS [7] , on an AMD Athlon 64 4000+ (2.4 GHz) running Gentoo Linux, 2.6.15 kernel. The BFGS and backtracking line search algorithms were implemented as outlined by Nocedal and Wright. [27] Sample code for the PLS and Jacobian computation are given in Section B. The objective function was supplemented by an optional term to simulate BIC, with the p and q terms of \u03ba p,q tunable parameters. The inverse of the sample standard deviation of each channel is generally used as the starting iterate, \u03bb (0) . The initial approximation to the Hessian is taken as some constant times the identity matrix. Termination of BFGS was triggered by the computation of a gradient smaller in norm than a lower bound, by an upper limit on the number of major iterates, function evaluations, or achievement of a lower bound on the change of the objective function. Response weighting (i.e., WPLS) p. 16/29 has not yet been implemented.",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 151,
                    "text": "[8]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 173,
                    "end": 176,
                    "text": "[7]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 353,
                    "end": 357,
                    "text": "[27]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 1050,
                    "end": 1055,
                    "text": "16/29",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Implementation Notes"
        },
        {
            "text": "Selection of optimal wavelengths was performed by minimization of a delete-d RMSECV or aBIC, with the different models determined by ordering of \u03bb or by diag (\u03b2) \u03bb, whichever is chosen by the information criterion. The trivial model (responses are normal with approximated means and variances and predictors are ignored) is also compared. An optional post-selection minimization is allowed on the selected channels. The final results consist of a subset of the available channels and predictor weightings for those channels. This bit of cheating allows the method to keep the advantages of properly weighted predictors. Note that the weighting is irrelevant and ignored in the case where the number of selected channels is equal to the number of latent factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation Notes"
        },
        {
            "text": "Unless specified by the user, the system must choose the cross validation groups. By default, this is done using the Monte Carlo framework suggested by Shao: 2m different partitions of m 3/4 calibration objects and m \u2212 m 3/4 test objects are randomly selected.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation Notes"
        },
        {
            "text": "SRCEK was tested on the three data sets used to compare wavelength selection procedures by Forina et al. [13, 10] As in the original publication, these are referred to as Moisture, Kalivas, and Artificial. The data set Moisture consists of moisture responses of 60 samples of soy flour, with predictors measured with a filter instrument and originally appeared in a paper by Forina et al. [12] . The data set Kalivas, originally from a paper by Kalivas [21] , consists of moisture responses of 100 samples of wheat flour, with 701 responses measured by an NIR spectrometer. The data set Artificial consists of 400 randomly generated objects with 300 channels. The channels are grouped into six classes, with a high degree of correlation between elements of the first five classes, each consisting of 10 channels. The response was generated by a linear combination of five of the responses (the first response in each of the first five classes), plus some noise; the 250 channels of the sixth class are entirely irrelevant to the responses. However, the level of noise in the response is large enough to mask the effects of the fifth relevant channel. The objects are divided into a training set of 100 objects, and an external set with the remainder.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 109,
                    "text": "[13,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 110,
                    "end": 113,
                    "text": "10]",
                    "ref_id": null
                },
                {
                    "start": 389,
                    "end": 393,
                    "text": "[12]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 453,
                    "end": 457,
                    "text": "[21]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "In order to allow meaningful comparison between the results found here and in the study of Forina et al., I report RMSECV values using the same CV groupings of that study. These were generated by dividing the objects into groups in their given order. Thus e.g., the first group consists of the 1 st , 6 th , 11 th objects and so on, the second group is the 2 nd , 7 th , 12 th objects and so on, etc. Five groups are used. [13, 10] However, the objective function was computed based on other groupings, as described below.",
            "cite_spans": [
                {
                    "start": 423,
                    "end": 427,
                    "text": "[13,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 428,
                    "end": 431,
                    "text": "10]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "Note that, in light of Shao's studies, the CV groupings used by Forina et al. seem sparse both in number and in the number deleted (m/5). For this reason, it may be meaningless to compare the different subset selection techniques based on the RMSECV for this grouping. However, since the channels retained by the different methods are not reported for the data sets Kalivas and Artificial, I can only compare the results of SRCEK to those of the other methods by the RMSECV of this grouping or by the aBIC based on that RMSECV. For Moisture, Forina et al. report the selected channels, making comparison based on Monte Carlo CV groupings possible. These are denoted by RMSEMCCV, and based on 120 delete-38 groupings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "p. 17/29 SRCEK was applied to Moisture with 120 delete-38 Monte Carlo CV groups, using RMSEMCCV as the objective function for 2 and 3 factor PLS. Termination was triggered by small relative change in the objective (relative tolerance in objective of 10 \u22125 ), which was achieved in both cases in at most 25 major iterates. Model selection is performed by minimization of aBIC. Results are summarized in Table 1 , and compared to the results found by Forina et al. For 2 factor PLS, SRCEK selects 2 channels, L14:2100 and L16:1940, the same choice made by SOLS and GA-OLS. For 3 factor PLS, SRCEK selects 3 channels, adding L20:1680 to the previous two channels. The 2 factor choice is preferred by both the CV error and aBIC. In this case, aBIC matches my intuitive ordering of these results. The results of SRCEK applied to Kalivas are summarized in Table 2 , and compared to the results from the previous study. Several experiments were attempted, each using 3-5 factors. The first experiment, (a), uses the same CV groupings as Forina et al., and minimizes and selects based on RMSECV for this grouping. In experiments (b) and (c), 240 delete-68 MC CV groups are used, RMSECV is minimized, and channel selection is based on, respectively, RMSECV and aBIC. In experiments (d) and (e), 120 delete-68 MC CV groups are used, the embedded aBIC (using \u03ba 1,2 ) is minimized, and selection is based on, respectively, aBIC and RMSECV. The final models of all experiments used to compute RMSECV for the same 200 delete-68 MC CV grouping, to facilitate comparison. The maximum acceptable number of channels was taken to be 50.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 402,
                    "end": 409,
                    "text": "Table 1",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 850,
                    "end": 857,
                    "text": "Table 2",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Experiments and Results"
        },
        {
            "text": "As puts them at a disadvantage when compared to SRCEK. A more serious objection is that a small RMSECV for the CV groupings of Forina et al. does not appear to imply a small RMSECV for the MC CV groupings, although the inverse implication does seem to hold. This gives confidence in the results of e.g., experiment (b)-5, which gives small RMSECV for both CV groupings. The effect of the objective function on the algorithm outcome for this data set is shown in Figure 3 on the next page. This graph shows the effective regression vector \u039b\u03b2 (\u03bb) , for the \u03bb found by BFGS minimization, for experiments (b) and (d), using 4 latent factors. When RMSECV alone is minimized, the regression vector has no clear structure. However, when aBIC is minimized, the regression vector divides the channels into two groups, those with 'low' relevance, and those with 'high' relevance. As expected from spectroscopic data, the relevance of relevant channels is more or less continuous. Note, however, that minimizing p. 19/29 on the information criterion selects some of the same channels as minimizing on RMSECV, but this relationship does not strictly hold. For example, some channels in the range 1-10 appear to be given high relevance by aBIC but not by RMSECV. I suspect that there is some dependence on the initial vector \u03bb, and the CV groups used. The results of SRCEK, and other methods, applied to Artificial are summarized in Table 3 . Two experiments were attempted, each using 3-5 factors. Each use autoscaling to generate the initial \u03bb, and 120 delete-75 MC CV groups for computing RMSECV. In both experiments, the embedded aBIC (using \u03ba 0.8,2.4 ) is minimized, and selection is based on aBIC and RMSECV, respectively, in experiments (f) and (g). The models built by these experiments are tested on a Monte Carlo CV grouping (200 delete-68 groups consisting of the 100 test data, not the external set), and the computed RMSEMCCV is shown in the table. Note that there is some mismatch between the RMSECV for the groups used by Forina et al. and the RMSEMCCV for these groupings. These measures produce different orderings of the models, casting some suspicion on the sparse CV groupings used by the previous study.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 462,
                    "end": 470,
                    "text": "Figure 3",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1420,
                    "end": 1427,
                    "text": "Table 3",
                    "ref_id": "TABREF6"
                }
            ],
            "section": "Experiments and Results"
        },
        {
            "text": "The post-optimized effective regression vector, \u039b\u03b2 (\u03bb) , is plotted in Figure 4 on page 22 for this experiment (g) with 3 factors, showing the discovered relevance of the channels. All but one of the 250 irrelevant channels for this data set are found to have a low weight. Selection by RMSECV picks 6 channels, numbers 4, 5, 10, 20, 24, 40, which includes channels from four of the five relevant correlated groups of channels. The 5 factor experiment picks channels 1, 2, 3, 4, 5, 9, 10, 11, 13, 14, 15, 19, 20, 24, 40 , 50, which includes channels from each of the five correlated relevant groups, and no irrelevant channels. This may be attributable only to chance, however, as the optimal effective regression vector \u039b\u03b2 (\u03bb) after optimization in this experiment attributes high weights to a number of irrelevant channels. ",
            "cite_spans": [
                {
                    "start": 467,
                    "end": 469,
                    "text": "1,",
                    "ref_id": null
                },
                {
                    "start": 470,
                    "end": 472,
                    "text": "2,",
                    "ref_id": null
                },
                {
                    "start": 473,
                    "end": 475,
                    "text": "3,",
                    "ref_id": null
                },
                {
                    "start": 476,
                    "end": 478,
                    "text": "4,",
                    "ref_id": null
                },
                {
                    "start": 479,
                    "end": 481,
                    "text": "5,",
                    "ref_id": null
                },
                {
                    "start": 482,
                    "end": 484,
                    "text": "9,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 485,
                    "end": 488,
                    "text": "10,",
                    "ref_id": null
                },
                {
                    "start": 489,
                    "end": 492,
                    "text": "11,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 493,
                    "end": 496,
                    "text": "13,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 497,
                    "end": 500,
                    "text": "14,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 501,
                    "end": 504,
                    "text": "15,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 505,
                    "end": 508,
                    "text": "19,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 509,
                    "end": 512,
                    "text": "20,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 513,
                    "end": 516,
                    "text": "24,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 517,
                    "end": 519,
                    "text": "40",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 71,
                    "end": 79,
                    "text": "Figure 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Experiments and Results"
        },
        {
            "text": "Foremost, it seems one should be able to optimize RMSECV with respect to response weightings, \u03b3, in addition to predictor weights \u03bb. One can easily alter Algorithm 3 to also compute the gradients with respect to \u03b3. The increased degrees of freedom increases the risk of overfitting. One should alter the embedded information criterion objective function described in Section 10 to balance this risk. Since it is assumed the data are distributed as y j \u223c X \u22a4 j \u03b2 + b 0 + N 0, \u03c3 2 /\u03b3 j , we have added m \u2212 1 new estimated parameters, viz. the separate variances of each observation. [22, 30] One strategy to embed the information criterion, then, is to let \u03c8 (\u03bb) = ln (MSECV (\u03bb)) + (\u03ba p,q (\u03bb) \u2212 \u03ba p,q (\u03b3)) ln (m) /(m \u2212 l \u2212 1). The initial estimate should be that of homoscedasticity. Comparison of models becomes tricky. Work is underway on this extension.",
            "cite_spans": [
                {
                    "start": 581,
                    "end": 585,
                    "text": "[22,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 586,
                    "end": 589,
                    "text": "30]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Directions for Further Study"
        },
        {
            "text": "A theoretical study of the asymptotic consistency of different model selection techniques for the case of reduced rank design matrix and PLS modeling would provide SRCEK with a more sound method for reversing the embedding, as well as a better objective function. As this problem seems intractable, a simulation study might be appropriate. If this theoretical study reveals that one should minimize some combination of RMSECV (for some tailored CV groupings) with model size, I am confident that that measure can be continuously embedded using the techniques of this note.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Directions for Further Study"
        },
        {
            "text": "The method of ordering the successively larger models based on the optimal \u03bb, or on the Kronecker product of \u03bb with the regression coefficient seems rather ad hoc. This step p. 21/29 would also benefit from some theory, or could perhaps be replaced by strategies cribbed from other channel selection techniques (e.g., IPW). Conversely, some of these techniques may benefit from a preliminary predictor weight optimization step via BFGS.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Directions for Further Study"
        },
        {
            "text": "The SRCEK method could also be extended to Kernel PLS regression, however I suspect this would require the kernel to also compute derivatives, which could be impractical. [20] I would be interested in an analysis of the structure of the RMSECV and embedded aBIC objective functions. For example, can either be shown to be convex (in \u03bb) in general, or under assumptions on the data X and y which are justifiable in the chemometric context? Moreover, can one find sufficient sizes for the CV groupings to sufficiently reduce dependence of the objective on the groupings? Will a sufficiently designed CV grouping make the objective function convex or nearly so?",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 175,
                    "text": "[20]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Directions for Further Study"
        },
        {
            "text": "The choice of CV groupings affects both the \"quality\" of the RMSECV measure (i.e., how accurately it rates subsets of channels) and the runtime of SRCEK. Shao's Monte Carlo scheme, using 2m groupings of m p objects in the calibration group, and the remainder in the test group, results in a total runtime of O m m 2p n(l \u2212 1) + m p+1 n flops for the computation of the gradient of RMSECV. Thus I would be interested to discover an acceptable lower bound on p which gives acceptable quality of RMSECV.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Directions for Further Study"
        },
        {
            "text": "The iterates of the BFGS algorithm for this objective function often display a zigzagging behaviour towards the minimum. Often this is the result of some elements of \u03bb \"overshooting\" zero. It would be interesting to see if this can be avoided by using other minimization techniques, for example the conjugate gradient method, or a proper constrained minimization implementation of BFGS. [26] Finally, the SRCEK method as described in this paper has many tweakable parameters:",
            "cite_spans": [
                {
                    "start": 387,
                    "end": 391,
                    "text": "[26]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Directions for Further Study"
        },
        {
            "text": "p. 22/29 initial iterate \u03bb (0) , initial approximation of the Hessian, termination condition for BFGS, choice of objective function, p and q for the continuous embedding of number of estimated parameters, etc. While these provide many possibilities to the researcher of the technique, they are an obstacle for the end user. Thus reasonable heuristics for setting these parameters which work well in a wide range of settings would be welcome.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Directions for Further Study"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Lemma 5.1. Let X (k) , \u0393, W (k) , T (k) , and P (k) be as in Algorithm 1",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "W (k) \u22a4 P (k) = 1, for k \u2265 1",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "X (k+j) W (k) = 0 for all j \u2265 1, and k \u2265 1",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "W (k+j) \u22a4 W (k) = 0 and P (k+j) \u22a4 W (k) = 0 for all j \u2265 1, and k \u2265 1",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "X (k+j) \u22a4 \u0393T (k) = 0 for all j \u2265 1, and k \u2265 0",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "\u0393T (k) = 0 for all j \u2265 1, and k \u2265 0",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "W (k+1) = W (k) \u2212 P (k) T (k) \u22a4 \u0393y for k \u2265 1, and thus P (k) \u2208 span",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "A new look at the statistical model identification",
            "authors": [
                {
                    "first": "Hirotsugu",
                    "middle": [],
                    "last": "Akaike",
                    "suffix": ""
                }
            ],
            "year": 1974,
            "venue": "IEEE Transactions on Automatic Control",
            "volume": "19",
            "issn": "",
            "pages": "716--723",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "The choice of variables in multivariate regression: a nonconjugate Bayesian decision theory approach",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Fearn",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Vannucci",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Biometrika",
            "volume": "86",
            "issn": "3",
            "pages": "635--648",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Model selection: Understanding AIC and multimodel inference, with constrasts to BIC",
            "authors": [
                {
                    "first": "Ken",
                    "middle": [],
                    "last": "Burnham",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Presented at the Amsterdam Workshop on Model Selection",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Generalizing the derivation of the Schwarz information criterion",
            "authors": [
                {
                    "first": "Joseph",
                    "middle": [
                        "E"
                    ],
                    "last": "Cavanaugh",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [
                        "A"
                    ],
                    "last": "Neath",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Communications in Statistics",
            "volume": "28",
            "issn": "",
            "pages": "49--66",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Simultaneous wavelength selection and outlier detection in multivariate regression of near-infrared spectra",
            "authors": [
                {
                    "first": "Da",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Xueguang",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Qingde",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Analytical Sciences",
            "volume": "21",
            "issn": "",
            "pages": "161--166",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Canonical partial least squares and continuum power regression",
            "authors": [
                {
                    "first": "Barry",
                    "middle": [
                        "M"
                    ],
                    "last": "Sijmen De Jong",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "Lawrence"
                    ],
                    "last": "Wise",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ricker",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Journal of Chemometrics",
            "volume": "15",
            "issn": "",
            "pages": "85--100",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Preface: Basic Linear Algebra Subprograms Technical (Blast) Forum Standard I. International Journal of High Performance Applications and Supercomputing",
            "authors": [
                {
                    "first": "Jack",
                    "middle": [],
                    "last": "Dongarra",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "16",
            "issn": "",
            "pages": "1--111",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Octave: Past, present, and future",
            "authors": [
                {
                    "first": "John",
                    "middle": [
                        "W"
                    ],
                    "last": "Eaton",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Proceedings of the 2nd International Workshop on Distributed Statistical Computing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Lanczos bidiagonalization I: Analysis of a projection method for multiple regression",
            "authors": [
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "Eld\u00e9n",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Computational statistics & data analysis",
            "volume": "46",
            "issn": "",
            "pages": "11--31",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Iterative predictor weighting (IPW) PLS: a technique for the elimination of useless predictors in regression problems",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Forina",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Casolino",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pizarro Millan",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Journal of Chemometrics",
            "volume": "13",
            "issn": "2",
            "pages": "11--31",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Transfer of calibration function in near-infrared spectroscopy",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Forina",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Drava",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Armanino",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Boggia",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lanteri",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Leardi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Corti",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Conti",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Giangiacomo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Galliena",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bigoni",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Quartari",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Serra",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ferri",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Leoni",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lazzeri",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "27",
            "issn": "",
            "pages": "189--203",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Selection of useful predictors in multivariate calibration",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Forina",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lanteri",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "Cerrato Oliveros",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pizarro Millan",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Analytical and Bioanalytical Chemistry",
            "volume": "380",
            "issn": "3",
            "pages": "397--418",
            "other_ids": {
                "DOI": [
                    "10.1007/s00216-004-2768-x"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Classification using partial least squares with penalized logistic regression",
            "authors": [
                {
                    "first": "Gersende",
                    "middle": [],
                    "last": "Fort",
                    "suffix": ""
                },
                {
                    "first": "Sophie",
                    "middle": [],
                    "last": "Lambert-Lacroix",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Bioinformatics",
            "volume": "21",
            "issn": "7",
            "pages": "1104--1111",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Notes on the history and nature of partial least squares (PLS) modelling",
            "authors": [
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Geladi",
                    "suffix": ""
                }
            ],
            "year": 1988,
            "venue": "Journal of Chemometrics",
            "volume": "2",
            "issn": "4",
            "pages": "231--246",
            "other_ids": {
                "DOI": [
                    "10.1002/cem.1180020403"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Limited-memory reduced-Hessian methods for large-scale unconstrained optimization",
            "authors": [
                {
                    "first": "Phillip",
                    "middle": [
                        "E"
                    ],
                    "last": "Gill",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [
                        "W"
                    ],
                    "last": "Leonard",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Weighted Partial Least Squares Method to Improve Calibration Precision for Spectroscopic Noise-limited Data",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Haaland",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "D T"
                    ],
                    "last": "Jones",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "The Eleventh International Conference on Fourier Transform Spectroscopy",
            "volume": "430",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Wavelength selection with Tabu search",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Hageman",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Streppel",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wehrens",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "M C"
                    ],
                    "last": "Buydens",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Journal of Chemometrics",
            "volume": "17",
            "issn": "",
            "pages": "427--437",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "On the structure of partial least-squares regression",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Helland",
                    "suffix": ""
                }
            ],
            "year": 1988,
            "venue": "Communications in Statistics-Simulation and Computation",
            "volume": "17",
            "issn": "2",
            "pages": "581--607",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Kernel PLS variants for regression",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hoegaerts",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Suykens",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vandewalle",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "De Moor",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the 11th European Symposium on Artificial Neural Networks (ESANN2003). 2003. URL citeseer",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Two data sets of near infrared spectra",
            "authors": [
                {
                    "first": "John",
                    "middle": [
                        "H"
                    ],
                    "last": "Kalivas",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "37",
            "issn": "",
            "pages": "255--259",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "On the use of AIC for the detection of outliers",
            "authors": [
                {
                    "first": "Genshiro",
                    "middle": [],
                    "last": "Kitagawa",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Technometrics",
            "volume": "21",
            "issn": "",
            "pages": "193--199",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "On the limited memory BFGS method for large scale optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "C"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Math. Programming",
            "volume": "45",
            "issn": "3",
            "pages": "503--528",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Line search algorithms with guaranteed sufficient decrease",
            "authors": [
                {
                    "first": "Jorge",
                    "middle": [
                        "J"
                    ],
                    "last": "Mor\u00e9",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [
                        "J"
                    ],
                    "last": "Thuente",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "ACM Transactions on Mathematical Software",
            "volume": "20",
            "issn": "3",
            "pages": "286--307",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "The prediction error in CLS and PLS: the importance of feature selection prior to multivariate calibration",
            "authors": [
                {
                    "first": "Boaz",
                    "middle": [],
                    "last": "Nadler",
                    "suffix": ""
                },
                {
                    "first": "Ronald",
                    "middle": [
                        "R"
                    ],
                    "last": "Coifman",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Journal of Chemometrics",
            "volume": "19",
            "issn": "",
            "pages": "107--118",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Theory of algorithms for unconstrained optimization",
            "authors": [
                {
                    "first": "Jorge",
                    "middle": [],
                    "last": "Nocedal",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "Acta Numerica",
            "volume": "",
            "issn": "1",
            "pages": "199--242",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Numerical Optimization. Springer Series in Operations Research",
            "authors": [
                {
                    "first": "Jorge",
                    "middle": [],
                    "last": "Nocedal",
                    "suffix": ""
                },
                {
                    "first": "Stephen",
                    "middle": [
                        "J"
                    ],
                    "last": "Wright",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Genetic algorithms as a tool for wavelength selection",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "M"
                    ],
                    "last": "Torbj\u00f6rn",
                    "suffix": ""
                },
                {
                    "first": "Janne",
                    "middle": [],
                    "last": "Nordling",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Koljonen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Jarmo",
                    "suffix": ""
                },
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Alander",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Geladi",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the Eleventh Finnish Artificial Intelligence Conference",
            "volume": "3",
            "issn": "",
            "pages": "99--113",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Wavelength selection and outlier detection in reduced rank linear models",
            "authors": [
                {
                    "first": "Steven",
                    "middle": [
                        "E"
                    ],
                    "last": "Pav",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "US Patent #",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Detection of outliers in regression analysis by information criteria",
            "authors": [
                {
                    "first": "Seppo",
                    "middle": [],
                    "last": "Pynn\u00f6nen",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Proceedings of the University of Vaasa, number 146",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "A strongly consistent procedure for model selection in a regression problem",
            "authors": [
                {
                    "first": "Radhakrishna",
                    "middle": [],
                    "last": "Rao",
                    "suffix": ""
                },
                {
                    "first": "Yuehua",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Biometrika",
            "volume": "76",
            "issn": "2",
            "pages": "369--374",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Overview and recent advances in partial least squares",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Rosipal",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kr\u00e4mer",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the 2005 PASCAL Workshop on Subspace, Latent Structure and Feature Selection Techniques (SLSFS 2005)",
            "volume": "3940",
            "issn": "",
            "pages": "24--51",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Linear model selection by cross-validation",
            "authors": [
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Journal of the American Statistical Association",
            "volume": "88",
            "issn": "422",
            "pages": "486--494",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "An asymptotic theory for linear model selection",
            "authors": [
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Statistica Sinica",
            "volume": "7",
            "issn": "",
            "pages": "221--264",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Run times of the fast analytical computation of the Jacobian of the residual are compared against a slow analytic and a numerical approximation. The number of channels, n is shown in the horizontal axis, while the vertical axis is CPU time in seconds. The number of objects in the calibration and test groups remained constant, at 40 and 10 throughout, as did the number of PLS factors, 5. Times are the mean of seven runs, and single standard deviation bars are plotted, although they are mostly too small to see. See Section 11 for details on the tested platform.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The absolute value of the elements of vector \u039b\u03b2 (\u03bb) are plotted for postoptimization \u03bb on data set Kalivas. At top, RMSECV for MC CV groups was used as the objective function (experiment (b), 4 latent factors). At bottom, the embedded aBIC using the same CV groups for the RMSECV part and \u03ba 1,2 , was minimized (experiment (d), 4 latent factors).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The vector \u039b\u03b2 (\u03bb) is plotted for post-optimization \u03bb on data set Artificial from SRCEK (3), experiment (g), using embedded aBIC as objective function. The 6 selected (by RMSECV) channels are indicated with crosses. Many of the 50 relevant channels are highly weighted, while most of the 250 irrelevant channels have very low weights, with one notable exception. That the highly weighted irrelevant channel was not selected may be attributable only to chance.",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "expected, when trained on the CV groups of Forina et al., SRCEK is able to produce small errors for that CV grouping, beating all the methods studied by Forina et al.. A number of caveats are necessary: the RMSECV values reported use predictor weighting to build and test the models. When the weights are not used, the RMSECV values are not as impressive. For example, for experiment (a), 4 factors, the reported 0.1869 becomes 0.2171 when the predictor weighting is not used. I think the objection here should not be that SRCEK uses predictor weighting, but that the methods studied previously did not, which",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Results from selection methods applied to data set Kalivas are shown ordered by decreasing aBIC, with results from SRCEK. The results from SRCEK are also tested against a MC CV grouping consisting of 200 delete-68 partitions, yielding the RMSEMCCV shown.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Results from selection methods applied to data set Artificial are shown ordered by decreasing aBIC, with results from SRCEK. The number of uninformative channels selected is shown as well as the RMSEP for the external set of 300 objects. The results from SRCEK are also tested against a MC CV grouping consisting of 200 delete-68 partitions, yielding the RMSEMCCV shown. The objective function used was the embedded aBIC.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The vector calculus appearing in this paper is not difficult, it is merely involved. To follow the derivations one need only have a good understanding of basic linear algebra and a fondness for the product derivative rule.First the gradient and Jacobian: given a scalar function f (\u03bb), its gradient is the vector \u2207 \u03bb f whose j th element is the partial derivative of f with respect to \u03bb j , that is \u2202f \u2202\u03bb j . Given a vector valued function f (\u03bb) which outputs an l-dimensional vector when given the mdimensional vector \u03bb as input, its Jacobian is an l \u00d7 m matrix whose (i, j) th element is the partial derivative of the i th component of f with respect to \u03bb j , that isRemarkably we need nothing more exotic than this. Some convenient rules to fill our toolbox:1. The gradient product rule: given functions f and g we haveThis follows because the j th element of \u2207 \u03bb (f g) is the partial of f g with respect to \u03bb j . Using the scalar product rule we have \u2202f g \u2202\u03bb j = f \u2202g \u2202\u03bb j + g \u2202f \u2202\u03bb j , from which our rule follows. The gradient quotient rule follows in a similar fashion: for given f and g:The gradient dot-product rule is similar; given vectors v and w we have2. The Jacobian scales: given a constant matrix M and a vector-valued function f we haveThis follows from the linearity of the matrix product and the derivative:3. The diagonal rule: given vector-valued function f , and letting \u039b p = (diag (\u03bb)) p thenAgain the proof is trivial:We have used \u2297i, j to be the Kronecker delta, which is one if i = j and zero otherwise. 4. The product rule for Jacobians: given scalar function f and vector function g, we haveThe proof is similar to above. 5. A useful composite rule; given vectors v and w and matrix M, thenThis rule follows from application of the dot-product and diagonal rules given above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A A Brief Review of Vector Calculus"
        },
        {
            "text": "I present Matlab tm compatible code for computing the preimage of the PLS regression coefficient and its Jacobian on the following page. This code is merely a realization of Algorithm 3. The code was tested in GNU Octave, and used to produce some of the results depicted in Figure 1 error('size mismatch.'); end %allocate storage V = zeros(m,l);dV = zeros(m,n,l-1);w = zeros(l,1);dw = zeros(l,n); q = zeros(l+1,1);dq = zeros(l+1,n); %used so much we compute once and store. Xlam = X * diags(lambda);XLXg = Xlam * Xlam' * diag(gam); %nm^2 hit. T0 = Tk = ones(m,1);dTk = zeros(m,n);Vk = y;dVk = zeros(m,n); t0 = sum(gam); for k=0:l rk = y' * (gam .* Tk); drk = dTk' * (gam .* y); tk = Tk' * (gam .* Tk); dtk = 2 * dTk' * (gam .* Tk); q(k+1) = rk/tk; dq(k+1,:) = (drk -q(k+1) * dtk)' / tk; if (k < l) Vk = Vk -q(k+1) * Tk;V(:,k+1) = Vk; dV(:,:,k+1) = (dVk = dVk -q(k+1)*dTk -Tk * dq(k+1,:)); Qk = XLXg * Vk;dQk = XLXg * dVk + 2 * Xlam * diag(X' * (gam .* Vk)); Uk = Qk .-(gam' * Qk / t0);dUk = dQk -T0 * (gam' * dQk / t0); w(k+1) = Tk' * (gam .* Uk) / tk; dw(k+1,:) = (dTk' * (gam .* Uk) + dUk' * (gam .* Tk) -w(k+1) .* dtk)' / tk; dTk = dUk -w(k+1) * dTk -Tk * dw(k+1,:); Tk = Uk -w(k+1) * Tk; end end p. 28/29 B CODE %now compute M\\q and its jacobian iMq = zeros(l,1);diMq = zeros(l,n); iMq(l) = q(l+1);diMq(l,:) = dq(l+1,:); for k=(l-1):-1:1 iMq(k) = q(k+1) -w(k+1) * iMq(k+1); diMq(k,:) = dq(k+1,:) -(w(k+1) .* diMq(k+1,:) + iMq(k+1) .* dw(k+1,:)); end %now compute VM\\q and its jacobian alpha = V * iMq;dalpha = V * diMq; %+ more stuff: %ack! no tensor product in octave/Matlab :( for k=2:l dalpha += iMq(k) .* dV(:,:,k); end %dV(:,:,1) is all zeros?%now the intercept b0 = gam' * (y -XLXg * alpha) ./ sum(gam); db0 = -(XLXg * dalpha + 2 * Xlam * diag(X' * (gam .* alpha)))' * gam ./ sum(gam); endfunction p. 29/29",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 274,
                    "end": 282,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "B Code"
        }
    ]
}