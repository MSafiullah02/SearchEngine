{
    "paper_id": "PMC7148218",
    "metadata": {
        "title": "You Can Teach an Old Dog New Tricks: Rank Fusion applied to\u00a0Coordination Level Matching for\u00a0Ranking in Systematic Reviews",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Harrisen",
                "middle": [],
                "last": "Scells",
                "suffix": "",
                "email": "h.scells@uq.net.au",
                "affiliation": {}
            },
            {
                "first": "Guido",
                "middle": [],
                "last": "Zuccon",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Bevan",
                "middle": [],
                "last": "Koopman",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "The goal of medical systematic review literature search is to retrieve all research publications relevant to a highly focused research question that satisfies an inclusion criteria. This is so that all literature relevant to the review\u2019s research question can be synthesised in the systematic review [23]. Search takes place using a Boolean query that is formulated by highly trained information specialists using their own intuition and domain knowledge, in order to capture the information need of the systematic review [8]. Afterwards, every study retrieved by the Boolean query is screened (assessed) for inclusion using the titles and abstracts of studies (abstract level assessment). Identified relevant abstracts are further processed by acquiring the full-text for additional assessment, information extraction and synthesis [23].",
            "cite_spans": [
                {
                    "start": 301,
                    "end": 303,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 523,
                    "end": 524,
                    "mention": "8",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 834,
                    "end": 836,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The process of creating a medical systematic review typically involves large monetary and temporal costs; the average Cochrane review costs $350K to create [35] and it takes up to two years to publish \u2013 thus often rendering the result of the systematic review already out-of-date at the time of publication. The process that incurs the most cost when creating a systematic review is the screening of studies retrieved by the Boolean query; often a large set of studies is retrieved, but only a handful are relevant.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 159,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A number of solutions have arisen to address the amount of time spent screening documents, including: screening prioritisation (which seeks to re-rank the set of retrieved documents to show more relevant documents first, thus starting the full-text screening earlier), and stopping estimation (which seeks to predict at what point continuing to screen will no longer contribute gain) [25, 26, 40]. In this paper, we propose and evaluate a Boolean query ranking function aimed at tackling these two tasks. The proposed method incorporates intuitions from both coordination level matching of Boolean queries and search engine rank fusion.",
            "cite_spans": [
                {
                    "start": 385,
                    "end": 387,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 389,
                    "end": 391,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 393,
                    "end": 395,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "This paper proposes an extension to coordination level matching (CLM) by exploiting the query-document relationship with rank fusion. CLM is a ranking function originally proposed for Boolean queries that scores documents using the occurrences of documents retrieved by different clauses of the query. The proposed extension, coordination level fusion (CLF), has many advantages over CLM that enable it to use multiple weighting schemes (rankers) and different fusion methods dependent on the Boolean clauses. We use CLF to rank studies in the screening prioritisation task of systematic reviews. We further plan to study the use of a cut-off threshold tuned on training data to control when the screening of studies should be stopped based on the CLF retrieval score. The empirical results obtained on the CLEF Technology Assisted Review datasets [25, 26] show that CLF significantly outperforms existing state-of-the-art methods that consider similar settings, including the ranking method currently used in PubMed (a popular database to search for literature for systematic reviews).",
            "cite_spans": [
                {
                    "start": 849,
                    "end": 851,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 853,
                    "end": 855,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Systematic reviews are costly and often out-of-date by the time they are published due to the amount of time involved in their creation. A wide range of systematic review creation processes have been considered for automation or improvement using semi-automatic techniques [40], including: query formulation [27, 46, 48], screening prioritisation [1\u20133, 7, 28, 29, 37, 47, 54, 56], stopping prediction [7, 16, 24], assessment of bias [33, 43], among others. This paper proposes a technique for screening prioritisation, thus the remainder of this section focuses on this specific task.",
            "cite_spans": [
                {
                    "start": 274,
                    "end": 276,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 309,
                    "end": 311,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 313,
                    "end": 315,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 317,
                    "end": 319,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 348,
                    "end": 349,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 350,
                    "end": 351,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 353,
                    "end": 354,
                    "mention": "7",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 356,
                    "end": 358,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 360,
                    "end": 362,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 364,
                    "end": 366,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 368,
                    "end": 370,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 372,
                    "end": 374,
                    "mention": "54",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 376,
                    "end": 378,
                    "mention": "56",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 402,
                    "end": 403,
                    "mention": "7",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 405,
                    "end": 407,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 409,
                    "end": 411,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 434,
                    "end": 436,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 438,
                    "end": 440,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Active learning has been explored extensively for screening prioritisation and automatic assessment [1, 10, 37, 56]. However, the main drawbacks of active learning are that a poor initial ranking will slow down the rate of learning, and that explicit human effort is required to update the ranking. While current practice prescribes all documents must be screened (therefore explicit assessments could be used for active learning), an initially poor ranking would require many assessments before the system is able to identify relevant documents. Thus the analysis of the full-text of eligible documents may be delayed. Automatic assessment has been suggested to be used in place of a second researcher performing screening [40]. Fully automatic methods of screening prioritisation allow for other processes of systematic reviews to begin earlier and do not require the effort of humans, saving more time (and costs). In this paper, we do not consider screening prioritisation methods based on active learning. However, we note that CLF could be used as the first pass ranking in the context of an active learning method. Then, active learning could be used to augment CLF to performing re-ranking in the presence of continuous, iterative relevance feedback. We leave the study of CLF in an active learning setting for future work.",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 102,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 104,
                    "end": 106,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 108,
                    "end": 110,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 112,
                    "end": 114,
                    "mention": "56",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 725,
                    "end": 727,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The CLEF Technology Assisted Reviews (TAR) track [25, 26] considers both screening prioritisation and stopping prediction tasks. The screening prioritisation task has gained substantial interest from CLEF participants, with submitted methods including active learning [12, 13], relevance feedback [4, 18, 21, 36, 38, 39, 52, 55], automatic supervised [9, 17, 30, 47, 51], and automatic unsupervised methods (which do not rely on any relevance feedback or human intervention) [2, 3, 7, 54]. Meanwhile, the stopping prediction task has seen little participation and na\u00efve techniques like static score-based cut-offs [24], as well as techniques based on continuous relevance feedback [16] are used. Many of the participants also do not use the Boolean queries directly, instead resorting only to the title of the review (a sentence), which is contrived and unrealistic in the context of systematic review literature search. This work overcomes these shortcomings by only using the Boolean query to rank documents, with no additional effort required by the information specialist.\n",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 52,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 54,
                    "end": 56,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 269,
                    "end": 271,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 273,
                    "end": 275,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 298,
                    "end": 299,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 301,
                    "end": 303,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 305,
                    "end": 307,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 309,
                    "end": 311,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 313,
                    "end": 315,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 317,
                    "end": 319,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 321,
                    "end": 323,
                    "mention": "52",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 325,
                    "end": 327,
                    "mention": "55",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 352,
                    "end": 353,
                    "mention": "9",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 355,
                    "end": 357,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 359,
                    "end": 361,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 363,
                    "end": 365,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 367,
                    "end": 369,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 476,
                    "end": 477,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 479,
                    "end": 480,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 482,
                    "end": 483,
                    "mention": "7",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 485,
                    "end": 487,
                    "mention": "54",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 615,
                    "end": 617,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 682,
                    "end": 684,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Several approaches to ranking documents retrieved by Boolean queries were proposed in the \u201880s and \u201890s outside of the context of systematic review creation. Most of these approaches rely on users explicitly weighting terms in the query [42], probabilistic retrieval using fuzzy set theory [6, 41] and term dependencies [15]. A drawback of these methods is their heavy reliance on the users to impose a ranking over retrieved documents (e.g., the requirement that users must specify individual term weightings). Users often are unable to provide such weights, or it creates an additional hindrance in using the retrieval system.",
            "cite_spans": [
                {
                    "start": 238,
                    "end": 240,
                    "mention": "42",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 291,
                    "end": 292,
                    "mention": "6",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 294,
                    "end": 296,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 321,
                    "end": 323,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "A ranking function for Boolean queries which relies solely on the structure of the Boolean query, without further user intervention, is Coordination Level Matching (CLM) [31]. The intuition behind CLM is that nested sub-clauses of a Boolean query could be considered as separate but related queries, and therefore documents that appear in multiple clauses should be ranked higher. For example, a very common way information specialists formulate Boolean queries for systematic review literature search is to break a search down into three of four categories based on the Population, Intervention, Controls, Outcomes (PICO) framework [8]. Query terms from each category become a clause in the Boolean query, grouped together by a single AND operator [8]. Formally, in CLM the score of a document d is the number of Boolean clauses of the query Q that are satisfied by it. A clause can be considered as both a single atomic keyword, and the grouping of several keywords or other nested groupings by a single Boolean operator (Boolean clause). Figure 1 visualises the differences between atomic clauses and Boolean clauses.",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 173,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 634,
                    "end": 635,
                    "mention": "8",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 750,
                    "end": 751,
                    "mention": "8",
                    "ref_id": "BIBREF54"
                }
            ],
            "section": "Related Work",
            "ref_spans": [
                {
                    "start": 1048,
                    "end": 1049,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Rankings produced by CLM typically perform poorly (as supported by our empirical findings in Sect. 5.1). This is because the amount of information about the query being exploited to produce a document ranking is low. CLM has been noted to be more effective when weighting occurrences of documents by, for example, IDF or TF-IDF [14]. Which weighting scheme to use for CLM is then unclear, and some documents may be ranked higher than others using different weighting schemes. Moreover, when computing scores, CLM does not account for the different Boolean operators present in the query, i.e., scores are summed in the same manner irrespective of the operator used, e.g., AND, OR.",
            "cite_spans": [
                {
                    "start": 329,
                    "end": 331,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The CLF method proposed in this paper exploits rank fusion [49], i.e., the combination of multiple document rankings, typically returned by different systems or weighting schemes for the same query (although recent work has applied fusion to different query variations [5]). There are many methods for fusion of rankings, and they can be classified into two main categories [22]: score-based [49] and rank-based [32]. Score-based methods fuse rankings using the original scores of documents in different rankings to infer the new fused ranking. As systems and weighting schemes will typically assign wildly different scores to documents, scores are often normalised before fusion (e.g., using min-max normalisation). Rank-based methods fuse rankings using only the rank positions of documents (similarly to electoral vote fusion [32]).",
            "cite_spans": [
                {
                    "start": 60,
                    "end": 62,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 270,
                    "end": 271,
                    "mention": "5",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 375,
                    "end": 377,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 393,
                    "end": 395,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 413,
                    "end": 415,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 830,
                    "end": 832,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The novelty of our contribution is that by combining insights from decades-old research about ranking documents directly with Boolean queries with relatively more recent research about the fusion of ranked lists, significant gains in effectiveness can be obtained.\n",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "We assume that a set R of rankings \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_1, r_2, \\ldots , r_k$$\\end{document} is available for each atomic Boolean clause (i.e., a term in the Boolean query, see Fig. 1) . These rankings could be produced by any weighting scheme available, e.g., IDF, BM25, etc. A ranking is an ordered list of documents: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r ={<}d_0,d_1, ..., d_k{>}$$\\end{document} with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s(d_i,r_j)$$\\end{document} representing the score of document \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_i$$\\end{document} within ranking \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_j$$\\end{document}. In CLF, these rankings are recursively fused, first at an atomic clause level, then at the level of (often nested) Boolean operators, until the highest level of the Boolean query is considered (typically represented by an AND operator): at this level, rankings are again fused together to produce a single, final ranking. This is achieved by applying the CLF fusion function to each document d as:1where R is the set of rankings associated with the clauses of the Boolean query considered at the current level, and T is the type of Boolean operator applied. In this work, we consider T as being either identifying an atomic clause, or the AND and OR operators. The queries we consider do not have NOT clauses (therefore we do not have a fusion method for this operator). According to Eq. 1, CLF performs CombSUM fusion [49] if the Boolean clause is AND (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T=$$\\end{document} AND). Likewise, CombMNZ fusion [49] is used when dealing with atomic clauses or the OR operator. Figure 2 visualises how fusion is performed for different Boolean clauses. When scoring exploded MeSH terms, the score provided by a weighting scheme is the summed score of each child in the subsumption (similar for phrases). Both CombSUM and CombMNZ boost the documents which multiple rankers estimate to be highly relevant (i.e., the chorus effect), however CombMNZ at the OR and atomic levels is used to combat less accurate estimates of relevance (i.e., the dark horse effect). That is, documents where only a single ranker estimates them as highly relevant are not boosted.\n",
            "cite_spans": [
                {
                    "start": 2632,
                    "end": 2634,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 2986,
                    "end": 2988,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                }
            ],
            "section": "Producing a Ranking ::: Coordination Level Fusion",
            "ref_spans": [
                {
                    "start": 432,
                    "end": 433,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 3058,
                    "end": 3059,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "The task of stopping prediction in systematic review literature search is that: given a ranking of the set of documents retrieved by the Boolean query, at what position should screening stop? We model this task with an equivalent description: given a set of documents retrieved by a Boolean query, what is the subset of documents which does not need to be screened? In this work, stopping prediction is performed by exploiting the scores of documents for each atomic term after fusion. Rather than setting a fixed cut-off on scores similar to participants in the CLEF TAR task [24], here a gain-based approach is used. Our approach is as follows: Given that researchers will screen documents starting at the first document and continuing to the next document for the entire list, they are accumulating gain from documents (equal to the document score) as they continue down the list of documents. Once enough gain from documents has been accumulated, they can stop screening. To model this, we use a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} parameter to control what percentage of the total gain a researcher can accumulate before stopping. The stopping point therefore becomes the position of the document in the ranked list where the cumulative gain exceeds the total allowable gain. When \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} is set to 1, no documents are discarded. In the task of screening prioritisation, where documents are assessed, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} is set to 1.",
            "cite_spans": [
                {
                    "start": 578,
                    "end": 580,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Stopping Prediction ::: Coordination Level Fusion",
            "ref_spans": []
        },
        {
            "text": "Evaluation is performed differently depending on the task. For the screening prioritisation task, rank-based measures are used. For comparison between the CLEF TAR participants (of which we acquired the runs), the MAP measure is included. The nDCG measure is included as a more realistic model of user behaviour. Reciprocal rank (RR) is used to demonstrate the effectiveness of systems in an active learning scenario (to show how soon the first relevant document would be shown and an update to the ranking potentially triggered). Precision after R documents (Rprec) is used to show the theoretical best possible precision obtainable in the stopping task, along with last relevant (Last Rel) that reports at what rank position the very last relevant document was shown. Participant runs are chosen for comparison if they are a fully automatic, unsupervised method, which does not use the training data or explicit relevance feedback, and do not set a threshold (as categorised in the TAR overview papers [25, 26]). Note that the tables in the CLEF TAR overview papers contain errors regarding these aspects, instead each of the participant\u2019s papers were considered to individually determine which runs to directly compare our methods to. For the stopping prediction task, several standard set-based measures are used: precision, recall, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_{\\beta =\\{0.5,1,3\\}}$$\\end{document}, total cost, and reliability [11]. Reliability is a loss measure (i.e., where smaller values are better) specifically designed for the TAR task. It has two components: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$loss_r=1-(\\text {recall})^2$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$loss_e=(n/(R+100)*100/N)^2$$\\end{document}, where n is the number of documents retrieved, N is the size of the collection, and R is the total number of relevant documents. Therefore, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Reliability}=loss_r+loss_e$$\\end{document}. Participants runs are chosen if they are fully automatic, supervised or unsupervised (thus we consider approaches that used training data), do not use explicit relevance feedback, and do set a threshold. Runs are evaluated using trec_eval or the evaluation scripts that are provided by the CLEF TAR organisers, where applicable.",
            "cite_spans": [
                {
                    "start": 1005,
                    "end": 1007,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1009,
                    "end": 1011,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1675,
                    "end": 1677,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Evaluation ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "When used for predicting when to stop screening, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} is tuned on training queries using a grid search to determine the best value. The parameter space searched in these experiments is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 0.9, 0.95\\}$$\\end{document}. Note that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} can be set at a clause-level, therefore it is possible for it to be adaptive based on the clause. We leave learning an adaptive \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} for future work, and here we fix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} to a set value across all clauses.",
            "cite_spans": [],
            "section": "Evaluation ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "Tables 1 and 2 present the results of the screening prioritisation task for the 2017 and 2018 CLEF TAR collections. Comparing CLM to CLF (without query expansion), CLF is statistically significantly better than CLM in all of the evaluation measures presented in both 2017 and 2018 tables (using a two-tails t-test where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.05$$\\end{document}). Comparing the CLM and CLF methods to the state-of-the-art PubMed ranking, CLM is often statistically significantly worse than the PubMed ranker, whereas some CLF-based methods are able to perform statistically significantly better than the PubMed method. Next, the best performing CLF method (CLF+weighting+PubMed+qe) and the best performing CLEF participant method for each year is compared. For 2017 topics, the best performing methods are Sheffield-run-2 (documents ranked with TF-IDF vector space model using terms from topic title and terms extracted from the Boolean query) and Sheffield-run-4 (same as Sheffield-run-2 except a PubMed stopword list is used) [3]. The CLF method does not perform statistically significantly better than these two methods in any evaluation measure considered (however in all measures apart from MAP and last relevant, CLF is better). For 2018 topics, the best performing method is Sheffield-general-terms (same as Sheffield-run-4 from 2017, however terms specifically designed to identify systematic reviews are added to the query) [2]. Comparing this method to CLF, the CLF method performs statistically significantly better in RR (and has gains in all evaluation measures apart from last relevant). Overall, CLF is able to obtain the highest MAP overall for 2018 topics, and the highest overall nDCG, RR, and Rprec for both 2017 topics and 2018 topics, performing statically significantly better than the state-of-the-art PubMed ranker.\n\n\n\n",
            "cite_spans": [
                {
                    "start": 1279,
                    "end": 1280,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1684,
                    "end": 1685,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Screening Prioritisation ::: Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 13,
                    "end": 14,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Tables 3 and 4 present the results of the stopping prediction task using the cut-off parameter \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document}. A \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} value of 0.4 through parameter tuning on training data was found to provide the least loss in Reliability, and was therefore chosen for the test queries for both 2017 and 2018. Results of the parameter tuning process on the training portion of the CLEF 2017 and 2018 topics are presented in Fig. 4. The CLF method used in this task was CLF+weighting+PubMed+qe as it obtained the highest performance on the screening task.",
            "cite_spans": [],
            "section": "Stopping Prediction ::: Results",
            "ref_spans": [
                {
                    "start": 980,
                    "end": 981,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 7,
                    "end": 8,
                    "mention": "3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 13,
                    "end": 14,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "Examining first Table 3, CLF obtains the highest precision, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_1$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_0.5$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_3$$\\end{document}, and lowest loss in reliability. CLF also obtains the second-lowest total cost, and maintains both a low total cost and reliability for this set of queries. Losses in recall are within a tolerable threshold [11].Table 4, reveals similar results to the 2017 topics. Significant improvements over the original queries in terms of precision, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_1$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_{0.5}$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_3$$\\end{document}, and total cost, with a tolerable reduction in recall can be observed. However, the Reliability on this set of queries is higher (thus worse). Given that the total cost is low, this indicates that the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$loss_r$$\\end{document} component of Reliability does not decrease at the same rate as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$loss_e$$\\end{document} increases for these topics. There were no participants which contributed a comparable run to the 2018 TAR task, therefore no comparisons to other systems can be made for this collection.",
            "cite_spans": [
                {
                    "start": 1139,
                    "end": 1141,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Stopping Prediction ::: Results",
            "ref_spans": [
                {
                    "start": 22,
                    "end": 23,
                    "mention": "3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 1149,
                    "end": 1150,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "While there is a drop in recall, there are real monetary savings associated with the increase in precision. Across the 2017 and 2018 topics, the CLF method provides savings between approximately USD$5000 and USD$12,000, according to estimates reported by McGowan et al. [35] when considering double screening.\n",
            "cite_spans": [
                {
                    "start": 271,
                    "end": 273,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Stopping Prediction ::: Results",
            "ref_spans": []
        },
        {
            "text": "In this paper, a novel approach to ranking documents for systematic review literature search using rank fusion applied to coordination level matching was presented. The method, dubbed Coordination Level Fusion (CLF), outperformed the current state of the art for two different tasks. For the screening prioritisation task, CLF significantly outperformed the existing PubMed ranking system, as well as participants that submitted comparable runs to the CLEF TAR tasks. The results of the screening prioritisation task demonstrate the applicability of CLF to systematic review literature search when prioritisation is considered, and suggest it may also be applied to obtain an effective early ranking in settings that consider active learning. For the stopping prediction task, CLF could significantly reduce the cost of screening with tolerable losses in recall. The results of the stopping prediction task demonstrate the applicability of CLF to specific systematic reviews where total recall is not essential, such as in rapid reviews [34].",
            "cite_spans": [
                {
                    "start": 1038,
                    "end": 1040,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        },
        {
            "text": "There are many aspects about CLF that require further investigation. First, we propose to study the effectiveness of CLF within an active learning setting. In this context, CLF can be used as the first ranker, before relevance feedback is collected. Then, feedback could be further weaved into CLF by devising and integrating weighting schemes that account for this. We also plan to investigate the use of CLF as a method for query performance prediction (e.g., as a post-retrieval predictor using reference lists [50], or as a candidate selection function in query transformation chain frameworks [48]). In terms of extending CLF, the weighting schemes themselves can be weighted (i.e., one weighting scheme may have more importance over others); e.g., using the linear combination fusion method [53] which assigns weights to each ranker being fused. The problem then is learning the weight to assign to each weighting scheme (ranker) used for rank fusion. Rather than using fusion methods like CombMNZ, it is foreseeable to use a different combination of weights for each Boolean clause considered.",
            "cite_spans": [
                {
                    "start": 515,
                    "end": 517,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 599,
                    "end": 601,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 798,
                    "end": 800,
                    "mention": "53",
                    "ref_id": "BIBREF48"
                }
            ],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Results for CLEF TAR 2017. The first row of results is obtained by issuing queries to PubMed, the next set of rows is are results of the various configurations of CLF, and the last set of rows are the relevant runs from participants for that year. Two-tailed t-test between the PubMed ranker and the other methods with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.05$$\\end{document} is indicated by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$*$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.01$$\\end{document} by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\dag $$\\end{document}.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Results for CLEF TAR 2018. Presentation of results and statistical significance is indicated the same was as in Table 1.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Results of CLF for stopping prediction for CLEF TAR 2017. The first row are the results from the original queries, the second row is when CLF with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa =0.4$$\\end{document}. Two-tailed t-test between the original results and the other methods with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.05$$\\end{document} is indicated by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$*$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.01$$\\end{document} by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\dag $$\\end{document}.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Results of CLF for stopping prediction for CLEF TAR 2018. The first row are the results from the original queries, the second row is when CLF with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa =0.4$$\\end{document}. Significance is indicated the same as in Table 3.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Types of clauses in a Boolean query. Dashed lines surround Boolean clauses, dotted lines surround atomic clauses.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Bottom-up visualisation of the fusion of ranked lists using the CLF method. First one or more ranked lists of an atomic clause are fused, then the results of each Boolean clause are fused. Each clause that has fusion applied to is encapsulated in a dashed box. The nested clauses which it encapsulates are included inside it. Each applicable fusion method is labelled within each respective box. Note that all atomic clauses use the same range of weighting schemes: in this figure only one is shown for space reasons.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Example query formatted to be issued to PubMed for re-ranking. Constructing the query like above ensures only the documents specified (e.g., document number 23593613) are retrieved, and therefore re-ranked.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 4.: Tuning the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} parameter on the training portions of the 2017 (left) and 2018 (right) CLEF TAR topics. Lowest value for both plots is 0.4.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Reducing workload in systematic review preparation using automated citation classification",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hersh",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Peterson",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yen",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "JAMIA",
            "volume": "13",
            "issn": "2",
            "pages": "206-219",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Exploiting the similarity of non-matching terms at retrieval time",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Crestani",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Inf. Retrieval",
            "volume": "2",
            "issn": "1",
            "pages": "27-47",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1009973415168"
                ]
            }
        },
        "BIBREF6": {
            "title": "Boolean queries and term dependencies in probabilistic retrieval models",
            "authors": [
                {
                    "first": "WB",
                    "middle": [],
                    "last": "Croft",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "J. Am. Soc. Inf. Sci.",
            "volume": "37",
            "issn": "2",
            "pages": "71-77",
            "other_ids": {
                "DOI": [
                    "10.1002/(SICI)1097-4571(198603)37:2<71::AID-ASI3>3.0.CO;2-4"
                ]
            }
        },
        "BIBREF7": {
            "title": "A study of an automatic stopping strategy for technologically assisted medical reviews",
            "authors": [
                {
                    "first": "GM",
                    "middle": [],
                    "last": "Di Nunzio",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "672-677",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Best match: new relevance search for pubmed",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Fiorini",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "PLoS Biol.",
            "volume": "16",
            "issn": "8",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pbio.2005343"
                ]
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Comparing rank and score combination methods for data fusion in information retrieval",
            "authors": [
                {
                    "first": "DF",
                    "middle": [],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Taksa",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Inf. Retrieval",
            "volume": "8",
            "issn": "3",
            "pages": "449-480",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-005-6994-4"
                ]
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "Boolean versus ranked querying for biomedical systematic reviews",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Karimi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pohl",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Scholer",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cavedon",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zobel",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "BMC MIDM",
            "volume": "10",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/1471-2210-10-1"
                ]
            }
        },
        "BIBREF20": {
            "title": "Learning-to-rank and relevance feedback for literature appraisal in empirical medicine",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lagopoulos",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anagnostou",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Minas",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tsoumakas",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction",
            "volume": "",
            "issn": "",
            "pages": "52-63",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Probabilistic retrieval and coordination level matching",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Losee",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "J. Am. Soc. Inf. Sci.",
            "volume": "38",
            "issn": "4",
            "pages": "239-244",
            "other_ids": {
                "DOI": [
                    "10.1002/(SICI)1097-4571(198707)38:4<239::AID-ASI4>3.0.CO;2-6"
                ]
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials",
            "authors": [
                {
                    "first": "IJ",
                    "middle": [],
                    "last": "Marshall",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kuiper",
                    "suffix": ""
                },
                {
                    "first": "BC",
                    "middle": [],
                    "last": "Wallace",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "J. Am. Med. Inform. Assoc.",
            "volume": "23",
            "issn": "",
            "pages": "193-201",
            "other_ids": {
                "DOI": [
                    "10.1093/jamia/ocv044"
                ]
            }
        },
        "BIBREF27": {
            "title": "Rapid reviews may produce different results to systematic reviews: a meta-epidemiological study",
            "authors": [
                {
                    "first": "IJ",
                    "middle": [],
                    "last": "Marshall",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Marshall",
                    "suffix": ""
                },
                {
                    "first": "BC",
                    "middle": [],
                    "last": "Wallace",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Brassey",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J. Clin. Epidemiol.",
            "volume": "109",
            "issn": "",
            "pages": "30-41",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jclinepi.2018.12.015"
                ]
            }
        },
        "BIBREF28": {
            "title": "Systematic reviews need systematic searchers (IRP)",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "McGowan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sampson",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "J. Med. Libr. Assoc.",
            "volume": "93",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "Reducing systematic review workload through certainty-based screening",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Miwa",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "O\u2019Mara-Eves",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ananiadou",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "JBI",
            "volume": "51",
            "issn": "",
            "pages": "242-253",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "Using text mining for study identification in systematic reviews: a systematic review of current approaches",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "O\u2019Mara-Eves",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "McNaught",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Miwa",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ananiadou",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Syst. Rev.",
            "volume": "4",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/2046-4053-4-5"
                ]
            }
        },
        "BIBREF35": {
            "title": "A probabilistic approach to information retrieval in systems with boolean search request formulations",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Radecki",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "J. Am. Soc. Inf. Sci.",
            "volume": "33",
            "issn": "6",
            "pages": "365-370",
            "other_ids": {
                "DOI": [
                    "10.1002/asi.4630330603"
                ]
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "Beyond medline: reducing bias through extended systematic review search",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Savoie",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Helmer",
                    "suffix": ""
                },
                {
                    "first": "CJ",
                    "middle": [],
                    "last": "Green",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kazanjian",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Int. J. Technol. Assess. Health Care",
            "volume": "19",
            "issn": "1",
            "pages": "168-178",
            "other_ids": {
                "DOI": [
                    "10.1017/S0266462303000163"
                ]
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF43": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "Query performance prediction using reference lists",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shtok",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Kurland",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Carmel",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ACM Trans. Inf. Syst. (TOIS)",
            "volume": "34",
            "issn": "4",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1145/2926790"
                ]
            }
        },
        "BIBREF46": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "Fusion via a linear combination of scores",
            "authors": [
                {
                    "first": "CC",
                    "middle": [],
                    "last": "Vogt",
                    "suffix": ""
                },
                {
                    "first": "GW",
                    "middle": [],
                    "last": "Cottrell",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Inf. Retrieval",
            "volume": "1",
            "issn": "3",
            "pages": "151-173",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1009980820262"
                ]
            }
        },
        "BIBREF49": {
            "title": "ECNU at 2018 eHealth task 2: technologically assisted reviews in empirical medicine",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Methods",
            "volume": "4",
            "issn": "5",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF50": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF51": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF52": {
            "title": "A general model of query processing in information retrieval systems",
            "authors": [
                {
                    "first": "DA",
                    "middle": [],
                    "last": "Buell",
                    "suffix": ""
                }
            ],
            "year": 1981,
            "venue": "Inf. Process. Manag.",
            "volume": "17",
            "issn": "5",
            "pages": "249-262",
            "other_ids": {
                "DOI": [
                    "10.1016/0306-4573(81)90019-4"
                ]
            }
        },
        "BIBREF53": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF54": {
            "title": "Systematic reviewing",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Clark",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Methods of Clinical Epidemiology",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF55": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}