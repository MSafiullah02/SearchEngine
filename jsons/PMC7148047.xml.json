{
    "paper_id": "PMC7148047",
    "metadata": {
        "title": "Crowdsourcing Truthfulness: The Impact of Judgment Scale and Assessor Bias",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "David",
                "middle": [],
                "last": "La Barbera",
                "suffix": "",
                "email": "labarbera.david@spes.uniud.it",
                "affiliation": {}
            },
            {
                "first": "Kevin",
                "middle": [],
                "last": "Roitero",
                "suffix": "",
                "email": "roitero.kevin@spes.uniud.it",
                "affiliation": {}
            },
            {
                "first": "Gianluca",
                "middle": [],
                "last": "Demartini",
                "suffix": "",
                "email": "g.demartini@uq.edu.au",
                "affiliation": {}
            },
            {
                "first": "Stefano",
                "middle": [],
                "last": "Mizzaro",
                "suffix": "",
                "email": "mizzaro@uniud.it",
                "affiliation": {}
            },
            {
                "first": "Damiano",
                "middle": [],
                "last": "Spina",
                "suffix": "",
                "email": "damiano.spina@rmit.edu.au",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "The credibility of information available online may vary and the presence of untrustworthy information has big implications on our safety online [5, 12, 15]. The recent increase of misinformation online is to be blamed on technologies that have enabled the next level of strategic politic propaganda. Social media platforms and their data allow for extreme personalization of content which makes it possible to individually customise information. Given that the majority of people access news from social media platforms [13] such strategies can be used towards the goal of influencing decision making processes [1, 14].",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 147,
                    "mention": "5",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 149,
                    "end": 151,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 153,
                    "end": 155,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 522,
                    "end": 524,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 613,
                    "end": 614,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 616,
                    "end": 618,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this constantly evolving scenario, it is key to understand how people perceive the truthfulness of information presented to them. To this end, in this paper we collect data from US-based crowd workers and compare it with expert annotation data generated by fact-checkers such as PolitiFact. Our dataset contains multiple judgments of truthfulness of information collected from several non-expert assessors to measure agreement levels and to identify controversial content. We also collect judgments over two different judgment scales and collect information about assessors\u2019 background that allows us to analyse assessment bias. The dataset we created is publicly available at https://github.com/KevinRoitero/crowdsourcingTruthfulness.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The results of our analysis indicate that: (1) crowd judgments can be aggregated to approximate expert judgments, (2) there is a political bias in crowd-generated truthfulness labels where crowd assessors tend to believe more to statements coming from speakers off the same political party they have voted for in the last election; and (3) there seems to be a preference for coarse-grained scales where crowd assessors tend to use the extreme values in the scale more often than other values.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Crowdsourcing has been previously used as a methodology in the context of information credibility research. For example, Zubiaga and Heng [17] looked at how tweet credibility can be assessed by means of Amazon MTurk workers in the context of disaster management. Their results show that it is difficult for crowd workers to properly assess the truthfulness of tweets in this context, but that the reliability of the source is a good indicator for trusted information. Kriplean et al. [6] analyse how volunteer crowdsourcing can be used for fact-checking by simulating the democratic process. The Fact-checking Lab at CLEF [3, 9] looks at this problem by defining the task of ranking sentences according to their need to be fact-checked. Maddalena et al. [7] focus on the ability of the crowd to assess news quality along eight different quality dimensions. Roitero et al. [10] use crowdsourcing to study user perception of fake news statements. As compared to previous studies looking at crowdsourcing for information credibility tasks, we look at bias in the data due to the assessor and the rating scale used to collected labels in the context of the truthfulness of statements by US politicians.",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 141,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 485,
                    "end": 486,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 623,
                    "end": 624,
                    "mention": "3",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 626,
                    "end": 627,
                    "mention": "9",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 755,
                    "end": 756,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 873,
                    "end": 875,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "In our study1 we use the PolitiFact dataset constructed by Wang [16]. This dataset contains 12800 statements by politicians with truth labels produced by expert fact-checkers on a 6-level scale: i.e., True, Mostly True, Half True, Barely True, False, and Lie.2 For this work, we selected a subset of 120 statements randomly sampled from the PolitiFact dataset to make sure that a balanced number of statements per class and per political party was included in the sample.",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 67,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Dataset Description ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "We crowdsourced 120 statements each judged by 10 distinct crowd workers across 400 HITs on Amazon MTurk asking US-based workers to label the truthfulness of statements from the dataset. Each HIT, rewarded $1.20 (i.e., $0.15 for each statement), consisted of 8 statements for which we asked an assessment either using the original 6-level scale (S6) or a 100-level scale (from 0 to 100) using a slider set by default at 50 (S100). The 8 statements contained 2 gold questions used to quality check the workers\u2019 responses by means of providing judgments consistent with the expert ground truth. Other than gold questions, each HIT contained 3 statements by Republican party speakers and 3 by Democratic party speakers. More than the judgments, crowd workers where also asked to provide a justification for each of their judgments, and a URL pointing to the source of information supporting their judgment. At the beginning of the HIT each worker was asked to complete a demographics questionnaire; it also included questions about their political orientation, used to classify crowd assessors as aligned to the US Democratic party (Dem) or the US Republican party (Rep).",
            "cite_spans": [],
            "section": "Crowdsourcing Setup ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Figure 1 shows the raw assessment score distributions given by crowd workers both for the S6 scale and the S100 scale. These results hint that workers tend to use fewer values than those available: the extremes of the scales are more used; for S100, the middle value (50) is also frequently used and some smaller peaks can be seen in correspondence of the multiples of 10. These outcomes are much less manifest in the aggregated scores (i.e., the arithmetic mean of the scores provided by the ten crowd workers judging the same statement), shown in Fig. 2: as usual, these are more evenly distributed. Also, values at the lower end of the scale are much less frequent. These outcomes suggest that perhaps a two- or three-level scale would be more appropriate for this task, although fine-grained scales have been successfully used in a crowdsourcing setting for relevance assessment [8, 11]. We intend to further address this issue in our future work also using the scale transformation techniques proposed by Han et al. [4].\n",
            "cite_spans": [
                {
                    "start": 884,
                    "end": 885,
                    "mention": "8",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 887,
                    "end": 889,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1022,
                    "end": 1023,
                    "mention": "4",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Judgment Distributions ::: Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 554,
                    "end": 555,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Figure 3 shows the crowd assessor labels as compared to expert judgment of truthfulness over both judgments scales. Crowd assessors seem able to distinguish among the different levels of the S6 scale as the median values are increasing following the expert assessments over the levels of the scale. A t-test comparing crowd assessor scores across expert judgment levels shows that crowd scores are significantly different (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.01$$\\end{document}) across all levels except for the class combinations Lie\u2013False, False\u2013Barely True, and Mostly True\u2013True. The crowd appears to be more lenient than experts in assessing the truthfulness of statements as scores for the lowest categories tend not to reach the bottom end of the scale in both S6 and S100. This suggests the need to align scales when used by crowd assessors and experts in order to identify misleading content using crowdsourcing. Overall, S6 seems more adequate than S100, not only because (as noted above) workers tend to use coarse-grained scales but also because the agreement with experts seems higher for S6: S100 presents wider boxplots and less separable categories, especially for the first three classes (Lie, False, Barely True).\n\n",
            "cite_spans": [],
            "section": "Crowd vs. Experts ::: Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Figure 4 shows how crowd assessors labelled statements as compared to ground truth expert labels based on their political background. We can see that crowd assessors who voted for the Rep party tend to assign higher truthfulness scores, especially for the Lie and False ground truth labels, showing how, on average, they believe to content more than crowd assessors who voted for the Dem party.",
            "cite_spans": [],
            "section": "Crowd Assessor Bias ::: Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "When comparing how crowd workers assess statements differently based on who the speaker is, we can observe that True statements obtain higher scores from crowd assessors who voted for the speaker\u2019s party. That is, Dem workers assigned an average score of 84.54 on S100 and 5.48 on S6 to True statements by Dem speakers and only 81.83 on S100 and 5.00 on S6 to True statements by Rep speakers. Rep workers assigned an average score of 81.89 on S100 and 5.35 on S6 to True statements by Rep speakers and only 73.24 on S100 and 4.73 on S6 to True statements by Dem speakers. While this is an expected behaviour, we also notice that Dem crowd assessors appear to be more skeptical than Rep crowd assessors by showing a lower average judgment score for untrue statements (e.g., Fig. 4, top row).",
            "cite_spans": [],
            "section": "Crowd Assessor Bias ::: Results",
            "ref_spans": [
                {
                    "start": 778,
                    "end": 779,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "In this paper we presented a dataset of crowdsourced truthfulness judgments for political statements and compared the collected judgments across different crowd assessors, judgments scales, and with expert judgments.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "Our results show that (1) crowd judgments, if properly aggregated, are comparable to expert ones (2) crowd assessors political background has an impact on how they label political statements: they show a tendency to be more lenient towards statements by politicians of the same political orientation as their own; and (3) crowd assessors seem to have a preference towards coarse-grained judgment scales for truthfulness judgements.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Most frequently used support URLs over both scales, with and without gold questions.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Individual score distributions: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_6$$\\end{document} (left, raw judgments), \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{100}$$\\end{document} (right, raw judgments). The red line shows the cumulative distribution of judgments. (Color figure online)",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Distribution of scores aggregated by mean: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_6$$\\end{document} (left), \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{100}$$\\end{document} (right). The red line shows the cumulative distribution of judgments. (Color figure online)",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Comparison of crowd labels with expert ground truth: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{6}$$\\end{document} (left), \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{100}$$\\end{document} (right).",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 4.: Comparison with ground truth for Dem workers (blue) and Rep workers (red): \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{6}$$\\end{document} (left), \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{100}$$\\end{document} (right). All statements (first row), Rep speaker statements (second row) and Dem speaker statements (third row). (Color figure online)",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Disinformation\u2019s spread: bots, trolls and all of us",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Starbird",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Nature",
            "volume": "571",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/d41586-019-02235-x"
                ]
            }
        },
        "BIBREF6": {
            "title": "Credibility in social media: opinions, news, and health information\u2014a survey",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Viviani",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pasi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Wiley Interdis. Rev.: Data Min. Knowl. Discov.",
            "volume": "7",
            "issn": "5",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Tweet, but verify: epistemic study of information verification on Twitter",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Soc. Netw. Anal. Min.",
            "volume": "4",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1007/s13278-014-0163-y"
                ]
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "Overview of the CLEF-2019 CheckThat! Lab: automatic identification and verification of claims",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction",
            "volume": "",
            "issn": "",
            "pages": "301-321",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "A survey of trust and reputation systems for online service provision",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "J\u00f8sang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ismail",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Boyd",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Decis. Support Syst.",
            "volume": "43",
            "issn": "2",
            "pages": "618-644",
            "other_ids": {
                "DOI": [
                    "10.1016/j.dss.2005.05.019"
                ]
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Overview of the CLEF-2018 CheckThat! lab on automatic identification and\u00a0verification of political claims",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction",
            "volume": "",
            "issn": "",
            "pages": "372-387",
            "other_ids": {
                "DOI": []
            }
        }
    }
}