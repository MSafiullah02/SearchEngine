{
    "paper_id": "b575fd33c74cb88c5fcb2256e06ef410ecf27f27",
    "metadata": {
        "title": "Attention-Based Graph Evolution",
        "authors": [
            {
                "first": "Shuangfei",
                "middle": [],
                "last": "Fan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Virginia Tech",
                    "location": {
                        "postCode": "24060",
                        "settlement": "Blacksburg",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "B",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Virginia Tech",
                    "location": {
                        "postCode": "24060",
                        "settlement": "Blacksburg",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Bert",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Virginia Tech",
                    "location": {
                        "postCode": "24060",
                        "settlement": "Blacksburg",
                        "country": "USA"
                    }
                },
                "email": "bhuang@vt.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Based on the recent success of deep generative models on continuous data, various new methods are being developed to generate discrete data such as graphs. However, these approaches focus on unconditioned generation, which limits their control over the generating procedure to produce graphs in context, thus limiting the applicability to real-world settings. To address this gap, we introduce an attention-based graph evolution model (AGE). AGE is a conditional graph generator based on the neural attention mechanism that can not only model graph evolution in both space and time, but can also model the transformation between graphs from one state to another. We evaluate AGE on multiple conditional graph-generation tasks, and our results show that it can generate realistic graphs conditioned on source graphs, outperforming existing methods in terms of quality and generality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "As a fundamental topic in graph modeling, graph generation has a long history that began as early as the 1950s [6] . However, most traditional methods rely on prior knowledge of the graph topology and are limited in capability of learning generative properties from observations. To solve this problem, researchers have recently been exploring trainable deep models for graph generation based on the effectiveness of graph neural networks-e.g., graph convolutional networks [14] -which have been applied to various kinds of data describing, for example, molecular chemicals for drug design and scientific publications for predicting citations [25, 31] . However, these approaches are unconditional generative models, which limits their control over the generating procedure and makes them unable to produce graphs in context. These limitations restrict the applicability of these approaches to real world settings where graphs transform from one state to another and evolve in dynamic network settings.",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 114,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 474,
                    "end": 478,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 643,
                    "end": 647,
                    "text": "[25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 648,
                    "end": 651,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Modeling graph evolution is an important task that can be applied to various practical applications. A model of graph evolution would be a powerful tool for both predicting the future and the transformation of networks. For example, a marketer aiming to post an advertisement on an online social network may only have access to short-hop ego networks around users, but they need to know how the information would spread into the extended network beyond these ego networks. In disease control and prevention, when an infectious disease emerges and starts to spread, it is important to understand how it may spread beyond the visible network. Because graph data represents real-world phenomena that is changing or incompletely observed, there are many other examples of problems that could benefit from new tools for modeling graph evolution. Yet existing methods lack the flexibility of deep generative models or the ability to condition on previous graph states.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To provide this missing capability, we introduce an attention-based graph evolution model (AGE). AGE is a model for conditional graph generation based on the attention mechanism that allows consideration of global information with parallel computation across all graph nodes. AGE adopts the encoder-decoder structure, where the encoder tries to learn the representation of conditioned graphs using a self-attention mechanism, and the decoder tries to generate the representation of the target graphs using the correlation with the conditioned graphs and also with itself. The decoder can thus capture both global and local information. This graph-conditioned generation framework greatly enriches the potential applications for graph generation. AGE can be used to model not only graph evolution in space and in time, but also the transformation between graphs from one state to another. To evaluate how AGE performs on this problem setting, we perform experiments on datasets in various areas. The experiment results in terms of both the evaluation metrics, show that AGE can not only generate extremely realistic graphs, but also has the strong ability to model the evolution of graphs as a powerful conditioned graph generative model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Graph generation is one of the core topics in graph analysis. Many methods have been proposed to solve this problem, which can be traced back to at least 1959 when Erd\u00f6s and R\u00e9nyi [6] first introduced the Erd\u00f6s-R\u00e9nyi (E-R) model for generating random graphs. The model is based on the assumption that each pair of nodes are connected with a fixed pre-defined probability. However, this assumption is not realistic in most real world networks. To mimic the structure of real graphs, Albert and Barab\u00e1si [2] proposed the preferential attachment model by further customizing the probability of each possible edge to be conditioned on current degrees of nodes. Separately, Airoldi et al. [1] proposed the mixedmembership stochastic block model (MMSB) to generate graphs that have a fixed number of communities based on a probability matrix to determine the possibility of a node pair from two communities been connected. This model is able to learn distributions from observed data, which makes it generate more useful random graphs based on basic assumptions. Other classical graph generative models include exponential random graph models (ERGMs) [21, 26] , the stochastic block model (SBM) [9] , the Watts-Strogatz model [29] , the Kronecker graph model [16] , and many more. These older approaches have limited ability to learn about graph distributions from collections of graphs. Recently, researchers also have proposed to use deep models to learn distributions for graph generation. These methods can be divided into two categories. Some of them are auto-regressive models, which generate the graph in a sequential manner. Examples of these are the DeepGMG model [18] and the GraphRNN model You et al. [31] . While some other methods are non-auto regressive models [22, 25] . Among them, many models are based on generative adversarial networks (GANs) [10] , which learn data distributions without explicitly defining a density function [3, 5, 7] . However, these deep models are either limited to generating small graphs with less than thirty nodes [18, 25] , or to generating specific types of graphs such as molecular graphs [5, 30] . More importantly, the overarching drawback of all these deep generative models is that they are unconditioned, which severely limits their applicability to real-world tasks.",
            "cite_spans": [
                {
                    "start": 180,
                    "end": 183,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 502,
                    "end": 505,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 684,
                    "end": 687,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1145,
                    "end": 1149,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1150,
                    "end": 1153,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1189,
                    "end": 1192,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1706,
                    "end": 1710,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1774,
                    "end": 1777,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1941,
                    "end": 1944,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1945,
                    "end": 1947,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1948,
                    "end": 1950,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 2054,
                    "end": 2058,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 2059,
                    "end": 2062,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 2132,
                    "end": 2135,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 2136,
                    "end": 2139,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "To further strengthen the power of graph generative models, Fan and Huang [7] proposed a conditioned model, which can generate graphs conditioned on discrete labels based on the conditional GAN frameworks [19, 20] . However, this approach cannot be applied to circumstances where we want to generate graphs conditioned on another graph, which the motivating case for graph evolution and graph transformation. Also Jin et al. [12] proposed a model with the junction tree encoder-decoder framework for graph to graph transformation. However, they only target the task of molecular optimization. We largely expand the applications of conditioned graph generative model to various interesting problems for graph transformation, such as predicting the graph evolution in space (e.g., how ego-networks would look if expanded to a larger radius) and predicting graph evolution in time (forecasting the changes of dynamic graphs).",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 77,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 205,
                    "end": 209,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 210,
                    "end": 213,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 425,
                    "end": 429,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "We define the prediction of graph evolution as taking an existing source graph with nodes (with or without label) as input and predicting, or generating, a transformed version of the graph, or target graph. The transformation can represent change over time in a dynamic graph, or expansion in space, such as how ego networks change as we expand out to more steps. Many powerful graph generation models are autoregressive, meaning they generate graphs by sequentially adding new nodes and evaluating relevant possible edges [18, 31] . We also adopt this approach and further incorporate an attention-based transformer [27] to process a source graph. We use an attention mechanism instead of a graph convolutional network [14] because attention models can overcome depth limitations of GCNs. Therefore, they can learn more powerful embeddings based on global context. We model the graph generation procedure as a sequential problem by adding new nodes one-at-a-time. Many other graph generative models such as GraphRNN [31] and DeepGMG [18] also use this same procedural structure; however, they all suffer from efficiency bottlenecks since the sequential procedure prohibits parallelization within instances during the training procedure. This drawback limits their applications on large graphs, especially for DeepGMG, which can only be applied to graphs with less than 30 nodes. To avoid these issues for large graphs with long sequences, we adopt the transformer framework, which instead processes the nodes ordered in a sequence in parallel while using the attention mechanism to incorporate information from all other nodes-even those far away in the sequence. By processing the nodes in parallel, we also significantly shorten the training time, making it much faster than other models.",
            "cite_spans": [
                {
                    "start": 523,
                    "end": 527,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 528,
                    "end": 531,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 617,
                    "end": 621,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 720,
                    "end": 724,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1017,
                    "end": 1021,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Attention-Based Graph Evolution Model"
        },
        {
            "text": "The architecture of AGE is illustrated in Fig. 1 . As in the standard transformer framework, AGE consists of two main components: an encoder E and a decoder D. The encoder learns hidden representations H s of source graphs through a multi-head attention mechanism (where N is the number of identical layers, we set N = 6 in the experiments). The decoder, which is an autoregressive model, then sequentially generates one new node at a time, with possible edges connecting to existing nodes (i.e., the nodes in source graphs and the ones generated previously) and also learns a hidden representations of the target graph H t . In our model, a graph is represented as G = (F , A, L) where F is the feature matrix of nodes in source graphs (if one is given), A is the adjacency matrix of source graphs, and L is the label matrix of the nodes in the graph (if labels are available). Among these three components, the adjacency matrix is essential. In some settings, we can leave out the features and labels if we do not have this information. The goal of AGE is therefore to learn a mapping from a source graph G sou to a target graph G tar .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 42,
                    "end": 48,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 670,
                    "end": 680,
                    "text": "(F , A, L)",
                    "ref_id": null
                }
            ],
            "section": "Attention-Based Graph Evolution Model"
        },
        {
            "text": "In AGE, the encoder and the decoder each have their own self-attention block, which is designed to learn high-level node representations based on other nodes within the same graph. In the encoder, the representation of node i in source graphs is updated based on the following rule:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Self Attention"
        },
        {
            "text": "where a i,j is the normalized weights the model learns between node v i and v j , h i is the hidden node feature of node i, N s is the number of nodes in the source graph, \u03c3 is a nonlinear activation and W s s is the linear transformation where the weights are learnable parameters separately instantiated for each attention step in the model. The edge weights between two nodes are computed based on the attention mechanism:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Self Attention"
        },
        {
            "text": "where a i,j is the normalized attention weight of e i,j , which is the attention weight of edge from node i to node j, W s and W s are linear transformations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Self Attention"
        },
        {
            "text": "To learn the correlations between the nodes in source graph and the ones to be generated by decoder, we apply a source-target (S-T) attention block after the self-attention operations. The representation of a predicted node j in generated graph is updated based on the learned embeddings of all nodes in the source graph using the following rule:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Source-Target Attention"
        },
        {
            "text": "where \u03c3 is a nonlinear activation function, W t s is a learnable linear transformation and a i,j is the normalized weights the model learned between node v i and v j . The edge weights between two nodes in different graphs are typically calculated in the same way as shown in Eq. 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Source-Target Attention"
        },
        {
            "text": "In AGE, the encoder takes in a source graph G sou represented by its initial representations G sou = [A s ; F s ] (we can leave out F s if it is not given) and maps it to a high-level embedding. Here A s and F s are the adjacency matrix and the feature matrix of the source graph, where the nodes are arranged in a breadthfirst-search (BFS) ordering. We concatenate the feature matrix if we have one. When available, we can use features to generate new node features in addition to the graph structure. In our experiments, we focus on undirected, unweighted graphs where the adjacency matrix A is a symmetric binary matrix with each element represents the connectivity of a pair of nodes, but our approach can be easily extended to both directed and weighted graphs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder"
        },
        {
            "text": "We use a fixed maximum number of nodes, N s for the source graph and N t for the target graph. AGE can learn about and generate structures with various sizes smaller than these maximums by ignoring isolated nodes in the generated graph. We also define a fixed minimum number of nodes for both source and target graphs to ensure that the input graph is not empty, and to ensure that there are some differences between the source and target graphs. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Encoder"
        },
        {
            "text": "The decoder is composed of several stacked attention modules that alternate selfattention and source-target attention layers. The input for the decoder includes two parts: the target graphs G tar and the learned embeddings H s of the source graph (provided by the encoder). The target graph G tar is represented by the shifted node representations (shifted to the right by one position): G tar = [A t ; F t ] (leaving out F t if it is not given), with a start token and an end token filled at the beginning and appended to the end to ensure that the decoder predicts the next node based on the previously generated set. Like the source graph, the nodes in the target graph are also arranged in a breadth-first-search (BFS) ordering at training time, and the model is expected to learn to generate BFS orders.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder"
        },
        {
            "text": "Given H s , the autoregressive decoder generates an output sequence of nodes one at a time, where each step is also conditioned on the previously generated nodes. The decoder maps the embedding to the space of adjacency matrices and space of label matrices (if the data has label information) to reconstruct the generated graphs. We use a generator which is a combination of a linear transformation and the sigmoid activation function to map H t to the adjacency matrix A t and we use a classifier which is a combination of a linear transformation and the softmax activation function to map H t to the label vectorL t :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder"
        },
        {
            "text": "For the predicted adjacency matrix, we use the binary cross-entropy loss function to measure the differences:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder"
        },
        {
            "text": "Moreover, if the data has the label information, we also added the loss on labels based on label smoothing using the KL divergence loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decoder"
        },
        {
            "text": "In this section, we compare AGE with other graph generation methods on various conditioned graph generation problems to demonstrate its wide applicability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In the following experiments, we extract 70% of the data as training set, 20% for the validation set and 10% for the test sets. We used six attention layers (N = 6) for both self-attention and source-target attention block and within each, we set the number of heads to eight.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Baselines. As we mentioned before, some other methods have been proposed to generate graphs using deep models. However, few of them can condition on existing graphs for general tasks. Therefore, we compare AGE against two categories of other relevant models. The first set consists of methods that can (or can be modified to) generate graphs conditionally, such as the Erd\u00f6s-R\u00e9nyi model (E-R) and the Barab\u00e1si-Albert (B-A) model. These generative models iteratively grow a graph, so they can start from an existing graph. The second set of more recent methods are unconditional graph generation models, such as the mixed-membership stochastic block models (MMSB), DeepGMG and GraphRNN, which include state-of-the-art deep generative models. Notice that due to the computational complexity of the DeepGMG model, we only perform experiments with it on small graphs. In our experiments, we train these models directly on the target graphs without the source graphs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Evaluation Metrics. We evaluate the generated graphs in two modes. First, we evaluate whether the distribution of generated target graphs is realistic, which captures how well the generative model captures variation in generated graphs. We compute the distances of the distributions of generated graphs and the target graphs using maximum mean discrepancy (MMD) [11] , following the evaluation procedure used by You et al. [31] . We compute MMD for four graph statistics: degree distribution, clustering coefficient distribution, node-label distribution (if labels are unavailable, the metric \"MMD label\" will be listed as N/A), and average orbit count statistics. A model that faithfully captures the conditional distribution over target graphs should have low MMD with the set of true target graphs. Secondly, we compute the similarity between the generated graph and the true target graph for each source graph. This metric evaluates the performance of conditional generation. We calculate the graph similarities using three graph kernels: the shortest path kernel [4] (GK st), the graphlet sampling kernel [24] (GK gs), and the SVM-\u03b8 kernel [13] (GK svm). A good conditional graph generator should generate graphs with high similarity to the true target graphs.",
            "cite_spans": [
                {
                    "start": 362,
                    "end": 366,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 423,
                    "end": 427,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1068,
                    "end": 1071,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1145,
                    "end": 1149,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Our first evaluation setting considers the graph evolution problem in space. In real-world networks, graph data is collected by subsampling from larger graphs. Due to resource constraints, data collection may not gather as large subsamples as needed. A generative model that can conditionally add nodes in a manner consistent with how graphs grow as one expands the subsample could enable larger analyses of semi-synthetic networks. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Evolution in Space"
        },
        {
            "text": "We test this problem setting on citation networks. The problem is to predict the expansion of ego networks with farther-hop neighbors. We used the Cora and Citeseer datasets [23] . We evaluated our models with different graph sizes. For small datasets (Cora small and Citeseer small), we extract one-hop (G sou = G 1 = {V 1 , E 1 }) and two-hop (G tar = G 2 = {V 2 , E 2 }) ego networks with 5 \u2264 |V 1 | \u2264 20 and 30 \u2264 |V 2 | \u2264 50 as the source and target graphs. For the large datasets (Cora and Citeseer), we extract two-hop (G sou = G 2 = {V 2 , E 2 }) and three-hop (G tar = G 3 = {V 3 , E 3 }) ego networks with 10 \u2264 |V 2 | \u2264 50 and 40 \u2264 |V 3 | \u2264 170 as the source and target graphs. Data construction for this problem is illustrated in Fig. 2 . The training data consists of graph pairs extracted from the datasets. The source graph G sou is the i-hop ego network where the initial embeddings is constructed by concatenating the adjacency matrix A s and the feature matrix F s (if F s is given). The target graphs G tar are the (i + 1)-hop ego networks of the same node v where the initial embeddings is constructed by concatenating the adjacency matrix A t and the feature matrix F t . Results are listed in Table 1 . (In all tables, values are rounded to two decimal places.) The metrics indicate that AGE is a strong graph generator in both its ability to mimic graph distributions and match the target graphs. Considering the evaluation of the distance between the distributions of generated graphs and target graphs, AGE achieves the best scores. AGE scores less than 0.1 MMD on all cases, with at least a 30% decrease compared to the second best method, GraphRNN on two datasets with different graph sizes. This result corroborates that, as a graph generative model, AGE can generate realistic graphs that appear to be from the same distribution as the true target graphs. Moreover, considering how well generated graphs match the specific target graphs, we also calculate the graph similarities between the generated graphs and the target graphs. The kernel similarity scores are normalized, so they range from 0 to 1. The graphs AGE generates consistently have the best similarity scores.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 740,
                    "end": 746,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1213,
                    "end": 1220,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Datasets."
        },
        {
            "text": "Many graph generation methods are designed for static graphs. However in practice, many networks are not static. Instead, they change and evolve over time, with the addition of new nodes and edges, such as in citation networks and collaboration networks, and also with the deletion of existing nodes and edges, such as in computer networks and social networks. [28] , the Bitcoin Networks [15] , and two citation networks in Physics: cit-HepPh and cit-HepTh [8] . We extract two-hop (G 2 = {V 2 , E 2 }) ego networks with 30 \u2264 |V 2 | \u2264 120 (or 20 \u2264 |V 2 | \u2264 50 for small data) at time t as the source graphs and the two-hop ego networks of the same node at time t+1 as the target graphs. Here, we have G t 2 \u2208 G t+1 2 , and the problem is to model how networks evolve (or grow) with actual time.",
            "cite_spans": [
                {
                    "start": 361,
                    "end": 365,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 389,
                    "end": 393,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 458,
                    "end": 461,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Graph Evolution in Time"
        },
        {
            "text": "We compare AGE with other graph generative models and the results are shown in Table 2 . The evaluation results show that AGE can accurately model the graph evolution or growth over time. We compute the distance between the distributions of generated graphs and target graphs, and, as before, AGE achieves the best scores among all the generative models regarding the realism of the generated graphs. Again, this is strong evidence that AGE can generate realistic graphs that appear to be from the same distribution of the target graphs. Considering the graph similarities between the generated graphs by all models and the target graphs, Table 2 shows that among all models, AGE is the only one that can reach similarity 0.9 for all three graph kernels, while the other methods cannot consistently score high across different kernels. This suggests some aspect of graph similarity is not satisfied by these other generation procedures. These results again demonstrate that AGE represents a significant step in our ability to model the evolution of graphs in time.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 79,
                    "end": 86,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 639,
                    "end": 646,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Graph Evolution in Time"
        },
        {
            "text": "To evaluate the performance of AGE on modeling the evolution of graphs with deletion, study cases where the source graphs evolves with not only addition of new nodes and edges, but also allows the deletion of existing nodes and edges. Datasets. We use the Computer Network dataset [17] , which is a network describing peering information inferred from Oregon route-views with nine different timestamps in total. We extract two-hop (G 2 = {V 2 , E 2 }) ego networks with 30 \u2264 |V 2 | \u2264 120 at the first and last timestamp, respectively, as the source and target graphs. In this experiment, we focus on the more difficult problem of modeling the evolution of graphs with deletion. The difference with the second experiment is that in this case, the condition G t 2 \u2286 G t+1 2 does not hold anymore. We compare AGE with other graph generative models, listing results in Table 3 . The evaluation results show that, even for this more complex problem, AGE still maintains a high-level performance compared to the other generative models in terms of both the realism of generated graphs and the similarity to the target ones. Therefore, together with the second experiment, we find that AGE is not only able to learn graph evolution through growth, but also the more complex setting of volatile evolution.",
            "cite_spans": [
                {
                    "start": 281,
                    "end": 285,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 865,
                    "end": 872,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Graph Evolution in Time with Deletion"
        },
        {
            "text": "In this work, we proposed attention-based graph evolution (AGE), a conditioned generative model for graphs based on the attention mechanism, which can model graph evolution in both space and time. AGE is capable of generating graphs conditioned on existing graphs. Our model can be useful for many applications in various domains, such as for predicting information propagation in social networks, disease control for healthcare, and traffic prediction in road networks. We model graph generation as a sequential problem, yet we are able to train AGE models in parallel by adopting the transformer framework. Our experimental results demonstrate that AGE is a powerful and efficient conditioned graph generative model, which outperforms all the other state-of-the-art deep generative models for graphs. In our several experiments on various datasets, AGE is to be able to adapt to various kinds of evolution or transformations between graphs, and it performs consistently well in terms of both the realism of its generated graphs and the similarity to ground-truth target graphs. Finally, AGE has a flexible structure that can be used to generate graphs with or without features and labels. This flexibility thus enables a wider range of applications by allowing it to model many forms of graph evolution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Mixed membership stochastic blockmodels",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "M"
                    ],
                    "last": "Airoldi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Fienberg",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "P"
                    ],
                    "last": "Xing",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "J. Mach. Learn. Res",
            "volume": "9",
            "issn": "",
            "pages": "1981--2014",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Statistical mechanics of complex networks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Albert",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Barab\u00e1si",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Rev. Mod. Phys",
            "volume": "74",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "NetGAN: generating graphs via random walks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bojchevski",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Shchur",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Z\u00fcgner",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "G\u00fcnnemann",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Shortest-path kernels on graphs",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "P"
                    ],
                    "last": "Kriegel",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Fifth IEEE International Conference on Data Mining",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "MolGAN: an implicit generative model for small molecular graphs",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "De Cao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kipf",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1805.11973"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "On random graphs I",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Erd\u00f6s",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "R\u00e9nyi",
                    "suffix": ""
                }
            ],
            "year": 1959,
            "venue": "Publicationes Math. Debrecen",
            "volume": "6",
            "issn": "",
            "pages": "290--297",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Labeled graph generative adversarial networks",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.03220"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Overview of the 2003 KDD cup",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gehrke",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ginsparg",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "ACM SIGKDD Explor. Newsl",
            "volume": "5",
            "issn": "2",
            "pages": "149--151",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A survey of statistical network models",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Goldenberg",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "X"
                    ],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Fienberg",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "M"
                    ],
                    "last": "Airoldi",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Found. Trends Mach. Learn",
            "volume": "2",
            "issn": "2",
            "pages": "129--233",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Generative adversarial nets",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2672--2680",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A kernel two-sample test",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gretton",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Rasch",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "J. Mach. Learn. Res",
            "volume": "13",
            "issn": "",
            "pages": "723--773",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Learning multimodal graph-to-graph translation for molecular optimization",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Barzilay",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Jaakkola",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Global graph kernels using geometric embeddings",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Johansson",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Jethava",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dubhashi",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bhattacharyya",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Semi-supervised classification with graph convolutional networks",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "N"
                    ],
                    "last": "Kipf",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Learning Representations (ICLR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "REV2: fraudulent user prediction in rating platforms",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hooi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Makhija",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Faloutsos",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Subrahmanian",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the ACM International Conferene on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "333--341",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Kronecker graphs: an approach to modeling networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chakrabarti",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Faloutsos",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ghahramani",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J. Mach. Learn. Res",
            "volume": "11",
            "issn": "",
            "pages": "985--1042",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Graphs over time: densification laws, shrinking diameters and possible explanations",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Faloutsos",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining",
            "volume": "",
            "issn": "",
            "pages": "177--187",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Learning deep generative models of graphs",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dyer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pascanu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Battaglia",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1803.03324"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Conditional generative adversarial nets",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mirza",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Osindero",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Conditional image synthesis with auxiliary classifier GANs",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Odena",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Olah",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shlens",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "2642--2651",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "An introduction to exponential random graph (p*) models for social networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Robins",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Pattison",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kalish",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lusher",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Soc. Netw",
            "volume": "29",
            "issn": "2",
            "pages": "173--191",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Designing random graph models using variational autoencoders with applications to chemical design",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Samanta",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "De",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ganguly",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gomez-Rodriguez",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1802.05283"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Collective classification in network data",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Namata",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bilgic",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Getoor",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Galligher",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Eliassi-Rad",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "AI Mag",
            "volume": "29",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Efficient graphlet kernels for large graph comparison",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shervashidze",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vishwanathan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Petri",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mehlhorn",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Borgwardt",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Artificial Intelligence and Statistics",
            "volume": "",
            "issn": "",
            "pages": "488--495",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "GraphVAE: towards generation of small graphs using variational autoencoders",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Simonovsky",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Komodakis",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1802.03480"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "New specifications for exponential random graph models",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "A"
                    ],
                    "last": "Snijders",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "E"
                    ],
                    "last": "Pattison",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "L"
                    ],
                    "last": "Robins",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Handcock",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Sociol. Methodol",
            "volume": "36",
            "issn": "1",
            "pages": "99--153",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "On the evolution of user interaction in Facebook",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Viswanath",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mislove",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cha",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "P"
                    ],
                    "last": "Gummadi",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the Workshop on Online Social Networks",
            "volume": "",
            "issn": "",
            "pages": "37--42",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Collective dynamics of small-world networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Watts",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Strogatz",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Nature",
            "volume": "393",
            "issn": "6684",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Graph convolutional policy network for goal-directed molecular graph generation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "You",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Pande",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "GraphRNN: generating realistic graphs with deep auto-regressive models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "You",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hamilton",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "5694--5703",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Springer Nature Switzerland AG 2020 H. W. Lauw et al. (Eds.): PAKDD 2020, LNAI 12084, pp. 436-447, 2020. https://doi.org/10.1007/978-3-030-47426-3_34",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The model architecture of AGE.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Data construction for graph evolution in space. The graph and matrix on the left represents the input source graph, which contains a portion of the full target graph on the right. The full target graph contains the adjacency and feature matrices of the source graph in this setting.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Comparison of AGE and other generative models on graph evolution in space using MMD evaluation metrics and graph kernel similarities.Degree Clustering Orbit Label GK st GKgs GKsvm Degree Clustering Orbit Label GK st GKgs GKsvm",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Comparison of AGE and other generative models on graph evolution in time using MMD evaluation metrics and graph kernel similarities.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Comparison of AGE and other generative models on graph evolution in time with deletion using MMD evaluation metrics and graph kernel similarities.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}