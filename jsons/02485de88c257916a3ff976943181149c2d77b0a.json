{
    "paper_id": "02485de88c257916a3ff976943181149c2d77b0a",
    "metadata": {
        "title": "An Empirical Model for n-gram Frequency Distribution in Large Corpora",
        "authors": [
            {
                "first": "Joaquim",
                "middle": [
                    "F"
                ],
                "last": "Silva",
                "suffix": "",
                "affiliation": {
                    "laboratory": "NOVA Laboratory for Computer Science and Informatics",
                    "institution": "",
                    "location": {
                        "settlement": "Caparica",
                        "country": "Portugal"
                    }
                },
                "email": ""
            },
            {
                "first": "Jose",
                "middle": [
                    "C"
                ],
                "last": "Cunha",
                "suffix": "",
                "affiliation": {
                    "laboratory": "NOVA Laboratory for Computer Science and Informatics",
                    "institution": "",
                    "location": {
                        "settlement": "Caparica",
                        "country": "Portugal"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Statistical multiword extraction methods can benefit from the knowledge on the n-gram (n \u2265 1) frequency distribution in natural language corpora, for indexing and time/space optimization purposes. The appearance of increasingly large corpora raises new challenges on the investigation of the large scale behavior of the n-gram frequency distributions, not typically emerging on small scale corpora. We propose an empirical model, based on the assumption of finite n-gram language vocabularies, to estimate the number of distinct n-grams in large corpora, as well as the sizes of the equal-frequency n-gram groups, which occur in the lower frequencies starting from 1. The model was validated for n-grams with 1 \u2264 n \u2264 6, by a wide range of real corpora in English and French, from 60 million up to 8 billion words. These are full nontruncated corpora data, that is, their associated frequency data include the entire range of observed n-gram frequencies, from 1 up to the maximum. The model predicts the monotonic growth of the numbers of distinct n-grams until reaching asymptotic plateaux when the corpus size grows to infinity. It also predicts the non-monotonicity of the sizes of the equal-frequency n-gram groups as a function of the corpus size.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The appearance of Web-scale corpora raised new challenges on the extraction of relevant expressions in natural languages, e.g. for indexing and time/space optimization, whose efficiency can benefit from the knowledge of the statistical regularities in real data. However, most studies only focus on single words, analyzing their occurrence frequencies. For example, function words such as \"the\", \"in\", \"of\", lacking semantic content and having a small and fixed vocabulary essentially related to a language grammar, tend to occur more often than words like \"oceanography\" or \"preferably\", whose appearance can be related to the semantic content of a text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "These studies should be extended with more generic approaches for the extraction of multiword expressions based on the properties of n-grams. An n-gram is a sequence of n \u2265 1 consecutive words, so, beyond single words, its characteristics can be related to the text phrases and sentences, e.g. \"History of Science\". In a given corpus one can observe distinct n-gram types, each one showing a certain number of instances. This requires an accurate estimation of the n-gram frequency distributions for any given corpus size, particularly important in Big Data extraction applications handling many Mega (10 6 ) and Giga (10 9 ) words.",
            "cite_spans": [
                {
                    "start": 601,
                    "end": 606,
                    "text": "(10 6",
                    "ref_id": null
                },
                {
                    "start": 618,
                    "end": 623,
                    "text": "(10 9",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We present a model that estimates, with good accuracy, the total numbers of distinct n-grams (1 \u2264 n) in real corpora for a wide range of sizes, in a given language. It also estimates the sizes of the frequency levels, i.e. the numbers of equal-frequency n-grams, for the extreme low frequencies, from the singletons onwards. The lower frequency n-grams are a significant proportion of the distinct n-grams across a wide range of corpora sizes, and a large part of the relevant expressions in a text. The model predicts the finite sizes of the n-gram vocabularies in a given language, from 1-grams to 6-grams. This range of n-gram sizes captures the most meaningful relevant expressions. It also predicts growth of the population of distinct n-grams towards asymptotic plateaux, for large enough corpus. The model also predicts that, for the lowest frequencies, the numbers of distinct n-grams with equal frequencies, instead of always growing with the corpus size, will present a non-monotonic behavior, reaching a maximum and then decreasing as corpus grows to infinity. Results were validated with full nontruncated data from English and French Wikipedia corpora from 60 Mega to 8 Giga words. We discuss background (Sect. 2), the model (Sect. 3), the results (Sect. 4) and conclusions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The empirical Zipf's Law [11] is a known model for word frequency distributions with a power law approximation in good agreement with data in a large range of frequencies, but significant deviations in the high and low frequencies. Most studies recognize difficulties for a generic model of the real data distributions in their entire frequency range [8] , and often do not consider the complete frequency distributions, e.g. an analysis of low frequency words is often omitted. These difficulties reinforce the importance of empirical approaches, leading to many models ( [1, 2, 5, 6, 10] , among others as surveyed in [7] ). Still, due to its simplicity and universality, Zipf's law is widely used, as a first approximation or as a basis for improvements. There is a lack of studies (e.g. [4] ) on n-grams (n > 1), carrying in their specifics a more focused semantic content, useful for relevant multiword extraction. Also, most studies are limited to corpora below a few million words. Due to the large orders of magnitude of the language vocabularies, much larger corpora are needed to investigate the n-gram behavior for n \u2265 1. There are recent studies on large corpora, [3, 9] but often exclude the lower n-gram frequencies, e.g., below 40, as in the 1.8 Tera word Google English n-gram corpus [3] for 1 \u2264 n \u2264 5, precluding model validation with real data.",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 29,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 351,
                    "end": 354,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 573,
                    "end": 576,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 577,
                    "end": 579,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 580,
                    "end": 582,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 583,
                    "end": 585,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 586,
                    "end": 589,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 620,
                    "end": 623,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 791,
                    "end": 794,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1176,
                    "end": 1179,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1180,
                    "end": 1182,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Background"
        },
        {
            "text": "We assume that the size of each language n-gram vocabulary, e.g., English, is practically fixed at each given temporal epoch, as new/old n-grams slowly emerge/disappear. For brevity, we omit an indication of the language L (English, French) and the n-gram size n (1..6) in the expressions but always assume each expression holds for a given (L, n) pair. Let V (L, n) (denoted as V ) be the language n-gram vocabulary size for each n-gram size (1 \u2264 n \u2264 6); and D(C; L, n) (denoted as D) be the number of distinct n-grams in a corpus of size C in language L, for each given n. We propose a model for estimating D that, as in growth and preferential attachment models [5, 10] , considers two processes: i) selecting new words from a language vocabulary; ii) or repeating existing words in a corpus. The model makes it explicit how the vocabulary finiteness influences the rate of appearance of new distinct n-grams as the corpus size grows. Regarding i) we follow [5] (whose complete model only applies to character-formed languages, e.g. Chinese) in the particular way those authors model the distinct n-grams from the language vocabulary that are still to appear in the corpus, represented by F 1 = (V \u2212 D)/V . Ratio F 1 monotonically decreases when the corpus grows, as the number of distincts (D) approaches the vocabulary size (V ). Regarding ii), ratio F 2 = C/D is the average number of occurrences per distinct n-gram. The larger F 2 , the stronger the tendency is for repeating existing n-grams in the corpus. Thus, we propose the rate of appearance of new distinct n-grams for each (L, n) to be \u221d F 1 \u00d7 1/F 2 , that is the outcome of multiplying F 1 by the reciprocal of F 2 . Assuming the validity of a continuum approximation, this rate corresponds to dD dC . Let K 1 > 0 be a real constant,",
            "cite_spans": [
                {
                    "start": 665,
                    "end": 668,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 669,
                    "end": 672,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 961,
                    "end": 964,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Estimating the Number of Distinct n-grams"
        },
        {
            "text": "with c 1 , c 2 as integration constants. As | V D | \u2265 1 and C > 0, with c 2 \u2212c 1 = ln(K 2 ),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Number of Distinct n-grams"
        },
        {
            "text": "Thus, the number of distinct n-grams for each (L, n) is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Number of Distinct n-grams"
        },
        {
            "text": "In Sect. 4, V , K 1 and K 2 are empirically determined for each (L, n) pair.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Number of Distinct n-grams"
        },
        {
            "text": "By Zipf's Law [11] , the frequency of the r th most frequent word in a corpus is",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 18,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Reviewing Zipf 's Law"
        },
        {
            "text": "where r is the word rank (from 1 to D(L, 1)) and \u03b1 is a constant close to 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reviewing Zipf 's Law"
        },
        {
            "text": "Observations in a wide range of large corpora show that the relative frequency of the most frequent 1-gram in English, \"the\", has small fluctuations around 0.06, being a fair approximation to its occurrence probability, p 1 . Thus, f (1) \u2248 p 1 C. From (2) , ln(f (r)) = ln(f (1)) \u2212 \u03b1 ln(r) so, ideally, ln(f (r)) decreases linearly with a slope \u03b1, as ln(r) increases. However, in general, real data show deviations from a straight line (e.g. Fig. 1 for real corpora: 62; 508; 8 600; in millions of words).",
            "cite_spans": [
                {
                    "start": 252,
                    "end": 255,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [
                {
                    "start": 442,
                    "end": 448,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Reviewing Zipf 's Law"
        },
        {
            "text": "The steps in the higher ranks in Fig. 1 correspond to equal-frequency words forming frequency levels (groups) of integer frequency k and size W (k). Only for the lowest k values starting from 1, there are frequency levels with multiple ranks. Figure 2 shows the log-log curve W (k) versus k (k \u2265 1), which can be approximated by a power law, but also exhibiting deviations from real data in both extremes of k [2, 6] . ",
            "cite_spans": [
                {
                    "start": 410,
                    "end": 413,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 414,
                    "end": 416,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 33,
                    "end": 39,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 243,
                    "end": 251,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Reviewing Zipf 's Law"
        },
        {
            "text": "Considering a generic level k, let r l k and r h k be its lowest and highest ranks. Thus, f (r l k ) = f (r h k ) = k. This model for estimating W (k) only applies to the higher ranks region of the real data distribution, as long as adjacent frequency levels have consecutive integer frequency values, that is f (",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Size W (k) of Each Frequency Level"
        },
        {
            "text": "We follow the functional structure of (2) due to its simplicity and assume, based on empirical observations, that it applies to n-grams n \u2265 1, with \u03b1 dependent on n for each language L, although we omit this in the expressions. In a first step, we assume an ideal straight line Zipf plot with slope \u03b1 z . In further steps, to address the Zipf plot deviations we model the dependencies of the \u03b1 parameter on the corpus size and the level frequency. Thus:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Size W (k) of Each Frequency Level"
        },
        {
            "text": "By analogy",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Size W (k) of Each Frequency Level"
        },
        {
            "text": "(4)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Size W (k) of Each Frequency Level"
        },
        {
            "text": "So, we can generalize (4) and (5), leading to:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Size W (k) of Each Frequency Level"
        },
        {
            "text": "where (k + m) : 1..k max with k max = f (1), and m is an integer offset starting from 0. With k = 1 in (6) we estimate the highest rank of level k + m for each m, as a function of rank r h1 , which is the number of distinct n-grams of size n in the corpus (D(C; L, n)). So, by subtracting r h k from r h k+1 , we estimate for each (L, n), the size W z (k) (subscript z denotes the Zipf assumption).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Estimating the Size W (k) of Each Frequency Level"
        },
        {
            "text": "To estimate W z (k), we first calculate D(C; L, n), requiring the K 1 and K 2 constants in (1), Sect. 4.1. Then we tune the \u03b1 z value that best fits the W z (1) value (by (7)) to the observed size W obs (k) of level k = 1 in a 508 million word corpus. Figure 2 shows the log-log curves of the Observed word frequency level sizes for different k for this corpus and the W z (k) estimates by (7) . The W obs (k) curve exhibits a regular decrease as k grows from 1 until the curve reaches a fluctuation zone, which becomes stronger for higher values of k (discussed ahead in Sect. 4.2). Before the fluctuation zone, the following dominant pattern is suggested: the deviation between the two curves is approximately proportional to ln(k). This leads us to an improved approximation, denoted by W (k), such that ln(W (k)) = ln(W z (k)) + \u03b2 ln(k), where \u03b2 is a real positive constant. Therefore Observed level sizes W z (k) estimates W(k) estimates after correction Fig. 2 . Word frequency level size W (k) vs k: observed and estimates by (7) and (8). \u03b2 is tuned, keeping the corpus fixed, to the best fit of W (k) to the observed level sizes. Curve W (k) estimates after correction by (8) is much closer to the observed one (Fig. 2 , Table 5 ). Similar behaviors were found for n-grams 1 < n \u2264 6.",
            "cite_spans": [
                {
                    "start": 390,
                    "end": 393,
                    "text": "(7)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 252,
                    "end": 260,
                    "text": "Figure 2",
                    "ref_id": null
                },
                {
                    "start": 960,
                    "end": 966,
                    "text": "Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 1219,
                    "end": 1226,
                    "text": "(Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 1229,
                    "end": 1236,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Estimating the Size W (k) of Each Frequency Level"
        },
        {
            "text": "Unlike the constant Zipf's \u03b1 z in (7), for real corpora, exponent \u03b1 in (2) depends both on the individual ranks of the distinct n-grams for each corpus and on the corpus size. Thus, W z (k) calculation in (7) should cope with the \u03b1 variation. Previously, the \u03b1 z value was tuned to fit W z (1), the size of level 1, in one of the available corpora (e.g. 508 million words corpus). This is a practical way to fit, with good approximation to the other frequency levels. The following are (corpus size; \u03b1 z ) pairs obtained, for 1-grams, for a set of different corpora: . These values show that \u03b1 z grows approximately an equal amount as the corpus size is doubled, suggesting a logarithmic proportionality between \u03b1 z and the corpus size. Let Q = log 2 (C 2 /C 1 ) and A = \u03b1 2 \u2212 \u03b1 1 with \u03b1 1 , \u03b1 2 associated, respectively, to C 1 , C 2 . Thus \u03bb \u0394Q = \u0394A, where \u03bb is a constant, so dA dQ = \u03bb and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Effect of the Corpus Size on the Level Size W (k)"
        },
        {
            "text": "where \u03b3 = \u03bb/ ln(2) and ct3 = ct2 \u2212 ct1. (ct 3 = 0 in the experiments.) This leads to (10) , with \u03b1 1 = \u03b1 z for some reference corpus with size C 1 . Any of the above (corpus size; \u03b1 z ) pairs can be used for this, e.g. for 1-grams in English, the C 1 and \u03b1 1 values of 508 571 317 and 1.155.",
            "cite_spans": [
                {
                    "start": 85,
                    "end": 89,
                    "text": "(10)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "The Effect of the Corpus Size on the Level Size W (k)"
        },
        {
            "text": "\u03b1(C) replaces \u03b1 z for calculating W z (k) in (7), now changing to W z (k, C):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Effect of the Corpus Size on the Level Size W (k)"
        },
        {
            "text": "which is reflected in W (k, C) of (8):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Effect of the Corpus Size on the Level Size W (k)"
        },
        {
            "text": "Equation (12) allows to predict the k-level size for n-grams 1 \u2264 n \u2264 6, given C. All expressions (1)-(12) apply to n-grams 1 \u2264 n \u2264 6 for corpora in a language L. The obtained \u03b1 z values are lower as the n-gram size increases from 1 to 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Effect of the Corpus Size on the Level Size W (k)"
        },
        {
            "text": "The corpora were built by random extraction of English Table 1 shows the corpora sizes and the distinct n-gram counts. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 55,
                    "end": 62,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Results"
        },
        {
            "text": "In order to find K 1 , K 2 and V (L, n) for a language L and an n-gram size, to obtain estimates by (1) we start by setting V (L, n) to 10 6 and successively increase it until the K 1 and K 2 values lead to the smallest relative error for two corpora of sizes close to the extremes of the corpora sizes range: 1/4 G and 4.3 G for English. Relative error is ((Est \u2212 Obs)/Obs) \u00d7 100%, for estimated (Est) and observed (Obs) numbers. This procedure stops when further increases of V (L, n) do not lead to significant changes in the relative error, and then that V (L, n) value is taken as an approximation to the n-gram vocabulary size. Table 2 shows K 1 and K 2 , and V (L, n) for English. In Table 3 , the left sub-column of each n-gram column shows acceptable values for the relative errors of the estimates D(C; L, n) by (1), in this range of corpora sizes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 634,
                    "end": 641,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 691,
                    "end": 698,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Predicting the Number of Distinct n-grams"
        },
        {
            "text": "The right sub-column shows the estimates of a Poisson based model given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Predicting the Number of Distinct n-grams"
        },
        {
            "text": "(1 \u2212 e \u2212f (r,L,C)) ); r is the n-gram rank and f (r, L, C)) is the expected frequency of r in corpus of size C by Zipf-Mandelbrot model (see [9] , where the parameters were tuned by the same procedure as described above in this section, for the empirical model). For n-gram sizes lower than 4, relative errors are considerably higher than the ones by D(C; L, n), e.g. reaching \u221231.1%, \u221224.5% and \u221212.4% for the largest corpus (8.6 Gw). For 5grams and 6-grams, relative errors are lower. Figure 3 shows that the curves for (1) predicts, e.g., about 99% of the distinct n-grams in each English n-gram vocabulary appear for C \u2248 6.7 \u00d7 10 12 words for 1-grams, and C \u2248 1.89 \u00d7 10 14 words for 6-grams. Table 4 shows the \u03b2 and \u03b3 values for calculating W (k, C) (12) and \u03b1(C) (10), 1 \u2264 n \u2264 6. Equation (12) provides a good approximation when the observed level sizes, W obs (k, C), decrease monotonically as k grows: W obs (k, C) > W obs (k + 1, C). For a fixed corpus size, that condition is not ensured when k exceeds a certain threshold, which is lower for smaller corpora and also for smaller ngram sizes. E.g., for the 62 Mw corpus the k threshold is 28 for 1-grams, and is 145 in the 8.6 Gw corpus for 2-grams. Above k threshold, due to W obs (k, C) non-monotonicity, model (12) only provides a rough approximation (Fig. 4) , in contrast to its good approximation in lower k. Table 5 shows error metrics for the W (k, C) estimates, considering the following basic set of k values: k \u2208 K, K = {1, 2, 3, 4, 5, 6, 7, 8, 16, 32, 64, 128}. Due to the k thresholds, the full set of k values was only used for the two corpora whose denoted sizes are above 4 Gw; the k value of 128 was not considered for the 1.1 Gw and 2.2 Gw corpora; 128 and 64 were not used for the 1/2 Gw and 1/4 Gw ones; 128, 64 and 32 were not considered for the remaining corpora, 1/8 Gw and 62 Mw. Table 5 shows two columns for each n-gram size: the left one indicates the average relative error, k\u2208K Err(k), where K is the set of k values used in the corpus as explained before, and Err(k) = |(W (k, C) \u2212 W obs (k, C))/W obs (k, C)|, which is the relative error (in its absolute value) for each k; each value in the right column, based on the root-mean-squared-deviation, is calculated as Figure 4 shows, e.g., for 1-grams and 3-grams, that the curves W (k, C) (\"Estimated\") are very close to the curves W obs (k, C) (\"Observed\") for each corpus. Likewise for other n-gram sizes (2, 4, 5, 6) . Beyond the k thresholds the observed curves W obs (k, C) enter non-monotonic fluctuation zones. The slope of the curves changes only slightly as the corpus size grows and the similar spacing between curves when C is doubled reflects a regular W obs (k, C) growth pattern.",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 144,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 2445,
                    "end": 2448,
                    "text": "(2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 2449,
                    "end": 2451,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 2452,
                    "end": 2454,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 2455,
                    "end": 2457,
                    "text": "6)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 487,
                    "end": 495,
                    "text": "Figure 3",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 696,
                    "end": 703,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1313,
                    "end": 1321,
                    "text": "(Fig. 4)",
                    "ref_id": null
                },
                {
                    "start": 1374,
                    "end": 1381,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 1863,
                    "end": 1870,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 2255,
                    "end": 2263,
                    "text": "Figure 4",
                    "ref_id": null
                }
            ],
            "section": "Predicting the Number of Distinct n-grams"
        },
        {
            "text": "Model (12) predicts that, for each of the lowest k values, W (k, C) grows with C until a maximum, then gradually decreases with increasing C (Fig. 5) . This results from the language vocabulary finiteness. E.g., for the singletons, W (1, C) keeps growing with C while n-grams remain to appear from the vocabulary. For a large enough corpus, the distinct n-gram plateau is reached (Fig. 3) and after this point, W (1, C) can not grow anymore. By further increasing C, the existing singleton n-grams will gradually move to the frequency level k = 2, until W (1, C) = 0, i.e. singletons disappear. Similar behavior is predicted for 1 < n \u2264 6 (Fig. 5 a) . By further increasing C, this process will affect successive k levels, e.g. k = 2 and so on, e.g., Fig. 5 b) . (k, C) ) and estimated (W (k, C)) (by (12)) values for different frequency levels and corpora sizes-English: a) 1-grams; b) 3-grams. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 141,
                    "end": 149,
                    "text": "(Fig. 5)",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 380,
                    "end": 388,
                    "text": "(Fig. 3)",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 639,
                    "end": 649,
                    "text": "(Fig. 5 a)",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 751,
                    "end": 760,
                    "text": "Fig. 5 b)",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 763,
                    "end": 769,
                    "text": "(k, C)",
                    "ref_id": null
                }
            ],
            "section": "The Evolution of the Frequency Level Sizes"
        },
        {
            "text": "Statistical extraction of relevant multiwords benefits with n-gram frequency distribution information from real corpora. This goes beyond the usual word frequency distribution (i.e. 1-grams) by including n-grams of sizes n > 1. This paper contributes with an empirical study on the n-gram frequency distribution from 1-grams to 6-grams, with large real corpora (English and French) from millions up to a few billion words. A distinctive aspect is that it analyzes the low frequency n-grams real data for such large corpora instead of relying on smoothing-based estimation. Low frequency n-grams represents the largest proportion of the distinct n-grams in a corpus for a wide range of corpora sizes, and are a significant part of the most relevant multiwords. This paper contributes with an empirical analysis and a model of the properties of the low frequency n-grams in large corpora, complementing studies on low frequency single words for smaller corpora. Assuming the finiteness of language n-gram vocabularies, we analyzed and modelled the total number of distinct n-grams for the above range of corpora. The model leads to good approximations to the real data distributions, with average relative errors of 5.6% and 2.9% respectively for the lower frequency n-gram distribution (namely the number of singleton n-grams), and the number of distinct n-grams. Moreover, the proposed model allows to predict the evolution of the numbers of distinct n-grams towards asymptotic plateaux for large enough corpora. Also, according to the model, the sizes of equal-frequency levels, for the lowest frequencies, initially grow with the corpus size until reaching a maximum and then decrease as the corpus grows to very large sizes. Overall, these results have practical implications for the estimation of the capacity of n-gram Big Data systems. Work is ongoing towards extending this empirical study up to hundreds of Giga word corpora.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A universal rank-size law",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ausloos",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cerqueti",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "PLoS ONE",
            "volume": "11",
            "issn": "11",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0166011"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Algorithmic information, complexity and Zipf's law",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "K"
                    ],
                    "last": "Balasubrahmanyan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Naranan",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Glottometrics",
            "volume": "4",
            "issn": "",
            "pages": "1--26",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Large language models in machine translation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brants",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Popat",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "J"
                    ],
                    "last": "Och",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Joint Conference on Empirical Methods in NLP and Computational Natural Language Learning",
            "volume": "",
            "issn": "",
            "pages": "858--867",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Multiword unit hybrid extraction",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dias",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "ACL Workshop on Multiword Expressions",
            "volume": "18",
            "issn": "",
            "pages": "41--48",
            "other_ids": {
                "DOI": [
                    "10.3115/1119282.1119288"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Deviation of Zipf's and heaps' laws in human languages with limited dictionary sizes",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "L\u00fc",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "K"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Sci. Rep",
            "volume": "3",
            "issn": "1082",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1038/srep01082"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "On the theory of word frequencies and on related Markovian models of discourse",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mandelbrot",
                    "suffix": ""
                }
            ],
            "year": 1953,
            "venue": "Structure of Language and its Mathematical Aspects",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A brief history of generative models for power law and lognormal distributions",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mitzenmacher",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Internet Math",
            "volume": "1",
            "issn": "2",
            "pages": "226--251",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Zipf's word frequency law in natural language: a critical review and future directions",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Piantadosi",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Psychon. Bull. Rev",
            "volume": "21",
            "issn": "5",
            "pages": "1112--1130",
            "other_ids": {
                "DOI": [
                    "10.3758/s13423-014-0585-6"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A theoretical model for n-gram distribution in big data corpora",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Silva",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gon\u00e7alves",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Cunha",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE International Conference on Big Data",
            "volume": "",
            "issn": "",
            "pages": "134--141",
            "other_ids": {
                "DOI": [
                    "10.1109/BigData.2016.7840598"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "On a class of skew distribution functions",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Simon",
                    "suffix": ""
                }
            ],
            "year": 1955,
            "venue": "Biometrika",
            "volume": "42",
            "issn": "3/4",
            "pages": "425--440",
            "other_ids": {
                "DOI": [
                    "10.2307/2333389"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Human Behavior and the Principle of Least-Effort",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "K"
                    ],
                    "last": "Zipf",
                    "suffix": ""
                }
            ],
            "year": 1949,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF1": {
            "text": "The observed word rank-frequency distributions",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "For a fair count of the distinct n-grams, still not modifying the text semantics, the corpora were pre-processed by separating words, through a space, from each of the following characters: {'<', '>', '\"', '!', '?', ':', ';', ',', '(', ')', '[', ']'}.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "61 \u00d7 10 \u221211 5.1 \u00d7 10 \u221211 2.66 \u00d7 10 \u221211 1.7835 \u00d7 10 \u221211 4.29 \u00d7 10 \u221212 6.5 \u00d7 10 \u221213 V 2.45 \u00d7 10 8 9.9 \u00d7 10 8 4.74 \u00d7 10 9 1.31 \u00d7 10 10 6.83 \u00d7 10 10 5.292 \u00d7 Numbers of distinct n-grams: observed and predicted (D(C; L, n), by(1)), versus the corpus size, in English.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Relative errors (%) of English distinct n-grams, estimated by: D(C; L, n), (1) (bold left col.); DistP oisson(L, n, C), 0.0 \u221226.8 \u22120.2 \u221219.7 0.0 \u221210.2 0.0 \u22126.6 0.2 \u22122.3 0.4 0.0 8.6 Gw \u22124.8 \u221231.1 \u22126.8 \u221224.5 \u22122.4 \u221212.4 \u22120.4 \u22127.8 3.3 \u22121.3 4.7 2.2 the observed and estimated values are quite close, for each n-gram size, across the analysed corpora. The predictions extend beyond the empirical corpora range, evolving to the n-gram vocabulary sizes plateaux. Equation",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Err(k) 2 and reflects how homogeneous the values of the relative error are for the different k values used in the estimates. The closer the left and right column values are, the greater the homogeneity. Global results exhibit homogeneity, showing also reasonably low average values. It should also be noted that the considered range of k values includes in all cases the lower frequency values, at least from 1 to 16.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "W (k, C) vs C: a) k = 1, 1-gram..6-gram; b) k \u2208 {1, 2, 3}, 1-gram, 3-gram.",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "The observed number of distinct n-grams for each corpus in English",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "K1, K2 and vocabulary sizes (V , in number of n-grams) for English",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Parameters \u03b2 and \u03b3 for each n-gram size and English",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Fig. 4. Observed (W obs",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}