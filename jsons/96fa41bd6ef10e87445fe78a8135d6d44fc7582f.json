{
    "paper_id": "96fa41bd6ef10e87445fe78a8135d6d44fc7582f",
    "metadata": {
        "title": "Generalized Multivariate Hawkes Processes",
        "authors": [
            {
                "first": "Tomasz",
                "middle": [
                    "R"
                ],
                "last": "Bielecki",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Illinois Institute of Technology Chicago",
                    "location": {
                        "postCode": "60616",
                        "region": "IL",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Jacek",
                "middle": [],
                "last": "Jakubowski",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Warsaw",
                    "location": {
                        "settlement": "Warszawa",
                        "country": "Poland"
                    }
                },
                "email": ""
            },
            {
                "first": "Mariusz",
                "middle": [],
                "last": "Niew\u0119g\u0142owski",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Technology",
                    "location": {
                        "settlement": "Warszawa",
                        "country": "Poland"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "This work contributes to the theory and applications of Hawkes processes. We introduce and examine a new class of Hawkes processes that we call generalized Hawkes processes, and their special subclass -the generalized multivariate Hawkes processes (GMHPs). GMHPs are multivariate marked point processes that add an important feature to the family of the (classical) multivariate Hawkes processes: they allow for explicit modelling of simultaneous occurrence of excitation events coming from different sources, i.e. caused by different coordinates of the multivariate process. We study the issue of existence of a generalized Hawkes process, and we provide a construction of a specific generalized multivariate Hawkes process. We investigate Markovian aspects of GMHPs, and we indicate some plausible important applications of GMHPs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Mathematics Subjects Classification (2010): 60G55, 60H99",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "A very interesting and important class of stochastic processes was introduced by Alan Hawkes in [11, 12] . These processes, called now Hawkes processes, are meant to model self-exciting and mutually-exciting random phenomena that evolve in time. The self-exciting phenomena are modeled as univariate Hawkes processes, and the mutually-exciting phenomena are modeled as multivariate Hawkes processes. Hawkes processes belong to the family of marked April 30, 2020",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 100,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 101,
                    "end": 104,
                    "text": "12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Generalized multivariate Hawkes processes 3 of 49 point processes, and, of course, a univariate Hawkes process is just a special case of the multivariate one. In this paper, which originates from Chapter 11 of [5], we define and study generalized multivariate Hawkes processes (GMHPs). These processes constitute a subclass of the family generalized Hawkes processes defined in this paper as well. In addition, we provide a novel construction of a generalized multivariate Hawkes process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "GMHPs are multivariate marked point processes that add an important feature to the family of the (classical) multivariate Hawkes processes: they allow for explicit modelling of simultaneous occurrence of excitation events coming from different sources, i.e. caused by different coordinates of the multivariate process. The importance of this feature is rather intuitive, and it will be illustrated in Section 6. In this regard, GMHPs differ from the multivariate Hawkes processes that were studied in Bremaud and Massouli [6] and Liniger [24] .",
            "cite_spans": [
                {
                    "start": 538,
                    "end": 542,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "We need to stress that we limit ourselves here to the case of linear GMHPs, that are counterpart of the linear classical Hawkes processes. That is to say, we do not study here what would be a counterpart of the nonlinear classical Hawkes processes. We refer to e.g. Chapter 1 in [33] for comparison of linear and nonlinear Hawkes processes. We also note that the generalized Hawkes processes introduced here should not be confused with those studied in [32] . In particular, we do not introduce any additional random factors, such as Brownian motions, into the compensators of the multivariate marked point process N showing in the Definition 3.1 below.",
            "cite_spans": [
                {
                    "start": 279,
                    "end": 283,
                    "text": "[33]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 453,
                    "end": 457,
                    "text": "[32]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "We also need to stress that we are not concerned in this study with stationarity and spectral properties of the GMHPs. This is the reason why in the definition of the Hawkes kernel \u03ba, of the generalized Hawkes process, we use integration over the interval (0, t) rather than integration over (\u2212\u221e, t). Please see also Remark 2.2 in this regard.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The paper is organized as follows. In Section 2 we define, prove existence of and provide some discussion of a generalized Hawkes process. Section 3 is devoted to study of the main object of this paper, namely the generalized multivariate Hawkes process. In Section 4 we provide a mathematical construction of and computational pseudo-algorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f (cf. (2.4)). Markovian aspects of a generalized multivariate Hawkes process are discussed in Section 5. Section 6 contains a brief description of possible applications of generalized multivariate Hawkes processes in seismology, epidemiology and finance. Finally, in the Appendix, we provide some needed technical results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In this paper we use various concepts and results from stochastic analysis. For a comprehensive study of these concepts and results we refer to e.g.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "[16], [22] and [18].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Let F N be the natural filtration of N , so F N := (F N t , t \u2265 0), where F N t is the Pcompleted \u03c3-field \u03c3(N ((s, r] \u00d7 A) : 0 \u2264 s < r \u2264 t, A \u2208 X ), t \u2265 0. In view of Theorem 2.2.4 in [22] the filtration F N satisfies the usual conditions. Moreover, N is F N -optional, so, using Proposition 4.1.1 in [22] we conclude that T n 's are F N -stopping times and X n are F Tn -measurable. In what follows we denote by P the F N -predictable \u03c3-field.",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 188,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 301,
                    "end": 305,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We recall that for a given filtration F a stochastic process X : \u2126\u00d7[0, \u221e) \u2192 R is said to be F-predictable if it is measurable with respect to the predictable sigma field P F on \u2126 \u00d7 [0, \u221e), which is generated by F-adapted processes whose paths are continuous (equivalently leftcontinuous, with the left limit at t = 0 defined as the value of the path at t = 0) functions of time variable. More generally, a function X : \u2126 \u00d7 [0, \u221e) \u00d7 X \u2192 R is said to be Fpredictable function if it is measurable with respect to the sigma field P F (X ) := P F \u2297 X on \u2126 \u00d7 [0, \u221e) \u00d7 X . The sigma field P F (X ) is generated by the sets A \u00d7 {0} \u00d7 X where A \u2208 F 0 and the sets of the form B \u00d7 (s, t] \u00d7 D where 0 < s \u2264 t, B \u2208 F s and D \u2208 X .",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We now consider a random measure \u03bd on (R + \u00d7 X , B(R + ) \u2297 X ) defined as \u03bd(\u03c9, dt, dy) := 1 ]]0,T\u221e(\u03c9)[[ (t)\u03ba(\u03c9, t, dy)dt,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "where, for A \u2208 X , \u03ba(t, A) = \u03b7(t, A) + (0,t)\u00d7X f (t, s, x, A)N (ds, dx), (2.4) \u03b7 is a finite kernel from (\u2126 \u00d7 [0, \u221e), P) to (X , X ), and f is a kernel from (\u2126 \u00d7 R + \u00d7 R + \u00d7 X , F \u2297 B(R + ) \u2297 B(R + ) \u2297 X ) to (X , X ). 1 We assume also that f is a kernel satisfying:",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 78,
                    "text": "(2.4)",
                    "ref_id": null
                },
                {
                    "start": 219,
                    "end": 220,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "1. f (t, s, x, A) = 0 for s \u2265 t,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "2. \u03b8 defined as \u03b8(t, A) := (0,t)\u00d7X f (t, s, x, A)N (ds, dx), t \u2265 0, A \u2208 X , is a kernel from (\u2126 \u00d7 [0, \u221e), P) to (X , X ), which is finite for t < T \u221e .",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Clearly, we have \u03b8(t, A) = n: Tn<t f (t, T n , X n , A). (2.5) Note that \u03ba(t, X ) is finite for any t < T \u221e . We additionally assume that \u03ba(t, X ) > 0 for all t \u2265 0 and that the integral [0,t] \u03ba(s, A)ds is finite for any A \u2208 X and any t < T \u221e . This last assumption is satisfied under mild boundedness conditions imposed on \u03b7 and f . ",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 62,
                    "text": "(2.5)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "A \u2208 X and thus it is F N -predictable. Consequently, \u03bd is a F N -predictable random measure.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "s)\u03ba(s, A)ds is continuous for any set"
        },
        {
            "text": "Before we proceed we recall that for a given filtration F the random measure \u03bd is said to be F-compensator of a random measure N if it is F-predictable random measure such that it holds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "s)\u03ba(s, A)ds is continuous for any set"
        },
        {
            "text": "for every non-negative F-predictable function F : \u2126 \u00d7 [0, \u221e) \u00d7 X \u2192 R.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "s)\u03ba(s, A)ds is continuous for any set"
        },
        {
            "text": "We are ready to state the underlying definition in this paper. Remark 2.2. We note that in our definition of the generalized Hawkes process the integral in (2.4) is taken over the interval (0, t). In the definition of the classic Hawkes process, the corresponding integral is taken over the interval (\u2212\u221e, t); see eg. [9] . The \"(0, t)\" convention is used by several authors, though, in many applications of classical Hawkes processes (such as in Example 3.8) that do not regard stationarity and spectral properties of these processes. We use this convention here since we are not considering stationarity and spectral properties of the generalized Hawkes processes. (ii) With a slight abuse of terminology we refer to \u03ba as to the Hawkes intensity kernel of N . Accordingly, we refer to the quantity \u03ba(t, A) as to the intensity at time t of the event regarding process N and amounting to the marks of N taking values in the set A, or, for short, as to the intensity at time t of marks of N taking values in A.",
            "cite_spans": [
                {
                    "start": 317,
                    "end": 320,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "s)\u03ba(s, A)ds is continuous for any set"
        },
        {
            "text": "Remark 2.4. Since F N 0 is a completed trivial \u03c3-field, then it is a consequence of Theorem 3.6 in [17] that the compensator \u03bd determines the law of N under P, and, consequently, the Hawkes kernel \u03ba determines the law of N under P.",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 103,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "s)\u03ba(s, A)ds is continuous for any set"
        },
        {
            "text": "We will now demonstrate that for an arbitrary measure \u03bd of the form (2.3) there exists a Hawkes process having \u03bd as F N -compensator. Towards this end we will consider the underlying canonical space. Specifically, we take (\u2126, F) to be the canonical space of multivariate marked point processes with marks taking values in X \u2202 . That is, \u2126 consists of elements \u03c9 = ((t n , x n )) n\u22651 , satisfying (t n , x n ) \u2208 (0, \u221e] \u00d7 X \u2202 and t n \u2264 t n+1 ; if t n < \u221e, then t n < t n+1 ; t n = \u221e iff x n = \u2202.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "The \u03c3-field F is defined to be the smallest \u03c3-field on \u2126 such that the mappings T n : \u2126 \u2192 ([0, \u221e], B[0, \u221e]), X n : \u2126 \u2192 (X \u2202 , X \u2202 ) defined by T n (\u03c9) := t n , X n (\u03c9) := x n are measurable for every n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "Note that the canonical space introduced above agrees with the definition of canonical space considered in [22] (see Remark 2.2.5 therein). On this space we denote by N a sequence of measurable mappings N = ((T n , X n )) n\u22651 , (2.7)",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "Clearly, these mappings satisfy 1. T n \u2264 T n+1 , and if T n < +\u221e then T n < T n+1 , 2. X n = \u2202 iff T n = \u221e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "We call such N a canonical mapping.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "The following result provides the existence of a probability measure P \u03bd on (\u2126, F) such that the canonical mapping N becomes a generalized Hawkes process with a given Hawkes kernel \u03ba, which in a unique way determines the compensator \u03bd.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "Theorem 2.5. Consider the canonical space (\u2126, F) and the canonical mapping N given by (2.7) . Let measures N and \u03bd be associated with this canonical mapping through (2.2) and (2.3)-(2.4), respectively. Then, there exists a unique probability measure P \u03bd on (\u2126, F), such that the measure \u03bd is an (F N , P \u03bd )-compensator of N . So, N is a generalized multivariate Hawkes process on (\u2126, F, P \u03bd ).",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 91,
                    "text": "(2.7)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "Proof. We will use Theorem 8.2.1 in [22] with X = X , \u03d5 = \u03c9, and with \u03b1(\u03c9, dt) := \u03bd(\u03c9, dt, X ) = 1 ]]0,T\u221e(\u03c9)[[ (t)\u03ba(\u03c9, t, X )dt, (2.8) from which we will conclude the assertion of theorem. Towards this end, we will verify that all assumptions of the said theorem are satisfied in the present case. As already observed, the random measure \u03bd is F N -predictable. Next, let us fix \u03c9 \u2208 \u2126. Given (2.8) we see that\u1fb1 satisfies the following equalities \u03b1(\u03c9, {0}) = 0,\u1fb1(\u03c9, {t}) = 0 \u2264 1, t \u2265 0, which correspond to conditions (4.2.6) and (4.2.7) in [22] , respectively. It remains to show that condition (4.2.8) holds as well, that is",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 129,
                    "end": 134,
                    "text": "(2.8)",
                    "ref_id": null
                },
                {
                    "start": 539,
                    "end": 543,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "where \u03c0 \u221e (\u03c9) := inf {t \u2265 0 :\u1fb1(\u03c9, (0, t]) = \u221e}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "To see this, we first note that (2.8) implies",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 37,
                    "text": "(2.8)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "Thus it suffices to show that \u03c0 \u221e (\u03c9) \u2265 T \u221e (\u03c9). By definition of\u1fb1 we can writ\u0113",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "If T \u221e (\u03c9) = \u221e, then we clearly have \u03c0 \u221e (\u03c9) = \u221e = T \u221e (\u03c9). Next, if T \u221e (\u03c9) < \u221e, then lim t\u2191T\u221e(\u03c9)\u1fb1 (\u03c9, (0, t]) = a. We need to consider two cases now: a = \u221e and a < \u221e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "If a = \u221e, then\u1fb1(\u03c9, (0, t]) = \u221e for t \u2265 T \u221e (\u03c9), and,\u1fb1(\u03c9, (0, t]) < \u221e for t < T \u221e (\u03c9) in view of our assumptions imposed on \u03ba in the beginning of this section. This implies that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": ", which implies that (2.9) holds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "Since \u03c9 was arbitrary, we conclude that for all \u03c9 \u2208 \u2126 conditions (4.2.6)-(4.2.8) in [22] are satisfied. So, applying Theorem 8.2.1 in [22] with \u03b2 = \u03bd, we obtain that there exists a unique probability measure P \u03bd such that \u03bd is a F N -compensator of N under P \u03bd .",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 88,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 134,
                    "end": 138,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Existence of a generalized Hawkes process"
        },
        {
            "text": "The classical Hawkes processes are conveniently interpreted, or represented, in terms of so called clusters. This kind of representation is sometimes called immigration and birth representation. We refer to [14] and [23] .",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 216,
                    "end": 220,
                    "text": "[23]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Cluster interpretation of the generalized Hawkes processes"
        },
        {
            "text": "Generalized Hawkes processes also admit cluster representation. The dynamics of cluster centers, or the immigrants, is directed by \u03b7. Specifically, \u03b7(t, A) is the time-t intensity of arrivals of immigrants with marks belonging to set A. The dynamics of the off-springs is directed by f . Specifically, f (t, s, x, A) represents the time-t intensity of births of offsprings with marks in set A of either an immigrant with mark x who arrived at time s, or of an offspring with mark x who was born at time s.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cluster interpretation of the generalized Hawkes processes"
        },
        {
            "text": "The cluster interpretation will be exploited in a follow-up work for asymptotic analysis of generalized Hawkes processes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cluster interpretation of the generalized Hawkes processes"
        },
        {
            "text": "We now introduce the concept of a generalized multivariate Hawkes process, which is a particular case of the concept of a generalized Hawkes process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalized multivariate Hawkes process"
        },
        {
            "text": "We first construct an appropriate mark space. Specifically, we fix an integer d \u2265 1 and we let (E i , E i ), i = 1, . . . , d, be some non-empty Borel spaces, and \u2206 be a dummy mark, the meaning of which will be explained below. Very often, in practical modelling, spaces E i are discrete. The instrumental rationale for considering a discrete mark space is that in most of the applications of the Hawkes processes that we are familiar with and/or we can imagine, a discrete mark space is sufficient to account for the intended features of the modeled phenomenon.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "We set E \u2206 i := E i \u222a \u2206, and we denote by E \u2206 i the sigma algebra on E \u2206 i generated by E i . Then, we define a mark space, say E \u2206 , as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "Moreover, denoting by \u2202 i the point which is external to E \u2206 i , we define E \u2202 i := E \u2206 i \u222a {\u2202 i }, and we denote by E \u2202 i the sigma algebra generated by E i and {\u2202 i }. Analogously we define",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "and by E \u2202 we denote the sigma field generated by E \u2206 and {\u2202}. Definition 3.1. A generalized Hawkes process N = ((T n , X n )) n\u22651 with the mark space X = E \u2206 given by (3.1) , and with X \u2202 = E \u2202 , is called a generalized multivariate Hawkes process (of dimension d).",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 173,
                    "text": "(3.1)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "Note that a necessary condition for generalized Hawkes processes to feature the selfexcitation and mutual-excitation is that f = 0. We refer to Example 3.9 for interpretation of the components \u03b7 and f of the kernel \u03ba in case of a generalized multivariate Hawkes process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "We interpret T n \u2208 (0, \u221e) and X n \u2208 E \u2206 as the event times of N and as the corresponding mark values, respectively. Thus, if T n < \u221e we have 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "Also, we interpret X i as the marks associated with i-th coordinate of N (cf. Definition 3.3). With this interpretation, the equality X i n (\u03c9) = \u2206 means that there is no event taking place with regard to the i-th coordinate of N at the (general) event time T n (\u03c9). In other words, no event occurs with respect to the i-th coordinate of N at time T n (\u03c9).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "Definition 3.2. We say that T n (\u03c9) is a common event time for a multivariate Hawkes process N if there exist i and j, i = j, such that X i n (\u03c9) \u2208 E i and X j n (\u03c9) \u2208 E j . We say that process N admits common event times if P \u03c9 \u2208 \u2126 : \u2203n such that T n (\u03c9) is a common event time > 0",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "Otherwise we say that process N admits no common event times. Definition 3.2 generalizes that in Bremaud and Massouli [6] and Liniger [24] . In particular, with regard to the concepts of multivariate Hawkes processes studied in Liniger [24] , the genuine multivariate Hawkes processes [24] admits no common event times, whereas in the case of pseudo-multivariate Hawkes process [24] all event times are common.",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 121,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 134,
                    "end": 138,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 236,
                    "end": 240,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 285,
                    "end": 289,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 378,
                    "end": 382,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Definition"
        },
        {
            "text": "We start with Definition 3.3. We define the i \u2212 th coordinate N i of N as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "for A \u2208 E i and t \u2265 0, where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Clearly, N i is a MPP and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Indeed, the i-th coordinate process N i can be represented as a sequence N i = (T i k , Y i k ) k\u22651 , which is related to the sequence (T n , X i n ) n\u22651 as follows",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "where k i = max{n : m i n < \u221e}, with m i defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "In particular this means that for the i-th coordinate N i the times T n (\u03c9) such that X i n (\u03c9) = \u2206 are disregarded as event times for this coordinate since the events occurring with regard to the entire N at these times do not affect the i-th coordinate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "We define the completed filtration [22] the filtration F N i satisfies the usual conditions.",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 39,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "We define the explosion time",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Clearly, T i \u221e \u2264 T \u221e . We conclude this section with providing some more insight into the properties of the measure N i . Towards this end, we first observe that the measure N i is both F N -optional and F N i -optional. Subsequently, we will derive the compensator of N i with respect to F N and the compensator of N i with respect to F N i . The following Proposition 3.4 and Proposition 3.7 come handy in this regard.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Proposition 3.4. Let N be a generalized multivariate Hawkes process with Hawkes kernel \u03ba. Then the (F N , P)-compensator, say \u03bd i , of measure N i defined in (3.2) is given as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Proof. According to Theorems 4.1.11 and 4.1.7 in [22] the i-th coordinate N i admits a unique F N -compensator, say \u03bd i , with a property that",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 53,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "For every n and A \u2208 E i the processes M i,n,A and M i,n,A given as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "are (F N , P)-martingales. Hence the process",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "is an F N -predictable martingale. Since it is of integrable variation and null at t = 0 it is null for all t \u2265 0 (see e.g. Theorem VI.6.3 in [16] ). From the above and the fact that",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 146,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "This proves the proposition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Remark 3.5. Note that for each i, the function \u03ba i defined in (3.7) is a measurable kernel from (\u2126 \u00d7 R + , P \u2297 B(R + )) to (E i , E i ). It is important to observe that, in general, there is no one-to-one correspondence between the Hawkes kernel \u03ba and all the marginal kernels \u03ba i , i = 1, . . . , d. We mean by this that may exist another Hawkes kernel, say \u03ba, such that \u03ba = \u03ba and The following important result gives the F N i -compensator of measure N i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Proposition 3.7. Let N be a generalized multivariate Hawkes process with Hawkes kernel \u03ba. Then the F N i -compensator of measure N i , say \u03bd i , is given as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "Proof. Using Theorems 4.1.9 and 3.4.6 in [22] , as well as the uniqueness of the compensator, it is enough to show that for any A \u2208 E i and any n \u2265 1 the process ",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 45,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "The i-th coordinate of a generalized multivariate Hawkes process N"
        },
        {
            "text": "We will provide now some examples of generalized multivariate Hawkes processes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples"
        },
        {
            "text": "For \u03c9 = (t n , x n ) n\u22651 , t \u2265 0 and A \u2208 E \u2206 we set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples"
        },
        {
            "text": "(3.10)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples"
        },
        {
            "text": "In all examples below we define the kernel \u03ba of the form (2.4) with \u03b7 and f properly chosen, so that we may apply Theorem 2.5 to the effect that there exists a probability measure P \u03bd on (\u2126, F) such that process N given by (3.10) is a Hawkes process with the Hawkes kernel equal to \u03ba. In other words, there exists a probability measure P \u03bd on (\u2126, F) such that \u03bd given in",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples"
        },
        {
            "text": "For a Hawkes process N with a mark space E \u2206 we introduce the following notation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples"
        },
        {
            "text": "Likewise, we denote for i = 1, . . . , d,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples"
        },
        {
            "text": "We take d = 1 and E 1 = {1}, so that E \u2206 = E 1 = {1}. As usual, and in accordance with (2.2), we identify N with a point process (N t ) t\u22650 . Now we take",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.8. Classical univariate Hawkes process"
        },
        {
            "text": "where \u03bb is positive, locally integrable function, and, for 0 \u2264 s \u2264 t, we take",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.8. Classical univariate Hawkes process"
        },
        {
            "text": "for some non-negative function w defined on R + (recall that f (t, s, 1, {1}) = 0 for s \u2265 t). Using these objects we define \u03ba by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.8. Classical univariate Hawkes process"
        },
        {
            "text": "In case of the classical univariate Hawkes process sufficient conditions under which the explosion time is almost surely infinite, that is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.8. Classical univariate Hawkes process"
        },
        {
            "text": "are available in terms of the Hawkes kernel. Specifically, sufficient conditions for no-explosion are given in [1] :",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 114,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Example 3.8. Classical univariate Hawkes process"
        },
        {
            "text": "\u03bb is locally bounded, and \u221e 0 w(u)du < \u221e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.8. Classical univariate Hawkes process"
        },
        {
            "text": "In the case of a generalized bivariate Hawkes process N we have d = 2 and the mark space is given as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "Here, in order to define kernel \u03ba, we take kernel \u03b7 in the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "where \u03b4 \u2206 is a Dirac measure, \u03b7 i for i = 1, 2 are probability kernels, from (R + , B(R + )) to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "Kernel f is given, for 0 \u2264 s \u2264 t and x = (x 1 , x 2 ), by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "The decay functions w i,j and the impact functions g i,j , i, j = 1, 2, c, are appropriately regular and deterministic. Moreover, the decay functions are positive and the impact functions are non-negative. In particular, this implies that the kernel f is deterministic and non-negative.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "In what follows we will need the concept of idiosyncratic group of I coordinates of a generalized bivariate Hawkes process N . For I = {1} we define Clearly, N idio,I is a MPP. For example, N idio,i is a MPP which records idiosyncratic events occurring with regard to X i ; that is, events that only regard to X i , so that X j n = \u2206 for j = i at times T n at which these events take place. Likewise, N idio,{1,2} is a MPP which records idiosyncratic events occurring with regard to X 1 and X 2 simultaneously. Let us note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "We will now interpret various terms that appear in the expressions for \u03b7 and f above:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "represents autonomous portion of the intensity, at time t, of marks of the coordinate N 1 taking values in the set dy 1 \u2282 E 1 and no marks occurring for N 2 ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "\u03b7 c (t, dy 1 , dy 2 ) represents autonomous portion of the intensity, at time t, of an event amounting to the marks of both coordinates N 1 and N 2 taking values in the set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "represents idiosyncratic impact of the coordinate N 1 alone on the intensity, at time t, of marks of the coordinate N 1 taking values in the set dy 1 \u2282 E 1 and no marks occurring for N 2 ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "(0,t)\u00d7E 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "represents idiosyncratic impact of the coordinate N 2 alone on the intensity, at time t, of an event amounting to the marks of coordinate N 1 taking value in the set dy 1 \u2282 E 1 and no marks occurring for N 2 ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "represents joint impact of the coordinates N 1 and N 2 on the intensity, at time t, of an event amounting to the marks of coordinate N 1 taking value in the set dy 1 \u2282 E 1 and no marks occurring for N 2 ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "represents idiosyncratic impact of the coordinate N 1 alone on the intensity, at time t, of an event amounting to the marks of both coordinates N 1 and N 2 taking values in the set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "represents joint impact of the coordinates N 1 and N 2 on the intensity, at time t, of an event amounting to the marks of both coordinates N 1 and N 2 taking values in the set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "In particular, the terms contributing to occurrence of common events are \u03b7 c (t, dy 1 , dy 2 ) and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "To complete this example we note that upon setting \u03b7 c = 0 and \u03c6 c = 0 we produce a generalized bivariate Hawkes process with no common event times.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 3.9. Generalized bivariate Hawkes process with common event times"
        },
        {
            "text": "Fix an arbitrary T > 0. In this section we first provide a construction of restriction to [0, T ] \u00d7 E \u2206 of a generalized multivariate Hawkes process, with deterministic kernels \u03b7 and f , via Poisson thinning, that is motivated by a similar construction given in [5] . Then, based on our construction, we present a computational pseudo-algorithm for simulation of a generalized multivariate Hawkes process restricted to [0, T ] \u00d7 E \u2206 . We are concerned here with a generalized multivariate Hawkes process admitting the Hawkes kernel of the form",
            "cite_spans": [
                {
                    "start": 262,
                    "end": 265,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Mathematical construction of and computational pseudoalgorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f"
        },
        {
            "text": ". We may, and we do, represent kernels \u03b7, f as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mathematical construction of and computational pseudoalgorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f"
        },
        {
            "text": "Note that Q 1 and Q 2 are deterministic probability kernels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mathematical construction of and computational pseudoalgorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f"
        },
        {
            "text": "Since we are concerned with a restricted Hawkes process we consider a Hawkes kernel \u03ba T which is a restriction to [0, T ] of \u03ba that is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mathematical construction of and computational pseudoalgorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f"
        },
        {
            "text": "For simplicity of notation we suppress T in the notation below. So, for example, we will write f rather than",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mathematical construction of and computational pseudoalgorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f"
        },
        {
            "text": "We make the following standing assumption:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mathematical construction of and computational pseudoalgorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f"
        },
        {
            "text": "for some measurable mapping f :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mathematical construction of and computational pseudoalgorithm for simulation of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f"
        },
        {
            "text": "Now we describe a construction of Hawkes process with Hawkes kernel given by (4.2) . This construction leads immediately to a pseudo-algorithm, presented in the next section, for simulation of such Hawkes process.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 82,
                    "text": "(4.2)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "In what follows we will define recursively a sequence of random measures (N k ) k\u22650 that provide building blocks for our Hawkes process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Towards this end we first let \u03b2 be the Borel isomorphism between the space E \u2202 and a Borel subset of R d \u222a \u2202, with the convention that \u03b2(\u2202) = \u2202.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Our construction will proceed in several steps.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Step 1). Let us consider an array ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "where we use the convention that 0 0 = 1. Therefore, for a random variable U uniformly distributed on (0, 1] the random variable D(\u03bb, U ) has Poisson distribution with parameter \u03bb \u2265 0, where we extend the concept of Poisson distribution by allowing \u03bb = 0. Moreover let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Existence of such functions G 1 and G 2 is asserted by Lemma 3.22 in [20] . We use the left open intervals of integration above so to be consistent with the the rest of the construction. The reason that we work with left open intervals in the rest of the construction is that the births of the offsprings occur after the appearance of their parents (e.g., after arrivals of the immigrants), see Section 2.2. This feature is explicitly accounted for in the construction presented here.",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Step 2). Using",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Then, we consider a sequence",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "j constructed as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Observe that S 0 j < S 0 j+1 on {S 0 j < \u221e}, and that the measure N 0 may be identified with the sequence",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "The representation (4.5) is more convenient for our needs than the representation (4.4). This is because the sequence (S 0 j , Y 0 j ) \u221e j=1 is ordered with respect to the first component, so that this sequence is a MPP and thus measure N 0 may also be considered as a MPP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Step 3). Now, we proceed by recurrence. So, for k \u2208 N suppose that we have constructed a random sequence",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "in the following way:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Note that the random variable P k+1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "otherwise, if S k j \u2265 T , these elements are all constant and equal to (\u221e, 0, \u2202). Moreover, they are \u03c3(N 0 , . . . , N k )-conditionally independent random elements, and the \u03c3(N 0 , . . . ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Similarly as above we observe that the random measure N k+1 can be identified with the random sequence (S k+1 n , Y k+1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Indeed, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Since the sequence S k+1 n , Y k+1 n \u221e n=1 forms a MPP, then N k+1 may be considered as a MPP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Step 4). Define a sequence of random measures H k , k \u2265 1, on B(R + ) \u2297 E \u2206 in terms of the previously constructed marked point processes (N j ) j\u22650 by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": ", can be associated with H k in a way analogous to how the sequence S k+1 n , Y k+1 n \u221e n=1 has been associated with N k+1 . Consequently, H k may be considered as an MPP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Step 5). Repeat Step 3 and Step 4 infinitely many times to obtain limiting random measure H \u221e on B(R + ) \u2297 E \u2206 given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "(4.11)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Remark 4.1. It is important to note that all random measures introduced in the construction above do not charge any set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Description of the construction"
        },
        {
            "text": "Now we will justify that the construction given in Steps 1-5 above delivers a generalized multivariate Hawkes process with the Hawkes kernel given in (4.1). Towards this end let us introduce the following filtrations:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Our first aim is to compute H \u221e -compensator of the limiting random measure H \u221e given in (4.11) . We begin with following key result, Proposition 4.2. i) The marked point process N 0 is an H 0 -doubly stochastic marked Poisson process. The random measure \u03bd 0 given by",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 95,
                    "text": "(4.11)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "is the H 0 0 -intensity kernel of N 0 . Moreover, \u03bd 0 is the H 0 -compensator of N 0 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "ii) For each j the marked point process N k+1 j is an H k+1 -doubly stochastic marked Poisson process. The random measure \u03bd k+1 j given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Proof. i). Note that from Lemma 7.2, by taking",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "it follows that N 0 is H 0 0 -conditional Poisson random measure with intensity measure \u03bd 0 given by (4.12) . Now, the assertion follows from the point i) of Proposition 7.3.",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 107,
                    "text": "(4.12)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "ii). We first note that from Lemma 7.2, by taking",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "it follows that N k+1 j defined by (4.6) is H k \u221e -conditionally Poisson random measure with intensity measure \u03bd k+1 j given by (4.13) .",
            "cite_spans": [
                {
                    "start": 128,
                    "end": 134,
                    "text": "(4.13)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "To complete the proof, in view of assertion ii) of Proposition 7.3, it suffices show that the marked point processes (N k+1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "are conditionally independent given \u03c3(N 0 , . . . , N k ). For this we first note that for each j the random measure N k+1 j is defined by (4.6), so it is constructed from the pair (S k j , Y k j ), which is \u03c3(N 0 , . . . , N k )-measurable and from the family",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": ". Now, using the fact that I 1 , I 2 , . . . are independent between themselves and also independent from \u03c3(N 0 , . . . , N k ), we conclude that N k+1 1 , N k+1 2 , . . . are (N 0 , . . . , N k )-conditionally independent. So we see that N k+1 j is a H k+1 0 -conditional Poisson random measure for any j \u2265 1, and that (N k+1 j ) j\u22651 are H k+1 0 -conditionally independent random measures. Thus we may use Proposition 7.3 to conclude that N k+1 j is an H k+1 -doubly stochastic marked Poisson process whose H k+1 0 -intensity kernel is \u03bd k+1 j given by (4.13)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "From Proposition 4.2 and from its proof we conclude that the random measure N k+1 given by (4.9) is a sum of H k \u221e -conditionally independent H k+1 -doubly stochastic marked Poisson processes. We will prove now that N k+1 is also an H k+1 -doubly stochastic marked Poisson process whose intensity kernel is simply the sum of intensity kernels of N k+1 j , j \u2265 0. for 0 \u2264 s \u2264 t, D \u2208 E \u2206 . Moreover, the intensity kernel \u03bd k+1 of N k+1 is the H k+1 -compensator of N k+1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Proof. To prove the first assertion, in view of Proposition 6.1.4 in [22] , it suffices to show that \u03bd k+1 is the H k+1 -compensator of N k+1 . Indeed this compensating property implies that",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "where the last equality follows from H k+1 0 -measurability of \u03bd k+1 . So, if \u03bd k+1 is H k+1compensator of N k+1 then it is H k+1 0 -intensity kernel and, thus, Theorem 6.1.4 in [22] implies the first assertion. Therefore it remains to show that \u03bd k+1 is H k+1 -compensator of N k+1 .",
            "cite_spans": [
                {
                    "start": 178,
                    "end": 182,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Towards this end we first note that from Proposition 4.2 it follows that N k+1 j is an H k+1doubly stochastic marked Poisson process with H k+1 -compensator \u03bd k+1 j given by (4.13) . So, for an arbitrary non-negative H k+1 -predictable function F :",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 180,
                    "text": "(4.13)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "This implies that (\u03c9, dt, dy)P(d\u03c9), \u00b5(dt, dy, d\u03c9) = N k+1 (\u03c9, dt, dy)P(d\u03c9) and once again for \u00b5 j (dt, dy, d\u03c9) = \u03bd k+1 j (\u03c9, dt, dy)P(d\u03c9), \u00b5(dt, dy, d\u03c9) = \u03bd k+1 (\u03c9, dt, dy)P(d\u03c9)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "we see that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "and since",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "we obtain that (4.14) holds. This concludes the proof of the first assertion. Now we will prove that the H k+1 0 -intensity kernel \u03bd k+1 of N k+1 is the H k+1 -compensator of N k+1 . For this, we first observe that from Theorem 6.1.4 in [22] it follows that the intensity kernel of N k+1 is the H k+1 -compensator of N k+1 . So, for an arbitrary non-negative In order to proceed we will need the following auxiliary result. Proof. The necessity follows from Proposition 5.9.1.1 in [19] . To prove sufficiency it is enough to show, again by Proposition 5.9.1.1 in [19] , that for every t \u2265 0 and every bounded F \u221e -measurable random variable \u03be it holds that",
            "cite_spans": [
                {
                    "start": 237,
                    "end": 241,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 481,
                    "end": 485,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 563,
                    "end": 567,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Fix \u03be, we need to show that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "for every A \u2208 F t \u2228 G t . Towards this end let us consider a family U of sets defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Note that U is a \u03c0-system of sets which generates F t \u2228 G t . Observe that family of all sets for which (4.18) holds is a \u03bb-system. Thus, by the Sierpinski's Monotone Class Theorem (cf. Theorem 1.1 in [20] ), which is also known as the Dynkin's \u03c0 \u2212 \u03bb Theorem, it suffices to prove (4.18) for the sets from U, which we will do now so to complete the proof. For A \u2208 U, we have",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 205,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "where the fourth equality follows from (4.17) for \u03b7 = 1 C .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "We are now ready to demonstrate the following proposition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "for every u \u2265 0 and every A \u2208 U, where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": ". . , D n are disjoint sets, and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Indeed, if (4.19) holds for A \u2208 U, then since U is a \u03c0-system which generates F N k+1 u the monotone class theorem implies that (4.17) holds. It remains to show (4.19) for A \u2208 U. Using Proposition 4.3 and invoking (7.11) we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "(4.20)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Since H k+1 0 = H k \u221e and \u03bd k+1 ((s i , t i ] \u00d7 D) is H k t i measurable we infer that the right hand side of (4.20) is H k tn -measurable and hence also H k u -measurable for arbitry u \u2265 t n . Consequently by taking conditional expectations with respect to H k u for u \u2265 t n we conclude that (4.19) holds for A \u2208 U. The proof is complete.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "We will determine now the compensators for H 0 := N 0 and for H k given by (4.10) for k \u2265 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Proposition 4.6. The H 0 -compensator of H 0 , is given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "where the kernel \u03b7 appears in (4.1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "The H k -compensator of H k , for k \u2265 1, is given by Proof. The proof goes by induction. Since H 0 = N 0 , then the form of H 0 -compensator of H 0 follows from assertion i) of Proposition 4.2 and from Proposition 6.1.4 [22] .",
            "cite_spans": [
                {
                    "start": 220,
                    "end": 224,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Suppose now that H k -compensator of H k is given by (4.21) . This means that for every D \u2208 E \u2206 the process",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 59,
                    "text": "(4.21)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "is an H k -local martingale. Proposition 4.5 implies that M k (D) is an H k+1 -local martingale. We know from Proposition 4.3 that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "is an H k+1 -local martingale. Thus M k (D) + L k+1 (D) is an H k+1 -local martingale. This H k+1 -local martingale can be written in the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "where the second equality follows from",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Note that the random measure \u03d1 k + \u03bd k+1 is H k+1 -predictable so it is the H k+1 -compensator of H k+1 . To complete the proof it suffices to show that \u03d1 k + \u03bd k+1 = \u03d1 k+1 . By the induction hypothesis on \u03d1 k and by (4.14) we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "The proof is complete.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "Before we conclude our construction of a generalized multivariate Hawkes process, we derive the following result. Proof. Proposition 4.5 and Proposition 4.6 imply that for every k \u2265 1, the H \u221e -compensator of H k is given by (4.21) . Thus, we see that for any k \u2265 1 and for an arbitrary non-negative",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 231,
                    "text": "(4.21)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Justification of the construction"
        },
        {
            "text": "This completes the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Using Lemma 7.5 in an analogous way as in the proof of Proposition 4.3 we obtain"
        },
        {
            "text": "We are now ready to conclude our construction of a generalized multivariate Hawkes process. Let T \u221e be the first accumulation time of H \u221e . 4 Then we have the following Proof. Let us define a sequence (T n , X n ) n\u22651 by",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 141,
                    "text": "4",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Using Lemma 7.5 in an analogous way as in the proof of Proposition 4.3 we obtain"
        },
        {
            "text": "and the random measure",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Using Lemma 7.5 in an analogous way as in the proof of Proposition 4.3 we obtain"
        },
        {
            "text": "where \u03d1 \u221e is given in (4.23 ",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 27,
                    "text": "(4.23",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Consequently, such restriction of H"
        },
        {
            "text": "Towards this end note that for arbitrary 0 \u2264 s < t \u2264 T and D \u2208 E \u2206 we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Consequently, such restriction of H"
        },
        {
            "text": "The second term above can be written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Consequently, such restriction of H"
        },
        {
            "text": "This and (4.25) imply that \u03d1 \u221e | ]]0,T\u221e[[\u00d7E \u2206 is an F N -predictable random measure such that for arbitrary non negative F N -predictable function F :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Consequently, such restriction of H"
        },
        {
            "text": "Thus N is a F N -Hawkes process (restricted to [0, T ] \u00d7 E \u2206 ) with the Hawkes kernel \u03ba.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Consequently, such restriction of H"
        },
        {
            "text": "In the description of the pseudo-algorithm below we use the objects \u03b7, f , \u03b7, f , G 1 and G 2 that underly the construction of our Hawkes process given in Section 4.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "The steps of the pseudo-algorithm are based on the steps presented in our construction of a generalized multivariate Hawkes process with deterministic kernels \u03b7 and f , and they are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "Step 0. Choose a positive integer K, set C 0 = \u2205.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "Step 1. Generate a realization, say p, of a Poisson random variable with parameter T \u03b7.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "Step 2. If p = 0, then go to Step 3. Else, if p > 0, then for i = 1, . . . , p :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "-Generate realizations u and v of independent random variables uniformly distributed on [0, 1]. Set t = T u, a = \u03b7.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "-If a \u2264 \u03b7(t, E \u2206 ), then generate a realization w of random variable uniformly distributed on [0, 1], compute x = G 1 (t, w) and include (t, x) into the cluster C 0 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "Step 3. Set N = C 0 , C prev = C 0 , k = 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "Step 4. While C prev = \u2205 and k \u2264 K :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "-For every (s, y) \u2208 C prev : * generate a realization p of Poisson random variable with parameter (T \u2212 s) f (s, y). * for i = 1, . . . , p:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "Generate realizations u and v of independent random variables uniformly",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "If a \u2264 f (t, s, y, E \u2206 ), then generate a realization w of random variable uniformly distributed on [0, 1], compute x = G 2 (t, s, y, w) and include (t, x) into the cluster C new .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "-Set k = k + 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "Step 5. Return N .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The pseudo-algorithm"
        },
        {
            "text": "The pseudo-algorithm presented above is implemented here in two cases. In the first case, presented in Example 4.9, we implemented the algorithm for a generalized bivariate Hawkes process with E 1 = E 2 = {1}. In the second case, presented in Example 4.10, we set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical examples via simulation"
        },
        {
            "text": "We used Python to run the simulations and to plot graphs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Numerical examples via simulation"
        },
        {
            "text": "Here we implement our pseudo-algorithm for a bivariate point Hawkes process, that is the generalized bivariate Hawkes process with E 1 = E 2 = {1}, and hence with",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "Moreover, we let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "and \u03b1 i , \u03b7 i (0), \u03b2 i are non-negative constants. We assume that, for 0 \u2264 s \u2264 t, the kernel f is given as in (3.11) with the decay functions w i,j in the exponential form:",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 116,
                    "text": "(3.11)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "with constant non-negative impact functions: g 1,1 (x 1 ) = \u03d1 1,1 , g 1,2 (x 2 ) = \u03d1 1,2 , g 1,c (x) = \u03d1 1,c , g 2,1 (x 1 ) = \u03d1 2,1 , g 2,2 (x 2 ) = \u03d1 2,2 , g 2,c (x) = \u03d1 2,c , g c,1 (x 1 ) = \u03d1 c,1 , g c,2 (x 2 ) = \u03d1 c,2 , g c,c (x) = \u03d1 c,c , and with Dirac kernels:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "Thus, the kernel f is of the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "The coordinates of N (cf. (3.2)) reduce here to counting (point) processes, so that . Simulated sample paths of N corresponding to the above setting are presented in Figure 1 and Figure 2 . Here we apply our pseudo-algorithm to Example 3.9 with d = 2 and E 1 = E 2 = R. We let: \u03b7 1 (t, dy 1 ) = \u03b1 1 \u03d5 \u00b5 1 ,\u03c3 1 (y 1 )dy 1 , \u03b7 2 (t, dy 2 ) = \u03b1 2 \u03d5 \u00b5 2 ,\u03c3 2 (y 2 )dy 2 , \u03b7 1 (t, dy 1 ) = \u03b1 c \u03d5 \u00b5c,\u03c3c (y 1 )\u03d5 \u00b5c,\u03c3c (y 2 )dy 1 dy 2 where \u03b1 i \u2265 0, i \u2208 {1, 2, c}, \u03d5 \u00b5,\u03c3 is the one dimensional Gaussian density function with mean \u00b5 and variance \u03c3 2 , and:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 174,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 179,
                    "end": 187,
                    "text": "Figure 2",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "Moreover, we set:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "and we take ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 4.9. Bivariate point Hawkes process"
        },
        {
            "text": "An important class of Hawkes processes considered in the literature is the one of Hawkes processes for which the Hawkes kernel is given in terms of exponential decay functions. See, e.g., [7] , [25] , [33] . One interesting and useful aspect of such processes is that they can be extended to Markov processes, a feature that we term the Markovian aspects of a generalized bivariate Hawkes process .",
            "cite_spans": [
                {
                    "start": 188,
                    "end": 191,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 194,
                    "end": 198,
                    "text": "[25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 201,
                    "end": 205,
                    "text": "[33]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "To simplify the presentation, we will discuss Markovian aspects of generalized bivariate Hawkes processes specified in Example 4.9. Using this specification we end up with the Hawkes kernel \u03ba of the form:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "where, for i = 1, 2, c, we have \u03bb i 0 := \u03b7 i (0) and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "We now refer to canonical space as in Section 2.1, and to the random measure \u03bd corresponding to \u03ba as in (2.3). So, using Theorem 2.5 we see that there exists a unique probability P \u03bd such that the canonical process N given as in (3.10) is a generalized multivariate Hawkes process with Hawkes kernel \u03ba. It is straightforward to verify (upon appropriate integration of the kernel \u03ba i.e. over {1} \u00d7 {1, \u2206} for N 1 and {1, \u2206} \u00d7 {1} for N 2 ) that the F N -intensity of process N i , say \u03bb i , is given as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "is the square bracket of N 1 , N 2 . Then, for i = 1, 2, c, the equality (5.2) can be written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "for t \u2265 0. This follows from the fact that [N 1 , N 2 ] counts common jumps of N 1 and N 2 , so for i = 1, 2 the processN i is counts the idiosyncratic jumps of N i , that is the jumps that do not occur simultaneously with the jumps of N j , j = i. In particular, expression (5.4) allows us to give the interpretation of the parameters \u03d1 i,j , i, j \u2208 {1, 2, c}, namely the parameter \u03d1 i,j describes the impact of the jump of the processN j on the intensity ofN i . Now, let us consider a bivariate counting process N := (N 1 , N 2 ). Note that we may, and we do, identify process N with our bivariate generalized Hawkes process N :",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 520,
                    "end": 530,
                    "text": "(N 1 , N 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "T 0 = 0, T n = inf {t > T n\u22121 : \u2206 N t = (0, 0)}, and for i = 1, 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "Also, note that we may, and we do, identify the process N with a random measure \u00b5 N on R + \u00d7 E, where E = {(1, 0), (0, 1), (1, 1)}, given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "Thus, we may slightly abuse terminology and call N a generalized bivariate Hawkes process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "Theorem 5.1. Let N be a Hawkes process defined as above. Then i) The process N = (N 1 t , N 2 t ) t\u22650 is not a Markov process. ii) The process Z = (\u03bb 1 t , \u03bb 2 t , \u03bb c t , N 1 t , N 2 t ) t\u22650 is a Markov process with the strong generator A acting on C \u221e c (R 5 + ) given by Av(\u03bb 1 , \u03bb 2 , \u03bb c , n 1 , n 2 ) (5.7) \u03bb 2 , \u03bb c , n 1 , n 2 ) + (v(\u03bb 1 + \u03d1 1,1 , \u03bb 2 + \u03d1 2,1 , \u03bb c + \u03d1 c,1 , n 1 + 1, n 2 ) \u2212 v(\u03bb 1 , \u03bb 2 , \u03bb c , n 1 , n 2 ))\u03bb 1 + (v(\u03bb 1 + \u03d1 1,2 , \u03bb 2 + \u03d1 2,2 , \u03bb c + \u03d1 c,2 , n 1 , n 2 + 1) \u2212 v(\u03bb 1 , \u03bb 2 , \u03bb c , n 1 , n 2 ))\u03bb 2 + (v(\u03bb 1 + \u03d1 1,c , \u03bb 2 + \u03d1 2,c , \u03bb c + \u03d1 c,c , n 1 + 1, n 2 + 1) \u2212 v(\u03bb 1 , \u03bb 2 , \u03bb c , n 1 , n 2 ))\u03bb c .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 313,
                    "end": 316,
                    "text": "\u03bb 2",
                    "ref_id": "FIGREF14"
                }
            ],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "Proof. i) From (5.4) and (5.6) we see that for any t > 0 the quantity \u03bd(dt, dy) given in (5.5) depends on the entire path of N until time t. Thus, by Theorem 4 in [15] , the process N is not a Markov process.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 167,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "ii) First note that (5.4) can be written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "Hence using stochastic integration by parts one can show that \u03bb i can be represented as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "This and (5.6) implies that the process Z is an F Z -semimartingale with characteristics (with respect to cut-off function h(x) = x1 |x|<1 )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": ", 0, 0 and \u03bd t (dy 1 , dy 2 , dy c , dz 1 , dz 2 ) (5.8) := \u03bb 1 u\u2212 \u03b4 (\u03d1 1,1 ,\u03d1 2,1 ,\u03d1 c,1 ,1,0) (dy 1 , dy 2 , dy c , dz 1 , dz 2 ) + \u03bb 2 u\u2212 \u03b4 (\u03d1 1,2 ,\u03d1 2,2 ,\u03d1 c,2 ,0,1) (dy 1 , dy 2 , dy c , dz 1 , dz 2 ) + \u03bb c u\u2212 \u03b4 (\u03d1 1,c ,\u03d1 2,c ,\u03d1c,c,1,1) (dy 1 , dy 2 , dy c , dz 1 , dz 2 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "This, by Theorem II.2.42 in [18] , implies that for any function v \u2208 C 2 b (R 5 ) the process M v given as",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 32,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "is an F Z -local martingale. Hence, for any v \u2208 C \u221e c (R 5 ) the process defined above is a martingale under P, since v and Av are bounded, which follows from the fact that v \u2208 C \u221e c (R 5 ) has compact support, and thus the local martingale M v is a martingale for such v. Consequently, the process Z solves martingale problem for (A, \u03c1) , where \u03c1 is the deterministic initial distribution of Z, that is \u03c1(dz) = \u03b4 Z 0 (dz).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 331,
                    "end": 337,
                    "text": "(A, \u03c1)",
                    "ref_id": null
                }
            ],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "We will now verify that Z is a Markov process with generator A given in (5.7) using Theorem 4.4.1 in [10] .",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 105,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "For this, we first observe that parameters determining A, i.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "are admissible in the sense of Definition 2.6 in [8] . Thus, invoking Theorem 2.7 in [8] we conclude that there exists a unique regular affine semigroup (P t ) t\u22650 with infinitesimal generator A given by (5.7). Hence, there exists a unique regular affine process with generator A and with transition function P defined by (P t ) t\u22650 . Since A is a generator of regular affine process it satisfies the Hille-Yosida conditions (cf. Theorem 1.2.6 in [10] ) relative to the Banach space B(R 5 ) of real valued, bounded and measurable functions on R 5 . Moreover, from Corollary 1.1.6 in [10] it follows that A is a closed operator. Now, using Theorem 4.4.1 in [10] we obtain that Z is a Markov process with generator A. Moreover, P is the transition function of Z.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 52,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 85,
                    "end": 88,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 447,
                    "end": 451,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 583,
                    "end": 587,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 656,
                    "end": 660,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "Let us note that using analogous argument as in the proof of Theorem 5.1 we can prove that the process Y 1 := (\u03bb 1 t + \u03bb c t , N 1 t ) t\u22650 is a Markov process in filtration F Z provided that parameters of \u03bb k , k \u2208 {1, c}, satisfy",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "Analogous statement is valid for Y 2 := (\u03bb 2 + \u03bb c , N 2 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Markovian aspects of a generalized bivariate Hawkes process"
        },
        {
            "text": "The are numerous potential applications of the generalized multivariate Hawkes processes. Here we present a brief description of possible applications in seismology, in epidemiology and in finance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Applications"
        },
        {
            "text": "In the Introduction to [27] the author writes:",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 27,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "\"Lists of earthquakes are published regularly by the seismological services of most countries in which earthquakes occur with frequency. These lists supply at least the epicenter of each shock, focal depth, origin time and instrumental magnitude.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "Such records from a self-contained seismic region reveal time series of extremely complex structure. Large fluctuations in the numbers of shocks per time unit, complicated sequences of shocks related to each other, dependence on activity in other seismic regions, fluctuations of seismicity on a larger time scale, and changes in the detection level of shocks, all appear to be characteristic features of such records. In this manuscript the origin times are mainly considered to be modeled by point processes, with other elements being largely ignored, except that the ETAS model and its extensions use data of magnitudes and epicenters.\"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "In particular, the dependence on (simultaneous) seismic activity in other seismic regions has been ignored in the classical univariate ETAS 6 model, and in all other models that we are aware of.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "The ETAS model is a univariate self-exciting point process, in which the shock intensity at time t, corresponding to a specific seismic location, is designed as (cf. Equation (17) in [27] )",
            "cite_spans": [
                {
                    "start": 183,
                    "end": 187,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "In the above formula, H t stands for the history of after-shocks at the given location, \u00b5 represents the background occurrence rate of seismic activity at his location, t m s are the times of occurrences of all after-shocks that took place prior to time t at the specific seismic location, and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "where M m is the magnitude of the shock occurring at time t m , and M 0 is the cut-off magnitude of the data set; we refer to [27] for details. As said above, dependence between (simultaneous) seismic activity in different seismic regions has been ignored in the classical univariate ETAS model. Below we suggest a possible method to construct a generalized multivariate Hawkes process that may offer a good way of modeling of joint seismic activities at various locations, accounting for dependencies between seismic activities at different locations and for consistencies with local data.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 130,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "We will now briefly describe this construction that leads to a plausible model, which we name the multivariate generalized ETAS model. Towards this end we consider a GMHP N (cf. Definition 3.1), where the index i = 1, . . . , d represents the i-th seismic location, and where the set E i = M i := {m 1 , m 2 , . . . , m n i } of marks is a discrete set whose elements represent possible magnitudes of seismic shocks with epicenter at location i. In the corresponding Hawkes kernel \u03ba the measure \u03b7(t, dy) represents the time-t background distribution of shocks' across all seismic regions, and the measure f (t, s, dy, x) represents the feedback effect.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "For the purpose of illustration, let d = 2. Suppose that local seismic data are collected for each location to the effect of producing local kernels of the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "In particular, the quantity \u03bb i (t) := \u03ba i (t, E i ) = y i \u2208M i \u03ba i (t, y i ) represents the time-t intensity of seismic activity at the i-th location.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "In order to produce an ETAS type model, we postulate that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "for j = 1, 2 and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "Thus,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "where t j,m s are the times of occurrences of after-shocks that took place prior to time t only at the i-th seismic location, and X j,t j,m is the magnitude of the after shock at location i that took place at time t j,m ; t c,m s are the times of occurrences of after-shocks that took place prior to time t both seismic locations, and X j,tc,m is the magnitude of the after shock at location i that took place at time t c,m .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "The classical univariate ETAS model has been extended in [26] to the (classical) univariate space-time ETAS model (see also Section 5 in [27] ). It is important to note that our generalized multivariate Hawkes process may also be used as an useful generalization of the space-time extension of the multivariate generalized ETAS model. In order to see this, let us consider the model (2.1) in [26] with g as in Section 2.1 in [26] , that is (in the original notation of [26] , which should not be confused with our notation)",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 61,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 137,
                    "end": 141,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 392,
                    "end": 396,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 425,
                    "end": 429,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 469,
                    "end": 473,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "Then, coming back to our generalized multivariate Hawkes process, let the seismic location i = 1, 2 be identified with a point in the plane with coordinates (a i , b i ) \u2208 R 2 . Next, let the set of marks E i be given as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "This will lead to a space-time generalized multivariate Hawkes process that will be studied elsewhere.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Seismology"
        },
        {
            "text": "It was already observed by Hawkes in [12] that Hawkes processes may find applications in epidemiology for modeling spread of epidemic diseases accounting for various types of cases, such as children or adults, that can be taken as marks. This insight has been validated over the years in numerous studies. We refer for example to [30, 28, 21] and the references therein.",
            "cite_spans": [
                {
                    "start": 37,
                    "end": 41,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 330,
                    "end": 334,
                    "text": "[30,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 335,
                    "end": 338,
                    "text": "28,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 339,
                    "end": 342,
                    "text": "21]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Epidemiology"
        },
        {
            "text": "It is important to account for the temporal and spatial aspects in the modeling of spread and intensity of epidemic and pandemic diseases, such as COVID-19. We believe that the variant of the generalized multivariate Hawkes process that we described at the end of Section 6.1 may offer a valuable tool in this regard. This will be investigated in a follow-up work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Epidemiology"
        },
        {
            "text": "Hawkes processes have found important applications in finance over the past two decades. We refer to [13] for a relevant survey. Here, we briefly discuss a possible application in finance of the generalized multivariate Hawkes processes.",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 105,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Finance"
        },
        {
            "text": "In a series of papers [2] , [3] , [1] introduced a multidimensional model for stock prices driven by (multivariate) Hawkes processes. The model for stock prices is formulated in [2] via a marked point process N = (T n , Z n ) n\u22651 , where Z n is a random variable taking values in {1, . . . , 2d}, and the compensator \u03bd of N has the form (it is assumed that",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 25,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 28,
                    "end": 31,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 34,
                    "end": 37,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 178,
                    "end": 181,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Finance"
        },
        {
            "text": "with \u00b5 i \u2208 R + and functions \u03c6 i,j from R + to R + . Let us define the processes N i , i = 1, . . . 2d, by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finance"
        },
        {
            "text": "Note that the above implies that N 1 , . . . , N 2d have no common jumps and the F N -intensity of N i is given by \u03bb i and can be written in the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finance"
        },
        {
            "text": "In [2] it is assumed that a d-dimensional vector of assets prices S = (S 1 , . . . , S d ) is based on N via representation",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 6,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Finance"
        },
        {
            "text": "The obvious interpretation is that N 2i\u22121 corresponds to an upward jump of the i-th asset whereas N 2i corresponds to an downward jump of i-th asset. Bacry et.al. [2] showed that within such framework some stylised facts about high frequency data, such as microstructure noise and the Epps effect, are reproduced.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 166,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Finance"
        },
        {
            "text": "Using the GMHPs we can easily generalize their model in several directions. In particular, a model of stock price movements driven by a generalized multivariate Hawkes process N allows for common jumps in upward and/or downward direction. This can be done by setting the multivariate mark space of N to be and where \u00b5 e \u2208 R + and \u03c6 e,x is a function from R + to R + . Including possibility of embedding co-jumps of the prices of various stocks in the book in the common excitation mechanism, may turn out to be important in modeling the book evolution in general, and in pricing basket options in particular.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finance"
        },
        {
            "text": "In this appendix we provide some auxiliary concepts and results that are needed in the rest of the paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "Let (\u2126, F, P) be a probability space and (X , X ) be a Borel space. For a given sigma field G \u2286 F, we define a G-conditionally Poisson random measure on (R + \u00d7 X , B(R + ) \u2297 X ) as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Definition 7.1. Let \u03bd be a \u03c3-finite random measure on (R + \u00d7 X , B(R + ) \u2297 X ). A random measure N on (R + \u00d7 X , B(R + ) \u2297 X ) is a G-conditionally Poisson random measure with intensity measure \u03bd if the following two properties are satisfied:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "1. For every C \u2208 B(R + ) \u2297 X such that \u03bd(C) < \u221e, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "2. For arbitrary n = 1, 2, . . . , and arbitrary disjoint sets C 1 , . . . , C n from B(R + ) \u2297 X , such that \u03bd(C m ) < \u221e, m = 1, . . . , n, the random variables",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Clearly \u03bd is G-measurable. Note that if G is trivial \u03c3-field (or if N is independent of G), then N is a Poisson random measure (see Chapter 4.19 in [29] ), which sometimes referred to as the Poisson process on R + \u00d7 X (see e.g. [20] ). In this case \u03bd is a deterministic \u03c3-finite measure. For G = \u03c3(\u03bd), the \u03c3(\u03bd)-conditional Poisson random measure is also known in the literature as Cox process directed by \u03bd (see [20] ). Now we will provide a construction of a G-conditional Poisson random measure with the intensity measure given in terms of a specific kernel g. In fact, the measure constructed below is supported on sets from B((0, T ]) \u2297 X , in the sense that for any set C that has an empty intersection with (0, T ] \u00d7 X the value of the measure is 0 almost surely.",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 152,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 228,
                    "end": 232,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 412,
                    "end": 416,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "We begin by letting g(t, y, dx) be a finite kernel from (R + \u00d7 Y, B(R + ) \u2297 Y) to (X , X ), where (Y, Y) and (X , X ) are Borel spaces, satisfying g(t, y, X ) = 0 for t > T.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "(7.1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Next, let \u2202 be an element external to X , and define kernel g \u2202 from (R + \u00d7 Y, B(R + ) \u2297 Y) to (X \u2202 , X \u2202 ) as g \u2202 (t, y, dx) = \u03bb(t, y)\u03b3(t, y, dx),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "where \u03bb(t, y) = g(t, y, X ), \u03b3(t, y, dx) = g(t, y, dx) g(t, y, X )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Suppose that Next, take Y to be a (Y, Y)-valued random element, which is G-measurable, and let Z and (U m , V m , W m ) \u221e m=1 be independent random variables uniformly distributed on (0, 1] and independent of G. We now define a random measure N on (R + \u00d7 X , B(R + ) \u2297 X ) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "where P , (T m , A m , X m ) \u221e m=1 are random variables defined by transformation of the sequence Z,(U m , V m , W m ) \u221e m=1 and the random element Y in the following way:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Using the above set-up we see that, for each m = 1, 2, . . . ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "where \u03b4 (\u221e,0,\u2202) is a Dirac measure. Note that even though the random elements X m , m = 1, 2, . . . , may take value \u2202, the measure N given in (7.2) is a random measure on (R + \u00d7 X , B(R + ) \u2297 X ) having support belonging to B([0, T ]) \u2297 X .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Given the above, we now have the following result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Lemma 7.2. The random measure N defined by (7.2) is a G-conditionally Poisson random measure with intensity measure \u03bd given by First we will prove that, conditionally on G, the random variable N ((s, t]\u00d7B) has the Poisson distribution with mean \u03bd((s, t] \u00d7 B). Towards this end we observe that P has, conditionally on G, the Poisson distribution with mean (T \u2212 (Y )) \u03bb(Y )1 { (Y )<T } (see (7. 3)), so",
            "cite_spans": [
                {
                    "start": 389,
                    "end": 392,
                    "text": "(7.",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Moreover, we conclude from (7.4) that for m = 1, 2, . . . ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "where the last equality follows from (7.1). Note that for u \u2208 R and m = 1, 2, . . . , we have 7",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "This and the G-conditional independence of P and (T m , A m , X m ) \u221e m=1 imply that Thus, the random variable N ((s, t] \u00d7 B) has the G-conditional Poisson distribution with mean equal to \u03bd((s, t] \u00d7 B).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Using standard monotone class arguments we obtain that for arbitrary C \u2208 B(R + ) \u2297 X random variable N (C) has, conditionally on G, the Poisson distribution with mean \u03bd(C).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Next, we will show that for 0 \u2264 s 1 < t 1 \u2264 s 2 < t 2 \u2264 . . . \u2264 s n < t n and for sets B 1 , . . . , B n \u2208 X the random variables",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "are conditionally independent given G. Towards this end let us define Using this representation we obtain that J := P(N ((s 1 , t 1 ] \u00d7 B 1 ) = l 1 , . . . , N ((s n , t n ] \u00d7 B n ) = l n |G)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "where l = n j=1 l j . Now, from (7.6), we see that the random vector",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "has, conditionally on G, the multinomial distribution with parameters p 1 , . . . , p n+1 given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "for j = 1, . . . , n, and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "Hence, using the fact that l = n j=1 l j , we deduce that J = \u221e r=l r! l 1 ! . . . l n !(r \u2212 l)! p l 1 1 \u00b7 \u00b7 \u00b7 p ln n p r\u2212l n+1 P(P = r|G) = 1 l 1 ! . . . l n ! p l 1 1 \u00b7 \u00b7 \u00b7 p ln n \u221e r=0 (r + l)! r! p r n+1 P(P = r + l|G)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "where the last equality follows from the fact that N ((s j , t j ] \u00d7 B j ) has the G-conditional Poisson distribution with mean equal to \u03bd((s j , t j ] \u00d7 B j ) = p j (T \u2212 (Y )) \u03bb(Y )1 { (Y )<T } , which is a consequence of (7.5), (7.6) and (7.8) . Using standard use the monotone class arguments we conclude from (7.7) that for arbitrary disjoint sets C 1 , . . . , C n \u2208 B(R + ) \u2297 X that random variables N (C 1 ), . . . , N (C n ) are G-conditionally independent. The proof is now complete.",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 245,
                    "text": "(7.8)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Conditional Poisson random measure: definition and specific construction"
        },
        {
            "text": "We begin by recalling (cf. Chapter 6 in [22] ) the concept of a doubly stochastic marked Poisson process. For this, we consider a filtration F on (\u2126, F, P). A marked point process N on (R + \u00d7 X , B(R + ) \u2297 X ) is an F-doubly stochastic marked Poisson process if there exist an F 0 -measurable random measure \u03bd on on (R + \u00d7 X , B(R + ) \u2297 X ) such that Hence, by analogy with the concept of the intensity of a Poisson random measure, the measure \u03bd is called the F 0 -intensity kernel of N (see Chapter 6 in [22] ). Let now N be marked point process on (R + \u00d7X , B(R + )\u2297X ), such that its F-compensator \u03bd is the F 0 -intensity kernel in a sense that the property analogous to (7.10) holds, Then, one can show (see Theorem 6.1.4 in [22] ) that N is an F-doubly stochastic marked Poisson process, i.e. the analog of (7.9) holds with N and \u03bd. The opposite statement is true as well (see Theorem 6.1.4 in [22] ): if N is an F-doubly stochastic marked Poisson process, then the F-compensator \u03bd of N is an F 0 -intensity kernel of N .",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 44,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 505,
                    "end": 509,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 729,
                    "end": 733,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 899,
                    "end": 903,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "Conditional Poisson random measures on (R + \u00d7 X , B(R + ) \u2297 X ) are closely related to F-doubly stochastic marked Poisson processes. It can be shown that if N is an F-doubly stochastic marked Poisson process with intensity kernel \u03bd, then N considered as a random measure is an F 0 -conditionally Poisson random measure with intensity kernel \u03bd.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "This implies that for sets B 1 , . . . , B n \u2208 X and for 0 \u2264 s 1 < t 1 \u2264 s 2 < t 2 \u2264 . . . \u2264 s n < t n \u2264 t, n \u2208 N, we have The next result, in a sense, complements our discussion of conditional Poisson random measures and doubly stochastic marked Poisson processes. Proposition 7.3. i) Let M be a marked point process on (R + \u00d7 X , B(R + ) \u2297 X ), which is a G-conditional Poisson random measure with intensity measure \u03bd, and let F M be a filtration defined by family of \u03c3-fields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "Then M is an F M -doubly stochastic marked Poisson process with F M 0 -intensity kernel \u03bd being also F M -compensator of M . ii) Let N = (N j ) j\u22651 be a family of marked point processes on (R + \u00d7 X , B(R + ) \u2297 X ), which are G-conditional Poisson random measures (each N j with intensity measure \u03bd j ), and let F N be a filtration defined by the family of \u03c3-fields",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "Suppose that (N j ) j\u22651 are G-conditionally independent. Then each N j is an F N -doubly stochastic marked Poisson process with F N 0 -intensity kernel \u03bd j being also F N -compensator of N j .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "In the proof of Proposition 7.3 we will use the following elementary result, whose derivation is omitted: Lemma 7.4. Let G be a sigma field and let A \u2208 G. Then for arbitrary measurable sets B and C which are conditionally independent given G we have E(1 B 1 A\u2229C ) = E(P(B|G)1 A\u2229C ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "Proof. (of Proposition 7.3) We will prove ii), the proof of i) is similar in spirit to the proof of ii) and in fact a bit simpler. Fix arbitrary j \u2265 1. By assumption N j is a G-conditional Poisson random measure, so we have for fixed 0 \u2264 s < t and D \u2208 X P(N j ((s, t] \u00d7 D) = i|G) = e \u2212\u03bd j ((s,t]\u00d7D) (\u03bd j ((s, t] \u00d7 D)) i i! , (7.12) where \u03bd j is G = F N 0 -measurable random measure. In view of the definition of F N -doubly stochastic marked Poisson process, of the above formula and of Proposition 6.1.4 in [22] it suffices to show that for arbitrary set F \u2208 F N s it holds for F N 0 -measurable random measure \u03bd j . So that N j is a F N -doubly stochastic marked Poisson process with F N 0 -intensity kernel \u03bd j . Then Proposition 6.1.4 in [22] implies that \u03bd j is F N -compensator of N j .",
            "cite_spans": [
                {
                    "start": 325,
                    "end": 331,
                    "text": "(7.12)",
                    "ref_id": null
                },
                {
                    "start": 508,
                    "end": 512,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 742,
                    "end": 746,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "To prove (7.13) we will use the Monotone Class Theorem. First note that sets F for which (7.13) holds constitute \u03bb-system. Thus it suffices to show the above equality for a \u03c0-system of sets which generates F N s . Towards this end consider family of sets:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "A s := A \u2229 C : A \u2208 G, C = \u2229 n r=1 \u2229 pr l=1 {N mr ((s r l , t r l ] \u00d7 D r l ) = k r l }, 0 \u2264 s r 1 < t r 1 \u2264 . . . \u2264 s r pr < t r pr \u2264 s, D r 1 , . . . D r pr \u2208 X , k r 1 , . . . , k r pr \u2208 N, 0 \u2264 p 1 \u2264 . . . \u2264 p r , 0 \u2264 m 1 \u2264 . . . \u2264 m r , r = 1, . . . , n, n \u2208 N .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "Clearly, A s is a \u03c0-system and \u03c3(A s ) = F N s . Let us take F \u2208 A s , so F = A \u2229 C, and let (s, t] \u00d7 D be disjoint with sets (s r l , t r l ] \u00d7 D r l which define C. This and G-conditional independence of {N j } j\u22651 imply that events {N j ((s, t] \u00d7 D) = k} and C are conditionally independent given G. Hence, by applying Lemma 7.4, we obtain that (7.13) holds for F \u2208 A s . Then, invoking the Monotone Class Theorem, we conclude that (7.13) holds for sets F \u2208 F N s . The proof is complete. Then \u00b5 is a measure. Moreover for any measurable non negative function F : X \u2192 R + we have X F d\u00b5 = lim n\u2192\u221e n k=1 X F d\u00b5 k Proof. The first part follows from the Nikodym convergence theorem (see e.g. Theorem 7.48 in Swartz [31] ).",
            "cite_spans": [
                {
                    "start": 715,
                    "end": 719,
                    "text": "[31]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Relation between conditional Poisson random measures and doubly stochastic marked Poisson processes"
        },
        {
            "text": "To prove the second assertion it suffices to consider simple step functions only, i.e. functions F of the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Additional Technical Result"
        },
        {
            "text": "For such F it holds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Additional Technical Result"
        },
        {
            "text": "Using usual approximation technique and the monotone convergence theorem we finish the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Additional Technical Result"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Some limit theorems for Hawkes processes and application to financial statistics",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Bacry",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Delattre",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hoffmann",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Muzy",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Stochastic Processes and their Applications",
            "volume": "123",
            "issn": "",
            "pages": "2475--2499",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Modelling microstructure noise with mutually exciting point processes",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Bacry",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Delattre",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hoffmann",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Muzy",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Quant. Finance",
            "volume": "13",
            "issn": "1",
            "pages": "65--77",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Hawkes model for price and trades high-frequency dynamics",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Bacry",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Muzy",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Quant. Finance",
            "volume": "14",
            "issn": "7",
            "pages": "1147--1166",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Semimartingales and shrinkage of filtration",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "R"
                    ],
                    "last": "Bielecki",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jakubowski",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jeanblanc",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nieweglowski",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Structured Dependence between Stochastic Processes",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "R"
                    ],
                    "last": "Bielecki",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jakubowski",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nieweglowski",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Stability of nonlinear Hawkes processes",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Br\u00e9maud",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Massouli\u00e9",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Ann. Probab",
            "volume": "24",
            "issn": "3",
            "pages": "1563--1588",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Probability and stochastics",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "\u00c7\u0131nlar",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Graduate Texts in Mathematics",
            "volume": "261",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Affine processes and applications in finance",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Duffie",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Filipovi\u0107",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Schachermayer",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Ann. Appl. Probab",
            "volume": "13",
            "issn": "3",
            "pages": "984--1053",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Multivariate Hawkes processes: an application to financial data",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Embrechts",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liniger",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "48A(New frontiers in applied probability: a Festschrift for S\u00f8ren Asmussen",
            "volume": "",
            "issn": "",
            "pages": "367--378",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Markov processes: Characterization and convergence",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "N"
                    ],
                    "last": "Ethier",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "G"
                    ],
                    "last": "Kurtz",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical Statistics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Point Spectra of Some Mutually Exciting Point Processes",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Hawkes",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological)",
            "volume": "33",
            "issn": "3",
            "pages": "438--443",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Spectra of Some Self-Exciting and Mutually Exciting Point Processes",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Hawkes",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "Biometrika",
            "volume": "58",
            "issn": "1",
            "pages": "83--90",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Hawkes processes and their applications to finance: a review",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Hawkes",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Quantitative Finance",
            "volume": "18",
            "issn": "2",
            "pages": "193--198",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "A cluster process representation of a self-exciting process",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "Hawkes",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Oakes",
                    "suffix": ""
                }
            ],
            "year": 1974,
            "venue": "J. Appl. Probab",
            "volume": "11",
            "issn": "",
            "pages": "493--503",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Two results on jump processes",
            "authors": [
                {
                    "first": "S.-W",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J.-G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "S\u00e9minaire de Probabilit\u00e9s XVIII 1982/83",
            "volume": "",
            "issn": "",
            "pages": "256--267",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Semimartingale Theory and Stochastic Calculus",
            "authors": [
                {
                    "first": "S.-W",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J.-G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Ji.-A",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Multivariate point processes: predictable projection, Radon-Nikod\u00fdm derivatives, representation of martingales",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jacod",
                    "suffix": ""
                }
            ],
            "year": 1974,
            "venue": "Z. Wahrscheinlichkeitstheorie und Verw. Gebiete",
            "volume": "31",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Limit theorems for stochastic processes",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jacod",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Shiryaev",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Grundlehren der Mathematischen Wissenschaften",
            "volume": "288",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Mathematical methods for financial markets",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jeanblanc",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yor",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chesney",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Foundations of modern probability. Probability and its Applications",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Kallenberg",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Real-time predictions of the 2018 -2019 Ebola virus disease outbreak in the Democratic Republic of the Congo using Hawkes point process models",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Kelly",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Harrigan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Epidemics",
            "volume": "28",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Marked point processes on the real line: The dynamic approach. Probability and its Applications",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Last",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Brandt",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Multivariate hawkes processes",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "J"
                    ],
                    "last": "Liniger",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "The Markovian self-exciting process",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Oakes",
                    "suffix": ""
                }
            ],
            "year": 1975,
            "venue": "J. Appl. Probability",
            "volume": "12",
            "issn": "",
            "pages": "69--77",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Space-time Point-process Models for Earthquake Occurrences",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ogata",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Ann. Inst. Math. Statist",
            "volume": "50",
            "issn": "",
            "pages": "379--402",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Seismicity Analysis through Point-process Modeling: A Review",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ogata",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Pure appl. geophys",
            "volume": "155",
            "issn": "",
            "pages": "471--507",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "SIR-Hawkes: Linking epidemic models and Hawkes processes to model diffusions in finite populations",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Rizoiu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mishra",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Carman",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "WWW '18: Proceedings of the 2018 World Wide Web Conference",
            "volume": "",
            "issn": "",
            "pages": "419--428",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "L\u00e9vy processes and infinitely divisible distributions",
            "authors": [
                {
                    "first": "K.-I",
                    "middle": [],
                    "last": "Sato",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Cambridge Studies in Advanced Mathematics",
            "volume": "68",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "A recursive point process model for infectious diseases",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "P"
                    ],
                    "last": "Schoenberg",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hoffmann",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "J"
                    ],
                    "last": "Harrigan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Annals of the Institute of Statistical Mathematics",
            "volume": "71",
            "issn": "",
            "pages": "1271--1287",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Multiplier convergent series",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Swartz",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Filtering and parameter estimation for partially observed generalized Hawkes processes",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vacarescu",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Nonlinear Hawkes Processes",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Note that the process \u03bd([0, \u00b7], A) = [0,\u00b7] 1 ]]0,T\u221e(\u03c9)[[ (",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Let N be the marked point process introduced in (2.1) with the corresponding random measure N defined in (2.2). We call N a generalized Hawkes process on (\u2126, F, P), if the (F N , P)-compensator of N , say \u03bd, is of the form (2.3). The kernel \u03ba is called the Hawkes kernel for N .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "(i) Recall that the compensator of a random measure is unique (up to equivalence). Thus, the compensator \u03bd of N is unique. However, the representation (2.3)-(2.4) is not unique, by any means, in general. For any given \u03b7 and f in the representation (2.3)-(2.4), one can always find \u03b7 = \u03b7 and f = f such that \u03ba(t, dy) = \u03b7(t, dy) + (0,t)\u00d7X f (t, s, x, dy)N (ds, dx).(2.6)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Remark 3.6. As we know from Remark 2.4 the Hawkes kernel \u03ba determines the law of N . However, in view of Remark 3.5, the kernel \u03ba i may not determine the law of N i . It remains to be an open problem for now to determine sufficient conditions under which the law of N i is determined by \u03ba i . This problem is a special case of a more general problem: what are general sufficient conditions under which characteristics of a semimartingale determine the law of this semimartingale.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "idio,{1} ((0, t], A) := N ((0, t], A \u00d7 \u2206), t \u2265 0, A \u2208 E 1 and, likewise, for I = {2} we define N idio,{2} ((0, t], A) := N ((0, t], \u2206 \u00d7 A), t \u2265 0, A \u2208 E 2 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": ", for I = {1, 2} we define N idio,{1,2} ((0, t], A) := N ((0, t], A), t \u2265 0, A \u2208 E 1 \u2297 E 2 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "j=1 of independent identically distributed random variables with uniform distribution on (0, 1]. Let D : [0, \u221e)\u00d7 (0, 1] \u2192 N be a measurable function such that (0,1]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "The marked point process N k+1 is an H k+1 -doubly stochastic marked Poisson process with intensity kernel \u03bd k+1 given by\u03bd k+1 ((s, t] \u00d7 D) = \u221e j=1 \u03bd k+1 j ((s, t] \u00d7 D) = t s (0,v)\u00d7E \u2206f (v, u, y, D)N k (du, dy)dv, (4.14)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": ", A), and \u03bd k+1 (\u03c9, A) j (dt, dy, d\u03c9) = N k+1 j",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Let F and G be filtrations in (\u2126, F, P). Then F is P-immersed in F \u2228 G if and only if for every t \u2265 0 and every bounded G t -measurable random variable \u03b7 we have E(\u03b7|F t ) = E(\u03b7|F \u221e ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "The H \u221e -compensator of H \u221e is given by\u03d1 \u221e ((s, t] \u00d7 D) (v, u, y, D)H \u221e (du, dy) dv.(4.23)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "The process N := 1 ]]0,T\u221e[[ H \u221e is an F N -Hawkes process with the Hawkes kernel 5 \u03ba(t, dx) = \u03b7(t, dx) + (0,t)\u00d7E \u2206 f (t, u, y, dx)N (du, dy).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "= N 2 ((0, t], {1}) = N ((0, t], {1, \u2206} \u00d7 {1}). Moreover, N idio,{1,2} -the MPP of idiosyncratic group of {1, 2} coordinates -reduces here to the process counting the number of occurrences of the common events: N c t = N idio,{1,2} ((0, t], {(1, 1)}) = N ((0, t], {(1, 1)}).We take the following values of parameters:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Bar plot of 10 paths of a bivariate point Hawkes process. Red bars represent common events, black bars represents idiosyncratic events.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Plot of a single path of counting processes associated with 2-variate Hawkes process.with the following values of the parameters:i \u03b1 i \u00b5 i \u03c3 i \u03b2 i a i g i,i g sample path is presented onFigure 3.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Plot of a simulated path of the bivariate Hawkes process specified in Example 4.10.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "The coordinates of N (cf. (3.2)) reduce here to counting (point) processes N 1 t = N 1 ((0, t], {1}) = N ((0, t], {1} \u00d7 {1, \u2206}), and N 2 t = N 2 ((0, t], {1}) = N ((0, t], {1, \u2206} \u00d7 {1}).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "\u2206 = {e = (e 1 , . . . , e 2d ) : e i \u2208 {1, \u2206}} \\ {(\u2206, . . . , \u2206)}, and the F N -compensator of N to be \u03bd(dt, dy) = 1 ]]0,T\u221e[[ (t) e\u2208E \u2206 \u03b4 e (dy)\u03bb e (t)dt, where \u03bb e (t) = \u00b5 e + E \u2206 \u00d7(0,t) \u03c6 e,x (t \u2212 s)N (ds \u00d7 dx), e \u2208 E \u2206 , t \u2265 0,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "A (\u0393(t, y, u))du, A \u2208 X , for some measurable mappings : Y \u2192 [0, T ] \u222a {\u221e}, \u03bb : Y \u2192 (0, \u221e) and \u0393 : R + \u00d7 Y \u00d7 (0, 1] \u2192 X . Existence of such mapping \u0393 is asserted by Lemma 3.22 in [20]. In addition, let D : [0, \u221e) \u00d7 (0, 1] \u2192 N be as in Step 1 of our construction done in Section 4.1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "To prove the result we consider N ((s, t] \u00d7 B) for fixed 0 \u2264 s \u2264 t, B \u2208 X . We haveN ((s, t] \u00d7 B) = \u221e m=1 \u03b4 (Tm,Xm) ((s, t] \u00d7 B)1 { (Y )<T,i\u2264P,Am\u2264\u03bb(Tm,Y )} = P m=1 1 { (Y )<T,s<Tm\u2264t,Xm\u2208B,Am\u2264\u03bb(Tm,Y )} .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "(e iuN ((s,t]\u00d7B) |G) = e (e iu \u22121)p(Y )(T \u2212 (Y )) \u03bb(Y )1 { (Y )<T } = e (e iu \u22121) t s 1 ( (Y ),\u221e) (v)g(v,Y,B)dv = e (e iu \u22121)\u03bd((s,t]\u00d7B) .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "m ((s, t] \u00d7 B), for r \u2208 N, 0 \u2264 s < t, B \u2208 X , where I m ((s, t] \u00d7 B) := 1 {s<Tm\u2264t, Xm\u2208B, Am\u2264\u03bb(Tm,Y )} . 7 In the ensuing two formulae i = \u221a \u22121. Note that the random variable N ((s, t] \u00d7 B) can be represented as N ((s, t] \u00d7 B) = S P ((s, t] \u00d7 B).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "(N ((s, t] \u00d7 B) = k|F s ) = e \u03bd((s,t]\u00d7B) (\u03bd((s, t] \u00d7 B)) k k! , 0 \u2264 s < t, B \u2208 X . (7.9)Thus, for 0 \u2264 s < t, B \u2208 X we have\u03bd((s, t] \u00d7 B) = E(N ((s, t] \u00d7 B)|F 0 ). (7.10)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": "\u03bd((s, t] \u00d7 B) = E( N ((s, t] \u00d7 B)|F 0 ), 0 \u2264 s < t, B \u2208 X .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF28": {
            "text": "((s i , t i ] \u00d7 B i ) = l i }|F 0 = n i=1 e \u03bd((s i ,t i ]\u00d7B i ) (\u03bd((s i , t i ] \u00d7 B i )) {N ((s i , t i ] \u00d7 B i ) = l i }|F 0 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF29": {
            "text": "{N j ((s,t]\u00d7D)=k} 1 F ) = E(P(N j ((s, t] \u00d7 D) = k|G)1 F ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF30": {
            "text": "j ((s, t] \u00d7 D) = i| F N s ) = e \u2212\u03bd j ((s,t]\u00d7D) (\u03bd j ((s, t] \u00d7 D)) i i! .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF31": {
            "text": "Let (\u00b5 k ) \u221e k=1 be a sequence of measures. Let \u00b5 be a mapping \u00b5 : X \u2192 [0,",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "F . This means that \u03bd k+1 is the H k+1 -compensator of N k+1 .",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "). So the compensator of the restriction of H \u221e to ]]0, T \u221e [[\u00d7E \u2206 is the restriction to ]]0, T \u221e [[\u00d7E \u2206 of compensator of H \u221e . Now we will prove that",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}