{
    "paper_id": "ea8048d190afec36c83714dd757797827b369f91",
    "metadata": {
        "title": "Mining Coronavirus (COVID-19) Posts in Social Media",
        "authors": [
            {
                "first": "Negin",
                "middle": [],
                "last": "Karisani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Emory University",
                    "location": {}
                },
                "email": "nkarisan@purdue.edu"
            },
            {
                "first": "Payam",
                "middle": [],
                "last": "Karisani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Emory University",
                    "location": {}
                },
                "email": "payam.karisani@emory.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "World Health Organization (WHO) characterized the novel coronavirus (COVID-19) as a global pandemic on March 11th, 2020. Before this and in late January, more specifically on January 27th, while the majority of the infection cases were still reported in China and a few cruise ships, we began crawling social media user postings using the Twitter search API. Our goal was to leverage machine learning and linguistic tools to better understand the impact of the outbreak in China. Unlike our initial expectation to monitor a local outbreak, COVID-19 rapidly spread across the globe. In this short article 1 we report the preliminary results of our study on automatically detecting the positive reports of COVID-19 from social media user postings using state-of-the-art machine learning models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "According to a tally by Johns Hopkins University 566,269 persons are tested positive and 25,423 persons have died around the globe as of today, March 27th. Approximately a third of the world's population is impacted by COVID-19. The United States became the epicenter of the virus pandemic on March 26thyesterday-and New York City with 23,112 confirmed cases is the epicenter of the US outbreak. The US House passed a $2 trillion stimulus bill to combat the negative impact of COVID-19 on the country's economy. Despite the devastating global impact of COVID-19, WHO has announced that the current pandemic would be the first pandemic in human history that could be controlled.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "The impact of COVID-19 on societies is unprecedented. Numerous countries in Asia and the EU, including Iran, Italy, and Spain are under a lock-down. In the US, states such as California and New York are experiencing the same situation. People are ordered to stay home, and are encouraged to practice social distancing. Psychologists advise the residents of the affected areas to practice certain routines to maintain their mental well-being. With people staying at home more often, the role of the internet, as a means of communication, has become even more critical. For instance, NextDoor, a hyperlocal social network, recently announced that the daily rate of its active users has increased by 80%.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "It has long been known that social networks are effective media for public health monitoring. Despite the well-understood limitations and biases present in the conclusions drawn from social media data (Olteanu et al., 2018) , they are proven to be invaluable resources (Paul and Dredze, 2017) . In this article, we report the preliminary results of our study on automatically mining the user postings related to COVID-19 on Twitter. Our goal is to find the extent in which machine learning models can distill the user generated data. As pointed out by previous studies (Karisani and Agichtein, 2018) , this can facilitate the related institutions' responses to the outbreak. In the next section, we focus on automatically detecting the positive reports of COVID-19 infections in the data that we have been collecting since January 27th, 2020.",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 223,
                    "text": "(Olteanu et al., 2018)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 269,
                    "end": 292,
                    "text": "(Paul and Dredze, 2017)",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 569,
                    "end": 599,
                    "text": "(Karisani and Agichtein, 2018)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "We started collecting Twitter data on January 27th, 2020. As of March 26th, we have collected 5,621,048 tweets. We used the Twitter search API to crawl the data, and our search keywords were initially \"coro- navirus\" and \"corona virus\". On mid-March we added the keywords \"COVID-19\" and \"COVID 19\" to the search criteria. We only collected the English user postings, and omitted retweets and replies. In order to evaluate the machine learning models we also manually inspected the data, and used stratified sampling to construct a dataset. Thus, for the tweets posted in February we randomly selected between 100 and 300 examples a day and collected the total of 6,090 tweets. This set of tweets constitute our training set. Additionally, we also randomly selected a set of 200 examples a day between March 3rd and March 12th, and collected the total of 2,000 tweets, which constitute our test set. We hired an annotator and annotated the data based on the following criteria: 1) If a tweet mentions individuals infected with COVID-19, and also explicitly or implicitly contains a time reference then it was labeled positive. 2) If a tweet does not mention any individual or lacks a time reference it was labeled negative.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "To validate the quality of the labels we hired a second annotator and randomly relabeled 10% of the tweets. The inter-agreement between the two annotators was 0.70 based on Cohen Kappa coefficient, which indicates a substantial correlation (Viera and Garrett, 2005) . Table 1 summarizes the training and test sets, and their corresponding percentage of the positive and negative examples. In the next section, we discuss the methods that we implemented and report their results.",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 265,
                    "text": "(Viera and Garrett, 2005)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 268,
                    "end": 275,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "We begin this section by describing the methods that we implemented, then we briefly discuss the training procedure, and finally report the results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We included seven methods in our experiments. One classic generative model (Naive Bayes), one classic discriminative model (Logistic Regression), one widely used neural network model (fasttext), and four models based on the state-of-the-art model Bidirectional Encoder Representations from Transformers (BERT). Below we briefly describe each one. NB. We included the Naive Bayes classifier. We incorporated the MALLET implementation (McCallum, 2002) of this classifier, and used the tweet unigrams and bigrams as features. LR. We included the Logistic Regression as the discriminative counterpart of Naive Bayes. We incorporated the MALLET implementation, and again used the tweet unigrams and bigrams as features. Fasttext. We included the neural model introduced in (Joulin et al., 2016) . This model is a shallow wide network, capable of updating the input word embeddings during the training. We used the pretrained word2vec vectors (Mikolov et al., 2013) as input features. The learning rate was empirically set to 0.5, and the window size was set to 2. BERT-BASE. We included the state-of-the-art model introduced in (Devlin et al., 2019) . This model is based on a multi-layer transformer encoder (Vaswani et al., 2017) . We used the pre-trained base variant, followed by one layer fully connected network. We applied the default model settings recommended in (Devlin et al., 2019) . We used the pytorch implementation of BERT introduced in (Wolf et al., 2019) . BERT-Twitter. Since our classification problem is defined on social media posts, we can expect that a model specifically exposed to the social media language model (through the Masked Language model task) would perform better than regularly pre-trained ones. Thus, we used a corpus of 35 million tweets collected between 2018 and 2019 through the Twitter streaming API to further pre-train BERT-BASE. We set the maximum window size to 160 and batch-size to 32-the rest of the settings were set to the default values, as suggested in (Devlin et al., 2019)-and pre-trained Table 2 : Average F1, precision, and recall in detecting positive reports of COVID-19 from Twitter data.",
            "cite_spans": [
                {
                    "start": 433,
                    "end": 449,
                    "text": "(McCallum, 2002)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 768,
                    "end": 789,
                    "text": "(Joulin et al., 2016)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 937,
                    "end": 959,
                    "text": "(Mikolov et al., 2013)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1123,
                    "end": 1144,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1204,
                    "end": 1226,
                    "text": "(Vaswani et al., 2017)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1367,
                    "end": 1388,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 2003,
                    "end": 2040,
                    "text": "(Devlin et al., 2019)-and pre-trained",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 1448,
                    "end": 1467,
                    "text": "(Wolf et al., 2019)",
                    "ref_id": null
                },
                {
                    "start": 2041,
                    "end": 2048,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Methods Compared"
        },
        {
            "text": "coronavirus is used in, it would perform better during the classification phase. Thus, we used the tweets that we collected between January 27th and March 3rd to pre-train BERT-BASE. We pre-trained this model for 400K steps-approximately 5 epochs. The pre-training settings were identical to BERT-Twitter.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods Compared"
        },
        {
            "text": "BERT-Corona-BiLSTM. Even though BERT already utilizes a sophisticated attention mechanism, we still experimented with sequence encoding models. Thus, we used a Bidirectional Long Short Term Memory Network (Hochreiter and Schmidhuber, 1997 ) on top of BERT-Corona, followed by one layer fully connected network. We empirically observed that if we set the size of the hidden dimensions of the BiLSTM to a half of the size of the hidden dimensions of BERT (i.e., 768) we would get the best performance.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 238,
                    "text": "(Hochreiter and Schmidhuber, 1997",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Methods Compared"
        },
        {
            "text": "We trained Fasttext for 100 iterations. The models based on BERT, i.e., BERT-BASE, BERT-Twitter, and BERT-Corona, were trained for 2 iterations-these models are already based on a pre-trained model. We trained BERT-Corona-BiLSTM for 3 iterations, since it has more parameters than the other BERT based baselines. For training the models, we used the default model optimizers proposed by the references. In none of the experiments we did any text pre-processing. Since there is a randomness in model initialization and drop-out regularization, we carried out all of the neural network experiments for five times. The results reported in the next section are the average over these experiments. The task that we defined is a binary classification problem-detecting the positive reports of COVID-19 from Twitter data. Since the class distribution is highly skewed, following the previous studies (McCreadie et al., 2019), we report the F1, Precision, and Recall of the models in the positive class. Table 2 summarizes the performance results. We see that the baselines based on the pre-trained models show the best results. The experiments validate our hypothesis about the effectiveness of pre-training BERT on domain specific data. We see that BERT-Corona has achieved the best F1 value. By comparing BERT-Twitter and BERT-Corona-BiLSTM we can also hypothesize that model initialization-through pretraining-can be potentially more effective than increasing model complexity. Even though validating this hypothesis require more comprehensive experiments.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 996,
                    "end": 1003,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Experimental Setup"
        },
        {
            "text": "We believe a robust social media surveillance system can be immensely helpful. Although the results are encouraging, there are still a lot of challenges to be addressed to build such a system for COVID-19. Automatically detecting positive reports, or even following up on the mental well-being of patients through social media posts can greatly enhance the concerned institutions' endeavor to monitor the public health and respond in timely manner.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "In this short article we reported the preliminary results of our study on the capability of machine learning models to distill social media posts related to COVID-19, namely we focused on automatically detecting the positive reports of this illness. We constructed a manually annotated dataset, and showed that stateof-the-art classifiers have encouraging results. Our pre-trained model and unlabeled data can be accessed",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "Sepp",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J\u00fcrgen",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural computation",
            "volume": "9",
            "issn": "8",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Bag of tricks for efficient text classification",
            "authors": [
                {
                    "first": "Armand",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "Edouard",
                    "middle": [],
                    "last": "Grave",
                    "suffix": ""
                },
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1607.01759"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Did you really just have a heart attack? towards robust detection of personal health mentions in social media",
            "authors": [
                {
                    "first": "Payam",
                    "middle": [],
                    "last": "Karisani",
                    "suffix": ""
                },
                {
                    "first": "Eugene",
                    "middle": [],
                    "last": "Agichtein",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 World Wide Web Conference",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Mallet: A machine learning for language toolkit",
            "authors": [
                {
                    "first": "Andrew Kachites",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Trec incident streams: Finding actionable information on social media",
            "authors": [
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Mccreadie",
                    "suffix": ""
                },
                {
                    "first": "Cody",
                    "middle": [],
                    "last": "Buntain",
                    "suffix": ""
                },
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Soboroff",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 16th International Conferenc e on Information Systems for Crisis Response and Management",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Distributed representations of words and phrases and their compositionality",
            "authors": [
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [
                        "S"
                    ],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "26",
            "issn": "",
            "pages": "3111--3119",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "A critical review of online social data: Biases, methodological pitfalls, and ethical boundaries",
            "authors": [
                {
                    "first": "Alexandra",
                    "middle": [],
                    "last": "Olteanu",
                    "suffix": ""
                },
                {
                    "first": "Emre",
                    "middle": [],
                    "last": "K\u0131c\u0131man",
                    "suffix": ""
                },
                {
                    "first": "Carlos",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Social monitoring for public health",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Michael",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Paul",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dredze",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Synthesis Lectures on Information Concepts, Retrieval, and Services",
            "volume": "9",
            "issn": "5",
            "pages": "1--183",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Ashish",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "Llion",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "Aidan",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "30",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Understanding interobserver agreement: the kappa statistic",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Viera",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Garrett",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Fam Med",
            "volume": "37",
            "issn": "5",
            "pages": "360--363",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Huggingface's transformers: State-of-theart natural language processing",
            "authors": [
                {
                    "first": "Thomas",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                },
                {
                    "first": "Lysandre",
                    "middle": [],
                    "last": "Debut",
                    "suffix": ""
                },
                {
                    "first": "Victor",
                    "middle": [],
                    "last": "Sanh",
                    "suffix": ""
                },
                {
                    "first": "Julien",
                    "middle": [],
                    "last": "Chaumond",
                    "suffix": ""
                },
                {
                    "first": "Clement",
                    "middle": [],
                    "last": "Delangue",
                    "suffix": ""
                },
                {
                    "first": "Anthony",
                    "middle": [],
                    "last": "Moi",
                    "suffix": ""
                },
                {
                    "first": "Pierric",
                    "middle": [],
                    "last": "Cistac",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Rault",
                    "suffix": ""
                },
                {
                    "first": "R&apos;emi",
                    "middle": [],
                    "last": "Louf",
                    "suffix": ""
                },
                {
                    "first": "Morgan",
                    "middle": [],
                    "last": "Funtowicz",
                    "suffix": ""
                },
                {
                    "first": "Jamie",
                    "middle": [],
                    "last": "Brew",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "TABREF0": {
            "text": "The number, and the percentage of the positive and negative tweets in the training and test sets.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "this model for 4.5 million steps-about 5 epochs. BERT-Corona. We hypothesized that if a model is already familiar with the contexts in which the word",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}