{
    "paper_id": "PMC7206174",
    "metadata": {
        "title": "Discretization and Feature Selection Based on Bias Corrected Mutual Information Considering High-Order Dependencies",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Puloma",
                "middle": [],
                "last": "Roy",
                "suffix": "",
                "email": "pulomaa92@gmail.com",
                "affiliation": {}
            },
            {
                "first": "Sadia",
                "middle": [],
                "last": "Sharmin",
                "suffix": "",
                "email": "sharmin@iut-dhaka.edu",
                "affiliation": {}
            },
            {
                "first": "Amin",
                "middle": [
                    "Ahsan"
                ],
                "last": "Ali",
                "suffix": "",
                "email": "aminali@iub.edu.bd",
                "affiliation": {}
            },
            {
                "first": "Mohammad",
                "middle": [],
                "last": "Shoyaib",
                "suffix": "",
                "email": "shoyaib@du.ac.bd",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "In classification tasks, the objective of feature selection (FS) process is to choose the most useful features that contribute to the prediction of class variable. Usually, all the features of a dataset do not have equal importance, rather some may create noise or be redundant. FS methods are used to remove such irrelevant and redundant features and can be divided into three broad categories namely Wrapper [14, 18], Embedded [20], and Filter methods [13, 15, 16]. Among these, filter methods do not depend on a classifier to select a feature. It thus works faster, which is preferable for handling large feature sets [12].",
            "cite_spans": [
                {
                    "start": 411,
                    "end": 413,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 415,
                    "end": 417,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 430,
                    "end": 432,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 455,
                    "end": 457,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 459,
                    "end": 461,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 463,
                    "end": 465,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 622,
                    "end": 624,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Again, Mutual information (MI) is usually popular in filter based methods. MI can capture non-linear relationships among features and class variable, can be computed for both categorical and numerical data, and can deal with multiple classes [7]. For these reasons, in this paper, we focus on MI based filter methods.",
            "cite_spans": [
                {
                    "start": 243,
                    "end": 244,
                    "mention": "7",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In MI based filter methods, the main goal is to select a subset of features S from the original feature set, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F=\\{f_1, f_2, f_3, ..., f_n \\}$$\\end{document} in such a way that it will maximize joint MI (I(S; C)) with the class variable, C as showed in Eq. 1.1However, the computation of I(S; C) is a NP-hard problem [7]. To overcome this problem, different approximations such as MIFS [1], mRMR [10], JMI [19], RelaxMRMR [17] have been proposed over the last decades. In these methods, MI terms such as feature relevancy(R), redundancy(r), conditional redundancy(c) and interaction(i) are considered in order to achieve a better approximation. However, none of the aforementioned methods correct \u201cbias\" due to finite samples in calculating MI terms. In a recent method mDSM [16], it is shown that incorporating bias correction for R, r, and c terms improves the classification performance. However, the interaction term is not considered in mDSM which needs to be addressed for better approximation [17].",
            "cite_spans": [
                {
                    "start": 585,
                    "end": 586,
                    "mention": "7",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 654,
                    "end": 655,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 664,
                    "end": 666,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 674,
                    "end": 676,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 690,
                    "end": 692,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1043,
                    "end": 1045,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1268,
                    "end": 1270,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Apart from the evaluation criteria, searching is an important step in the FS methods to find out the combination of feature subset that performs well. Most popular searching techniques are forward selection, backward elimination, genetic algorithms (GA) based search [11]. Forward selection and backward elimination are greedy searching strategy that select/delete a feature one at a time. The limitation of these approaches are after selecting/deleting a feature, it cannot be deleted/re-selected later which may add redundant features [6]. On the other hand, GA based methods are computationally expensive and for a dataset with large number of features, it is not feasible to apply. Convex based Relaxation Approximation (COBRA) is proposed in [7] which provides a global solution for MI based FS. Another search strategy is introduced in mDSM where a small subset of features is selected using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} based forward selection that uses dynamic discretization. However, it cannot deselect a feature once it is already selected and do not show whether it is possible to use \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} based search for interaction term. Considering the aforementioned issues, we propose a method called Discretization and feature Selection based on bias corrected MI (DSbM) and make the following major contribution: First, we calculate bias for the interaction terms and propose to use it for FS. Second, we show that the interaction terms follow \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} distribution and proposed to use it in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} based search. Third, to obtain reduced number of feature, keeping similar performances with DSbM we propose a new method for simultaneous forward selection and backward elimination (DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document}).",
            "cite_spans": [
                {
                    "start": 268,
                    "end": 270,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 538,
                    "end": 539,
                    "mention": "6",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 748,
                    "end": 749,
                    "mention": "7",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The main objective of MI based features selection methods is to determine a subset of features that have maximum dependency with the given class as shown in Eq. 1. Alternatively, this problem can be formulated for incremental feature selection that is to add one feature at a time in the selected subset to maximize I(S; C). From a given set F with n number of features, a new feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} is added to the selected set, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S = \\{f_1,f_2,.....,f_{m-1}\\}$$\\end{document}, that maximizes the score for a feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document}:2Since I(S; C) remains constant with respect to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document}, we choose \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} that maximizes \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I(f_m; C \\mid S)$$\\end{document}. Using MI identities, this term can be expressed as3here, the terms \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I(f_m;C)$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I(f_m;S)$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I (f_m;S|C)$$\\end{document} represent feature relevancy, redundancy and conditional redundancy respectively [2]. Hence the score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$J(f_m)$$\\end{document} increases if the relevancy of the feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} is large and redundancy with the existing features is low. However, the score also increases if the conditional redundancy is higher than the redundancy term. Hence, there is a trade-off, and the overall score is what needs to be maximized. Brown et al. in [2] further shows under the assumption that (a) the selected features in S are independent given the feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} and (b) the selected features are class-conditionally independent given the feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} and removing terms that have no effect on the choice of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} one can obtain the following equivalent score function:4with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta =1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma =1$$\\end{document}, this is what we call the Rrc criterion. It can then be easily shown that the incremental FS criterion or score function of well known MI based method such as MIFS [1], mRMR [10], Extended mRMR [9], JMI [19], and MIM [5] can be derived from this parameterized version of the score function. For example, JMI [19] criteria can be derived setting the value of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta = \\gamma = \\frac{1}{\\mid S\\mid }$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 3341,
                    "end": 3342,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 4242,
                    "end": 4243,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 6176,
                    "end": 6177,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 6186,
                    "end": 6188,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 6206,
                    "end": 6207,
                    "mention": "9",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 6215,
                    "end": 6217,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 6229,
                    "end": 6230,
                    "mention": "5",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 6320,
                    "end": 6322,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Information Theoretic Feature Selection Methods",
            "ref_spans": []
        },
        {
            "text": "In [17], the authors propose a new criterion by relaxing the the first assumption. They show under the relaxed assumption that the selected features are conditionally independent given the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} and another feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} in S, the redundancy term can be approximated as the following5where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varOmega $$\\end{document} is not dependent on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document}. Instead of finding a feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} to condition on, they propose to average the right-hand side over all \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i \\in S$$\\end{document}, resulting in the following score function6here, the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I(f_m;f_j\\mid f_i)$$\\end{document} terms are the second order interaction term between the features. It should be noted that sum of the second order terms is normalized by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{1}{\\mid S\\mid \\mid S-1 \\mid }$$\\end{document} instead of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{1}{\\mid S\\mid }$$\\end{document}. The authors note that this is to prevent this sum to out-weight other terms. It can be seen that one can approximate the redundancy term using 3rd or higher order interaction terms by further relaxing the assumption. However, it is shown that the joint MI is more influenced by lower-order interaction terms in case of forward selection methods [4].",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 6,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 3625,
                    "end": 3626,
                    "mention": "4",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Information Theoretic Feature Selection Methods",
            "ref_spans": []
        },
        {
            "text": "Practically, all aforementioned MI terms that have been used for the approximation need bias correction due to the finite number of samples. To solve this issue, a recent method namely, mDSM [16] is proposed where bias corrected MI has been used for calculating relevancy, redundancy and complementary term. They show incorporating bias correction improves the accuracy of classification. Also, it is theoretically shown that these three terms follow \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^{2}$$\\end{document} distributions.7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} J_{mDSM}(f_m) = I(f_m ; C) - \\frac{(\\mathcal {M}-1)(\\mathcal {K}-1)}{2N\\ln {2}}+ \\frac{1}{\\mid S\\mid }\\sum _{f_i \\in S} (I(f_m;f_i\\mid C) -\\\\ \\frac{(\\mathcal {M}-1)(\\mathcal {I}-1)\\mathcal {K}}{2N\\ln {2}} - I(f_m;f_i) + \\frac{(\\mathcal {M}-1)(\\mathcal {I}-1)}{2N\\ln {2}}) \\end{aligned}$$\\end{document}here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {M}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {I}$$\\end{document} are the number of intervals in feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} respectively. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {K}$$\\end{document} is number of class and N is total number of samples. The limitation of mDSM is that it does not consider the interaction term while proposing bias corrected MI to calculate the feature score which is necessary for better approximation of joint MI.",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 194,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Information Theoretic Feature Selection Methods",
            "ref_spans": []
        },
        {
            "text": "Bias is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{(\\mathcal {M}-1)(\\mathcal {J}-1)\\mathcal {I}}{2N\\ln {2}}$$\\end{document} for Interaction \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I(f_m;f_j\\mid f_i)$$\\end{document} among the features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_j$$\\end{document} given feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {I}$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {J}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {M}$$\\end{document} are the number of intervals in feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} , \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_j$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} respectively.",
            "cite_spans": [],
            "section": "Theorem 1 ::: Discretization and Feature Selection Based on Bias Corrected Mutual Information (DSbM) ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Incorporating this bias corrected Interaction term with Eq. 7, DSbM uses the following criteria for discretization and feature selection.8\n",
            "cite_spans": [],
            "section": "Theorem 1 ::: Discretization and Feature Selection Based on Bias Corrected Mutual Information (DSbM) ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ I(f_m;f_j \\mid f_i) $$\\end{document} follows \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\chi ^2 $$\\end{document} distribution with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(\\mathcal {M}-1)(\\mathcal {J}-1)\\mathcal {I}$$\\end{document} degrees of freedom if \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_j$$\\end{document} are statistically independent.",
            "cite_spans": [],
            "section": "Theorem 2 ::: Discretization and Feature Selection Based on Bias Corrected Mutual Information (DSbM) ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "Based on Theorem 2, the critical value of the Interaction term will be as Eq. 99As the other three terms of Eq. 6 also follows \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} distribution, we can use their critical values (shown in [16]) for selecting a new feature. \n",
            "cite_spans": [
                {
                    "start": 478,
                    "end": 480,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Theorem 2 ::: Discretization and Feature Selection Based on Bias Corrected Mutual Information (DSbM) ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The overall process of DSbM is given in Algorithm 1. First, each feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m \\in F$$\\end{document} is discretized with minimum number of intervals (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_m$$\\end{document}) for which its relevancy with the class variable (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$J_{rel}(f_m) = I(f_m ; C)- \\frac{(\\mathcal {M}-1)(\\mathcal {K}-1)}{2N\\ln {2}}$$\\end{document}) is significant. If the feature is not significant even with some predefined maximum number of intervals (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_{max}$$\\end{document}), it is dropped. The selected candidate features (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_c$$\\end{document}) are then sorted according to their relevance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$J_c$$\\end{document} in descending order (line 2\u201312 in Algorithm 1). The first feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_1$$\\end{document} is then included to the final selected feature set S. The remaining features of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_c$$\\end{document} are evaluated incrementally maximizing the Rrci criteria. The score of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$J_{DSbM}$$\\end{document} (Eq. 8) is compared (in line 15) with its\u2019 critical value (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2_{(Rrci)}$$\\end{document}), to select a new feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} if it is not significantly redundant. Otherwise, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} is discarded considering that it does not contribute to the score significantly. While selecting a new feature, its discretization level is also shifted by a small value \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta $$\\end{document} from its original value (as selected previously based on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$J_{rel}$$\\end{document} as shown in line 16\u201321). This process helps to select the discretization level of features dynamically considering its dependency with other feature. In this way, all the features are discretized and selected simultaneously.",
            "cite_spans": [],
            "section": "Theorem 2 ::: Discretization and Feature Selection Based on Bias Corrected Mutual Information (DSbM) ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "DSbM follows \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} based forward searching strategy where a feature can not be discarded once it is added to the selected subset S. When a candidate feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} is found redundant with respect to the selected features from S, DSbM does not consider \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} for selection. However, it may happen that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} is more important and contains extra information compared to the already selected features. In this case, removing the redundant features from S is more appropriate. Therefore, we modify DSbM by including backward elimination and propose DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document} where simultaneous selection and elimination is incorporated.",
            "cite_spans": [],
            "section": "DSbM with Simultaneous Forward Selection and Backward Elimination (DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document}) ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "The process of backward elimination is described in Algorithm 2. Here, the redundant candidate feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} is rechecked based on its interaction value to decide whether this feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} is able to replace some features from S. This checking can be done by several ways such as considering all possible combination of three way interaction of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_j$$\\end{document} and selecting the feature pair whose replacement can increase the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$J_{DSbM}$$\\end{document} score significantly. However, it is computationally expensive to check all possible combination pairs of features. Hence, we consider the pair for which we obtain the highest interaction value (line 9\u201315) and replace that feature pair with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_m$$\\end{document} if their removal from S passes the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} value and increases the total score (line 17\u201318). As a result, DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document} obtains a smaller subset of features compared to DSbM. \n",
            "cite_spans": [],
            "section": "DSbM with Simultaneous Forward Selection and Backward Elimination (DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document}) ::: Proposed Method",
            "ref_spans": []
        },
        {
            "text": "In this experiment, twenty benchmark datasets collected from UCI Machine Learning Repository [3] are used as they are also employed in [16] and [19]. The description of these datasets are given in Table 1. For classification, we use SVM and KNN, and conduct 10-fold cross-validation on each dataset. We compare DSbM with four state-of-the-art methods namely mDSM, JMI, JMI with COBRA search (JC) and RelaxMRMR. Here, DSbM, mDSM and JC are feature selection method, however, JMI and RelaxMRMR are feature ranking method. Hence, the number of selected feature obtained in DSbM are used to generate the results for these two methods. For JMI and RelaxMRMR, we use forward selection whereas, JC performs COBRA search and mDSM uses \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} based search. For comparing the methods we use three metrics namely accuracy, Score (defined in Eq. 10) and Pareto Optimality(PO). PO returns a set of non-dominant candidate solutions.10\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Score = \\frac{\\sum _{i=1}^n w_i * \\alpha _i}{\\sum _{i=1}^n w_i} \\end{aligned}$$\\end{document}here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_i$$\\end{document} indicates the performance evaluation criteria and weights respectively. For our method \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _2$$\\end{document} indicates the percentage accuracy, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _2$$\\end{document} = \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(N_t - N_s)/N_t$$\\end{document} is the percentage of reduction features. Here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_t$$\\end{document} is the total number of features in a dataset and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_s$$\\end{document} is the number of selected features. We use equal weights. To calculate PO, we use \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _2$$\\end{document} and to perform Friedman test we use Score to incorporate the joint impact of number of selected features and the corresponding accuracy. We also calculate Win/Tie/Loss which indicates the number of datasets for which comparing method performs better/equally-well/worse than other methods unless otherwise stated. To determine whether the wins are statistically significant we perform t-test at 0.05 significance level.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 95,
                    "mention": "3",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 136,
                    "end": 138,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 145,
                    "end": 147,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Dataset Description and Implementation Details ::: Experimental Result",
            "ref_spans": [
                {
                    "start": 203,
                    "end": 204,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "To understand the impact of simultaneous forward selection and backward elimination using DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document}, let us consider Fig. 1a and Fig. 1b. We observe, in most of the cases DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document} selects less features than DSbM (number of selected features is given on the top of each bar and on the x-axis the index of datasets are given according to their order in Table 1). These figures also illustrate that when the total number of features for a dataset is comparatively small then the performance of both DSbM and DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document} are similar in terms of number of selected features and accuracy (e.g., Iris, Yeast, Glass etc.). Note that in some cases such as in Cardio, Arrhythmia etc., DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document} selects fewer features with higher accuracy.\n\n",
            "cite_spans": [],
            "section": "Impact of DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document} over DSbM. ::: Results and Discussion ::: Experimental Result",
            "ref_spans": [
                {
                    "start": 414,
                    "end": 415,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 426,
                    "end": 427,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 942,
                    "end": 943,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Furthermore, a limitation of mDSM is that, the set of selected features might contain a subset for which better accuracy can be found. DSbM also has similar problem which can be observed in Fig. 2a. This issue is resolved to some extent in DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document}. Here, we get 74.19% accuracy with 84 selected features (see Fig. 2b) while DSbM obtains an accuracy of 72.79% with 107 features.",
            "cite_spans": [],
            "section": "Impact of DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document} over DSbM. ::: Results and Discussion ::: Experimental Result",
            "ref_spans": [
                {
                    "start": 195,
                    "end": 196,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 608,
                    "end": 609,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "In this paper, we propose a method DSbM which includes bias correction for high-order dependencies among features and use \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} based search that also consider high-order dependencies. Results over a large amount of dataset demonstrate that DSbM outperforms current state-of-the-art methods. Beside this, a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\chi ^2$$\\end{document} based simultaneous forward and backward search is also proposed here that shows similar performances with DSbM with less number of features. This method can be applied for different applications such as activity recognition and cancer classification for gene expression data. Incorporation of further high-order terms might improve the overall performance which require further theoretical analysis and experimentation with global feature selection which will be addressed in future work.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Dataset description\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Comparison of different methods (Win/Tie/Loss)\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Comparison among different methods based on its accuracy. (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$*$$\\end{document}) and (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\circ $$\\end{document}) represents that DSbM wins and loses significantly from that method respectively and bold values represent the overall win among all methods.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Ranking of existing feature selection criteria.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: DSbM(black bar) vs. DSbM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$_\\mathrm{fb}$$\\end{document}(white bar)",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Accuracy (SVM) vs. Number of features for Arrhythmia dataset",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Using mutual information for selecting features in supervised neural net learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Battiti",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "IEEE Trans. Neural Netw",
            "volume": "5",
            "issn": "4",
            "pages": "537-550",
            "other_ids": {
                "DOI": [
                    "10.1109/72.298224"
                ]
            }
        },
        "BIBREF1": {
            "title": "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "8",
            "issn": "",
            "pages": "1226-1238",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2005.159"
                ]
            }
        },
        "BIBREF2": {
            "title": "Hybrid feature selection method based on the genetic algorithm and pearson correlation coefficient",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Saidi",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Bouaguel",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Essoussi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Machine Learning Paradigms: Theory and Application",
            "volume": "",
            "issn": "",
            "pages": "3-24",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "A new maximum relevance-minimum multicollinearity (MRmMC) method for feature selection and ranking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Senawi",
                    "suffix": ""
                },
                {
                    "first": "HL",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "SA",
                    "middle": [],
                    "last": "Billings",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Pattern Recogn.",
            "volume": "67",
            "issn": "",
            "pages": "47-61",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2017.01.026"
                ]
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Simultaneous feature selection and discretization based on mutual information",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sharmin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shoyaib",
                    "suffix": ""
                },
                {
                    "first": "AA",
                    "middle": [],
                    "last": "Ali",
                    "suffix": ""
                },
                {
                    "first": "MAH",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chae",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Pattern Recogn.",
            "volume": "91",
            "issn": "",
            "pages": "162-174",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2019.02.016"
                ]
            }
        },
        "BIBREF8": {
            "title": "Can high-order dependencies improve mutual information based feature selection?",
            "authors": [
                {
                    "first": "NX",
                    "middle": [],
                    "last": "Vinh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bailey",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Pattern Recogn.",
            "volume": "53",
            "issn": "",
            "pages": "46-58",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2015.11.007"
                ]
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Pocock",
                    "suffix": ""
                },
                {
                    "first": "MJ",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Luj\u00e1n",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "J. Mach. Learn. Res.",
            "volume": "13",
            "issn": "1",
            "pages": "27-66",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "A comparison of optimization methods and software for large-scale L1-regularized linear classification",
            "authors": [
                {
                    "first": "GX",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "KW",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "CJ",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "CJ",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J. Mach. Learn. Res.",
            "volume": "11",
            "issn": "",
            "pages": "3183-3234",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Mutual information-based multi-label feature selection using interaction information",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "DW",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Exp. Syst. Appl.",
            "volume": "42",
            "issn": "4",
            "pages": "2013-2025",
            "other_ids": {
                "DOI": [
                    "10.1016/j.eswa.2014.09.063"
                ]
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Orthogonal forward selection and backward elimination algorithms for feature subset selection",
            "authors": [
                {
                    "first": "KZ",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "IEEE Trans. Syst. Man Cybern. Part B",
            "volume": "34",
            "issn": "1",
            "pages": "629-634",
            "other_ids": {
                "DOI": [
                    "10.1109/TSMCB.2002.804363"
                ]
            }
        },
        "BIBREF17": {
            "title": "A semidefinite programming based search strategy for feature selection with mutual information measure",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Naghibi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hoffmann",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pfister",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "37",
            "issn": "8",
            "pages": "1529-1541",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2014.2372791"
                ]
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}