{
    "paper_id": "cada7b3b2da7bdb315104d2ad3541d7cd1a533e9",
    "metadata": {
        "title": "Hybridizations of Metaheuristics With Branch & Bound Derivates",
        "authors": [
            {
                "first": "Christian",
                "middle": [],
                "last": "Blum",
                "suffix": "",
                "affiliation": {
                    "laboratory": "ALBCOM research group Universitat Polit\u00e8cnica de Catalunya",
                    "institution": "",
                    "location": {}
                },
                "email": "cblum@lsi.upc.edu"
            },
            {
                "first": "Carlos",
                "middle": [],
                "last": "Cotta",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universidad de M\u00e1laga",
                    "location": {}
                },
                "email": "ccottap@lcc.uma.es"
            },
            {
                "first": "Antonio",
                "middle": [
                    "J"
                ],
                "last": "Fern\u00e1ndez",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universidad de M\u00e1laga",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Jos\u00e9",
                "middle": [
                    "E"
                ],
                "last": "Gallardo",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universidad de M\u00e1laga",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Monaldo",
                "middle": [],
                "last": "Mastrolilli",
                "suffix": "",
                "affiliation": {},
                "email": "monaldo@idsia.ch"
            }
        ]
    },
    "abstract": [
        {
            "text": "An important branch of hybrid metaheuristics concerns the hybridization with branch & bound derivatives. In this chapter we present examples for two different types of hybridization. The first one concerns the use of branch & bound features within construction-based metaheuristics in order to increase their efficiancy. The second example deals with the use of a metaheuristic, in our case a memetic algorithm, in order to increase the efficiancy of branch & bound, respectively branch & bound derivatives such as beam search. The quality of the resulting hybrid techniques is demonstrated by means of the application to classical string problems: the longest common subsequence problem and the shortest common supersequence problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "One of the basic ingredients of an optimization technique is a mechanism for exploring the search space, that is, the space of valid solutions to the considered optimization problem. Algorithms belonging to the important class of constructive optimization techniques tackle an optimization problem by exploring the search space in form of a tree, a so-called search tree. The search tree is generally defined by an underlying solution construction mechanism. Each path from the root node of the search tree to one of the leaves corresponds to the process of constructing a candidate solution. Inner nodes of the tree can be seen as partial solutions. The process of moving from an inner node to one of its child nodes is called a solution construction step, or extension of a partial solution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The class of constructive optimization techniques comprises approximate methods as well as complete methods. Recall that complete algorithms are guaranteed to find for every finite size instance of a combinatorial optimization problem an optimal solution in bounded time. This is in contrast to incomplete methods such as heuristics and metaheuristics where we sacrifice the guarantee of finding optimal solutions for the sake of getting good solutions in a significantly reduced amount of time. A prominent example of a deterministic constructive heuristic is a greedy heuristic. Greedy heuristics make use of a weighting function that gives weights to the child nodes of each inner node of the search tree. At each construction step the child node with the highest weight is chosen.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Apart from greedy heuristics, the class of constructive optimization techniques also includes metaheuristics such as ant colony optimization (ACO) [12] and greedy randomized adaptive search procedures (GRASP) [13] . 1 They are iterative algorithms that employ repeated probabilistic (randomized) solution constructions at each iteration. For each child node of an inner node of the tree they compute the probability of performing the corresponding construction step. These probabilities may depend on weighting functions and/or the search history of the algorithm. They are sometimes called transition probabilities and define a probability distribution over the search space. In GRASP, this probability distribution does not change during run-time, while in ACO the probability distribution is changed during run-time with the aim of biasing the probabilistic construction of solutions towards areas of the search space containing high quality solutions.",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 151,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 209,
                    "end": 213,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 216,
                    "end": 217,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In addition to the methods mentioned above, the class of constructive optimization techniques also includes complete algorithms such as backtracking [36] and branch & bound [25] . A common backtracking scheme is implemented in the depth-first search (DFS) algorithm. The un-informed version of DFS starts from the root node of the search tree and progresses by always moving to the best unexplored child of the current node, going deeper and deeper until a leaf node is reached. Then the search backtracks, returning to the most recently visited node of which remain unexplored children, and so on. This systematic search method explicitly visits all possible solutions exactly once.",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 153,
                    "text": "[36]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 173,
                    "end": 177,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Branch & bound algorithms belong to the class of implicit enumeration techniques. The branch & bound view on the search tree is slightly different to that exhibited by the algorithms mentioned before. More specifically, the subtree rooted at an inner node of the search tree is seen as a subspace of the search space. Accordingly, the subspaces represented by the subtrees rooted at the children of an inner node consitute a partition of the subspace that is represented by the inner node itself. The partitioning of the search space is called branching. A branch & bound algorithm produces for each inner node of the search tree an upper bound as well as a lower bound of the objective function values of the solutions contained by the corresponding subspace. These bounds are used to decide if the whole subspace can be discarded, or if it has to be further partitioned. As in backtracking, there are different schemes such as depth-first search or breadth-first search for traversing over the search tree.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "An interesting heuristic version of a breadth-first branch & bound is beam search [33] . Instead of considering all nodes of a certain level of the search tree, beam search restricts the search to a certain number of nodes based on the bounding information (lower bounds for minimization, and upper bounds for maximization).",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 86,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Each of the algorithms mentioned above has advantages as well as disadvantages. Greedy heuristics, for example, are usually easy to implement and fast in execution. However, the obtained solution quality is often not sufficient. Metaheuristics, on the other side, can find good solutions in a reasonable amount of time, without providing performance guarantees. However, metaheuristics can generally not avoid visiting the same solution more than once, which might lead to a waste of computation time. Finally, complete techniques guarantee to find an optimal solution. However, a user might not be prepared to accept overly large running times. In recent years it has been shown that a hybridization of concepts originating from different types of algorithms can often result in more efficient techniques. For example, the use of backtracking in metaheuristics is relatively wide-spread. Examples of their use in construction-based metaheuristics are [2, 3, 9] . Backtracking is also used in evolutionary algorithms (see, for example, [10, 24] ), and even in tabu search settings [32] . The hybridization of metaheuristics with branch & bound (respectively, beam search) concepts is rather recent. We distinguish between two different lines of hybridization. On one side, it is possible to use branch & bound concepts within construction-based metaheuristics in order to increase the efficiency of the metaheuristics search process. On the other side, metaheuristics can be used within branch & bound in order to reduce the space and time consumption of branch & bound. This chapter is dedicated to outline representative examples of both types of hybrid algorithms. The reader interested in a broader discussion on the combination of metaheuristics and exact techniques is referred to [34] .",
            "cite_spans": [
                {
                    "start": 952,
                    "end": 955,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 956,
                    "end": 958,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 959,
                    "end": 961,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1787,
                    "end": 1791,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "1. Bounding information is used for evaluating partial solutions; sometimes also for choosing among different partial solutions, or discarding partial solutions. 2. The extension of partial solutions is allowed in more than one way. The number of nodes which can be selected at each search tree level is hereby limited from above by a parametric constraint, resulting in parallel and non-independent solution constructions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Using Branch & Bound Concepts Within Construction-Based Metaheuristics"
        },
        {
            "text": "This type of hybrid algorithm includes probabilistic beam search (PBS) [6] , Beam-ACO algorithms [4, 5] , and approximate and non-deterministic tree search (ANTS) procedures [27, 28, 29] . 2 These works give empiricial evidence of the usefulness of including the two features mentioned above in the construction process of construction-based metaheuristics. 3",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 74,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 97,
                    "end": 100,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 101,
                    "end": 103,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 174,
                    "end": 178,
                    "text": "[27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 179,
                    "end": 182,
                    "text": "28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 183,
                    "end": 186,
                    "text": "29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Using Branch & Bound Concepts Within Construction-Based Metaheuristics"
        },
        {
            "text": "In the following we first give a motivation of why the above mentioned branch & bound features should be incorporated in construction-based metaheuristics. Afterwards, we present some representative approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Using Branch & Bound Concepts Within Construction-Based Metaheuristics"
        },
        {
            "text": "The following tree search model captures the essential elements common to all constructive procedures. In general, we are given an optimization problem P and an instance x of P. Typically, the search space S x is exponentially large in the size of the input x. Without loss of generality we intend to maximize the objective function f : S x \u2192 R + . The optimization goal is to find a solution y \u2208 S x to x with f (y) as great as possible. Assume that each element y \u2208 S x can be viewed as a composition of l y,x \u2208 N elements from a set \u03a3. From this point of view, S x can be seen as a set of strings over an alphabet \u03a3. Any element y \u2208 S x can be constructed by concatenating l y,x elements of \u03a3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A Tree Search Model"
        },
        {
            "text": "The following method for constructing elements of S x is instructive: A solution construction starts with the empty string . The construction process consists of a sequence of construction steps. At each construction step, we select an element of \u03a3 and append it to the current string t. The solution construction may end for two reasons. First, it may end in case t has no feasible extensions. This happens in case t is already a complete solution, or when no solution of S x has prefix t. Second, a solution construction ends in case of available upper bound information that indicates that each solution with prefix t is worse than any solution that is already known. Henceforth we denote the upper bound value of a partial solution t by UB(t). if UB(w) <f then 6: v := null The application of such an algorithm can be equivalently described as a walk from the root v 0 of the corresponding search tree to a node at level l y,x . The search tree has nodes for all y \u2208 S x and for all prefixes of elements of S x . The root of the tree is the empty string, that is, v 0 corresponds to . There is a directed arc from node v to node w if w can be obtained by appending an element of \u03a3 to v. Note that henceforth we identify a node v of the search tree with its corresponding string t. We will use both notations interchangeably. The set of nodes that can be reached from a node v via directed arcs are called the children of v, denoted by C(v). Note, that the nodes at level i correspond to strings of length i. If w is a node corresponding to a string of length l > 0 then the length l \u22121 prefix v of w is also a node, called the father of w denoted by F(w). Thus, every y \u2208 S x corresponds to exactly one path of length l y,x from the root node of the search tree to a specific leaf. The above described solution construction process is pseudo-coded in Algorithm 7. In the following we assume function ChooseFrom(C(v)) of this algorithm to be implemented as a probabilistic choice function.",
            "cite_spans": [
                {
                    "start": 765,
                    "end": 767,
                    "text": "6:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A Tree Search Model"
        },
        {
            "text": "The analysis provided in the following assumes that there is a unique optimal solution, represented by leaf node v d of the search tree, also referred to as the target node. Let us assume that -without loss of generality -the target node v d is at the maximum level d \u2265 1 of the search tree. A probabilistic constructive optimization algorithm is said to be successful, if it can find the target node v d with high probability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal and Dual Problem Knowledge"
        },
        {
            "text": "In the following let us examine the success probability of repeated applications of Algorithm 7 in which function ChooseFrom(C(v)) is implemented as a probabilisitc choice function. Such solution constructions are employed, for example, within the ACO metaheuristic. The value of the inputf is not important for the following analysis. Given any node v i at level i of the search tree, let p(v i ) be the probability that a solution construction process includes node v i . Note that there is a single path from v 0 , the root node, to v i . We denote the corresponding sequence of nodes by (v 0 , v 1 , v 2 , ..., v i ). Clearly, p(v 0 ) = 1 and p(v i ) = i\u22121 j=0 p(v j+1 |v j ). Let Success(\u03c1) denote the event of finding the target node v d within \u03c1 applications of Algorithm 7. Note that the probability of Success(\u03c1) is equal to 1 \u2212 (1 \u2212 p(v d )) \u03c1 , and it is easy to check that the following inequalities hold:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal and Dual Problem Knowledge"
        },
        {
            "text": "By (1), it immediately follows that the chance of finding node v d is large if and only if \u03c1p(v d ) is large, namely as soon as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal and Dual Problem Knowledge"
        },
        {
            "text": "In the following, we will not assume anything about the exact form of the given probability distribution. However, let us assume that the transition probabilities are heuristically related to the attractiveness of child nodes. In other words, we assume that in a case in which a node v has two children, say w and q, and w is known (or believed) to be more promising, then p(w|v) > p(q|v). This can be achieved, for example, by defining the transition probabilities proportional to the weights assigned by greedy functions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal and Dual Problem Knowledge"
        },
        {
            "text": "Clearly, the probability distribution reflects the available knowledge on the problem, and it is composed of two types of knowledge. If the probability p(v d ) of reaching the target node v d is \"high\", then we have a \"good\" problem knowledge. Let us call the knowledge that is responsible for the value of p(v d ) the primal problem knowledge (or just primal knowledge). From the dual point of view, we still have a \"good\" knowledge of the problem if for \"most\" of the wrong nodes (i.e. those that are not on the path from v 0 to v d ) the probability that they are reached is \"low\". We call this knowledge the dual problem knowledge (or just dual knowledge). Note that the quality of the dual knowledge grows with the valuef that is provided as input to Algorithm 7. This means, the better the solution that we already know, the higher is the quality of the dual knowledge. Observe that the two types of problem knowledge outlined above are complementary, but not the same. Let us make an example to clarify these two concepts. Consider the search tree of Figure 1 , where the target node is v 5 . Let us analyze two different probability distributions:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1058,
                    "end": 1066,
                    "text": "Figure 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Primal and Dual Problem Knowledge"
        },
        {
            "text": "Case (a) For each v and w \u2208 C(v) let p(w|v) = 0.5. Moreover, let us assume that no upper bound information is available. This means that each solution construction is performed until a leaf node is reached. When probabilistically constructing a solution the probability of each child is therefore the same at each construction step. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal and Dual Problem Knowledge"
        },
        {
            "text": "In general, the transition probabilities are defined as in case (a), with one exception. Let us assume that the available upper bound indicates that the subspaces represented by the black nodes do not contain any better solutions than the ones we already know, that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case (b)"
        },
        {
            "text": "where v is a black node. Accordingly, the white children of the black nodes have probability 0 to be reached.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case (b)"
        },
        {
            "text": "Note that in both cases the primal knowledge is \"scarce\", since the probability that the target node v d is reached by a probabilistic solution construction decreases exponentially with d, that is, p(v d ) = 2 \u2212d . However, in case (b) the dual knowledge is \"excellent\", since for most of the wrong nodes (i.e. the white nodes), the probability that any of them is reached is zero. Viceversa, in case (a) the dual knowledge is \"scarce\", because there is a relatively \"high\" probability that a white node is reached.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case (b)"
        },
        {
            "text": "By using the intuition given by the provided example, let us try to better quantify the quality of the available problem knowledge. Let V i be the set of nodes at level i, and let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case (b)"
        },
        {
            "text": "Note that (i) is equal to the probability that the solution construction process reaches level i of the search tree. Observe that the use of the upper bound information makes the probabilities (i) smaller than one. Case (b) was obtained from case (a) by decreasing (i) (for i = 1, . . . , d) down to 2 i\u22121 (and without changing the probability p(v i ) of reaching the ancestor v i of the target node at level i), whereas in case (a) it holds that (i) = 1 (for i = 1, . . . , d). In general, good dual knowledge is supposed to decrease (i) without decreasing the probability of reaching the ancestor v i of the target node v d . This discussion may suggest that a characterization of the available problem knowledge can be given by the following knowledge ratio:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case (b)"
        },
        {
            "text": "The larger this ratio the better the knowledge we have on the target node v d . In case (a) it is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case (b)"
        },
        {
            "text": "Finally, it is important to observe that the way of (repeatedly) constructing solutions in a probabilistic way does not exploit the dual problem knowledge. For example in case (b), although the available knowledge is \"excellent\", the target node v d is found after an expected number of runs that is proportional (2)), which is the same as in case (a). In other words, the number of necessary probabilistic solution constructions only depends on the primal knowledge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case (b)"
        },
        {
            "text": "The problem of Algorithm 7 is clearly the following one: When encountering a partial solution whose upper bound is less or equal to the value of the best solution found so far, the construction process is aborted, and the computation time invested in this construction is lost. Generally, this situation may occur very often. In fact, the probability for the abortion of a solution construction is 1\u2212p(v d ) in the example outlined in the previous section, which is quite high.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "How to Exploit the Dual Knowledge?"
        },
        {
            "text": "In the following let us examine a first simple extension of Algorithm 7. The corresponding algorithm -henceforth denoted by PSC(\u03b1,f ) -is pseudocoded in Algorithm 8. Hereby, \u03b1 denotes the maximum number of allowed extensions of partial solutions at each construction step; in other words, \u03b1 is the maximum number of solutions to be constructed in parallel. We use the following additional notation: For any given set S of search tree nodes let C(S) be the set of children of the nodes in S. Morever, B i denotes the set of reached nodes of tree level i. Recall that the root node v 0 is the only node at level 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "How to Exploit the Dual Knowledge?"
        },
        {
            "text": "The algorithm works as follows. Given the selected nodes B i of level i (with |B i | \u2264 \u03b1)), the algorithm probabilistically chooses at most \u03b1 solutions from C := C(B i ), the children of the nodes in B i . The probabilistic choice of a Algorithm 8 Parallel solution construction: PSC(\u03b1,f ) 1: input: \u03b1 \u2208 Z + , the best known objective function valuef 2: initialization: i := 0, 10: else 11: if f (w) >f then z := w,f := f (z) end if 12: end if 13: C := C \\ {w} 14: end for 15: i := i + 1 16: end while 17: output: z (which might be null) child is performed in function ChooseFrom(C) proportionally to the following probabilities:",
            "cite_spans": [
                {
                    "start": 378,
                    "end": 381,
                    "text": "10:",
                    "ref_id": null
                },
                {
                    "start": 387,
                    "end": 390,
                    "text": "11:",
                    "ref_id": null
                },
                {
                    "start": 433,
                    "end": 436,
                    "text": "12:",
                    "ref_id": null
                },
                {
                    "start": 444,
                    "end": 447,
                    "text": "13:",
                    "ref_id": null
                },
                {
                    "start": 461,
                    "end": 464,
                    "text": "14:",
                    "ref_id": null
                },
                {
                    "start": 473,
                    "end": 476,
                    "text": "15:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "How to Exploit the Dual Knowledge?"
        },
        {
            "text": "Remember that F(w) denotes the father of node w. After choosing a node w it is first checked if w is a complete solution, or not. In case it is not a complete solution, it is checked if the available bounding information allows the further extension of this partial solution, in which case w is added to B i+1 . However, if w is already a complete solution, it is checked if its value is better than the value of the best solution found so far. The algorithm returns the best solution found, in case it is better than thef value that was provided as input. Otherwise the algorithm returns null.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "How to Exploit the Dual Knowledge?"
        },
        {
            "text": "Observe that when \u03b1 = 1, PSC(\u03b1,f ) is equivalent to SC(f ). In contrast, when \u03b1 > 1 the algorithm constructs (maximally) \u03b1 solutions nonindependently in parallel. Concerning the example outlined in the previous section with the probability distribution as defined in case(b), we can observe that algorithm PSC(\u03b1,f ) with \u03b1 > 1 solves this problem even within one application. At each step i, B i will only contain the brother of the corresponding black node, because the upper bound information does not allow the inclusion of the black nodes in B i . This shows that algorithm PSC(\u03b1,f ), in contrast to algorithm SC(f ), benefically uses the dual problem knowledge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "How to Exploit the Dual Knowledge?"
        },
        {
            "text": "For practical optimization, algorithm PSC(\u03b1,f ) has some drawbacks. First, in most cases algorithms for optimization are applied with the goal of finding a solution as good as possible, without having a clue beforehand about the value of good solutions. Second, the available upper bound function might not be very tight. For both reasons, solution constructions that lead to unsatisfying solutions are discarded only at very low levels of the search tree, that is, close to the leaves. Referring to the example of Section 2.2, this means that black nodes will only appear close to the leaves. In those cases, algorithm PSC(\u03b1,f ) will have practically no advantage over algorithm SC(f ). It might even have a disadvantage due to the amount of computation time invested in choosing children from bigger sets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Probabilistic Beam Search"
        },
        {
            "text": "The following simple extension can help in overcoming the drawbacks of algorithm PSC(\u03b1,f ). At each algorithm iteration we allow the choice of \u00b5 \u00b7 \u03b1 nodes from B i , instead of \u03b1 nodes. \u00b5 \u2265 1 is a parameter of the algorithm. Moreover, after the choice of the child nodes we restrict set B i+1 to the (maximally) \u03b1 best solutions with respect to the upper bound information. This results in a so-called (probabilistic) beam search algorithm -henceforth denoted by PBS(\u03b1,\u00b5,f ) -pseudo-coded in Algorithm 9. Note that algorithm PBS(\u03b1,\u00b5,f ) is a generalization of algorithm PSC(\u03b1,f ), that is, when \u00b5 = 1 both algorithms are equivalent. Algorithm PBS(\u03b1,\u00b5,f ) is also a generalization of algorithm SC(f ), which is obtained by \u03b1 = \u00b5 = 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Probabilistic Beam Search"
        },
        {
            "text": "In general, algorithm PBS(\u03b1,\u00b5,f ) can be expected to produce good solutions if (at least) two conditions are fullfilled: Neither the greedy function nor the upper bound function are misleading. If at least one of these two functions is misleading, the algorithm might not be able to find solutions above a certain threshold. One possibility of avoiding this drawback is to add a learning component to algorithm PBS(\u03b1,\u00b5,f ), that is, adding a mechanism that is supposed to adapt the primal knowledge, the dual knowledge, or both, over time, based on accumulated search experience.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Adding a Learning Component: Beam-ACO"
        },
        {
            "text": "Ant colony optimization (ACO) [12] is the most prominent constructionbased metaheuristics that attempts to learn the primal problem knowledge during run-time. ACO is inspired by the foraging behavior of ant colonies. At the core of this behavior is the indirect communication between the ants by means of chemical pheromone trails, which enables them to find short paths between their nest and food sources. This characteristic of real ant colonies is exploited in ACO algorithms in order to solve, for example, combinatorial optimization problems. In general, the ACO approach attempts to solve an optimization problem by iterating the following two steps: Algorithm 9 Probabilistic beam search: PBS(\u03b1,\u00b5f ) 1: input: \u03b1, \u00b5 \u2208 Z + , the best known objective function valuef 2: initialization: i := 0, 10: else 11: if f (w) >f then z := w,f := f (z) end if 12: end if 13: C := C \\ {w} 14: end for 15: Restrict B i+1 to the (maximally) \u03b1 best nodes w.r.t. their upper bound 16: i := i + 1 17: end while 18: output: z (which might be null)",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 34,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 799,
                    "end": 802,
                    "text": "10:",
                    "ref_id": null
                },
                {
                    "start": 808,
                    "end": 811,
                    "text": "11:",
                    "ref_id": null
                },
                {
                    "start": 854,
                    "end": 857,
                    "text": "12:",
                    "ref_id": null
                },
                {
                    "start": 865,
                    "end": 868,
                    "text": "13:",
                    "ref_id": null
                },
                {
                    "start": 882,
                    "end": 885,
                    "text": "14:",
                    "ref_id": null
                },
                {
                    "start": 894,
                    "end": 897,
                    "text": "15:",
                    "ref_id": null
                },
                {
                    "start": 970,
                    "end": 973,
                    "text": "16:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Adding a Learning Component: Beam-ACO"
        },
        {
            "text": "\u2022 At each iteration, a certain number of \u03b1 candidate solutions is probabilistically constructed. The respective probability distribution is derived from an available greedy function and from the values of so-called pheromone trail parameters, the pheromone values. The set of pheromone trail parameters is denoted by T . \u2022 The constructed candidate solutions are used to modify the pheromone values in a way that is deemed to bias future solution constructions towards areas of the search space containing high quality solutions. Hereby, the greedy function can be seen as the a priori available primal knowledge, whereas the pheromone values are used to modify (ideally, to improve) this a priori given primal knowledge over time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Adding a Learning Component: Beam-ACO"
        },
        {
            "text": "While standard ACO algorithms use \u03b1 applications of algorithm SC(f ) at each iteration for the probabilistic construction of solutions, the idea of Beam-ACO [4, 5] is to use one application of probabilistic beam search PBS(\u03b1,\u00b5,f ) instead.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 160,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 161,
                    "end": 163,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Adding a Learning Component: Beam-ACO"
        },
        {
            "text": "A related ACO approach is labelled ANTS (see [27, 28, 29] ). The characterizing feature of ANTS is the use of upper bound information for defining the primal knowledge. The latest version of ANTS [29] uses at each iteration algorithm PSC(\u03b1,f ) to construct candidate solutions.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "text": "[27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 50,
                    "end": 53,
                    "text": "28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 54,
                    "end": 57,
                    "text": "29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 196,
                    "end": 200,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Adding a Learning Component: Beam-ACO"
        },
        {
            "text": "The longest common subsequence (LCS) problem is one of the classical string problems. Given a problem instance (S, \u03a3), where S = {s 1 , s 2 , . . . , s n } is a set of n strings over a finite alphabet \u03a3, the problem consists in finding a longest string t * that is a subsequence of all the strings in S. Such a string t * is called a longest common subsequence of the strings in S. Note that a string t is called a subsequence of a string s, if t can be produced from s by deleting characters. For example, dga is a subsequence of adagtta. If n = 2 the problem is polynomially solvable, for example, by dynamic programming [17] . However, when n > 2 the problem is in general NP-hard [26] . Traditional applications of this problem are in data compression, syntactic pattern recognition, and file comparison [1] , whereas more recent applications also include computational biology [38] .",
            "cite_spans": [
                {
                    "start": 623,
                    "end": 627,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 684,
                    "end": 688,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 808,
                    "end": 811,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 882,
                    "end": 886,
                    "text": "[38]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "In order to apply algorithm PBS(\u03b1,\u00b5,f ) to the LCS problem, we have to define the solution construction mechanism, the greedy function that defines the primal knowledge, and the upper bound function that defines the dual knowledge. We use the construction mechanism of the so-called Best-Next heuristic [15, 22] for our algorithm. Given a problem instance (S, \u03a3), this heuristic produces a common subsequence t sequentially by appending at each construction step a letter to t such that t maintains the property of being a common subsequence of all strings in S. Given a common subsequence t of the strings in S, we explain in the following how to derive the children of t. For that purpose we introduce the following notations:",
            "cite_spans": [
                {
                    "start": 303,
                    "end": 307,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 308,
                    "end": 311,
                    "text": "22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "i be the partition of s i into substrings s A i and s B i such that t is a subsequence of s A i and s B i has maximal length. Given this partition, which is well-defined, we introduce position pointers p i := |s A i | for i = 1, . . . , n (see Figure 2 for an example). 2. The position of the first appearance of a letter a \u2208 \u03a3 in a string s i \u2208 S after the position pointer p i is well-defined and denoted by i a . In case a letter a \u2208 \u03a3 does not appear in s B i , i a is set to \u221e (see Figure 2 ). 3. A letter a \u2208 \u03a3 is called dominated, if exists at least one letter b \u2208 \u03a3 such that i b < i a for i = 1, . . . , n; 4. \u03a3 nd t \u2286 \u03a3 henceforth denotes the set of non-dominated letters of the alphabet \u03a3 with respect to a given t. Moreover, for all a \u2208 \u03a3 nd t it is required that i a < \u221e, i = 1, . . . , n. Hence, we require that in each string s i a letter a \u2208 \u03a3 nd t appears at least once after position pointer p i . The children C(t) of a node t are then determined as follows:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 244,
                    "end": 252,
                    "text": "Figure 2",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 487,
                    "end": 495,
                    "text": "Figure 2",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "The primal problem knowledge is derived from the greedy function \u03b7(\u00b7) that assigns to each child v = ta \u2208 C(t) the following greedy weight: The child with the highest greedy weight is considered the most promising one. Instead of the greedy weights themselves, we will use the corresponding ranks. More in detail, the child v = ta with the highest greedy weight will be assigned rank 1, denoted by r(v) = 1, the child w = tb with the second-highest greedy weight will be assigned rank 2 (that is, r(w) = 2), and so on.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "In the following we explain the implementation of function ChooseFrom(C) of algorithm PBS(\u03b1,\u00b5,f ). Remember that C denotes the set of children obtained from the nodes that are contained in the beam B i (that is, C := C(B i )). For evaluating a child v \u2208 C we use the sum of the ranks of the greedy weights that correspond to the construction steps performed to construct string v. Let us assume that v is on the i-th level of the search tree, and let us denote the sequence of characters that forms string v by v 1 ",
            "cite_spans": [
                {
                    "start": 513,
                    "end": 514,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "where v 1 . . . v j denotes the substring of v from position 1 to position j. With this definition, Equation 5 can be defined for the LCS problem as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "Finally, we outline the upper bound function UB(\u00b7) that the PBS(\u03b1,\u00b5,f ) algorithm requires. Remember that a given subsequence t splits each string s i \u2208 S into a first part s A i and into a second part s B i , that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "i | a denotes the number of occurrences of letter a in s B i for all a \u2208 \u03a3. Then,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "that is, for each letter a \u2208 \u03a3 we take the minimum of the occurrences of a in s B i , i = 1, . . . , n. Summing up these minima and adding the result to the length of t results in the upper bound. This completes the description of the implementation of the PBS(\u03b1,\u00b5,f ) algorithm for the LCS problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "In the following, we use algorithm PBS(\u03b1,\u00b5,f ) in two different ways: First, we use PBS(\u03b1,\u00b5,f ) in a multi-start fashion as shown in Algorithm 10, denoted by MS-PBS(\u03b1,\u00b5). Second, we use PBS(\u03b1,\u00b5,f ) within a Beam-ACO algorithm as explained in the following. if v = null then z := v,f := |z| 7: end while 8: output: z The first step of defining a Beam-ACO approach -and, in general, any ACO algorithm -consists in the specification of the set of pheromone values T . In the case of the LCS problem T contains for each position j of a string s i \u2208 S a pheromone value 0 \u2264 \u03c4 ij \u2264 1, that is, T = {\u03c4 ij | i = 1, . . . , n, j = 1, . . . , |s i |}. A value \u03c4 ij \u2208 T indicates the desirability of adding the letter at position j of string i to a solution: the greater \u03c4 ij , the greater is the desirability of adding the corresponding letter. In addition to the definition of the pheromone values, we also introduce a solution representation that is suitable for ACO. Any common subsequence t of the strings in S can be translated into an ACO-solution T = {T ij \u2208 {0, 1} | i = 1, . . . , n, j = 1, . . . , |s i |} where T ij = 0 if the letter at position j of string i was not added to t during the solution construction, and T ij = 1 otherwise. Note that the translation of t into T is well-defined due to the construction mechanism. For example, given solution t = abcdd for the problem instance of Figure 2 , the corresponding ACO-solution is T 1 = 101101001, T 2 = 011001101, and T 3 = 011111000, where T i refers to the sequence T i1 . . . T i|si| . In the following, for each given solution, the lower case notation refers to its string representation, and the upper case notation refers to its binary ACO representation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1393,
                    "end": 1401,
                    "text": "Figure 2",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "The particular ACO framework that we used for our algorithm is the socalled MAX \u2212 MIN ant system (MMAS) algorithm implemented in the hyper-cube framework (HCF), an ACO variant that generally performs very well. One of the particularities of this algorithm is the use of an upper bound for the pheromone values with the aim of preventing convergence to a solution, see [7] . A high level description of the algorithm is given in Algorithm 11. The data structures used, in addition to counters and to the pheromone values, are: (1) the best-so-far solution T bs , i.e., the best solution generated since the start of the algorithm; (2) the restart-best solution T rb , that is, the best solution generated since the last restart of the algorithm; (3) the convergence factor cf, 0 \u2264 cf \u2264 1, which is a measure of how far the algorithm is from convergence; and (4) the Boolean variable bs update, which becomes true when the algorithm reaches convergence.",
            "cite_spans": [
                {
                    "start": 368,
                    "end": 371,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "Roughly, the algorithm works as follows. First, all the variables are initialized. In particular, the pheromone values are set to their initial value 0.5. Each algorithm iteration consists of the following steps. First, algorithm PBS(\u03b1,\u00b5,f ) is applied withf = 0 to generate a solution T pbs . The setting off = 0 is chosen, because in ACO algorithms it is generally useful to learn also from solutions that are worse than the best solution found so far. The only change in algorithm PBS(\u03b1,\u00b5,f ) occurs in the definition of the choice probabilities. Instead of using Equation 8, these probabilities are now defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "Remember in this context, that i a was defined as the next position of letter a after position pointer p i in string s i . The intuition of choosing the minimum of the pheromone values corresponding to the next positions of a letter in the n given strings is as follows: If at least one of these pheromone values is low, the corresponding letter should not yet be appended to the string, because apparently there is another letter that should be appended first.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "The second action at each iteration concerns the pheromone update conducted in the ApplyPheromoneUpdate(cf , bs update, T , T pbs , T rb , T bs ) procedure. Third, a new value for the convergence factor cf is computed. Depending on this value, as well as on the value of the Boolean variable bs update, a decision on whether to restart the algorithm or not is made. If the algorithm is restarted, all the pheromone values are reset to their initial value (that is, 0.5). The algorithm is iterated until the CPU time limit is reached. Once terminated, the algorithm returns the string version t bs of the best-so-far ACO-solution T bs . In the following we describe the two remaining procedures of Algorithm 11 in more detail.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "ApplyPheromoneUpdate(cf ,bs update,T ,T pbs ,T rb ,T bs ): In general, three solutions are used for updating the pheromone values. These are the solution T pbs generated by the PBS algorithm, the restart-best solution T rb , and the best-so-far solution T bs . The influence of each solution on the pheromone update depends on the state of convergence of the algorithm as measured by the convergence factor cf. Each pheromone value \u03c4 ij \u2208 T is updated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "where \u03be ij := \u03ba pbs \u00b7 T pbs ij + \u03ba rb \u00b7 T rb ij + \u03ba bs \u00b7 T bs ij ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "where \u03ba pbs is the weight (that is, the influence) of solution T pbs , \u03ba rb is the weight of solution T rb , \u03ba bs is the weight of solution T bs , and \u03ba pbs + \u03ba rb + \u03ba bs = 1. After the application of the pheromone update rule (Equation 11), pheromone values that exceed \u03c4 max = 0.999 are set back to \u03c4 max (similarly for \u03c4 min = 0.001). This is done in order to avoid a complete convergence of the algorithm, which is a situation that should be avoided. Equation 12 allows to choose how to schedule the relative influence of the three solutions used for updating the pheromone values. For our application we used a standard update schedule as shown in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 653,
                    "end": 660,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "ComputeConvergenceFactor(T ): The convergence factor cf , which is a function of the current pheromone values, is computed as follows: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "In this way, cf = 0 when the algorithm is initialized (or reset), that is, when all pheromone values are set to 0.5. On the other side, when the algorithm has converged, then cf = 1. In all other cases, cf has a value in (0, 1). This completes the description of our Beam-ACO approach for the LCS problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Longest Common Subsequence Problem"
        },
        {
            "text": "We implemented algorithms MS-PBS(\u03b1,\u00b5) and Beam-ACO in ANSI C++ using GCC 3.2.2 for compiling the software. The experimental results that we outline in the following were obtained on a PC with an AMD64X2 4400 processor and 4 Gb of memory. We applied algorithm MS-PBS(\u03b1,\u00b5) with three different settings:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "1. \u03b1 = \u00b5 = 1: The resulting algorithm corresponds to a multi-start version of algorithm SC(f ); see Algorithm 7. In the following we refer to this algorithm by MS-SC. 2. \u03b1 = 10, \u00b5 = 1: This setting corresponds to a multi-start version of algorithm PSC(\u03b1,f ); see Algorithm 8. We refer henceforth to this algorithm by MS-PSC. 3. \u03b1 = 10, \u00b5 > 1: These settings generate a multi-start version of algorithm PBS(\u03b1,\u00b5,f ); see Algorithm 9. This algorithm version is referred to simply by MS-PBS. Note that the setting of \u00b5 will depend on the alphabet size, that is, the number of expected children of a partial solution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "In addition we applied Beam-ACO with \u03b1 = 10 and with the same settings for \u00b5 as chosen for MS-PBS.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "For the experimentation we used a set of benchmark instances that was generated as explained in the following. Given h \u2208 {100, 200, . . . , 1000} and \u03a3 (where |\u03a3| \u2208 {2, 4, 8, 24}), an instance is produced as follows. First, a string s of length h is produced randomly from the alphabet \u03a3. String s is in the following called base string. Each instance contains 10 strings. Each of these strings is produced from the base string s by traversing s and by deciding for each letter with a probabilitiy of 0.1 whether to remove it, or not. Note that the 10 strings of such an instance are not necessarily of the same length. As we produced 10 instances for each combination of h and |\u03a3|, 400 instances were generated in total. Note that the values of optimal solutions of these instances are unknown. However, a lower bound is obtained as follows. While producing the 10 strings of an instance, we record for each position of the base string s, whether the letter at that position was removed for the generation of at least one of the 10 strings. The number of positions in s that were never removed constitutes the lower bound value henceforth denoted by LB I with respect to an instance I.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "We applied each of the 4 algorithms exactly once for h/10 seconds to each problem instance. We present the results averaged over the 10 instances for each combination of h (the length of the base string that was used to produce an instance) and the alphabet size |\u03a3|. Two measures are presented:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "1. The (average) length of the solutions expressed in deviation (percentage) from the respective lower bounds, which is computed as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "where f is the length of the solution achieved by the respective algorithm. 2. The computation time of the algorithms, which refers to the time the best solution was found within the given CPU time (averaged over the 10 instances of each type).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "The results are shown graphically in Figure 3 . for i := 1 to offsize do 8: if recombination is performed then end if 15: if mutation is performed then 16 ",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 75,
                    "text": "8:",
                    "ref_id": null
                },
                {
                    "start": 118,
                    "end": 121,
                    "text": "15:",
                    "ref_id": null
                },
                {
                    "start": 152,
                    "end": 154,
                    "text": "16",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 37,
                    "end": 45,
                    "text": "Figure 3",
                    "ref_id": "FIGREF16"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "In this section, we present a collaborative technique that integrates a population based metaheuristics, a memetic algorithm (MA) [31, 19, 23] , with the beam search variant of the branch & bound procedure. MAs are based on the systematic exploitation of knowledge about the problem being solved, and the synergistic combination of ideas taken from both population-based techniques and trajectory-based metaheuristics. A very common way to achieve this combination is using the template of an evolutionary algorithm, endowing it with local search add-ons. A general sketch of this kind of MA is shown in Algorithm 12. Several things must be noted: firstly, initialization is very often done by means of problem-dependent constructive heuristics, thus ensuring that a good starting point is used for the evolutionary search. Local search can be also used in this initialization stage (to supplement the lack of an adequate constructive heuristic, or to complement the latter). The remaining components of the algorithm are typically chosen so that they incorporate problem-knowledge (if possible) as well.",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 134,
                    "text": "[31,",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 135,
                    "end": 138,
                    "text": "19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 139,
                    "end": 142,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Using Metaheuristics Concepts Within Branch & Bound"
        },
        {
            "text": "According to the previous description, it is clear that MAs are specifically concerned with exploiting as much problem-knowledge as available. This makes MAs specifically suited for taking part in hybrid approaches, either integrative or collaborative [34] . In this case, we have considered an approach in which the control flows of BS and MA are intertwined: phases of BS and MA alternate, and both processes share the best known solution. The technique provides the following benefits:",
            "cite_spans": [
                {
                    "start": 252,
                    "end": 256,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Using Metaheuristics Concepts Within Branch & Bound"
        },
        {
            "text": "\u2022 The best known solution can be used by the beam search part to purge its problem queue, by not expanding partial nodes whose upper bound is worse than the one obtained by the MA. The resulting algorithm for a minimization problem is pseudo-coded in Algorithm 13. The procedure performs a standard beam search procedure (B i is used to maintain the beam at level i of the search tree and \u03b1 is the beam width, i.e., the maximum number of partial solutions to be expanded at each level). After spreading out each level, if a level dependent problem specific condition is fulfilled (represented in the pseudocode by the runMA variable), the MA is run with a population that is initialized using the best nodes (w.r.t some criteria) in the current beam. Note that nodes in the beam are partial solutions, whereas the MA population consists of complete solutions, so a problem specific procedure must be used to complete them. After the MA stabilizes, if the solution it provides improves the incumbent one, this one is updated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Using Metaheuristics Concepts Within Branch & Bound"
        },
        {
            "text": "The Shortest Common Supersequence Problem (SCSP) is a well-known problem in the area of string analysis. Essentially, given a certain alphabet \u03a3 and a set L of strings from \u03a3, the aim is to find a minimal-length sequence s, such that all strings in the given set L can be embedded in s. The SCSP can be shown to be NP\u2212hard, even if strong constraints are posed on L, or on \u03a3. For example, it is NP\u2212hard in general when all s i have length two [39] , or when the alphabet size |\u03a3| is two [30] . This combinatorial problem is interesting as it constitutes a formalization of different real-world problems. For example, it has many implications in bioinformatics [18] : it is a problem with a close relationship to multiple sequence alignment [37] , and to probe synthesis during microarray production [35] . Besides this, it also has applications in planning [14] and data compression [39] , among other fields.",
            "cite_spans": [
                {
                    "start": 443,
                    "end": 447,
                    "text": "[39]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 487,
                    "end": 491,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 660,
                    "end": 664,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 740,
                    "end": 744,
                    "text": "[37]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 799,
                    "end": 803,
                    "text": "[35]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 857,
                    "end": 861,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 883,
                    "end": 887,
                    "text": "[39]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "Formally, the notion of embedding can be described as follows. Let s and r be two strings of symbols taken from \u03a3. String s can be said to embed string r (denoted as s r) using the following recursive definition: for w \u2208 C(B i ) do 6: if |C(w)| > 0 then 7: if LB(w) <f then B i+1 := B i+1 \u222a {w} end if 8: else 9: if f (w) <f then z := w,f := f (z) end if 10: end if 11: end for 12: Restrict B i+1 to the (maximally) \u03b1 best nodes 13: if runMA then 14: pop := select popsize best nodes from B i+1 15: for j = 1, . . . , popsize do 16: complete partial solution Formally, an instance I = (\u03a3, L) for the SCSP is given by a finite alphabet \u03a3 and a set L of m strings {s 1 , \u00b7 \u00b7 \u00b7 , s m }, s i \u2208 \u03a3 * . The problem consists of finding a string s of minimal length that embeds each string in L (s s i , \u2200s i \u2208 L and |s| is minimal).",
            "cite_spans": [
                {
                    "start": 232,
                    "end": 234,
                    "text": "6:",
                    "ref_id": null
                },
                {
                    "start": 254,
                    "end": 256,
                    "text": "7:",
                    "ref_id": null
                },
                {
                    "start": 302,
                    "end": 304,
                    "text": "8:",
                    "ref_id": null
                },
                {
                    "start": 310,
                    "end": 312,
                    "text": "9:",
                    "ref_id": null
                },
                {
                    "start": 355,
                    "end": 358,
                    "text": "10:",
                    "ref_id": null
                },
                {
                    "start": 366,
                    "end": 369,
                    "text": "11:",
                    "ref_id": null
                },
                {
                    "start": 378,
                    "end": 381,
                    "text": "12:",
                    "ref_id": null
                },
                {
                    "start": 429,
                    "end": 432,
                    "text": "13:",
                    "ref_id": null
                },
                {
                    "start": 447,
                    "end": 450,
                    "text": "14:",
                    "ref_id": null
                },
                {
                    "start": 495,
                    "end": 498,
                    "text": "15:",
                    "ref_id": null
                },
                {
                    "start": 529,
                    "end": 532,
                    "text": "16:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "A branch & bound algorithm to solve an instance I = (\u03a3, L) of the SCSP can start from a single node containing as tentative solution . In order to implement function C(w), |\u03a3| subproblems are generated, each of them obtained by appending a symbol from \u03a3 to the partial solution w. Nodes with unproductive characters (i.e., not contributing to embedding any string in L) are pruned from the search tree. To obtain a lower bound for a node s t , the set of remaining strings in L not embedded by s t must first be calculated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "where s r = (r e , r r ) if r e is the longest initial segment of string r embedded by s and r r is the remaining part of r not embedded by s. Let M (\u03b1, R) be the maximum number of occurrences of symbol \u03b1 in any string in R: 4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "Clearly, every common supersequence for the remaining strings must contain at least M (\u03b1, R) copies of the symbol \u03b1. Thus a lower bound can be obtained by summing the length of the tentative solution and the maximum number of occurrences in any string in R of each symbol of the alphabet:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "In order to rank nodes in the branch & bound queue, the following quality function was used for each node:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "so that tentative solutions embedding more symbols in L are selected. As all tentative solutions in the same level of the search tree have the same length, the algorithm selects nodes that provide good initial segments for constructing a short supersequence. Before being injected into the MA population, solutions were randomly completed and repaired using the following function: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "where MM is the Majority Merge algorithm (see Algorithm 14) described in [8] . This is a greedy algorithm that constructs a supersequence incrementally by adding the symbol most frequently found at the front of the strings in L, and removing these symbols from the corresponding strings. Note that, apart from completing a string in order to have a valid supersequence, this function also removes unproductive steps from the repaired string, acting thus as a local searcher.",
            "cite_spans": [
                {
                    "start": 46,
                    "end": 59,
                    "text": "Algorithm 14)",
                    "ref_id": null
                },
                {
                    "start": 73,
                    "end": 76,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "Preliminary tests show that partial good solutions were only obtained after descending a substantial number of levels in the beam search tree. This led us to the following strategy for interleaving the MA and the branch & bound in the hybrid algorithm: start by running in isolation the branch & bound part of the algorithm for a initial number of levels, and then periodically interleave both algorithms afterwards. To be precise, an estimation for the SCSP solution s 0 was calculated using the Weighted Majority Merge (WMM) algorithm [8] and its length was used to set l 0 = 0.7 \u00b7 |s 0 |. The condition for running the MA was (i > l 0 ) and (i mod l = 0), where variable i (see s := s\u03b2 12: until si\u2208L |s i | = 0 13: output: s algorithm, and parameter l controls the balance between the MA and beam search, i.e., an execution of the MA is performed every l iterations of the beam search. A sensitivity analysis of the parameters was done in a similar way to that described in [16] and, based on it, the following values were used for the different parameters of the algorithm: \u03b1 = 10000 and l = 10.",
            "cite_spans": [
                {
                    "start": 537,
                    "end": 540,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 978,
                    "end": 982,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "As to the MA used, it evolves sequences in |\u03a3| \u03bb , where \u03bb = si\u2208L |s i |. Before being evaluated, sequences in the population are repaired using the \u03c1 function. After this repairing, raw fitness (to be minimized) is simply com- ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "Selection in the MA was performed by binary tournament selection, the mutation operator randomly flips a character in the sequence, and recombination is carried out using an standard uniform crossover operator. The local search technique used is based on the neighborhood defined by the Del k : \u03a3 * \u00d7 (\u03a3 * ) m \u2192 \u03a3 * operation [35] . The functioning of this procedure is as follows:",
            "cite_spans": [
                {
                    "start": 326,
                    "end": 330,
                    "text": "[35]",
                    "ref_id": "BIBREF35"
                }
            ],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "This operation thus removes the k-th symbol from a string, and then submits it to the repair function so that all strings in L can be embedded. Notice that the repairing function can actually find that the sequence is feasible, hence resulting in a reduction of length by one symbol. A full local-search scheme is defined by iterating this operation until no single deletion results in length reduction (see Algorithm 15) . The improvement in solution quality attainable via the application of this LS operator comes obviously at the expenses of an increased computational cost. This additional cost might be too high if k := k + 1 10: end if 11: end while 12: output: s LS were massively applied. On the other hand, the extreme option of simply removing LS handicaps the search capabilities of the algorithm. A pragmatic solution can be found in the use of partial lamarckism [21] , namely using LS but with some intermediate probability. Preliminary experiments were conducted with probabilities to apply local search in {0, 0.01, 0.1, 0.5, 1} (see [11] ), and setting this parameter to 0.01 provided a better tradeoff between the attainable improvement, and the additional computational cost implied.",
            "cite_spans": [
                {
                    "start": 408,
                    "end": 421,
                    "text": "Algorithm 15)",
                    "ref_id": null
                },
                {
                    "start": 632,
                    "end": 635,
                    "text": "10:",
                    "ref_id": null
                },
                {
                    "start": 877,
                    "end": 881,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Example: Shortest Common Supersequence Problem"
        },
        {
            "text": "In this section, we do a experimental comparison of the beam search and MA hybrid algorithm with respect to the probabilistic beam search (PBS) algorithm for the SCSP described in [6] . For this purpose, two sets of benchmark instances have been used:",
            "cite_spans": [
                {
                    "start": 180,
                    "end": 183,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "The first one -henceforth referred to as RandomSet -consists of random strings with different alphabet lengths. To be precise, each instance is composed of eight strings, four of them of length 40, and the other four of length 80. Each of these strings is randomly generated, using an alphabet \u03a3. The benchmark set consists of 5 classes of each 5 instances characterized by different alphabet sizes, namely |\u03a3| = 2, 4, 8, 16, and 24. Thus, the benchmark set consists of 25 different problem instances.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "A second set of instances -henceforth referred to as RealSet -is composed of strings obtained from molecular sequences, comprising both DNA sequences (|\u03a3| = 4) and protein sequences (|\u03a3| = 20). In the first case, we have taken two DNA sequences of the SARS coronavirus from a genomic database 5 ; these sequences are 158 and 1269 nucleotides long. As to the protein sequences, we have considered four of them, extracted from Swiss-Prot 6 :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "\u2022 Oxytocin: quite important in pregnant women, this protein causes contraction of the smooth muscle of the uterus and of the mammary gland. The sequence is 125-aminoacid long. \u2022 p53 : this protein is involved in the cell cycle, and acts as tumor suppressor in many tumor types; the sequence is 393-aminoacid long. \u2022 Estrogen: involved in the regulation of eukaryotic gene expression, this protein affects cellular proliferation and differentiation; the sequence is 595-aminoacid long. \u2022 Myelin: this sequence correspond to a transcription factor of myelin, and is associated with neuronal differentiation. The sequence is 1186-aminoacid long.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "Problem instances in RealSet are obtained from the target sequence by removing symbols from the latter with a certain probability p % (p \u2208{10 %,15 %,20 %} in our experiments). Figure 4 shows results for RandomSet. Results are averaged over 5 independent runs for each problem instance and further averaged over 5 different problem instances with the same alphabet length. For the beam search and MA hybrid, executions were performed on a Pentium IV PC (2400MHz and 512MB of main memory), and a time limit of 600 seconds per execution was imposed. As to the PBS, tests were performed on a AMD64X2 4400 processor and 4 Gb of memory. The time limit was set to 350 seconds, that roughly corresponds to the time given to other algorithm on a different machine. Results show that PBS performs better for this instance set, as it finds better solutions except for |\u03a3| = 2. Note that PBS performs several iterations of the beam search part of the algorithm, and thus exhausts the allowed time, whereas the beam search and MA hybrid only performs a beam search execution, and does not necessarily use all the permitted time. We also studied the performance of a variation of the beam search and MA hybrid (labelled MA-BS 2 in Figure 4 ) that exhausts the allowed time by performing several iterations of the beam search. In order to introduce more randomness in the algorithm, each time the MA was executed, its population was initialized by selecting nodes from the beam using binary tournament selection. Results show that MA-BS 2 outperforms PBS for |\u03a3| \u2208 {2, 24}, and is slightly worse for |\u03a3| = 4. Figure 5 shows results for RealSet. In this case, the beam search and MA hybrid performs better, as it always finds the presumed optimal solution in all runs (except for the MYELIN instance with p %=20 %). Note that in this latter instance, PBS finds a non-optimal better result. We also make note that the second version of the MA-BS hybrid does not improve these results because the allowed time is exhausted in the first iteration of the beam search; for this reason the results obtained by MA-BS 2 are not shown in Figure 5 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 176,
                    "end": 184,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 1217,
                    "end": 1225,
                    "text": "Figure 4",
                    "ref_id": null
                },
                {
                    "start": 1594,
                    "end": 1602,
                    "text": "Figure 5",
                    "ref_id": null
                },
                {
                    "start": 2113,
                    "end": 2121,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "In this chapter we have dealt with hybridizations of branch & bound derivatives (i.e., beam search) and metaheuristic techniques and have shown that the resulting hybrid algorithms provide better results than their counterparts working alone. Particularly, we have highlighted two different proposals: in the first one, a construction-based metaheuristic is enriched with branch & bound features. Here, we start from a (parallel) solution construction method with a probabilistic component in the election of the next step to execute (i.e., the set of nodes that can be reached from the current state). Then we improve it by incorporating first a beam search component in the election, resulting in a probabilistic beam search algorithm, and second adding a learning component to adjust the knowledge acquired from the accumulated experience.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Our second proposal consists of a branch & bound technique that collaborates in an interleaved way with a metaheuristic, namely a memetic algorithm. Here, the branch & bound technique is used to identify the promising regions of the search space in which the optimal solution can be found. The metaheuristics is then used to exploit this knowledge in order to improve the bounds employed by the branch & bound technique to force further branch pruning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Our hybrid algorithms have been first described in detail and then applied on practical problems to show their effectiveness. This paper clearly shows that both exact techniques such as branch & bound (including non-complete derivatives such as beam search) and metaheuristics can clearly benefit one from each other. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Data structures and algorithms",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Aho",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hopcroft",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ullman",
                    "suffix": ""
                }
            ],
            "year": 1983,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Backtracking ant system for the traveling salesman problem",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Al-Shihabi",
                    "suffix": ""
                },
                {
                    "first": ";",
                    "middle": [
                        "M"
                    ],
                    "last": "Dorigo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Birattari",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Blum",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "M"
                    ],
                    "last": "Gambardella",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Mondada",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "St\u00fctzle",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of ANTS 2004 -4th International Workshop on Ant Colony Optimization and Swarm Intelligence",
            "volume": "3172",
            "issn": "",
            "pages": "318--325",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Ant colony optimization for the maximum edgedisjoint paths problem",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Blesa",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Blum",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Applications of Evolutionary Computing, Proceedings of EvoWorkshops",
            "volume": "3005",
            "issn": "",
            "pages": "160--169",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Beam-ACO-hybridizing ant colony optimization with beam search: an application to open shop scheduling",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Blum",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Computers and Operations Research",
            "volume": "32",
            "issn": "",
            "pages": "1565--1591",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Beam-ACO applied to assembly line balancing",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Blum",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bautista",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pereira ; M. Dorigo",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "M"
                    ],
                    "last": "Gambardella",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Martinoli",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Poli",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "St\u00fctzle",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of ANTS 2006 -Fifth International Workshop on Swarm Intelligence and Ant Algorithms",
            "volume": "2463",
            "issn": "",
            "pages": "14--27",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "A probabilistic beam search algorithm for the shortest common supersequence problem",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Blum",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cotta",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Gallardo",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of EvoCOP 2007 -Seventh European Conference on Evolutionary Computation in Combinatorial Optimisation",
            "volume": "4446",
            "issn": "",
            "pages": "36--47",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The hyper-cube framework for ant colony optimization",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Blum",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dorigo",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics -Part B",
            "volume": "34",
            "issn": "2",
            "pages": "1161--1172",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Improved heuristics and a genetic algorithm for finding short supersequences",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Branke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Middendorf",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schneider",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "OR-Spektrum",
            "volume": "20",
            "issn": "",
            "pages": "39--45",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "GRASPing the examination scheduling problem",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Casey",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thompson",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of PATAT 2002 -4th International Conference on Practice and Theory of Automated Timetabling",
            "volume": "2740",
            "issn": "",
            "pages": "232--246",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Protein structure prediction using evolutionary algorithms hybridized with backtracking",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cotta",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the 7th International Work-Conference on Artificial and Natural Neural Networks (IWANN 2003)",
            "volume": "2687",
            "issn": "",
            "pages": "321--328",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Memetic algorithms with partial lamarckism for the shortest common supersequence problem",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cotta",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Artificial Intelligence and Knowledge Engineering Applications: a Bioinspired Approach, number 3562 in Lecture Notes in Computer Science",
            "volume": "",
            "issn": "",
            "pages": "84--91",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Ant Colony Optimization",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dorigo",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Stuetzle",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Greedy randomized adaptive search procedures",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "A"
                    ],
                    "last": "Feo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "G C"
                    ],
                    "last": "Resende",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Journal of Global Optimization",
            "volume": "6",
            "issn": "",
            "pages": "109--133",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Theory and algorithms for plan merging",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "E"
                    ],
                    "last": "Foulser",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Artificial Intelligence",
            "volume": "57",
            "issn": "2-3",
            "pages": "143--181",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Subsequences and supersequences of strings",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "B"
                    ],
                    "last": "Fraser",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "On the hybridization of memetic algorithms with branch-and-bound techniques",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Gallardo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cotta",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Fern\u00e1ndez",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics",
            "volume": "37",
            "issn": "1",
            "pages": "77--83",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Algorithms on Strings, Trees, and Sequences. Computer Science and Computational Biology",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gusfield",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "An integrated complexity analysis of problems from computational biology",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Hallet",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Recent Advances in Memetic Algorithms",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "E"
                    ],
                    "last": "Hart",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Krasnogor",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Unifying local and exhaustive search",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "N"
                    ],
                    "last": "Hooker",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of ENC 2005 -Sixth Mexican International Conference on Computer Science",
            "volume": "",
            "issn": "",
            "pages": "237--243",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Empirical investigation of the benefits of partial lamarckianism",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Houck",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Joines",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "G"
                    ],
                    "last": "Kay",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Wilson",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Evolutionary Computation",
            "volume": "5",
            "issn": "1",
            "pages": "31--60",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Fast algorithms for finding the common subsequences of multiple sequences",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Tseng",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the International Computer Symposium",
            "volume": "",
            "issn": "",
            "pages": "1006--1011",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A tutorial for competent memetic algorithms: model, taxonomy, and design issues",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Krasnogor",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "E"
                    ],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "IEEE Transactions on Evolutionary Computation",
            "volume": "9",
            "issn": "5",
            "pages": "474--488",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Evolutionary algorithms combined with deterministic search",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "B"
                    ],
                    "last": "Lamont",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "H"
                    ],
                    "last": "Gates",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proceedings of Evolutionary Programming VII, 7th International Conference",
            "volume": "1447",
            "issn": "",
            "pages": "517--526",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Branch and bound methods: A survey",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Lawler",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wood",
                    "suffix": ""
                }
            ],
            "year": 1966,
            "venue": "Operations Research",
            "volume": "4",
            "issn": "4",
            "pages": "669--719",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "The complexity of some problems on subsequences and supersequences",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Maier",
                    "suffix": ""
                }
            ],
            "year": 1978,
            "venue": "Journal of the ACM",
            "volume": "25",
            "issn": "",
            "pages": "322--336",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Exact and Approximate Nondeterministic Tree-Search Procedures for the Quadratic Assignment Problem",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Maniezzo",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "INFORMS Journal on Computing",
            "volume": "11",
            "issn": "4",
            "pages": "358--369",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "An ANTS heuristic for the frequency assignment problem",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Maniezzo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Carbonaro",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Future Generation Computer Systems",
            "volume": "16",
            "issn": "",
            "pages": "927--935",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "An ant-based framework for very strongly constrained problems",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Maniezzo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Milandri",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Proceedings of ANTS 2002: 3rd International Workshop on Ant Algorithms",
            "volume": "2463",
            "issn": "",
            "pages": "222--227",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "More on the complexity of common superstring and supersequence problems",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Middendorf",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Theoretical Computer Science",
            "volume": "125",
            "issn": "",
            "pages": "205--228",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "A gentle introduction to memetic algorithms",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Moscato",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cotta",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Handbook of Metaheuristics",
            "volume": "",
            "issn": "",
            "pages": "105--144",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "A fast taboo search algorithm for the job-shop problem",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Nowicki",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Smutnicki",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Management Science",
            "volume": "42",
            "issn": "2",
            "pages": "797--813",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Filtered beam search in scheduling",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Ow",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "E"
                    ],
                    "last": "Morton",
                    "suffix": ""
                }
            ],
            "year": 1988,
            "venue": "International Journal of Production Research",
            "volume": "26",
            "issn": "",
            "pages": "297--307",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Combining metaheuristics and exact algorithms in combinatorial optimization: A survey and classification",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Puchinger",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "R"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Artificial Intelligence and Knowledge Engineering Applications: a Bioinspired Approach, number 3562 in Lecture Notes in Computer Science",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Alvarez",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "41--53",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "The shortest common supersequence problem in a microarray production setting",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rahmann",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Bioinformatics",
            "volume": "19",
            "issn": "",
            "pages": "156--161",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Artificial Intelligence: A Modern Approach",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Russell",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Norvig",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "The consensus string problem for a metric is NPcomplete",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Sim",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Journal of Discrete Algorithms",
            "volume": "1",
            "issn": "1",
            "pages": "111--117",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Identification of common molecular subsequences",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Waterman",
                    "suffix": ""
                }
            ],
            "year": 1981,
            "venue": "Journal of Molecular Biology",
            "volume": "147",
            "issn": "1",
            "pages": "195--197",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Complexity of common subsequence and supersequence problems and related problems",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "G"
                    ],
                    "last": "Timkovsky",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Cybernetics",
            "volume": "25",
            "issn": "",
            "pages": "565--580",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Solution construction: SC(f ) 1: input: the best known objective function valuef (which might be 0) 2: initialization: v := v 0 3: while |C(v)| > 0 and v = null do",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "10: end while 11: output: v (which is either a complete solution, or null)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Example of a search tree. v5 is the unique optimal solution.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Given is the problem instance (S = {s1, s2, s3}, \u03a3 = {a, b, c, d}) where s1 = acbcadbbd, s2 = cabdacdcd, and s3 = babcddaab. Let us assume that t = abcd. (a), (b), and (c) show the corresponding division of si into s A i and s B i , as well as the setting of the pointers pi and the next positions of the 4 letters in s B i . Note that in case a letter does not appear in s B i (for example, letter a does not appear in s B 1 ), the corresponding pointer is set to \u221e. For example, as letter a does not appear in s B 1 , we set 1a := \u221e.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Algorithm 10 Multi-start probabilistic beam search: MS-PBS(\u03b1,\u00b5) 1: input: \u03b1, \u00b5 \u2208 Z + 2: z := null 3:f := 0 4: while CPU time limit not reached do 5: v := PBS(\u03b1,\u00b5,f ) {see Algorithm 9} 6:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Algorithm 11Beam-ACO for the LCS problem 1: input: \u03b1, \u00b5 \u2208 Z + 2: T bs := null, T rb := null, cf := 0, bs update := false 3: \u03c4 ij := 0.5, i = 1, . . . , n, j = 1, . . . , |s i | 4: while CPU time limit not reached do5:    T pbs := PBS(\u03b1,\u00b5,0) {see Algorithm 9}6:    if |t pbs | > |t rb | then T rb := T pbs 7:if |t pbs | > |t bs | then T bs := T pbs 8:ApplyPheromoneUpdate(cf ,bs update,T ,T pbs ,T rb ,T bs ) 9:cf := ComputeConvergenceFactor(T )10:    if cf > 0.99 then11:    if bs update = true then12:    \u03c4 ij := 0.5, i = 1, . . . , n, j = 1, . . . , |s i | 19: end while 20: output: t bs (that is, the string version of ACO-solution T bs )",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "The graphics on the left hand side show the algorithm performance (in percentage of deviation from the lower bound), and the graphics on the right hand side show the computation times. The following observations are of interest. First, while having a comparable computation time, algorithm MS-PBS is always clearly better than algorithms MS-PSC and MS-SC. Second, algorithm Beam-ACO is consistently the best algorithm of the comparison. This shows that it can pay off adding a learning component to algorithm (MS-)PBS. The advantage of Beam-ACO over MS-PBS grows with growing alphabet size, that is, with growing problem complexity. This advantage of Beam-ACO comes with a slight increase in computational cost. However, this is natural: due to the learning component, Beam-ACO has a higher probability than MS-PBS of improving on the best solution found even at late stages of a run. Finally, a last interesting remark concerns the comparison of MS-PSC with MS-SC. Despite of the construction of solutions in parallel, MS-PSC is always slightly beaten by MS-SC. This is due to fact that the used upper bound function is not tight at all, which results in the fact that constructing solutions in parallel in the way of algorithm (MS-)PSC is rather a waste of computation time.Algorithm 12 Pseudocode of a memetic algorithm 1: for i := 1 to popsize do 2: pop[i] := Heuristic Solution(ProblemData) 3: pop[i] := Local Search(pop[i]) 4: Evaluate(pop[i]) 5: end for 6: while allowed runtime not exceeded do 7:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "The beam search can guide the search of the MA by injecting information about more promising regions of the search space into the MA population.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": ", if r = \u03b1s \u03b1r = s r \u03b1s \u03b2r = s \u03b2r, if \u03b1 = \u03b2 (14) Algorithm 13 Beam Search and MA Hybrid: Hybrid(\u03b1,f ) 1: input: \u03b1 \u2208 Z + , the best known objective function valuef 2: initialization: i := 0, B i := { }, z := null 3: while B i = \u2205 do",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "f (sol) <f then z := sol,f := f (z) := i + 1 22: end while 23: output: z (which might be null)Plainly, s r means that all symbols in r are present in s in the very same order (although not necessarily consecutive).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "\u03b1s , L) = \u03c1(s , L), if i : s i = \u03b1s i \u03c1 (\u03b1s , L) = \u03b1\u03c1(s , L| \u03b1 ), if \u2203i : s i = \u03b1s i \u03c1 ( , L) = MM(L), if \u2203i : s i =",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Algorithm 13) is the current level explored by the beam search part of the Algorithm 14 Majority Merge algorithm 1: input: L = {s 1 , \u00b7 \u00b7 \u00b7 , s m } 2: \u2190 max \u22121 {\u03bd(\u03b1) | \u03b1 \u2208 \u03a3} 8:for s i \u2208 L, s i = \u03b2s i do",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "puted as: fit (s, L) = 0, if \u2200i : s i = fit (\u03b1s , L) = 1 + fit(s , L| \u03b1 ), if \u2203i : s i =",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Local search for MA(s, L) 1: input: s \u2208 \u03a3 * , L = {s 1 , \u00b7 \u00b7 \u00b7 , s m } 2: initialization: k := 1 3: while k < |s| do 4: r := Del k (s, L) 5:if fit(r, L) < fit(s, L)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Results and computation times of algorithms MS-SC, MS-PSC, MS-",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Comparison of two versions of MA-BS Hybrid Algorithm and PBS on random instances for different alphabet sizes. Figures show relative improvements with respect to solutions provided by MM. A \u00d7 sign indicates the mean solution, whereas a marks the best solution. Standard deviations of distribution are also depicted. Comparison of MA-BS Hybrid Algorithm and PBS on different real instances and gap \u2208 {10 %, 15 %, 20 %} (from left to right for each algorithm). Figures show relative distances to optimal solutions. A \u00d7 sign indicates the mean solution, whereas a marks the best solution. Standard deviations of distribution are also depicted.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Setting of \u03ba pbs , \u03ba rb , \u03ba bs , and \u03c1 depending on the convergence factor cf and the Boolean control variable bs update",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Algorithm % improvement to MM (c) |\u03a3| = 8 Algorithm % improvement to MM (d) |\u03a3| = 16",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}