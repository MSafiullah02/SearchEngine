{
    "paper_id": "45c9c1022e0345f344a37f8e1522a048b57ed805",
    "metadata": {
        "title": "Learning Discriminative Neural Sentiment Units for Semi-supervised Target-Level Sentiment Classification",
        "authors": [
            {
                "first": "Jingjing",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Yao",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {},
                "email": "yaoyang@tencent.com"
            },
            {
                "first": "Guansong",
                "middle": [],
                "last": "Pang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Adelaide",
                    "location": {
                        "settlement": "Adelaide",
                        "country": "Australia"
                    }
                },
                "email": "pangguansong@gmail.com"
            },
            {
                "first": "Lei",
                "middle": [],
                "last": "Lv",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Hong",
                "middle": [],
                "last": "Shang",
                "suffix": "",
                "affiliation": {},
                "email": "hongshang@tencent.com"
            },
            {
                "first": "Zhongqian",
                "middle": [],
                "last": "Sun",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Wei",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {},
                "email": "willyang@tencent.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "Target-level sentiment classification aims at assigning sentiment polarities to opinion targets in a sentence, for which it is significantly more challenging to obtain large-scale labeled data than sentence/document-level sentiment classification due to the intricate contexts and relations of the target words. To address this challenge, we propose a novel semi-supervised approach to learn sentiment-aware representations from easily accessible unlabeled data specifically for the finegrained sentiment learning. This is very different from current popular semi-supervised solutions that use the unlabeled data via pretraining to generate generic representations for various types of downstream tasks. Particularly, we show for the first time that we can learn and detect some highly sentiment-discriminative neural units from the unsupervised pretrained model, termed neural sentiment units. Due to the discriminability, these sentiment units can be leveraged by downstream LSTM-based classifiers to generate sentiment-aware and context-dependent word representations to substantially improve their sentiment classification performance. Extensive empirical results on two benchmark datasets show that our approach (i) substantially outperforms state-of-the-art sentiment classifiers and (ii) achieves significantly better data efficiency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Target-level sentiment classification (TSC) is the task of classifying sentiment polarities on opinion targets in sentences. It can provide more detailed insights into sentence polarities, but it involves significantly more intricate sentiment relations than sentence/document-level sentiment analysis. For example, the sentence \"The voice quality of this phone is not good, but the battery life is long\" holds negative sentiment on the target \"voice quality\" but is positive on the target \"battery life\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In recent years, deep neural network-based methods have been extensively explored for target-level sentiment classification to learn the representations of sentences and/or targets. Recurrent neural networks are one of the most popular approaches for this task because of their strong capability of learning sequential representations [2, 9] .",
            "cite_spans": [
                {
                    "start": 335,
                    "end": 338,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 339,
                    "end": 341,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, these methods fail to distinguish the importance of each word to the target. A range of attention mechanisms are introduced to address this issue, such as target-to-sentence attention [2] , fine-grained word-level attention [3] , and multiple attentions [4] . Convolutional neural network (CNN)-based models are also recently used for this task because of the capability to extract the informative n-grams features [5] . All the aforementioned methods focus on exploiting labeled data to build the classification model, whose performance is often largely limited. This is because they normally require large-scale high-quality labeled data to be well trained, but in practice we have only small target-level labeled data since it is very difficult and costly to collect due to the complex nature of the task, e.g., fine granularity, co-existence of multiple targets in a sentence, and context-sensitive sentiment. Two main methods to address this issue include: (i) generating and incorporating extra sentiment-informative representations by using auxiliary knowledge resources, e.g., sentiment lexicons [17, 28] ; and (ii) pretraining the embeddings of words or the parameters of networks using large-scale unlabeled data [3, 16] . However, both methods can't capture context-dependent sentiment. For example, the opinion \"long \" can have completely opposite sentiment in different contexts, e.g., it is positive in \" battery life is long \" but negative in \"the start-up time is too long \". Additionally, the sentiment lexicons require very expensive human involvement to handle data with evolving and highly diversified linguistics, so the pretraining method is more plausible.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 196,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 233,
                    "end": 236,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 263,
                    "end": 266,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 424,
                    "end": 427,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "text": "[17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1118,
                    "end": 1121,
                    "text": "28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1236,
                    "end": 1239,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The pretraining aims at generating generic representations for different learning tasks, which can often extract some transferable features for a particular task. However, due to the generic learning objective, it can also extract a large number of features that are irrelevant or even noisy w.r.t. a given task such as sentiment classification, leading ineffective use of the unlabeled data. In this study, we introduce a novel approach to associate the feature learning on unlabeled data with the downstream sentiment classification to extract highly relevant features w.r.t. sentiment classification. Specifically, besides pretraining on unlabeled data, we take a step further to learn and extract highly sentiment-discriminative neural units from a pretrained model, e.g., long short-term memory (LSTM)based Variational Autoencoder (VAE) [11] . The selective sentiment-aware units, termed Neural Sentiment Units (NeSUs), can generate highly relevant sentimentaware representations, which are then leveraged by LSTM networks to perform sentiment classification on small labeled data. This enables LSTM networks to achieve significantly improved data efficiency and to learn context-dependent sentiment representations, resulting in substantially improved LSTM networks. In summary, this paper makes the following two main contributions: -We discover for the first time that feature learning on unlabeled data can be associated with downstream sentiment classification to learn some highly sentiment-discriminative neural units (NeSUs). These NeSUs can be leveraged by LSTM-based classifiers to generate sentiment-aware and contextdependent representations, carrying substantially more task-dependent information than the generic representations obtained by pretraining. -We further propose a novel LSTM-based target-level sentiment classifier called NeaNet that effectively incorporates the most discriminative NeSU to exemplify the applications of the NeSUs. Extensive empirical results on two benchmark datasets show that NeaNet (i) substantially outperforms 13 (semi-) supervised state-of-the-art sentiment classifiers and (ii) achieves significantly better data efficiency.",
            "cite_spans": [
                {
                    "start": 842,
                    "end": 846,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Many methods have been introduced for target-level sentiment analysis, including rule-based approaches [1, 6] , statistical approaches [7, 8] and deep approaches [9, 21] . Due to page limits, below we discuss two closely relevant research lines.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 106,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 107,
                    "end": 109,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 135,
                    "end": 138,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 139,
                    "end": 141,
                    "text": "8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 162,
                    "end": 165,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 166,
                    "end": 169,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Recursive neural network is one popular network architecture explored at the early stage [29] , which heavily relies on the effectiveness of syntactic parsing tree. Recurrent neural networks have also shown expressive performance in this task. TD-LSTM [9] incorporated target information into LSTM and modeled preceding and following contexts of the target to boost the performance. Target-sensitive memory networks (TMNs) [21] were proposed to capture the sentiment interaction between targets and contexts to address the context-dependent sentiment problem. However, these models fail to identify the contribution of each word to the targets. The attention mechanism [2, 4, 10, 22] is then applied to address this issue. For example, A target-to-sentence attention mechanism, ATAE-LSTM [2] , was introduced to explore the connection between the target and its context; IARM [22] leveraged recurrent memory networks with multiple attentions to generate target-aware sentence representations. As CNN can capture the informative n-grams features, convolutional memory networks were explored in [18] to incorporate an attention mechanism to sequentially compute the weights of multiple memory units corresponding to multi-words. Instead of attention networks, [5] proposed a component to generate target-specific representations for words, and employed a CNN layer as the feature extractor relying on a mechanism of preserving the original contextual information. Some other works [20] exploited human reading cognitive process for this task. These neural network-based methods stand for the current state-of-the-art techniques, but their performance are generally limited by the amount of high-quality labeled data.",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 93,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 252,
                    "end": 255,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 423,
                    "end": 427,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 669,
                    "end": 672,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 673,
                    "end": 675,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 676,
                    "end": 679,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 680,
                    "end": 683,
                    "text": "22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 788,
                    "end": 791,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 876,
                    "end": 880,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1093,
                    "end": 1097,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1258,
                    "end": 1261,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1479,
                    "end": 1483,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Deep Methods."
        },
        {
            "text": "Semi-supervised Methods. Many semi-supervised methods have been explored on sentence-level sentiment classification, such as pretraining with Restricted Boltzmann Machine or autoencoder [23, 26] , auxiliary task learning [24] and adversarial training [25, 27] . However, there are only few studies [16, 19] on semi-supervised target-level sentiment classification. [19] explored both pretraining and multi-task learning for transferring knowledge from document-level data, which is much less expensive to obtain. [16] used a Transformer-based VAE for pretraining, which modeled the latent distributions via variational inference. However, it failed to distinguish the relevant and irrelevant features with respect to the sentiment.",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 190,
                    "text": "[23,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 191,
                    "end": 194,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 221,
                    "end": 225,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 251,
                    "end": 255,
                    "text": "[25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 256,
                    "end": 259,
                    "text": "27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 298,
                    "end": 302,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 303,
                    "end": 306,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 365,
                    "end": 369,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 513,
                    "end": 517,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Deep Methods."
        },
        {
            "text": "We introduce a novel semi-supervised framework to learn sentimentdiscriminative neural units (NeSUs) on large-scale unlabeled data to enhance downstream classifiers on small labeled data. Unlike the widely-used pretraining approaches that learn generic representations, our proposed approach is specifically designed for fine-grained sentiment classification, by incorporating sentiment-aware neural units hidden in the pretrained model into downstream LSTM-based classifiers. This enables us to have a substantially more effective use of the unlabeled data, greatly lifting the sentiment classification on limited labeled data. The procedure of our framework is presented in Fig. 1 , which consists of four modules, including LSTM-based VAE pretraining, measuring neuron sentiment discriminability, detection of NeSUs, and NeSU-enabled sentiment classification. The details of each module are introduced below.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 676,
                    "end": 682,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "The Proposed Framework"
        },
        {
            "text": "VAE is composed of an encoder and a decoder. The encoder maps an input x into a latent space and outputs the representation z. The decoder decodes z to generate the input x. LSTM-based VAE is used to pretrain for two main reasons: (i) VAE retains sentiment-related features which are important to generate sentences. (ii) LSTMs use an internal memory to remember semantic information, which can help learn intricate context-dependent opinions in sentiment analysis. VAE is trained on unlabeled data DS unlabel by minimizing reconstruction loss and KL divergence loss. And we obtain H neuron units for the encoding/decoding stage. We then exploit small labeled data to examine the discriminability of each neuron unit as follows. Firstly, an LSTM-based VAE is trained on unlabeled data DS unlabel . We then evaluate the discriminability of each encoding LSTM neuron unit using labeled data. A distribution separation measure d(\u00b7) is further applied to find a set of NeSUs (F ) that have the best discriminability. Since NeSUs are often redundant to each other, only the most discriminative NeSU (C ) is leveraged by the downstream classifiers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "LSTM-Based VAE Pretraining"
        },
        {
            "text": "sentences with negative sentiment, then we define the discriminability measure function d(\u00b7) w.r.t. a neuron unit C i as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Measuring Neuron Sentiment"
        },
        {
            "text": "where \u03b7 i : DS \u2192 R M +K returns a vector that contains the last hidden states of the neuron unit C i for all the sentences in the set DS = {DS pos , DS neg }, i.e., for M positive sentences and K negative sentences; the unit C i has a scalar output; \u03c6(\u00b7, \u00b7) is a measure that evaluates the separability of hidden states' distributions resulted by the samples of the two classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Measuring Neuron Sentiment"
        },
        {
            "text": "The main intuition of Definition 1 is that if a neuron unit has good discriminability, its hidden state distributions of different classes' samples should be well separable. Motivated by the fact that Gaussian distribution is the most general distribution for fitting values drawn from Gaussian/non-Gaussian variables according to the central limit theorem, we specify \u03c6 using Bhattacharyya distance to measure the separability of two distributions, which assumes the resulting hidden states in the neuron unit C i for each class's samples follow a Gaussian distribution. Accordingly, the discriminability of C i is calculated as follows: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Measuring Neuron Sentiment"
        },
        {
            "text": "where \u03be is a threshold hyperparameter and F is a set of discriminative NeSUs. Since each NeSU is an LSTM neural unit, it works as a none-linear mapping function \u03b7 : R D \u2192 R which is the same \u03b7 as Eq. 1 and can be formally defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 2 (Neural Sentiment Units"
        },
        {
            "text": "where v t is an embedding vector of the t-th word and s t is a scalar sentiment indication value with larger s t indicating more positive sentiment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 2 (Neural Sentiment Units"
        },
        {
            "text": "In Fig. 1(b) , we illustrate the discriminability values of all encoding LSTM neural units on a dataset Laptop. It is clear that only a small number of neural units are sentiment-aware. Most units do not capture much sentiment information. Therefore, simply using all units may disregard discriminative information. Instead, as defined in Eq. ( 3), we only retain selective sentiment-aware neural units based on their discriminability to fully exploit the unlabeled data.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Fig. 1(b)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Definition 2 (Neural Sentiment Units"
        },
        {
            "text": "The parameter \u03be can be tuned via cross validation using the labeled data. We find that retaining the single most discriminative neural sentiment unit (NeSU) always results in the best downstream classification performance; adding more NeSUs does not perform better. This demonstrates that NeSUs in F capture similar transferable features, so they are often redundant to each other. We therefore only extract NeSU below for the downstream classification:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 2 (Neural Sentiment Units"
        },
        {
            "text": "where the unit C , denoted by \u03b7 , is the only neural sentiment unit incorporated into downstream classifiers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 2 (Neural Sentiment Units"
        },
        {
            "text": "We further introduce a novel NeSU-enabled attention Network, namely NeaNet, by using two parallel LSTMs to fully exploit the NeSU and generate sentimentaware representations for target-level sentiment classification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "NeSU-Enabled LSTMs for Sentiment Classification"
        },
        {
            "text": "Task Statement. The target-level sentiment analysis is to predict a sentiment category for a (sentence, target) pair. Given a sentence-target pair x = (w, w T ), where w = {w 1 , w 2 , . . . , w n }, w T = w T 1 , w T 2 , . . . , w T m , and w T is a subsequence of w. The goal of this task is to predict a sentiment polarity y \u2208 {P, N, O} of the sentence w w.r.t. the target w T , where P , N , and O denote \"positive\", \"negative\" and \"neutral\" sentiments respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "NeSU-Enabled LSTMs for Sentiment Classification"
        },
        {
            "text": "The architecture of NeaNet is shown in Fig. 2 . The bottom is an embedding layer, which maps the words in an input sequence w to a word vectors ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 39,
                    "end": 45,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "NeSU-Enabled LSTMs for Sentiment Classification"
        },
        {
            "text": "Since NeSU can discriminate the sentiment of the input words, we integrate it into the memory computation of LSTM to generate sentiment-aware word representations. Moreover, the sentiment information can be carried forward along with word sequences due to the LSTM structure. Besides the three gates (input, forget and output gates) in the vanilla LSTM, we define an additional read gate r t \u2208 [0, 1] to control the sentiment information captured by the NeSU \u03b7 . This yields a NeSU-enabled Sentiment LSTM. The NeSU works like a sentiment prior, so we call the whole module NeSU-based Sentiment Prior (SUSP), which is defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "where \u03c3 refers to sigmoid activation function and tanh refers to hyperbolic tangent function; i t , f t , o t \u2208 R H respectively denote the input, forget and output gates; v t is the t-th word embedding and h t\u22121 is the hidden state at time step",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "and z su \u2208 R H are the network weights, where H is the number of hidden cells; s t = \u03b7 (v t ) denotes the sentiment value output by the retained NeSU mapping function \u03b7 as in Eq. ( 4); denotes element-wise multiplication. Essentially, SUSP uses the NeSU \u03b7 , via the underlined parts in Eq. ( 8-9) to capture context-dependent sentiment information and propagate this information to generate the context-dependent representation h t .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "The position information between the target and its context is also used to weight opinion words. The position weight l i of w i is calculated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "where k is the index of the first target word, m is the length of the target, and C is a constant associated with datasets. Finally h t is weighted with l t as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "SUCR: Using NeSU as a Context Reinforcer. Due to the integrated computation of Sentiment LSTM, some original context information might be lost.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "To preserve the genuine context, we parallelly employ a Context LSTM initialized with the VAE encoder to learn the generic word representation, and further incorporate NeSU with the position l to sentimentally reinforce the context representations generated by the Context LSTM. We call this whole module NeSU-based Context Reinforcer (SUCR) and define it as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "where h et is the hidden state generated by the Context LSTM at the t-th time step and s t is a sentiment value output by \u03b7 as in Eq. ( 4).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SUSP: Using NeSU as Sentiment Prior."
        },
        {
            "text": "We further consolidate the word-level representations generated by SUSP and SUCR via summation to form the final sentiment-aware and context-sensitive word representations. Then we apply a standard attention layer to fuse the semantic information of the context and the target. Particularly, let h Tm be the target representation generated by SUSP, h t and h et respectively denote the word representations generated by SUSP and SUCR. The input of attention layer is given as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dual LSTMs for Classifying Target Sentiment."
        },
        {
            "text": "We evaluate our method on two benchmark datasets: Laptop and Rest from SemEval 2014 [30] , containing reviews in laptop and restaurant domains. Following previous works [4, 5] , we remove the samples labeled \"conflict\". For VAE pretraining, a relatively large unlabeled dataset was collected, including Laptop, Rest. and Elec.. The unlabeled data Laptop and Rest. are respectively obtained from the Amazon Product Reviews 1 and Kaggle 2 , while Elec. is from [14] . The statistics of all datasets and the detailed hyperparameters are listed in Table 1 .",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 88,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 169,
                    "end": 172,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 173,
                    "end": 175,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 459,
                    "end": 463,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 544,
                    "end": 551,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Settings"
        },
        {
            "text": "For both labeled and unlabeled data, any punctuation is treated as space. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "To understand the discriminability of NeSU, this section demonstrates the sentiment NeSU perceives for each word in different sentences. It is clear that NeSU responds to the sentiment word \"long \" adaptively depending on the context, i.e., it is positive in Fig. 3 (a) and negative in Fig. 3(b) . In Figs. 3(a) , benefiting from the LSTM, the target \"battery life\" can arouse the NeSU memory from \"long \", generating a higher value. Fig. 3 (c) shows an example with subjunctive style, a challenging task for [5] . The NeSU can correctly assign a negative value for the positive sentiment word \"friendly \", and a downtrend/uptrend for \"bit\"/\"more\", demonstrating NeSU is also aware of implicit semantics.",
            "cite_spans": [
                {
                    "start": 509,
                    "end": 512,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 259,
                    "end": 265,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 286,
                    "end": 295,
                    "text": "Fig. 3(b)",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 301,
                    "end": 311,
                    "text": "Figs. 3(a)",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 434,
                    "end": 440,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Visualizing and Understanding NeSU"
        },
        {
            "text": "Overall Performance. The results are shown in Table 2 . On both datasets, our model NeaNet consistently achieves the best performance in both accuracy (ACC) and macro-F1 compared to all 13 supervised and semi-supervised methods. E.g., compared to RAM, MGAN, TNet and ASVAET, which are the best competing methods in the overall ACC, NeaNet substantially outperforms them by 1.18%-3.05% in Laptop and 2.64%-4.61% in Rest. The superiority of NeaNet is mainly due to the incorporation of the NeSU-driven SUCR and SUSP components that effectively leverage the discriminability of the NeSU to capture context-dependent sentiment information, which enables the LSTM networks to classify the sentiment of opinion targets more correctly. Particularly, as PRET+MULT is pretrained on document-level labeled sentiment data, its pretraining may introduce ambiguity for fine-grained sentiment task, leading to significantly less effective performance than NeaNet. ASVAET is also pretrained on unlabeled data, and generates generic representations only, which are much less expressive than the NeSU-enabled sentiment-aware representations.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 46,
                    "end": 53,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Comparison to State-of-the-Art Methods"
        },
        {
            "text": "Breakdown Performance. NeaNet obtains the best F1 performance in the negative class on both Rest. and Laptop, achieving 8.69% and 2.43% improvements over the best competing methods respectively. And NeaNet performs very competitive to the best results in positive and neutral classes. These results indicate that NeaNet well leverages unlabeled data to capture fine-grained sentiment features and achieves impressive improvements by using SUCR and SUSP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison to State-of-the-Art Methods"
        },
        {
            "text": "This section is to answer whether the discriminability of NeSU enables NeaNet to achieve a more data-efficient learning. We evaluate the performance of NeaNet with randomly reduced training data, with RAM and TNet as the baselines. The results are shown in Fig. 4 . NeaNet performs significantly better than RAM and TNet in both ACC and macro-F1 with different amount of labeled training data on both Laptop and Rest. Particularly, even when NeaNet is trained using 50% less labeled data, it can obtain the ACC and/or macro-F1 performance that is comparable well to, or better than, RAM on both datasets. Similarly, NeaNet achieves comparable well performance to TNet even if 25% less training data is used in training NeaNet. This justifies that NeaNet can leverage the sentiment-aware property of NeSU to achieve substantially more effective exploitation of the small labeled data.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 257,
                    "end": 263,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Data Efficiency"
        },
        {
            "text": "NeaNet is compared with its four ablations as follows to investigate the contribution of its different components.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ablation Study"
        },
        {
            "text": "-aLSTM*: aLSTM* is a simple semi-supervised version of aLSTM by initializing with our pretrained VAE encoder. -aLSTM*+NeSU: aLSTM*+NeSU is a simple NeSU-enabled aLSTM*, in which the NeSU-based sentiment value is added into the attention layer. -SUCR-enabled aLSTM*: It is an enhanced aLSTM* with its plain LSTM replaced with SUCR. It is equivalent to NeaNet with SUSP removed. -SUSP-enabled aLSTM*: It improves aLSTM* by replacing its LSTM with SUSP. It is a simplified NeaNet with SUCR removed. The results are given in the last group in Table 2 . aLSTM* performs significantly better than aLSTM on all datasets, showing that the pretrained VAE can extract highly transferable features from unlabeled data. aLSTM*+NeSU, SUCR-enabled aLSTM* and SUSP-enabled aLSTM* outperform aLSTM* in all performance measures, which indicates that the discriminability of NeSU can enhance the downstream classifiers in various ways, e.g., to enhance the attention as in aLSTM*+NeSU or the memory architecture of LSTM as in SUCR/SUSPenabled aLSTM*. SUCR/SUSP-enabled aLSTM* performs much better than aLSTM*+NeSU, indicating that SUSP and SUCR can exploit the power of NeSU more effectively; both of them underperform NeaNet, so both SUSP and SUCR are important to NeaNet. Particularly, SUSP-enabled aLSTM* performs consistently better than SUCR-enabled aLSTM*, revealing that, SUSP leverages the sentiment-aware property of NeSU to learn better representations than SUCR.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 539,
                    "end": 546,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Ablation Study"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A holistic lexicon-based approach to opinion mining",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Xiaowen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Philip",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Attention-based LSTM for aspect-level sentiment classification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Multi-grained attention network for aspect-level sentiment classification",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Recurrent attention network on memory for aspect sentiment analysis",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bing",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Transformation networks for target-oriented sentiment classification",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bing",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lam",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Using bilingual knowledge and ensemble techniques for unsupervised Chinese sentiment analysis",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Target-dependent twitter sentiment classification",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "NRC-Canada-2014: Detecting aspects and sentiment in customer reviews",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kiritchenko",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "SemEval",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Target-dependent sentiment classification with long short term memory",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Aspect level sentiment classification with deep memory network",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Generating sentences from a continuous space",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Bowman",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Vilnis",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Jozefowicz",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Effective attention modeling for aspectlevel sentiment classification",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "T"
                    ],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dahlmeier",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Adam: a method for stochastic optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Semi-supervised convolutional neural networks for text categorization via region embedding",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "On a measure of divergence between two statistical populations defined by their probability distributions",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bhattacharyya",
                    "suffix": ""
                }
            ],
            "year": 1943,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Variational semi-supervised aspect-term sentiment analysis via transformer",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Sentiment lexicon enhanced attention-based LSTM for sentiment classification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lei",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Convolution-based memory network for aspect-based sentiment analysis",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Qinghong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Gui",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Exploiting document knowledge for aspect-level sentiment classification",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "T"
                    ],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dahlmeier",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "A human-like semantic cognition network for aspect-level sentiment classification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lei",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Target-sensitive memory networks for aspect sentiment classification",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mazumder",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "IARM: inter-aspect relation modeling with memory networks in aspect-based sentiment analysis",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Majumder",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Poria",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Variational pretraining for semisupervised text classification",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gururangan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Card",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "A"
                    ],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Semi-supervised learning with auxiliary evaluation component for large scale e-commerce text classification",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Adversarial training methods for semisupervised text classification",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Miyato",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "I"
                    ],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Fuzzy deep belief networks for semi-supervised sentiment classification",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Neurocomputing",
            "volume": "131",
            "issn": "",
            "pages": "312--322",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Learning adversarial networks for semi-supervised text classification via policy gradient",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Attention and lexicon regularized LSTM for aspect-based sentiment analysis",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bao",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lambert",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Badia",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Phrasernn: phrase recursive neural network for aspectbased sentiment analysis",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "H"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shirai",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Semeval-2016 task 5: Aspect based sentiment analysis",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pontiki",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Neuron Discriminability). Let DS pos = {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x M } be the sentence set with positive sentiment and DS neg = {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x K } be the",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The proposed NeSU-enabled target-level sentiment classification framework.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "where C pos i \u223c N(\u03bc p i , (\u03c3 p i ) 2 ) contains the hidden state values of C i w.r.t. all the sentences with positive polarity; Similarly, C neg i \u223c N (\u03bc n i , (\u03c3 n i ) 2 ) contains the hidden state values for the negative polarity. A larger \u03c6 indicates greater separability between two hidden state distributions, thus, better discriminability.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The framework of NeaNet. SUSP and SUCR are the two NeSU-driven modules (NeSU-M). h S i is the integrated word representation of SUSP and SUCR, which carries context-dependent sentiments w.r.t. the target hT m . {v 1 , v 2 , . . . , v n } according to an embedding lookup table L \u2208 R D\u00d7V generated by the pretrained VAE, where D is the dimension of word vectors and V is the vocabulary size. The middle part consists of two core components which exploit NeSU to generate sentiment-aware representations, namely NeSU as Sentiment Prior (SUSP) and NeSU as Context Reinforcer (SUCR). The top parts are an attention layer and a softmax layer to combine the dual NeSU-driven modules to extract informative features for classification.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Visualization of the NeSU value for each word, as defined in Eq. ( 4). The red/green lines are to highlight where positive/negative sentiment concentrates. (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Results with decreasing training data.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "). Let C = {C 1 , C 2 . . . , C H } be the encoding LSTM neural unit set. Then neural sentiment units are defined as the neuron units with significantly large discriminability values:",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Basic statistics of datasets and settings of hyperparamters.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Results of all models on two benchmark datasets. The top two performance for each column are boldfaced. F1 is short for macro-F1. Our methods (NeaNet and its variants) aLSTM* 80.27 69.50 69.79 50.77 87.94 73.82 69.39 64.94 58.90 84.32 aLSTM*+NeSU 80.62 70.78 72.69 51.09 88.56 74.45 70.68 65.79 61.20 85.04 SUCR-enabled aLSTM* 81.25 71.54 74.37 51.17 89.07 75.24 71.28 67.69 60.12 86.03 SUSP-enabled aLSTM* 82.05 72.58 74.75",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This paper introduces a novel semi-supervised approach to leverage large-scale unlabeled data for target-level sentiment classification on small labeled data. We discover for the first time that a few neuron units in encoding LSTM cells of the pretrained VAE demonstrate highly sentiment-discriminative capability. We further explore two effective ways to incorporate the most discriminative neural sentiment unit (NeSU) into attention networks to develop a novel LSTMbased target-level sentiment classifier. Empirical results show that our NeSUenabled classifier substantially outperforms 13 state-of-the-art methods on two benchmark datasets and achieves significantly better data efficiency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ]
}