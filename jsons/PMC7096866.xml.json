{
    "paper_id": "PMC7096866",
    "metadata": {
        "title": "Twitter sentiment classification for measuring public health concerns",
        "authors": [
            {
                "first": "Xiang",
                "middle": [],
                "last": "Ji",
                "suffix": "",
                "email": "xj25@njit.edu",
                "affiliation": {}
            },
            {
                "first": "Soon",
                "middle": [
                    "Ae"
                ],
                "last": "Chun",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Zhi",
                "middle": [],
                "last": "Wei",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "James",
                "middle": [],
                "last": "Geller",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Public health surveillance is critical to monitoring the spread of infectious diseases and deploying rapid responses when there is an indication of an epidemic emerging. Different surveillance strategies have been developed to meet different needs. These strategies include sentinel surveillance systems, household surveys, laboratory-based surveillance, and most recently Integrated Disease Surveillance and Response (IDSR) (DCP 2014). Besides monitoring the spread of a disease itself, monitoring emotional changes of the general public, brought about by epidemics, is becoming increasingly important for public health specialists.",
            "cite_spans": [
                {
                    "start": 430,
                    "end": 434,
                    "mention": "2014",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The importance of monitoring the public\u2019s concerns about an epidemic is illustrated by the recent Ebola scare in the United States. Since the end of September 2014, Ebola concerns have spread in the United States after a Liberian visitor to Dallas became the first person to be diagnosed in the USA. The immigration examination and the medical system\u2019s ability to deal with Ebola were widely questioned by the general public (Reuters 2014) due to a series of missteps when the Liberian was issued a visitor visa and was not diagnosed by a Dallas hospital. For example, a tweet on October 15th of 2014, stated that, \u201cCo-worker LEGITIMATELY thinks #Ebola was caused by one of two things: (1) Gov\u2019t attempts at population control. (2) ISIS THIS IS NOT A JOKE\u201d. As the public opinion will potentially affect the government\u2019s public health decisions, President Obama attempted to calm the public by stating that \u201cThis is a serious disease, but we can\u2019t give into hysteria or fear\u201d (Reuters 2014).",
            "cite_spans": [
                {
                    "start": 434,
                    "end": 438,
                    "mention": "2014",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 985,
                    "end": 989,
                    "mention": "2014",
                    "ref_id": "BIBREF49"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Zhu et al. (2008) studied the changes in mental state of the Chinese public during the outbreak of SARS (2003). They found that, during the outbreak, most of the people surveyed (96.4 %) reported emotional changes and negative emotions such as panic (54.8 %), nervousness (34.0 %), and fear (7.6 %). Psychological changes might lead to unpredictable behavior. Of all the people surveyed, 23.3 % admitted to \u201cirrational\u201d behaviors such as going on a shopping spree, or to actions such as seeking shelter, preparing provisions, etc.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 16,
                    "mention": "2008",
                    "ref_id": "BIBREF71"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Another example is the public\u2019s reaction to Japan\u2019s nuclear emergency in March 2011 (Guardian 2011). Text messages about nuclear plumes spread throughout Asia. In China, the rumors that iodized salt could help ward off radiation poisoning amid Japan\u2019s nuclear emergency triggered panic buying all over the country. In Vietnam, students were kept indoors by schools, some companies allowed staff to leave early to avoid rainfall after the rumor spread that rain would burn the skin and cause cancer. A university in Manila canceled classes due to a similar scare.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 98,
                    "mention": "2011",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "As the above examples illustrate, monitoring public panic about health issues is critical not only to public health specialists but also to government decision makers. However, for traditional public health surveillance systems, it is hard to detect and monitor health-related concerns and changes in public attitudes to health-related issues. Due to their expenses, the existing surveillance methods, such as questionnaires and clinical tests, can only cover a limited number of people and results often appear with significant delays. To supplement the current surveillance systems, a novel tool must be developed. This tool must be able to track real-time statistics of emotions related to different health matters, such as epidemics, to provide early warning, and to help the government decision makers prevent or respond to potential social crises that might be the impact of these health-related emergencies.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The Web has created unprecedented resources for tracking threats to public health. Ginsberg et al. (2009) relied exclusively on search engine logs, in which users submitted queries in reference to issues that they were concerned about, to approach this problem. Their thread of research led to the realization that an aggregation of large numbers of queries might show patterns that are useful for the early detection of epidemics. Twitter, a micro-blog service provider, shows several advantages over search engines for disease surveillance. It is up-to-date and it has more than 500 million users in total. There are more than 340 million tweets posted by Twitter users per day (Twitter 2014a). Most tweets are public and the Twitter API (Twitter 2014b) enables researchers to retrieve the tweets as well as related information, such as geographical location and hyperlinks included.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 104,
                    "mention": "2009",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 689,
                    "end": 694,
                    "mention": "2014a",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 749,
                    "end": 754,
                    "mention": "2014b",
                    "ref_id": "BIBREF61"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We explore the potential of mining social network data, such as tweets, to provide a tool for public health specialists and government decision makers to gauge the measure of concern (MOC) expressed by Twitter users under the impact of diseases. To derive the MOC from Twitter, we developed a two-step classification approach to analyze sentiments in disease-related tweets. We first distinguish Personal from News (Non-Personal) tweets. Many news articles released by online media organizations are used for \u2018re-tweets\u2019 by Twitter users. We consider these News tweets as Non-Personal, as opposed to Personal tweets posted by individual Twitter users. We refer to the former as News tweets and the latter as Personal tweets. In the second stage, the sentiment analysis is applied only to Personal tweets to distinguish Negative from Non-Negative tweets.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Although News tweets may also express concerns about a certain disease, they tend not to reflect the direct emotional impact of that disease on people. A person re-tweeting a News message about a disease, which is comparable to forwarding an email message, is most likely not directly affected by it, while a user sending out a Personal tweet with emotional expressions might be directly affected. Note that the two-step sentiment classification problem we present is different from the traditional Twitter sentiment classification, which is categorizing tweets into positive/negative or positive/neutral/negative tweets (Zhang et al. 2011; Liu and Zhang 2012; Mohammad et al. 2013; Saif et al. 2013; Aramaki et al. 2011) without distinguishing Personal from Non-Personal tweets first. Our sentiment classification method is able to identify Personal tweets (including Personal Negative and Personal Non-Negative) and News (Non-Personal) tweets. In addition, we subsequently use the results of the classification to compute the correlation between sentiment-carrying tweets and News tweets, as the classification results provide all the necessary data for this computation.",
            "cite_spans": [
                {
                    "start": 635,
                    "end": 639,
                    "mention": "2011",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 655,
                    "end": 659,
                    "mention": "2012",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 677,
                    "end": 681,
                    "mention": "2013",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 695,
                    "end": 699,
                    "mention": "2013",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 716,
                    "end": 720,
                    "mention": "2011",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We need to differentiate between the spread of concern about a disease and the spread of the disease itself. For example, the tweet: \u201cWiz looks like he got the measles and Ross just dark as hell. I can\u2019t tell if they\u2019re tattoos or wrinkles http://twitpic.com/4geuc2\u201d is annotated as a Non-Negative tweet, because it shows no concern. However, it is a strong clue to track the spread of measles. We focus on studying the Twitter users\u2019 concerns about diseases instead of the outbreak of the disease itself, which has been extensively studied (Brownstein et al. 2008; Collier and Doan 2012; Signorini et al. 2011; Aramaki et al. 2011; Lampos and Cristianini 2010).",
            "cite_spans": [
                {
                    "start": 560,
                    "end": 564,
                    "mention": "2008",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 583,
                    "end": 587,
                    "mention": "2012",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 606,
                    "end": 610,
                    "mention": "2011",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 627,
                    "end": 631,
                    "mention": "2011",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 656,
                    "end": 660,
                    "mention": "2010",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Using the sentiment classification results, we quantify the MOC based on the number of Personal Negative tweets per day. The MOC increases with the relative growth of Personal Negative tweets and with the absolute growth of Personal Negative tweets. Previous research (Sha et al. 2014; Ji et al. 2013) found that sentiment surges co-occurred with health events on a timeline. Different from the previous work, we calculated the correlation between MOC timeline (i.e., change over time) and News timeline and the correlation between Non-Negative timeline and News timeline using the Jaccard Coefficient (Liben Nowell and Kleinberg 2007). Using the MOC to track public health concerns can help government officials to make timely decisions to refute rumors, and thus prevent potential social crises such as the past case of Chinese panic buying of salt. Monitoring of the public concern using social network data can provide public health specialists with a surveillance capability for large segments of the population, in real-time, and with low expenses.",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 284,
                    "mention": "2014",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 296,
                    "end": 300,
                    "mention": "2013",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 630,
                    "end": 634,
                    "mention": "2007",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We summarize our contributions as follows:We developed a two-step sentiment classification method by combining clue-based search and Machine Learning (ML) methods by first automatically labeling the training datasets, and then building classifiers for Personal tweets and classifiers for tweet sentiments. As previously discussed, the traditional Twitter sentiment classification methods classify the tweets into positive/negative or positive/neutral/negative. Our two-step algorithm is different from the traditional methods because it filters out the Personal tweets and News tweets in the first step, and then the Personal Negative tweets are used for defining a MOC. To the best of our knowledge, while previous research has extracted objective tweets, it has not explicitly extracted the News tweets, and has not utilized Personal Negative, Personal Non-Negative, and News tweets to define a MOC to quantify the sentiment trends on the timeline. Thus, using this novel method, one can combine the sentiment classification results into a MOC to reveal the sentiment timeline trends.We quantified the MOC using the results of sentiment classification, and used it to reveal the timeline trends of sentiments of tweets. We both quantitatively and qualitatively correlated the sentiment timeline trends and the News timeline trends, and calculated the correlation between MOC timeline and News timeline and the correlation between Non-Negative timeline and News timeline using the Jaccard Coefficient. We performed the correlation analysis among different tweet sentiment classes. The experimental results show that the peaks of the MOC and the peaks of NN (Non-Negative) tweets are weakly correlated with the peaks on the News timeline without any appreciable time delay/lead.We applied our sentiment classification method and the MOC to other topical domains, such as mental health monitoring and crisis management. The experimental results support the hypothesis that our approach is generalizable to other domains.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The rest of the paper is organized as follows. In Sect. 2, related work and open problems are discussed. In Sect. 3, we give formal definitions of the concepts used in this paper. In Sect. 4, sentiment classification methods and results are introduced in detail. In Sect. 5, the sentiment timeline trend analysis results are illustrated, interpreted, and discussed. Section 6 contains conclusions and suggestions for future research.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Since the year 2008, concepts and systems have been developed to monitor disease outbreaks and emergencies with Twitter. Artman et al. (2011) introduced the concept of dialogical emergency management, which emphasizes the screening of vast and quickly spread information on the Internet, to help the emergency management staff gain a better strategic awareness of the public. The Alert4All Screening of New Media (SNM) tool (Johansson et al. 2012) was developed based on this concept to analyze emotion recognition/affect in social media, e.g., Twitter and Facebook, regarding crisis management. Brownstein et al. (2008) used online News to perform surveillance of epidemics. Their system, Healthmap, collects reports from online News aggregators, such as Google News. By categorizing the News into epidemics-related and epidemics-unrelated reports, and filtering the epidemics-related documents into \u201cbreaking News\u201d, \u201cwarnings\u201d, and \u201cold News\u201d, the system is able to trigger alerts based on \u201cbreaking News\u201d. To detect disease outbreaks and monitor their progression over time and location, we have previously implemented the Epidemics Outbreak and Spread Detection System (EOSDS) (Ji et al. 2012) by monitoring social media data (specifically, Twitter data). EOSDS provides the functionality to perform geographic visual analysis of tweets with three different kinds of maps. The static map shows each individual tweet\u2019s location. The distribution (intensity) map displays absolute and relative frequencies of tweets from every USA state. The filter map provides users with a functionality to monitor the spread of a particular epidemic.",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 140,
                    "mention": "2011",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 442,
                    "end": 446,
                    "mention": "2012",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 615,
                    "end": 619,
                    "mention": "2008",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "mention": "2012",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Disease and emergency monitoring with Twitter ::: Related work",
            "ref_spans": []
        },
        {
            "text": "The other thread of research focused on building models, primarily supervised learning models, to detect disease and emergency events from Twitter. Collier and Doan (2012) developed a model to automatically classify Twitter messages into six fixed classes of syndromes, such as Respiratory and Gastrointestinal. Aramaki et al. (2011) applied a Support Vector Machine (SVM) to distinguish influenza-related tweets from tweets that are irrelevant. Signorini et al. (2011) also used an SVM-based estimator to analyze H1N1-related tweets, and estimated the Influenza-like Illness (ILI) rate, which is usually regarded as the ground truth, preceding the official announcement of an H1N1 outbreak by 1\u20132 weeks. Similarly, (Culotta 2010b) experimented with a number of regression models to correlate Twitter messages with statistics from the Center for Disease Control and Prevention (CDC) and provided a relatively simple method to track the ILI rate using a large number of Twitter messages (Culotta 2010a). Lampos and Cristianini (2010) used an approach to automatically learn a set of markers to help compute flu scores, and achieved a high correlation with the HPA flu score, which is the equivalent of the CDC score in the UK. All of the above research projects studied how to use Twitter to detect the outbreak of diseases instead of the sentiment trend caused by epidemics, which is the focus of this paper.",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 170,
                    "mention": "2012",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 328,
                    "end": 332,
                    "mention": "2011",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 464,
                    "end": 468,
                    "mention": "2011",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 725,
                    "end": 730,
                    "mention": "2010b",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 995,
                    "end": 1000,
                    "mention": "2010a",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1027,
                    "end": 1031,
                    "mention": "2010",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Disease and emergency monitoring with Twitter ::: Related work",
            "ref_spans": []
        },
        {
            "text": "Sentiment Analysis has been an active research area since the 2000s. With an increasing number of datasets from various data sources, such as blogs, review sites, News articles, and micro-blogs available, researchers have become interested in mining high-level sentiments from them. Sentiments are also closely related to information spread. Their relationship was shown in different contexts, such as social transmission (Berger 2011), News broadcasts (Heath 1996), and online social media, such as Twitter (Stieglitz and Dang-Xuan 2013). By analyzing the sentiments of opinion leaders, the public health officials will be able to monitor the viral effects in social media communication, and take early actions to prevent unnecessary panic.",
            "cite_spans": [
                {
                    "start": 430,
                    "end": 434,
                    "mention": "2011",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 460,
                    "end": 464,
                    "mention": "1996",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 533,
                    "end": 537,
                    "mention": "2013",
                    "ref_id": "BIBREF59"
                }
            ],
            "section": "Sentiment analysis ::: Related work",
            "ref_spans": []
        },
        {
            "text": "A survey of sentiment analysis was done by Pang and Lee (2008). According to the target of analysis, the research on sentiment analysis can be categorized into the following levels: document-level (Pang et al. 2002), blog-level (Mishne 2005), sentence-level (Wilson et al. 2005), tweet-level (Johansson et al. 2012; Brynielsson et al. 2014; Saif et al. 2014) with the sub-category non-English tweet-level (Refaee and Rieser 2014), and tweet-entity-level (Saif et al. 2012). Due to the large number of available tweets and their real-time nature, tweets are ideal for sentiment classification and quantification for disease monitoring, and more broadly, for crisis monitoring.",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 61,
                    "mention": "2008",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 210,
                    "end": 214,
                    "mention": "2002",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 236,
                    "end": 240,
                    "mention": "2005",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 273,
                    "end": 277,
                    "mention": "2005",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 310,
                    "end": 314,
                    "mention": "2012",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 335,
                    "end": 339,
                    "mention": "2014",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 353,
                    "end": 357,
                    "mention": "2014",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 424,
                    "end": 428,
                    "mention": "2014",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 467,
                    "end": 471,
                    "mention": "2012",
                    "ref_id": "BIBREF52"
                }
            ],
            "section": "Sentiment analysis ::: Related work",
            "ref_spans": []
        },
        {
            "text": "Extensive research has been done in the sub-area of Twitter sentiment classification since 2009 (Barbosa and Feng 2010; Bifet and Frank 2010; Pak and Paroubek 2010; Jiang et al. 2011; Mohammad et al. 2013; Zhou et al. 2014; Brynielsson et al. 2014). Most of this thread of research used Machine Learning-based approaches such as Na\u00efve Bayes, Multinomial Na\u00efve Bayes, and Support Vector Machine. The Na\u00efve Bayes classifier is a derivative of the Bayes decision rule (Fukunaga 1990), and it assumes that all features are independent from each other. Good performance of Na\u00efve Bayes (NB) was reported in several sentiment analysis papers (Barbosa and Feng 2010; Brynielsson et al. 2014; Zhou et al. 2014). Multinomial Naive Bayes (MNB) is a model that works well on sentiment classification (Bifet and Frank 2010; Pak and Paroubek 2010; Zhou et al. 2014). MNB takes into account the number of occurrences and relative frequency of each word. Support Vector Machine (Cortes and Vapnik 1995) is also a popular ML-based classification method that works well on tweets (Jiang et al. 2011; Brynielsson et al. 2014). In Natural Language Processing, SVM with a polynomial kernel is more popular (Chang and Lin 2011).",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 118,
                    "mention": "2010",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 136,
                    "end": 140,
                    "mention": "2010",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 159,
                    "end": 163,
                    "mention": "2010",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 178,
                    "end": 182,
                    "mention": "2011",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 200,
                    "end": 204,
                    "mention": "2013",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 218,
                    "end": 222,
                    "mention": "2014",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 243,
                    "end": 247,
                    "mention": "2014",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 475,
                    "end": 479,
                    "mention": "1990",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 653,
                    "end": 657,
                    "mention": "2010",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 678,
                    "end": 682,
                    "mention": "2014",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 696,
                    "end": 700,
                    "mention": "2014",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 805,
                    "end": 809,
                    "mention": "2010",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 828,
                    "end": 832,
                    "mention": "2010",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 846,
                    "end": 850,
                    "mention": "2014",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 981,
                    "end": 985,
                    "mention": "1995",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "mention": "2011",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1101,
                    "end": 1105,
                    "mention": "2014",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1200,
                    "end": 1204,
                    "mention": "2011",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Twitter sentiment classification ::: Related work",
            "ref_spans": []
        },
        {
            "text": "Mohammad et al. (2013) explored an extensive list of features such as clusters, negation, and n-grams, and used a Support Vector Machine (SVM) to classify Twitter messages into positive, negative, and neutral. Barbosa and Feng (2010) focused on automation of the training data generation process. Their work combined sentiment-labeled tweets coming from three sources: Twendz, Twitter Sentiment, and Tweet Feel. A moderate Cohen\u2019s Kappa Coefficient served as evidence that the combination of several sources reduced the bias of the individual sources. In this way, the combination improved the polarity classification.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 21,
                    "mention": "2013",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 228,
                    "end": 232,
                    "mention": "2010",
                    "ref_id": "BIBREF44"
                }
            ],
            "section": "Twitter sentiment classification ::: Related work",
            "ref_spans": []
        },
        {
            "text": "The above sentiment classification studies have two drawbacks:They classified Twitter messages into either positive/negative or positive/negative/neutral with the assumption that all Twitter messages express ones\u2019 opinion. However, this assumption does not hold in many situations, especially when the tweets are about epidemics or more broadly, about crises. In these situations, as we found when we randomly sampled 100 tweets, many tweets (up to 30 %) of the samples are repetitions of the News without any personal opinion. Since they are not explicitly labeled with re-tweet symbols, it is not easy for a stopword-based pre-processing filter to detect them. We attempt to solve a different problem, which is how to classify tweets into three categories: Personal Negative tweets, Personal Non-Negative tweets, and News tweets (tweets that are non-Personal tweets). We are not singling out positive tweets, as few people would post positive tweets about a spreading epidemic.Instead of identifying News tweets, Brynielsson et al. (2014) used manual labeling to classify tweets into \u201cangry\u201d, \u201cfear\u201d, \u201cpositive\u201d, or \u201cother\u201d (irrelevant). Salathe and Khandelwal (2011) also identified irrelevant tweets together with sentiment classifications. Without considering irrelevant tweets, they calculated the H1N1 vaccine sentiment score from the relative difference of positive and negative messages. As we will show later, by the two-step classification method, we can automatically extract News tweets and perform the sentiment analysis, and the results of sentiment classification are the input for computing the correlation between sentiments and News trends. In this way, the goals of sentiment classification and measuring the public concern can be achieved in an integrated framework.Secondly, although the above research approaches have developed sophisticated models to improve the precision and recall of sentiment classification, they did not quantify the results of the sentiment classification to measure timeline trends, and correlate them with real-world incidents, to provide insights for public health specialists and government decision makers. We developed the MOC to quantify the sentiments, and we correlate sentiment trends and News trends to provide better knowledge of Twitter users\u2019 reactions toward crises, such as epidemics, mental health problems, clinical science problems, etc.\n",
            "cite_spans": [
                {
                    "start": 1035,
                    "end": 1039,
                    "mention": "2014",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "mention": "2011",
                    "ref_id": "BIBREF56"
                }
            ],
            "section": "Twitter sentiment classification ::: Related work",
            "ref_spans": []
        },
        {
            "text": "The objective of sentiment quantification is to convert natural language text to a numerical value or a timeline of numerical values to gain insights into the sentiment trends. Zhuang et al. (2006) generated a quantification of sentiments about movie elements, such as special effects, plot, dialog, etc. Their quantification contains a positive score and a negative score toward a specific movie element.",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 196,
                    "mention": "2006",
                    "ref_id": "BIBREF72"
                }
            ],
            "section": "Quantifying Twitter sentiment on timeline ::: Related work",
            "ref_spans": []
        },
        {
            "text": "For tweet-level sentiment quantification on a timeline, Chew and Eysenbach (2010) used a statistical approach to computing the relative proportion of all tweets expressing concerns about H1N1 and visualized the temporal trend of positive/negative sentiments based on their proportion. Similar research was done by O\u2019Connor et al. (2010). In their thread of research, they quantified the sentiments as a timeline by deriving a day-to-day (positive and negative) sentiment score simply by counting how many positive and negative words of one tweet appear in the subjectivity lexicon of OpinionFinder (Wilson et al. 2005), which is a list containing words marked as positive or negative. By analyzing Chinese micro-blogs, Sha et al. (2014) found that the sentiment fluctuations on a timeline were associated with the announcements of new regulations or government actions.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 80,
                    "mention": "2010",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 331,
                    "end": 335,
                    "mention": "2010",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 613,
                    "end": 617,
                    "mention": "2005",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 731,
                    "end": 735,
                    "mention": "2014",
                    "ref_id": "BIBREF57"
                }
            ],
            "section": "Quantifying Twitter sentiment on timeline ::: Related work",
            "ref_spans": []
        },
        {
            "text": "There are two drawbacks of the existing Twitter sentiment quantification research: (1) the clue-based sentiment extraction models used by the above researchers are often too limited. As pointed out by Wiebe and Riloff (2005), identifying positive or negative tweets by counting words in a dictionary usually has high precision but low recall. In the case of Twitter sentiment analysis, the performance will be even worse, since many words in tweets are not recorded in a dictionary. For example, LMAO is a positive \u201cword\u201d in Twitter, but it does not match any word in MPQA (Riloff and Wiebe 2003), which is a popular sentiment dictionary. (2) The correlation between sentiments and News events are only studied visually by observing their co-occurrence on a timeline (Sha et al. 2014; O\u2019Connor et al. 2010), but to the best of our knowledge, there is no prior work that both quantitatively and qualitatively studies these correlations between Twitter sentiment and the News in Twitter to identify concerns caused by diseases and crisis.",
            "cite_spans": [
                {
                    "start": 219,
                    "end": 223,
                    "mention": "2005",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 591,
                    "end": 595,
                    "mention": "2003",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 779,
                    "end": 783,
                    "mention": "2014",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 801,
                    "end": 805,
                    "mention": "2010",
                    "ref_id": "BIBREF40"
                }
            ],
            "section": "Quantifying Twitter sentiment on timeline ::: Related work",
            "ref_spans": []
        },
        {
            "text": "As we summarized the Twitter sentiment classification and Twitter sentiment quantification research, there is a research gap between them. More specifically, the existing sentiment classification research does not quantify sentiment timeline trends from the classification results to provide insights into the sentiments. On the other hand, the existing sentiment quantification research often used a clue-based model, which has a low recall in terms of identifying sentiment tweets. In addition, the existing sentiment quantification work has only qualitatively correlated the sentiment timeline with real-world events, but has not provided a comprehensive, quantitative correlation between the sentiment timeline trend and the News timeline trend. This work is our attempt to fill this gap.",
            "cite_spans": [],
            "section": "Quantifying Twitter sentiment on timeline ::: Related work",
            "ref_spans": []
        },
        {
            "text": "There are two objectives to achieve. The first objective is to automatically label datasets for training a Twitter sentiment classifier together with identifying News (Non-Personal) tweets. The purpose of identifying News tweets is that it can help filtering them out in the first step and then the Negative vs. Non-Negative classifier can be applied only to the Personal tweets. The second objective is to quantify the sentiment trends and News timeline trends from sentiment classification results, and compute a quantitative measure of correlation between them, to better understand the sentiment timeline trends relative to events in the real world.",
            "cite_spans": [],
            "section": "Quantifying Twitter sentiment on timeline ::: Related work",
            "ref_spans": []
        },
        {
            "text": "A Personal\nTweet is defined to be one that expresses its author\u2019s private states (Wilson and Wiebe 2003; Wilson et al. 2005). A private state can be a sentiment, opinion, speculation, emotion, or evaluation, and it cannot be verified by objective observation. In addition, if a tweet talks about a fact observed by the Twitter user, it is also defined as a Personal Tweet.",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 103,
                    "mention": "2003",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 119,
                    "end": 123,
                    "mention": "2005",
                    "ref_id": "BIBREF68"
                }
            ],
            "section": "Definition 1 (Personal Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "\u201cThe boyfriend is STILL sick from the @fatburger he ate last Thursday. The doctor suspects listeria. :(\u201d",
            "cite_spans": [],
            "section": "Example (Personal Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "The purpose of this definition is to distinguish the tweets written word-by-word by the Twitter users from the News tweets redistributed in the Twitter environment, as mentioned above.",
            "cite_spans": [],
            "section": "Example (Personal Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "A News Tweet (denoted with NT) is a tweet that is not a Personal Tweet. A News Tweet states an objective fact.",
            "cite_spans": [],
            "section": "Definition 2 (News Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "\u201cMeasles outbreak reported in Honiara, Solomon Islands | Outbreak News Today http://fb.me/1hMxpNmrh\u201d",
            "cite_spans": [],
            "section": "Example (News Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "If a tweet is a Personal Tweet, and it expresses negative emotions or attitude, it is a Personal Negative Tweet (denoted as PN). Otherwise, it is a Personal Non-Negative Tweet (denoted as PNN). Personal Non-Negative Tweets include personal neutral or personal positive tweets. A Personal Tweet is either a PN or a PNN.",
            "cite_spans": [],
            "section": "Definition 3 (Personal Negative Tweet and Personal Non-Negative Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "A Raw Tweet\ntw is defined as a tuple1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$tw \\, = \\, < tid, \\, te, \\, ty, \\, h, \\, t >$$\\end{document}tw=<tid,te,ty,h,t>where tid is the unique tweet identifier; te is the tweet text; ty is the tweet type, which is a disease or crisis as the topic of this tweet (e.g., \u201cSwine Flu\u201d or \u201cBipolar Disorder\u201d, etc.); h is the tweet holder (\u201ctweeter\u201d), which is the person who posted the tweet tw; and t is the time when tw was posted.",
            "cite_spans": [],
            "section": "Definition 4 (Raw Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "\ntw = <TFare1,\u201cEverytime someone writes #tb i think of tuberculosis\u201d, tuberculosis, StealYoKidney, 10/8/2014>.",
            "cite_spans": [],
            "section": "Example (Raw Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "It means that the user \u201cStealYoKidney\u201d posted a tweet of \u201ctuberculosis\u201d on 10/8/2014, and the tweet\u2019s text is \u201cEverytime someone writes #tb i think of tuberculosis\u201d.",
            "cite_spans": [],
            "section": "Example (Raw Tweet) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Given a Raw Tweet tw, O(tw) is defined as the Label of the tweet, where2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$O\\left( {tw} \\right)\\left\\{ {PN, \\, PNN, \\, NT} \\right\\}$$\\end{document}OtwPN,PNN,NTsuch that PN is a Personal Negative Tweet, PNN is a Personal Non-Negative Tweet, and NT is a News Tweet.",
            "cite_spans": [],
            "section": "Definition 5 (Label) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Given a Raw Tweet tw, a Tweet Label\nts is defined as a tuple3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ts \\, = \\, < ty\\left( {tw} \\right), \\, O\\left( {tw} \\right), \\, h, \\, t >$$\\end{document}ts=<tytw,Otw,h,t>where ty(tw) is the type of tweet tw (e.g., listeria); O(tw) is the Label as defined in Definition 5; h is the Label holder, who is the person who posted the tweet tw; and t is the time when tw was posted.",
            "cite_spans": [],
            "section": "Definition 6 (Tweet Label) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Tweet Label for the tweet \u201cThen I gotta go get the damn tuberculosis test?\u201d is <tuberculosis, PN, user1, 4/5/2014>;",
            "cite_spans": [],
            "section": "Example (Tweet Label) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Tweet Label for the tweet \u201cSigned off again with viral meningitis! Nice one\u201d is <meningitis, PNN, user2, 5/7/2014>;",
            "cite_spans": [],
            "section": "Example (Tweet Label) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Tweet Label for the tweet \u201cListeria risk prompts recall of cheese from Unicer Foods in Canada\u201d is <listeria, NT, user3, 6/8/2014>.",
            "cite_spans": [],
            "section": "Example (Tweet Label) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "A Tweet Label Dataset is defined as a set of Tweet Labels of the same type collected at a specific time t.4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$TS_{t} = \\, \\left\\{ {ts_{1} , \\, ts_{2} , \\ldots ts_{n} } \\right\\}$$\\end{document}TSt=ts1,ts2,\u2026tsn\n",
            "cite_spans": [],
            "section": "Definition 7 (Tweet Label Dataset) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "<1, \u201cBack to the wonderful world of listeria! WOO HOO! Here I come!\u201d, workin_with_S, 6/23/2014>",
            "cite_spans": [],
            "section": "Example (Tweet Label Dataset) two Raw Tweets were collected ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "<2,\u201c#Listeria outbreak causes Roos Foods to shut plant #FoodSafety http://t.co/mUOJMZqbUB #LegalUpdates\u201d, Nlegal_IMC, 6/23/2014>",
            "cite_spans": [],
            "section": "Example (Tweet Label Dataset) two Raw Tweets were collected ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "The Tweet Label Dataset TS\n6/23/2014 = {<listeria, PNN, workin_with_S, 6/23/2014>, <listeria, NT, Nlegal_IMC, 6/23/2014>}, where there are two Tweet Labels of \u201clisteria\u201d type on 6/23/2014.",
            "cite_spans": [],
            "section": "Example (Tweet Label Dataset) two Raw Tweets were collected ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Given is TS\ni as a Tweet Label Dataset (as defined in Definition 7) of a particular type (e.g., listeria) at a time i. The MOC M\ni is defined as follows:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M_{i} = {{\\left( {\\mathop \\sum \\nolimits_{1}^{n} \\sigma (ts_{j} )} \\right)^{2} } \\mathord{\\left/ {\\vphantom {{\\left( {\\mathop \\sum \\limits_{1}^{n} \\sigma (ts_{j} )} \\right)^{2} } {\\left| {TS_{i} } \\right|}}} \\right. \\kern-0pt} {\\left| {TS_{i} } \\right|}}$$\\end{document}Mi=\u22111n\u03c3(tsj)2TSiwhere \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma \\left( {ts_{j} } \\right) = 1$$\\end{document}\u03c3tsj=1 if O(ts\nj) = PN and ts\nj \u220a TS\ni; \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma \\left( {ts_{j} } \\right) = 0$$\\end{document}\u03c3tsj=0 otherwise. Intuitively, M\ni is the square of the total number of Personal Negative tweets that are posted at time i, divided by the total number of Raw Tweets of a particular type at the same time i. The MOC increases with the relative growth of Personal Negative tweets and with the absolute growth of Personal Negative tweets.",
            "cite_spans": [],
            "section": "Definition 8a (Measure of Concern) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Similarly, the Non-Negative Sentiment\nNN\ni is defined as follows:6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$NN_{i} = {{\\left( {\\mathop \\sum \\nolimits_{1}^{n} \\alpha (ts_{j} )} \\right)^{2} } \\mathord{\\left/ {\\vphantom {{\\left( {\\mathop \\sum \\limits_{1}^{n} \\alpha (ts_{j} )} \\right)^{2} } {\\left| {TS_{i} } \\right|}}} \\right. \\kern-0pt} {\\left| {TS_{i} } \\right|}}$$\\end{document}NNi=\u22111n\u03b1(tsj)2TSiwhere \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha \\left( {ts_{j} } \\right) = 1$$\\end{document}\u03b1tsj=1 if O(ts\nj) = PNN and ts\nj \u220a TS\ni; \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha \\left( {ts_{j} } \\right) = 0$$\\end{document}\u03b1tsj=0 otherwise. Intuitively, NN\ni is the square of the total number of Personal Non-Negative tweets that are posted on time i, divided by the total number of Raw Tweets of a particular type at the same time i.",
            "cite_spans": [],
            "section": "Definition 8b (Non-Negative Sentiment) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Finally, the News Count\nNE\ni is defined as follows:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$NE_{i} = \\mathop \\sum \\nolimits_{1}^{n} \\beta \\left( {ts_{j} } \\right)$$\\end{document}NEi=\u22111n\u03b2tsjwhere \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta \\left( {ts_{j} } \\right) = 1$$\\end{document}\u03b2tsj=1 if O(ts\nj) = NT and ts\nj \u220a TS\ni; \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta \\left( {ts_{j} } \\right) = 0$$\\end{document}\u03b2tsj=0 otherwise. NE\ni is the total number of News Tweets at the time i. Note that the News Count is not normalized by the total number of Raw Tweets. The reason is that we are interested in studying the relationship between sentiment trends and News popularity trends (see Sect. 5). An absolute News Count is able to better represent the popularity of News.",
            "cite_spans": [],
            "section": "Definition 8c (News Count) ::: Definitions",
            "ref_spans": []
        },
        {
            "text": "Given a series of time points T = (1, 2, \u2026, j, k, \u2026 n), where j < k, the Measure of Concern Timeline is defined as the time series8\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$MOC\\left[ {1:n} \\right] \\, = \\, \\left( {M_{1} , \\, M_{2} , \\, M_{i} , \\, \\ldots M_{n} } \\right)$$\\end{document}MOC1:n=M1,M2,Mi,\u2026Mnwhere M\ni is the MOC at time i. Similarly, NN[1:n] and NE[1:n] are defined as the Non-Negative Sentiment Timeline, and the News Count Timeline, respectively. Figure 5 in Sect. 5 will show the visualization of an MOC Timeline.",
            "cite_spans": [],
            "section": "Definition 9 (Measure of Concern Timeline) ::: Definitions",
            "ref_spans": [
                {
                    "start": 695,
                    "end": 696,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "Given a timeline, a value X\ni on the timeline is defined as a peak if and only if X\ni is the largest value in a given time interval [i \u2212 b, i + a]. The time intervals a > 0, b > 0 can be chosen according to each specific case to limit the number of peaks. Peaks are defined for MOC timelines, Non-Negative timelines, and News Count timelines. Figure 5 in Sect. 5 will show the peaks as red or black dots on an MOC Timeline.",
            "cite_spans": [],
            "section": "Definition 10 (Peak) ::: Definitions",
            "ref_spans": [
                {
                    "start": 350,
                    "end": 351,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "In cases of disease surveillance on Twitter, the classical division of sentiments into positive and negative is inappropriate, because diseases are generally classified as negative. Positive emotions could arise as a result of relief about an epidemic subsiding, but we ignore this possibility. Thus, a two-point \u201cLikert scale\u201d with the points positive and negative would not cover this spectrum well. Rather, we started with an asymmetric four-point Likert scale of \u201cstrongly negative\u201d, \u201cnegative\u201d, \u201cneutral\u201d, and \u201cpositive\u201d. We then combined \u201cstrongly negative\u201d and \u201cnegative\u201d into one category, and \u201cneutral\u201d and \u201cpositive\u201d into another. We use \u201cNegative\u201d as the name of the first category and \u201cNon-Negative\u201d for the second one. Thus, the problem reduces to a two-class classification problem, and a Personal tweet can either be a Negative tweet or a Non-Negative tweet.",
            "cite_spans": [],
            "section": "Pre-processing of features ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "Some features need to be removed or replaced. We first deleted the tweets starting with \u201cRT\u201d, which indicates that they are re-tweets without comments to avoid duplications. For the remaining tweets, the special characters were removed. The URLs in Twitter were replaced by the string \u201curl\u201d. Twitter\u2019s special character \u201c@\u201d was replaced by \u201ctag\u201d. For punctuations, \u201c!\u201d and \u201c?\u201d were substituted by \u201cexcl\u201d and \u201cques\u201d, respectively, and any of \u201c.,:;\u2212|+=/\u201d were replaced by \u201csymb\u201d. Twitter messages were transformed into vectors of words, such that every word was used as one feature, and only unigrams were utilized for simplicity.",
            "cite_spans": [],
            "section": "Pre-processing of features ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "The clue-based classifier parses each tweet into a set of tokens and matches them with a corpus of Personal clues. There is no available corpus of clues for Personal versus News classification, so we used a subjective corpus MPQA (Riloff and Wiebe 2003) instead, on the assumption that if the number of strongly subjective clues and weakly subjective clues in the tweet is beyond a certain threshold (e.g., two strongly subjective clues and one weakly subjective clue), it can be regarded as Personal tweet, otherwise it is a News tweet. The MPQA corpus contains a total of 8221 words, including 3250 adjectives, 329 adverbs, 1146 any-position words, 2167 nouns, and 1322 verbs. As for the sentiment polarity, among all 8221 words, 4912 are negatives, 570 are neutrals, 2718 are positives, and 21 can be both negative and positive. In terms of strength of subjectivity, among all words, 5569 are strongly subjective words, and the other 2652 are weakly subjective words.",
            "cite_spans": [
                {
                    "start": 248,
                    "end": 252,
                    "mention": "2003",
                    "ref_id": "BIBREF51"
                }
            ],
            "section": "Clue-based tweet Labeling ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "Twitter users tend to express their personal opinions in a more casual way compared with other documents, such as News, online reviews, and article comments. It is expected that the existence of any profanity might lead to the conclusion that the tweet is a Personal tweet. We added a set of 247 selected profanity words (Ji 2014a) to the corpus described in the previous paragraph. USA law, enforced by the Federal Communication Commission, prohibits the use of a short list of profanity words in TV and radio broadcasts (FederalCommunicationsCommittee 2014). Thus, any word from this list in a tweet clearly indicates that the tweet is not a News item.",
            "cite_spans": [
                {
                    "start": 325,
                    "end": 330,
                    "mention": "2014a",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 554,
                    "end": 558,
                    "mention": "2014",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Clue-based tweet Labeling ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "We counted the number of strongly subjective terms and the number of weakly subjective terms, checked for the presence of profanity words in each tweet and experimented with different thresholds. A tweet is labeled as Personal if its count of subjective words surpasses the chosen threshold; otherwise it is labeled as a News tweet.",
            "cite_spans": [],
            "section": "Clue-based tweet Labeling ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "In clue-based classification, if the threshold is set too low, the precision might not be good enough. On the other hand, if the threshold is set too high, the recall will be decreased. The advantage of a clue-based classifier is that it is able to automatically extract Personal tweets with more precision when the threshold is set to a higher value.",
            "cite_spans": [],
            "section": "Clue-based tweet Labeling ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "Because only the tweets fulfilling the threshold criteria are selected for training the \u201cPersonal vs. News\u201d classifier, we would like to make sure that the selected tweets are indeed Personal with high precision. Thus, the threshold that leads to the highest precision in terms of selecting Personal tweets is the best threshold for this purpose.",
            "cite_spans": [],
            "section": "Clue-based tweet Labeling ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "The performance of the clue-based approach with different thresholds on human-annotated test datasets is shown in Table 1. More detailed information about the human-annotated dataset is shown in Sect. 4.3.2.2. Among all the thresholds, s3w3 (3 strong, 3 weak) achieves the highest precision on all three human annotated datasets. In other words, when the threshold is set so that the minimum number of strongly subjective terms is 3 and the minimum number of weakly subjective terms is 3, the clue-based classifier is able to classify Personal tweets with the highest precision of 100 % but with a low recall (15 % for epidemic, 7 % for mental health, 1 % for clinical science).\n",
            "cite_spans": [],
            "section": "Clue-based tweet Labeling ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 120,
                    "end": 121,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "To overcome the drawback of low recall in the clue-based approach, we combined the high precision of clue-based classification with Machine Learning-based classification in the Personal vs. News classification, as shown in Fig. 2. Suppose that the collection of Raw Tweets of a unique type (e.g., tuberculosis) is T. After the pre-processing step, which filters out non-English tweets, re-tweets, and near-duplicate tweets, the resulting tweet dataset is T\u2032 =  {tw\n1\n, tw\n2\n, tw\n3\n,\u2026, tw\nn}, which is a subset of T, and is used as the input for the clue-based method for automatically labeling datasets for training a Personal vs. News classifier as shown in Fig. 2.\n",
            "cite_spans": [],
            "section": "Machine learning classifiers for personal tweet classification ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 228,
                    "end": 229,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 664,
                    "end": 665,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "In the clue-based step for labeling training datasets, each tw\ni of T\u2032 is compared with the MPQA dictionary (Riloff and Wiebe 2003). If tw\ni contains at least three strongly subjective clues and at least three weakly subjective clues, tw\ni is labeled as a Personal tweet. Similarly, tw\ni is compared with a News stopword list (Ji 2014b) and a profanity list (Ji 2014a). The News stopword list contains 20+ names of highly influential public health News sources and the profanity list has 340 commonly used profanity words. If tw\ni contains at least one word from the News stopword list and does not contain any profanity word, tw\ni is labeled as a News tweet. For example, the tweet \u201cAtlanta confronts tuberculosis outbreak in homeless shelters: By David Beasley ATLANTA (Reuters)\u2014Th\u2026 http://yhoo.it/1r88Lnc #Atlanta\u201d is labeled as a News tweet, because it contains at least one word from the News stopword list and does not contain any profanity word. We mark the set of labeled Personal tweets as T\np\u2032, and the set of labeled News tweets as T\nn\u2032, note that (T\np\u2032 \u222a T\nn\u2032) \u2286 T\u2019.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 130,
                    "mention": "2003",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 330,
                    "end": 335,
                    "mention": "2014b",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 362,
                    "end": 367,
                    "mention": "2014a",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Machine learning classifiers for personal tweet classification ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "The next step is the Machine Learning-based method. The two classes of data \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{p}^{\\prime }$$\\end{document}Tp\u2032 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{n}^{\\prime }$$\\end{document}Tn\u2032 from the clue-based labeling are used as training datasets to train the Machine Learning models. We used three popular models: Na\u00efve Bayes, Multinomial Na\u00efve Bayes, and polynomial-kernel Support Vector Machine. After the Personal vs. News classifier is trained, the classifier is used to make predictions on each tw\ni in T\u2032, which is the preprocessed tweets dataset. The goal of Personal vs. News classification is to obtain the Label for each tw\ni in the tweet database T\u2032, where the Label O(ts\ni) is either Personal or NT (News Tweet). Label was introduced in Definition 5, whereby Personal could be PN or PNN.",
            "cite_spans": [],
            "section": "Machine learning classifiers for personal tweet classification ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "As shown in Fig. 1, after a classifier for Personal tweets in step 1 is built, the second step in the sentiment classification is to classify the set of Personal tweets \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T\\prime \\prime = \\left\\{ {tw_{i} :O\\left( {tw_{i} } \\right) = {\\text{Personal}}, tw_{i} \\in T\\prime } \\right\\}$$\\end{document}T\u2033=twi:Otwi=Personal,twi\u2208T\u2032 into Personal Negative (PN) or Personal Non-Negative (PNN) tweets. Figure 3 shows the process of classification in this second step. In the rest of this section, Negative is used to refer to the Personal Negative and Non-Negative is used to refer to the Personal Non-Negative.\n",
            "cite_spans": [],
            "section": "Negative sentiment classifier ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 17,
                    "end": 18,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 666,
                    "end": 667,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "In terms of training the classifier for Negative vs. Non-Negative classification, the ideal training dataset must be large and contain little noise. Manual annotation of a training dataset is possible, but this process usually requires different annotators to independently label each tweet and to calculate their degree of agreement. This limits the fast generation of large-sized training datasets. Pang and Lee (2008) listed a few annotated corpuses used in previous work in the field of sentiment analysis. These corpuses cover topics such as customer reviews of products and restaurants. However, to the best of our knowledge, there is no disease-related annotated corpus that can be used as a training dataset to distinguish Negative tweets from Non-Negative tweets.",
            "cite_spans": [
                {
                    "start": 415,
                    "end": 419,
                    "mention": "2008",
                    "ref_id": "BIBREF42"
                }
            ],
            "section": "Negative sentiment classifier ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "In order to build the training datasets for Negative versus Non-Negative classification (TR-NN), we formed a whitelist and blacklist of stopwords using predefined emoticons. An emoticon is a combination of characters that form a pictorial expression of one\u2019s emotions. Emoticons have been used as important indicators of sentiments in previous research. We combined the emoticon lists used by Go et al. (2009), Pak and Paroubek (2010), and Agarwal et al. (2011). A partial list of emoticons is in Table 2.\n",
            "cite_spans": [
                {
                    "start": 404,
                    "end": 408,
                    "mention": "2009",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 429,
                    "end": 433,
                    "mention": "2010",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 456,
                    "end": 460,
                    "mention": "2011",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Negative sentiment classifier ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 503,
                    "end": 504,
                    "mention": "2",
                    "ref_id": "TABREF11"
                }
            ]
        },
        {
            "text": "The whitelist and blacklist of stopwords for building TR-NN are described in Table 3. The whitelist is used for extracting while the blacklist is used for eliminating information. A tweet is extracted as a Negative tweet if and only if this tweet contains at least one stopword (or emoticon) from the Negative whitelist, and does not contain any stopword (or emoticon) from the Negative blacklist. A tweet is extracted as Non-Negative using similar lists, a Non-Negative whitelist, and a corresponding blacklist. For example, the tweet \u201cThey are going to take fluid from around the spinal cord to see if she has meningitis\u2026 :(\u201d is extracted as a Negative tweet, because it contains at least one stopword from the Negative whitelist and no words from the Negative blacklist.\n",
            "cite_spans": [],
            "section": "Negative sentiment classifier ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 83,
                    "end": 84,
                    "mention": "3",
                    "ref_id": "TABREF12"
                }
            ]
        },
        {
            "text": "As shown in Fig. 3, the emoticons contained in the tweets are used to generate the training dataset TR-NN. Tweets were labeled as PN or PNN based on the emoticons they contained. More specifically, if a tweet contains at least one negative emoticon or at least one word from the profanity list that has 247 selected profanity words (Ji 2014a), it is labeled as PN. If a tweet contains at least one non-negative emoticon or at least one positive emoticon, it is labeled as a PNN. These two categories (PN and PNN) of labeled tweets were combined into the training dataset TR-NN for Negative vs. Non-Negative classification. Table 4 shows examples of tweets in TR-NN. The set of labeled PN tweets is marked as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{ne}^{\\prime \\prime }$$\\end{document}Tne\u2033, and the set of labeled PNN tweets is marked as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{nn}^{\\prime \\prime }$$\\end{document}Tnn\u2033, and (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{ne}^{\\prime \\prime }$$\\end{document}Tne\u2033 \u222a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{nn}^{\\prime \\prime }$$\\end{document}Tnn\u2033) \u2286 T\u2032. Similarly, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{ne}^{\\prime \\prime }$$\\end{document}Tne\u2033 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{nn}^{\\prime \\prime }$$\\end{document}Tnn\u2033 are used to train the Negative vs. Non-Negative classifier, and the classifier is used to make predictions on each tw\ni in T\u2033, which is the set of Personal tweets. The goal of Negative vs. Non-Negative classification is to obtain the Label for each tw\ni in the tweet database T\u2033, where the Label O(tw\ni) is either PN or PNN. (There are no News tweets at this stage).\n",
            "cite_spans": [
                {
                    "start": 336,
                    "end": 341,
                    "mention": "2014a",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Negative sentiment classifier ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 17,
                    "end": 18,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 629,
                    "end": 630,
                    "mention": "4",
                    "ref_id": "TABREF13"
                }
            ]
        },
        {
            "text": "After step 1 (Personal tweets classification) and step 2 (sentiment classification), for a unique type of tweets (e.g., tuberculosis), the Raw Tweet dataset T is transformed into a series of Tweet Label datasets TS\ni. Recall from the definition section that TS\ni is the Tweet Label dataset for time i, and TS\ni = {ts\n1, ts\n2, ts\n3,\u2026, ts\nn}, where O(ts\ni) is either PN, or PNN, or NT.",
            "cite_spans": [],
            "section": "Negative sentiment classifier ::: Tweet sentiment classification ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "We implemented a data collector using the Twitter API version 1.1 and Twitter4J library (Twitter4J 2014) to collect real-time tweets containing certain specified health-related keywords (e.g., listeria), along with associated user profile information for subsequent analysis. The overall data collection process can be described as \u201cETL\u201d (Extract-Transform-Load) approach, as it is widely used in Data Warehousing. The data was collected in JSON format from the Twitter Streaming API. (This is the \u201cExtract\u201d step). Then the raw JSON data was parsed into relational data, such as tweets, tweet_mentions, tweet_place, tweet_tags, tweet_urls, and users (Transform step). Finally, the relational data were stored into our MySQL relational database (Load step).",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 103,
                    "mention": "2014",
                    "ref_id": "BIBREF62"
                }
            ],
            "section": "Data collection and description ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "The current prototype system has collected a total of 15+ million tweets in 12 datasets. These datasets include six infectious diseases: Listeria, influenza, swine flu, measles, meningitis, and tuberculosis; four mental health problems: Major depression, generalized anxiety disorder, obsessive\u2013compulsive disorder, and bipolar disorder; one crisis: Air disaster; and one clinical science issue: Melanoma experimental drug. The core component uses the Twitter Streaming API for collecting epidemics-related real-time tweets. The tweets were collected from March 13 2014 to June 29 2014. The statistics of the collected datasets are shown in Table 5.\n",
            "cite_spans": [],
            "section": "Data collection and description ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 647,
                    "end": 648,
                    "mention": "5",
                    "ref_id": "TABREF14"
                }
            ]
        },
        {
            "text": "For each tweet type, the tweets were collected according to the keywords of the dataset. These keywords are shown in the \u201cAppendix\u201d Section. The language of tweets is automatically identified by Twitter4J library during the data collection phase. For example, if the value of the tweet attribute \u201clang\u201d is \u201cen\u201d, that means this tweet is an English tweet. If the value of tweet attribute is \u201cfr\u201d, it means that this tweet is a French tweet. Only English tweets are used in our experiments. As shown in Table 5, some datasets have a larger portion of non-English tweets, for example, influenza, swine flu, and tuberculosis compared with other datasets.",
            "cite_spans": [],
            "section": "Data collection and description ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 507,
                    "end": 508,
                    "mention": "5",
                    "ref_id": "TABREF14"
                }
            ]
        },
        {
            "text": "The pre-processing step filters out re-tweets and near-duplicate tweets. Two tweets are considered near-duplicates of each other, if they contain the same tokens (words) in the same order; however, they may contain different capitalization of words, different URLs and different special characters such as @, # etc. For example, the two tweets (1) \u201cSEVEN TONS OF #HUMMUS RECALLED OVER LISTERIA FEARS\u2026 http://t.co/IUU5SiJgjG\u201d and (2) \u201cseven tons of hummus recalled over @listeria fears\u2014http://t.co/dBgAk1heo4.\u201d are near-duplicates, thus only one tweet (randomly chosen) is kept in the database.",
            "cite_spans": [],
            "section": "Data collection and description ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "The clue-based annotation of the test dataset was done as follows. We first automatically extracted the Personal tweets and News tweets by the clue-based approach described in Sect. 4.2.1 and labeled them as Personal or News. Then we randomly divided the labeled dataset into three partitions and used two partitions for training the three different classifiers. Finally, we compared the different classifiers\u2019 accuracies on the third partition of labeled data. For example, for Dataset 3 in Table 5, in the classification step, 2899 Personal tweets and 508 News tweets were automatically extracted using the MPQA corpus (Riloff and Wiebe 2003). We randomly divided these tweets into training and test datasets, resulting in 1933 Personal and 339 News tweets as training dataset, and the remaining 966 Personal tweets and 169 News tweets as test dataset. A similar emoticon-based approach was used to automatically generate a training dataset and a test dataset for Negative vs. Non-Negative classification.",
            "cite_spans": [
                {
                    "start": 639,
                    "end": 643,
                    "mention": "2003",
                    "ref_id": "BIBREF51"
                }
            ],
            "section": "Clue-based annotation for test dataset ::: Evaluation ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 498,
                    "end": 499,
                    "mention": "5",
                    "ref_id": "TABREF14"
                }
            ]
        },
        {
            "text": "Because the clue-based annotation method is automatic, it is relatively easy to generate large samples. However, the drawback is that the training and testing datasets are extracted by the same clue-based annotation rule, thus the results might carry a certain bias. In order to more fairly evaluate the usability of our approach, we created a second test dataset by human annotation, which is described as follows.",
            "cite_spans": [],
            "section": "Human annotation for test dataset ::: Evaluation ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "We extracted three test data subsets by random sampling from all tweets from the three domains epidemic, clinical science, and mental health, collected in the year 2015. Each of these subsets contains 200 tweets. Note that the test tweets are independent from the training tweets that were collected in the year 2014. One professor and five graduate students annotated the tweets, with each tweet annotated by three people. The instructions for annotators are shown in the \u201cAppendix\u201d. Annotators were asked to assign a value of 1 if they considered a tweet to be Personal, and a value of 0 if they considered it to be News, according to the instructions they were given. If a tweet was labeled as a Personal tweet by an annotator, s/he was asked to further label it as Personal Negative or Personal Non-Negative tweet. We utilized Fleiss\u2019 Kappa (Fleiss 1971) to measure the inter-rater agreement between the three annotators of each tweet. Table 6 presents the agreement between human annotators. For each tweet, if at least two out of three annotators agreed on a Label (Personal Negative, Personal Non-Negative, or News), we labeled the tweet with this sentiment. Table 7 shows the numbers of tweets with different labels. For example, the fraction 25/200 for Negative tweets in \u201cepidemic\u201d means that out of the 200 human-annotated epidemic tweets, 25 tweets were labeled as Personal Negative tweets. The total number of tweets in each dataset does not add up to 200, because in some cases each of the three annotators classified a tweet differently. Tweets for which no majority existed were omitted from the analysis.\n\n",
            "cite_spans": [
                {
                    "start": 853,
                    "end": 857,
                    "mention": "1971",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Human annotation for test dataset ::: Evaluation ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 946,
                    "end": 947,
                    "mention": "6",
                    "ref_id": "TABREF15"
                },
                {
                    "start": 1172,
                    "end": 1173,
                    "mention": "7",
                    "ref_id": "TABREF16"
                }
            ]
        },
        {
            "text": "We compared the previously discussed classifiers: Two-Step Na\u00efve Bayes, Two-Step Multinomial Na\u00efve Bayes, and Two-Step Polynomial-Kernel Support Vector Machine. As previously discussed, the labeled dataset was randomly divided into three partitions and we used two partitions for training the three different classifiers. The detailed training and test dataset sizes are shown in Table 8. Note that the test datasets for each classifier in step 2 can be different. The reason is that different classifiers extract different numbers of Personal tweets in the first step, thus the test data in the second step, which is extracted from the previously extracted Personal tweets, can also be different for the three classifiers. The two-step sentiment classification accuracy on individual datasets (1\u201312) is shown in Table 9 and confusion matrices of the best classifiers in terms of accuracy are shown in Table 10; similarly, the classification accuracy and confusion matrices of the best classifiers for the three domains (epidemic, mental health, clinical science) are shown in Tables 11 and 12, respectively.\n\n\n\n\n",
            "cite_spans": [],
            "section": "Results with clue-based annotated test dataset ::: Classification results ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 386,
                    "end": 387,
                    "mention": "8",
                    "ref_id": "TABREF17"
                },
                {
                    "start": 819,
                    "end": 820,
                    "mention": "9",
                    "ref_id": "TABREF18"
                },
                {
                    "start": 908,
                    "end": 910,
                    "mention": "10",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1084,
                    "end": 1086,
                    "mention": "11",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 1091,
                    "end": 1093,
                    "mention": "12",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "On individual datasets, all three two-step methods show good performance. SVM is slightly better than the other two classifiers for most of the datasets. For the domain datasets, which combine individual datasets according to their domains, all three two-step methods also exhibit good performance. SVM again slightly outperforms the other two classifiers in all three domains.",
            "cite_spans": [],
            "section": "Results with clue-based annotated test dataset ::: Classification results ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "In order to evaluate the usability of two-step classification, Personal vs. News classification and Negative vs. Non-Negative classification were also evaluated with human annotated datasets.\nPersonal vs. News Classification We compared our Personal vs. News classification method with three baseline methods. (1) A na\u00efve algorithm that randomly picks a class. (2) The clue-based classification method described in Sect. 4.2.1. Recall that in the clue-based method, if a tweet contains more than a certain number of strongly subjective terms and a certain number of weakly subjective terms, it is regarded as a Personal tweet, otherwise as a News tweet. (3) A URL-based method. In URL-based method, if a tweet contains an URL, it is classified as a News tweet; otherwise the tweet is classified as a Personal tweet. The classification accuracies of different methods and confusion matrices of the best classifiers are presented in Tables 13 and 14, respectively. The results show that 2S-MNB and 2S-NB outperform all three baselines in most of the cases. Surprisingly, 2S-SVM does not perform as well as on the clue-based annotated test dataset. It is possible that SVM overfitted to the clue-based annotated dataset, since SVM is a relatively complex model and it infers too much from the training datasets. Overall, all methods exhibit a better performance on the epidemic dataset than on the other two datasets. In addition, as we compare the ML-based approaches (2S-MNB, 2S-NB, 2S-SVM), the ML-based approaches outperform the clue-based approaches in most of the cases. This means that although the ML-based approaches utilize the simple clue-based rules to automatically label the training data, they also learn some emotional patterns that cannot be distinguished by MPQA corpus. Some unigrams are learned by the ML-based methods and are shown to be useful for the classification, which will be discussed later.\n\n\nNegative vs. Non-Negative Classification The second step in the two-step classification algorithm is to separate Negative tweets from Non-Negative tweets. As discussed in Sect. 4.2, the training datasets are automatically labeled with emoticons and words from a profanity list, and then the classifier is trained by one of the three models, Multinomial Na\u00efve Bayes (MNB), Na\u00efve Bayes (NB), and Support Vector Machine (SVM). The accuracies of Negative vs. Non-Negative classification and confusion matrices of the best classifiers for human annotated datasets are shown in Tables 15 and 16, respectively. 2S-MNB outperforms the other two algorithms on the epidemic dataset, and 2S-NB outperforms the other two algorithms on the mental health and clinical science datasets. All three classifiers perform better than the random-select baseline, which generates an average of 50 % accuracy. We can see that although the classifier is trained with tweets containing profanity and tweets containing emoticons, the classifier is still able to perform with an average accuracy of 70+% on human annotated test datasets. Overall, 2S-NB and 2S-MNB both achieved good Negative vs. Non-Negative classification accuracy in terms of accuracy and simplicity, followed by 2S-SVM.\n\n\n",
            "cite_spans": [],
            "section": "Results with human annotated test dataset ::: Classification results ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 938,
                    "end": 940,
                    "mention": "13",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 945,
                    "end": 947,
                    "mention": "14",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 2499,
                    "end": 2501,
                    "mention": "15",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 2506,
                    "end": 2508,
                    "mention": "16",
                    "ref_id": "TABREF7"
                }
            ]
        },
        {
            "text": "We analyzed the output of sentiment classification. As discussed in Sect. 4.3.2, we manually annotated 600 tweets as Personal Negative, Personal Non-Negative, and News. We used 2S-MNB, which achieved the best accuracy in our experiments described in Sect. 4.3.3, to classify each of the 600 manually annotated tweets as Personal Negative, Personal Non-Negative, or News. Then we analyzed the tweets that were assigned different labels by 2S-MNB and by the human annotators.",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "For the Personal vs. News classification, we found two major types of errors.The tweet is in fact a Personal tweet, but is classified as a News tweet. By manually checking the content, we found that these tweets are often users\u2019 comments on News items (Pointing by URL) or users are citing the News. There are 27 out of all 140 errors belonging to this type. One possible solution to reduce this type of error is that we can calculate what percentage of the tweet text appears in the web page pointed to by the URL. If this percentage is low, it is probably a Personal tweet since most of the tweet text is the user\u2019s comment or discussion, etc. Otherwise, if the percentage is near 100 %, it is more likely a News tweet since the title of a news article is often pasted into the tweet text.The tweet is in fact a News item, but is classified as a Personal tweet. Those misclassified tweets are News items that have \u201cpersonal\u201d titles, and mostly have a question as title. There are 48 out of all 140 errors belonging to this type. One possible solution is to check the similarity between the tweet text and the title of the web page content pointed to by the URL. If both are highly similar to each other, the tweet is more likely a News item. Those two types of errors together cover 54 % (75/140) of the errors in Personal vs. News classification.\n",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "For Negative vs. Non-Negative classification, in 50 % (30/60) of all errors, the tweet is in fact Negative, but is classified as Non-Negative. One possible improvement is to incorporate \u201cNegative phrase identification\u201d to complement the current ML paradigm. The appearance of negative phrases such as \u201cI feel bad\u201d, \u201cpoor XX\u201d, and \u201cno more XX\u201d are possible indicators of Negative tweets. Examples of misclassified tweets are as follows:",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "\n\u201cThis is the scariest chart I\u2019ve made in awhile http://t.co/3MH5exZjSh\nhttp://t.co/oc9lyEO0XY\n\u201d (Personal tweet classified as News tweet).",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "\n\u201cMy OCD has been solved! Get our newsletter here: http://t.co/fAxsHjaIn4\nhttp://t.co/1Jhkbta2Px\n\u201d (Personal tweet classified as News tweet).",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "\n\u201cWhat is Generalized Anxiety Disorder? (GAD #1) http://t.co/y32GmkYhkh #Celebrity #Charity http://t.co/EYDupOLxY8\n\u201d (News tweet classified as Personal tweet).",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "\n\u201cBasal Cell Carcinoma is the most common form of skin cancer. Do you know what to look for? http://t.co/hmofWTApG9\n\u201d (News tweet classified as Personal tweet).",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "\u201c@Jonathan_harrod I know there is some research going on, but\u2026 Measles kills and us easily spread. @mercola\u201d (Negative tweet classified as Non-Negative tweet).",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "\n\u201cHaving a boyfriend with diagnosed OCD is not easy task, let me tell ya\u201d (Negative tweet classified as Non-Negative tweet).",
            "cite_spans": [],
            "section": "Error analysis of sentiment classification output ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "In order to illustrate which unigrams are most useful for the classifiers\u2019 predictions, ablation experiments were performed on Personal vs. News classification and Negative vs. Non-Negative classification on the three human annotated test datasets. The classifier 2S-MNB was used since it took less time to train and has one of the best average accuracies on human-annotated test dataset. 2S-MNB was trained with the automatically generated data from the Epidemic, Mental Health, and Clinical Science domains collected in the year 2014. Then the trained classifiers were used to classify the sentiments of human annotated datasets collected in the year 2015, where unigrams were removed from the test dataset one at a time, in order to study each removed unigram\u2019s effect on accuracy. The change of classification accuracy was recorded each time, and the unigram that leads to the largest decrease in accuracy (when removed) is the most useful one for predictions. Table 17 shows the ablation experiments for Personal vs. News classification. For example, the unigrams \u201ci\u201d, \u201cplz\u201d, \u201clol\u201d are not in MPQA corpus but are learned by the ML classifier 2S-MNB as the most important unigrams contributing to classification. Some words that are closely related to sentiment polarity are also shown in the list. For example, \u201cbitch\u201d, \u201clove\u201d, and \u201crisk\u201d are strong indicators for Personal vs. News classification. We did not find any useful unigram in Negative vs. Non-Negative classification by this ablation experiment.\n",
            "cite_spans": [],
            "section": "Contribution of unigrams ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": [
                {
                    "start": 971,
                    "end": 973,
                    "mention": "17",
                    "ref_id": "TABREF8"
                }
            ]
        },
        {
            "text": "Twitter may give a biased view, since people who are tweeting are not necessarily a very representative sample of the population. As pointed out by Bruns and Stieglitz (2014), there are two questions to be addressed in terms of generalizing collected Twitter data. (1) Does Twitter data represent Twitter? (2) Does Twitter represent society? To answer the first question, according to the documentation (Twitter 2014b), the Twitter Streaming API returns at most 1 % of all the tweets produced on Twitter at any given time. Once the number of tweets matching given parameters (keywords, geographical boundary, user ID) is beyond the 1 % of all the tweets, Twitter will begin to sample the data that it returns to the user. To mitigate this, we utilized highly specific keywords (e.g., h1n1, h5n1) for each tweet type (e.g., flu) to increase the coverage of collected data (Morstatter et al. 2013). These keywords are shown in \u201cAppendix\u201d Section. As for the second question, Mislove et al. (2011) has found that the Twitter users significantly over-represent the densely populated regions of the USA, are predominantly male, and represent a highly non-random sample of the race/ethnicity distribution. To reduce the bias of collected Twitter data, we defined the MOC in relative terms in Sect. 3. It depends on the fraction of all tweets obtained during the day that have been classified as \u201cPersonal Negative\u201d tweets. The MOC analysis will be discussed in more detail in Sect. 5.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 173,
                    "mention": "2014",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 412,
                    "end": 417,
                    "mention": "2014b",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 890,
                    "end": 894,
                    "mention": "2013",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 989,
                    "end": 993,
                    "mention": "2011",
                    "ref_id": "BIBREF37"
                }
            ],
            "section": "Bias of Twitter data ::: Experimental results of the classification approach ::: Two-step sentiment classification",
            "ref_spans": []
        },
        {
            "text": "Table 18 summarizes the number of peaks in each of the three time series: MOC (Negative sentiment), NN sentiment, and News. The best Jaccard Coefficient between MOC peaks and News peaks for a given dataset was computed as follows: Firstly, we directly computed the JC between MOC peaks and News peaks without any time delay or lead, and we recorded the result. Secondly, we added 1, 2, or 3 days of lead to the original MOC, computed the correlation between the revised MOC peaks and the original News peaks, respectively, and recorded these three results. Thirdly, we added 1, 2, or 3 days of delay to the original MOC, and we recorded three more results. Finally, we chose the highest measure from the above seven results as the best correlation between MOC and News. The best correlation between NN sentiment and News was computed similarly.\n",
            "cite_spans": [],
            "section": "Quantitative correlation of peaks ::: Concern sentiment trend analysis in public health",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 8,
                    "mention": "18",
                    "ref_id": "TABREF9"
                }
            ]
        },
        {
            "text": "The best Jaccard Coefficients between MOC peaks vs. News peaks and between NN peaks vs. News peaks for the three domain datasets are shown in Table 18. The +t time means that we delay all MOC peaks or NN peaks to t days later, and the -t time means that we move all MOC or NN peaks to t days earlier. Note that two peaks overlap with each other if and only if the two peaks happen on exactly the same day.",
            "cite_spans": [],
            "section": "Quantitative correlation of peaks ::: Concern sentiment trend analysis in public health",
            "ref_spans": [
                {
                    "start": 148,
                    "end": 150,
                    "mention": "18",
                    "ref_id": "TABREF9"
                }
            ]
        },
        {
            "text": "From Table 18, we can see that without any time delay/lead, the peaks of MOC and the peaks of NN correlated with the peaks of News in all datasets with a Jaccard Coefficient of 0.2\u20130.3. One exception is in the clinical science dataset, where the peaks of MOC do not correlate with the peaks of News. One possible reason is that there are only two peaks for MOC and three peaks for News.",
            "cite_spans": [],
            "section": "Quantitative correlation of peaks ::: Concern sentiment trend analysis in public health",
            "ref_spans": [
                {
                    "start": 11,
                    "end": 13,
                    "mention": "18",
                    "ref_id": "TABREF9"
                }
            ]
        },
        {
            "text": "We also qualitatively studied the surges in News and MOC, and how those surges co-occurred with the surges of TV and Internet broadcasts and newspaper articles about real-world events. The timeline trends of (1) listeria, (2) bipolar disorder, and (3) air disaster are shown in Fig. 6, where the MOC, NN, and NE are min\u2013max normalized, and a 10-day moving average is used to reduce the spikes in values. The top 5 most frequently mentioned topic terms (hash tags) for the tweets on each peak date are also shown in Fig. 6. For listeria in Fig. 6a, the News Peak 1 occurred because on that same day, several food items produced by Parkers Farm were recalled due to a listeria contamination (FoxNews 2014a). We observe that there was a surge in MOC as well. News Peak 2 was caused by the News broadcast that a company is voluntarily recalling more than 14,000 pounds of hummus and dips due to listeria concerns (FoxNews 2014b).\n",
            "cite_spans": [
                {
                    "start": 698,
                    "end": 703,
                    "mention": "2014a",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 918,
                    "end": 923,
                    "mention": "2014b",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Qualitative correlation of peaks ::: Concern sentiment trend analysis in public health",
            "ref_spans": [
                {
                    "start": 283,
                    "end": 284,
                    "mention": "6",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 520,
                    "end": 521,
                    "mention": "6",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 544,
                    "end": 545,
                    "mention": "6",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "For bipolar disorder in Fig. 6b, the News Peak 3 was recorded on 03/25/2014. On that day, researchers reported creating stem cells from the skin of people with bipolar disorder to directly measure cellular differences between people with bipolar disorder and people without (Discover 2014). It is surprising to find that the MOC peaks correlated well with this News peak. For air disasters in Fig. 6c, the News Peak 4 was recorded on 07/17/2014. On that day, Malaysian Airlines flight MH17 crashed in the Ukraine (Independent 2014). There are surges of MOC on the same day as well. This qualitative correlation reveals that in most of the cases, the surges of News generated by our method indeed correlated well with the surges of TV, Internet, and newspaper reports of real-world events. Surprisingly, the surges of MOC also correlate with the surges of News, which shows that the general public tends to express negative emotions according to News peaks during these circumstances.",
            "cite_spans": [
                {
                    "start": 284,
                    "end": 288,
                    "mention": "2014",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 526,
                    "end": 530,
                    "mention": "2014",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Qualitative correlation of peaks ::: Concern sentiment trend analysis in public health",
            "ref_spans": [
                {
                    "start": 29,
                    "end": 30,
                    "mention": "6",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 398,
                    "end": 399,
                    "mention": "6",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "To monitor the timeline and geographic distribution of public concern, we expanded the Epidemics Outbreak and Spread Detection System visual analytics tools with (1) a concern timeline chart to track the public concern trends on the timeline; (2) a tag cloud for discovering the popular topics within a certain time period; and (3) a concern map that shows the geographic distribution of concern. The public health specialists can utilize the concern timeline chart, as shown in Fig. 7a, to monitor (e.g., identify concern peaks) and compare public concern timeline trends for various diseases. Then the specialists might be interested in what topics people are discussing on social media during the \u201cunusual situations\u201d discovered with the help of the concern timeline chart. To answer this question, they can use the tag cloud, as shown in Fig. 7b to browse the top topics within a certain time period for different diseases. In addition, the concern map, as shown in Fig. 7c, can help public health specialists and government officials to identify parts of the country with different MOCs toward a particular disease or crisis; thus appropriate preventive actions can be taken in high-concern regions.\n",
            "cite_spans": [],
            "section": "Prototype system ::: Concern sentiment trend analysis in public health",
            "ref_spans": [
                {
                    "start": 484,
                    "end": 485,
                    "mention": "7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 847,
                    "end": 848,
                    "mention": "7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 975,
                    "end": 976,
                    "mention": "7",
                    "ref_id": "FIGREF6"
                }
            ]
        },
        {
            "text": "We discussed the difficulties of measuring and monitoring public health concerns by traditional public health surveillance systems, due to high expenses, limited coverage, and significant time delays. To address these problems, we used the MOC, derived from the social network site Twitter, to monitor the public\u2019s concern about common health and disaster issues.",
            "cite_spans": [],
            "section": "Conclusions and future work",
            "ref_spans": []
        },
        {
            "text": "To derive the MOC and understand its relationship with the News Count timeline on Twitter, we developed a two-step sentiment classification approach: In the first step, we classify health tweets into Personal tweets versus News tweets. This step separates the tweets that carry the personal opinions of tweeters from those that are third-party factual reports such as News articles. It uses a subjective clue-based lexicon and News stopwords to automatically extract training datasets: labeled Personal tweets and labeled News tweets. These auto-generated training datasets are then used to train Machine Learning models to classify whether a tweet is Personal or News. After filtering out News tweets, in the second step, we utilized an emotion-oriented clue-based method to automatically extract training datasets and generate another classifier to predict whether a Personal tweet is Negative or Non-Negative.",
            "cite_spans": [],
            "section": "Conclusions and future work",
            "ref_spans": []
        },
        {
            "text": "We used the MOC to quantify the health concerns of the tweeting public, and defined a method to both qualitatively categorize and quantitatively measure the correlation between MOC timeline and News Count timeline.",
            "cite_spans": [],
            "section": "Conclusions and future work",
            "ref_spans": []
        },
        {
            "text": "In order to more fairly evaluate the two-step classification method, we created a test dataset by human annotation for three domains: epidemic, clinical science, and mental health. The Fleiss\u2019s Kappa values between annotators were 0.40, 0.54, and 0.33 for epidemic, clinical science, and mental health, respectively. According to the criteria presented by Landis and Koch (1977), the annotators reached a moderate agreement on the epidemic and clinical science datasets, and a fair agreement on the mental health dataset. This result illustrates the complexity of the sentiment classification task, since even humans exhibit relatively low agreement on the labels of tweets.",
            "cite_spans": [
                {
                    "start": 373,
                    "end": 377,
                    "mention": "1977",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Conclusions and future work",
            "ref_spans": []
        },
        {
            "text": "Experimental results show that (1) in sentiment classification, by combining a clue-based method with a Machine Learning method, our two-step algorithm is able to classify a tweet as Personal Negative, Personal Non-Negative, or News tweet with good accuracy. This overcomes the drawbacks of the clue-based method and the Machine Learning methods when used separately. (2) Quantitatively, the peaks of MOC and the peaks of NN (Non-Negative) correlated with the peaks of News with Jaccard Coefficients of 0.2\u20130.3. Note that this range of Jaccard Coefficient is still too low to make useful predictions. (3) Qualitatively, as we expected, the peaks of News correlated well with the surges of TV, Internet, and newspaper reports about real-world events. Surprisingly, the surges of MOC also correlated with the surges of News in some cases. This suggests that the general public tends to express negative emotions according to News peaks during these circumstances. (4) As shown in the experiments, our method to derive the MOC is generic and can be applied to topics in other domains, such as mental health monitoring, and clinical science.",
            "cite_spans": [],
            "section": "Conclusions and future work",
            "ref_spans": []
        },
        {
            "text": "Future work includes the following topics.Negation and irony in tweets. In logic, negation is an operation that transforms a proposition p into another proposition \u201cnot p\u201d. Wiegand et al. (2010) presented a survey on the role of negation in sentiment analysis and investigated several negation models. In our sentiment classification approach, we utilized profanity words to automatically annotate Negative tweets with the assumption that profanity words indicate a negative emotion. However, this assumption may not hold in a negative context. Kiritchenko et al. (2014) showed that in a negative context, both positive and negative terms tend to convey a negative sentiment (e.g., \u201cI know what it feels like to make a thousand dollars in one day, thanks to my tax returns. I\u2019m still not satisfied\u201d). We also observe that profanity words sometimes reverse their polarity when used to modify a positive term (e.g., \u201cdamn\u201d is a positive term in \u201cMarriage feels pretty damn great!\u201d). To further improve our two-step sentiment classification, we plan to utilize the negated and affirmative context lexicons (Kiritchenko et al. 2014) to give different polarities and sentiment scores to a word depending on whether it appears in a negated context or in an affirmative context.Irony is another difficult problem, since an ironic statement is used to express the opposite of what is being said (Quintilien and Butler 1953). Utsumi (1996) proposed one of the first computational theories to formalize an ironic environment but the model is too abstract to represent a non-hearer\u2013listener interaction (Reyes et al. 2013). In addition, irony detection requires knowledge of cultural and social stereotypes and tends to be subjective and personal. For the sentiment classification task, the appearance of irony often indicates the opposite of the literal meaning of a statement. Recently, a few techniques were proposed to detect irony in tweets, by investigating irony features. Reyes et al. (2013) proposed four types of conceptual features, which include signatures, unexpectedness, style, and emotional scenarios. More recently, Barbieri and Saggion (2014) designed another set of features that take into account the sentiments. We plan to investigate these features to better reveal the real sentiments underneath the literal ones to further improve the accuracy of sentiment classification.Measure of Concern is currently based on the number of Personal Negative tweets and total number of tweets on the same day. The MOC was used to define the fraction of tweets that are Personal Negative tweets. We plan to fine grain this definition to quantify the number of tweets expressing real concern. Previous research has been done to classify fine-grained emotions. Brynielsson et al. (2014) manually labeled angry, fear, and positive tweets and trained classifiers with the labeled data to predict which emotion category each tweet belongs to. In the future, we will extend the current work using a more specific sentiment lexicon, such as LIWC (Pennebaker and Francis 1999), to automatically label the tweets that express concerns. In this way, the classifier will be better able to directly identify the tweets with concerns.To improve the performance of classification, we plan to extend the current feature set to include more features specific to micro-blogs, such as slang terms and intensifiers to capture the unique language in micro-blogs. Slang replacement (Piskorski et al. 2013) is able to reveal the semantics and sentiment by translating the slang terms into their original meaning. For example, \u201cugh\u201d is translated into \u201cdisgusted\u201d. As reported by Kouloumpis et al. (2011), the presence of intensifiers, such as all caps and character repetitions in micro-blogs, is also a useful feature for sentiment classification. In Personal vs. News classification, we chose to work in the Machine Learning-based paradigm. However, we note that some lightweight knowledge-based approaches could possibly produce competitive results. For example, if the tweet is of the form \u201cTEXT URL\u201d and the TEXT appears on the web page that the URL points to, the tweet is a News Tweet. The intuition behind this approach is that the title of a news article is often pasted into the tweet body followed by the URL to that news article. We would like to perform a comparison of these knowledge-based approaches with our ML approach in the future.Although it is difficult to find the ground truth for sentiment trends, we would like to conduct a systematic experiment on comparing the sentiments derived by our methods with the epidemic cases reported by other available tools, and with authoritative data sources, such as Health Map and CDC reports. The sentiment trends for topics will also be studied by combining the sentiment analysis algorithms with topic modeling algorithms.\n",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 193,
                    "mention": "2010",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 565,
                    "end": 569,
                    "mention": "2014",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1123,
                    "end": 1127,
                    "mention": "2014",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1410,
                    "end": 1414,
                    "mention": "1953",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "mention": "1996",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 1606,
                    "end": 1610,
                    "mention": "2013",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 1983,
                    "end": 1987,
                    "mention": "2013",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 2144,
                    "end": 2148,
                    "mention": "2014",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 2777,
                    "end": 2781,
                    "mention": "2014",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 3061,
                    "end": 3065,
                    "mention": "1999",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 3477,
                    "end": 3481,
                    "mention": "2013",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 3674,
                    "end": 3678,
                    "mention": "2011",
                    "ref_id": "BIBREF30"
                }
            ],
            "section": "Conclusions and future work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table\u00a01: Results of Personal tweets classification with different thresholds (Precision/Recall)\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table\u00a010: Confusion matrices of the best classifier on each dataset (Step 1 positive class is Personal and Negative class is News, Step 2 positive class is Personal Negative and Negative class is Personal Non-Negative)\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table\u00a011: Results of S1A/S2A (S1A step one accuracy and S2A step two accuracy) on individual domain\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table\u00a012: Confusion matrices of the best classifier on individual domain (Step 1 positive class is Personal and Negative class is News, Step 2 positive class is Personal Negative and Negative class is Personal Non-Negative)\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table\u00a013: Accuracy of Personal vs. News classification on human annotated datasets\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table\u00a014: Confusion matrices of the best Personal vs. News classifier on human annotated datasets (positive class is Personal and Negative class is News)\n",
            "type": "table"
        },
        "TABREF6": {
            "text": "Table\u00a015: Negative vs. Non-Negative classification results on human annotated datasets\n",
            "type": "table"
        },
        "TABREF7": {
            "text": "Table\u00a016: Confusion matrices of the best Personal Negative vs. Personal Non-Negative classifier on human annotated datasets (Positive class is Personal Negative and Negative class is Personal Non-Negative)\n",
            "type": "table"
        },
        "TABREF8": {
            "text": "Table\u00a017: Most important unigrams in Personal vs. News classification\n",
            "type": "table"
        },
        "TABREF9": {
            "text": "Table\u00a018: The correlation results of Measure of Concern vs. News and Non-Negative vs. News\n",
            "type": "table"
        },
        "TABREF10": {
            "text": "Table\u00a019: Keywords for collecting tweets in each dataset\n",
            "type": "table"
        },
        "TABREF11": {
            "text": "Table\u00a02: Partial list of the emoticons used\n",
            "type": "table"
        },
        "TABREF12": {
            "text": "Table\u00a03: Whitelist and blacklist of stop words for building TR-NN\n",
            "type": "table"
        },
        "TABREF13": {
            "text": "Table\u00a04: Examples of Personal Negative and Personal Non-Negative tweets in training dataset TR-NN\n",
            "type": "table"
        },
        "TABREF14": {
            "text": "Table\u00a05: The statistics of the collected dataset\n",
            "type": "table"
        },
        "TABREF15": {
            "text": "Table\u00a06: Agreement between human annotators\n",
            "type": "table"
        },
        "TABREF16": {
            "text": "Table\u00a07: Statistics regarding human annotated dataset\n",
            "type": "table"
        },
        "TABREF17": {
            "text": "Table\u00a08: Size of experimental training and test datasets for two-step classification (PN is Personal Negative and PNN is Personal Non-Negative)\n",
            "type": "table"
        },
        "TABREF18": {
            "text": "Table\u00a09: Results of S1A/S2A (S1A = step one accuracy and S2A = step two accuracy) on individual dataset (rounded to 2 decimal places)\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig.\u00a01: Overview of the two-step sentiment classification and quantification method",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig.\u00a02: Personal vs. News (Non-Personal) Classification",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig.\u00a03: Negative vs. Non-Negative Classification",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig.\u00a04: Correlation between sentiment trends and News trends",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig.\u00a05: An example of calculating the Jaccard Coefficient between peaks of MOC and peaks of News",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Fig.\u00a06: Measure of Concern timeline trend (green) vs. News Timeline Trend (purple): in a listeria, b bipolar disorder and c air disaster with most frequent topic terms in different peaks",
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Fig.\u00a07: EOSDS visual analytics tools for public concern monitoring a sentiment timeline chart, b topics cloud, c concern map",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Emotion classification of social media posts for estimating people\u2019s reactions to communicated alert messages during crises",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Brynielsson",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Johansson",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jonsson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Westling",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Security Inf",
            "volume": "3",
            "issn": "1",
            "pages": "1-11",
            "other_ids": {
                "DOI": [
                    "10.1186/2190-8532-3-1"
                ]
            }
        },
        "BIBREF2": {
            "title": "LIBSVM: a library for support vector machines",
            "authors": [
                {
                    "first": "C-C",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "C-J",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "ACM Trans Intell Syst Technol",
            "volume": "2",
            "issn": "3",
            "pages": "1-27",
            "other_ids": {
                "DOI": [
                    "10.1145/1961189.1961199"
                ]
            }
        },
        "BIBREF3": {
            "title": "Pandemics in the age of Twitter: content analysis of Tweets during the 2009 H1N1 outbreak",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chew",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Eysenbach",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "PLoS One",
            "volume": "5",
            "issn": "11",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0014118"
                ]
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Support-Vector Networks",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cortes",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vapnik",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Mach Learn",
            "volume": "20",
            "issn": "3",
            "pages": "273-297",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Measuring nominal scale agreement among many raters",
            "authors": [
                {
                    "first": "JL",
                    "middle": [],
                    "last": "Fleiss",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "Psychol Bull",
            "volume": "76",
            "issn": "5",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1037/h0031619"
                ]
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Detecting influenza epidemics using search engine query data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ginsberg",
                    "suffix": ""
                },
                {
                    "first": "MH",
                    "middle": [],
                    "last": "Mohebbi",
                    "suffix": ""
                },
                {
                    "first": "RS",
                    "middle": [],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Brammer",
                    "suffix": ""
                },
                {
                    "first": "MS",
                    "middle": [],
                    "last": "Smolinski",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Brilliant",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Nature",
            "volume": "457",
            "issn": "7232",
            "pages": "1012-1014",
            "other_ids": {
                "DOI": [
                    "10.1038/nature07634"
                ]
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "The WEKA data mining software: an update",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hall",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Frank",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Holmes",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pfahringer",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Reutemann",
                    "suffix": ""
                },
                {
                    "first": "IH",
                    "middle": [],
                    "last": "Witten",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "SIGKDD Explor Newsl",
            "volume": "11",
            "issn": "1",
            "pages": "10-18",
            "other_ids": {
                "DOI": [
                    "10.1145/1656274.1656278"
                ]
            }
        },
        "BIBREF20": {
            "title": "Do people prefer to pass along good or bad news? Valence and relevance of news as predictors of transmission propensity",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Heath",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Organ Behav Hum Decis Process",
            "volume": "68",
            "issn": "2",
            "pages": "79-94",
            "other_ids": {
                "DOI": [
                    "10.1006/obhd.1996.0091"
                ]
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "Twitter sentiment analysis: the good the bad and the omg!",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kouloumpis",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Wilson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Moore",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Icwsm",
            "volume": "11",
            "issn": "",
            "pages": "538-541",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "The link prediction problem for social networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liben Nowell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J Am Soc Inform Sci Technol",
            "volume": "58",
            "issn": "7",
            "pages": "1019-1031",
            "other_ids": {
                "DOI": [
                    "10.1002/asi.20591"
                ]
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "Opinion mining and sentiment analysis",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Found Trends Inf Retr",
            "volume": "2",
            "issn": "1\u20132",
            "pages": "1-135",
            "other_ids": {
                "DOI": [
                    "10.1561/1500000011"
                ]
            }
        },
        "BIBREF43": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF49": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF50": {
            "title": "A multidimensional approach for detecting irony in twitter",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Reyes",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Rosso",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Veale",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Lang Resour Eval",
            "volume": "47",
            "issn": "1",
            "pages": "239-268",
            "other_ids": {
                "DOI": [
                    "10.1007/s10579-012-9196-x"
                ]
            }
        },
        "BIBREF51": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF52": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF53": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF54": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF55": {
            "title": "Arousal increases social transmission of information",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Berger",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Psychol Sci",
            "volume": "22",
            "issn": "7",
            "pages": "891-893",
            "other_ids": {
                "DOI": [
                    "10.1177/0956797611413294"
                ]
            }
        },
        "BIBREF56": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF57": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF58": {
            "title": "The use of Twitter to track levels of disease activity and public concern in the U.S. during the influenza A H1N1 pandemic",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Signorini",
                    "suffix": ""
                },
                {
                    "first": "AM",
                    "middle": [],
                    "last": "Segre",
                    "suffix": ""
                },
                {
                    "first": "PM",
                    "middle": [],
                    "last": "Polgreen",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "PLoS One",
            "volume": "6",
            "issn": "5",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0019467"
                ]
            }
        },
        "BIBREF59": {
            "title": "Emotions and information diffusion in social media\u2014sentiment of microblogs and sharing behavior",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Stieglitz",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Dang-Xuan",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J Manag Inf Syst",
            "volume": "29",
            "issn": "4",
            "pages": "217-248",
            "other_ids": {
                "DOI": [
                    "10.2753/MIS0742-1222290408"
                ]
            }
        },
        "BIBREF60": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF61": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF62": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF63": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF64": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF65": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF66": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF67": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF68": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF69": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF70": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF71": {
            "title": "Changes in emotion of the Chinese public in regard to the SARS period",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Miao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Social Behav Personal",
            "volume": "36",
            "issn": "4",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.2224/sbp.2008.36.4.447"
                ]
            }
        },
        "BIBREF72": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF73": {
            "title": "Surveillance Sans Frontieres: internet-based emerging infectious disease intelligence and the HealthMap project",
            "authors": [
                {
                    "first": "JS",
                    "middle": [],
                    "last": "Brownstein",
                    "suffix": ""
                },
                {
                    "first": "CC",
                    "middle": [],
                    "last": "Freifeld",
                    "suffix": ""
                },
                {
                    "first": "BY",
                    "middle": [],
                    "last": "Reis",
                    "suffix": ""
                },
                {
                    "first": "KD",
                    "middle": [],
                    "last": "Mandl",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "PLoS Med",
            "volume": "5",
            "issn": "7",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pmed.0050151"
                ]
            }
        },
        "BIBREF74": {
            "title": "Twitter data: What do they represent?",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bruns",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Stieglitz",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "It Inf Technol",
            "volume": "56",
            "issn": "5",
            "pages": "240-245",
            "other_ids": {
                "DOI": []
            }
        }
    }
}