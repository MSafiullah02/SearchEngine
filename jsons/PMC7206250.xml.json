{
    "paper_id": "PMC7206250",
    "metadata": {
        "title": "Multi-information Source HIN for Medical Concept Embedding",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Yuwei",
                "middle": [],
                "last": "Cao",
                "suffix": "",
                "email": "ycao43@uic.edu",
                "affiliation": {}
            },
            {
                "first": "Hao",
                "middle": [],
                "last": "Peng",
                "suffix": "",
                "email": "penghao@act.buaa.edu.cn",
                "affiliation": {}
            },
            {
                "first": "Philip",
                "middle": [
                    "S."
                ],
                "last": "Yu",
                "suffix": "",
                "email": "psyu@uic.edu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Analogous to how word embedding [17, 18] empowers natural language processing (NLP) [13], medical concepts embedding is indispensable for machine learning to show its enormous potential in healthcare [1]. Embeddings of medical concepts enable the studies of correlations between concepts, such as co-occurrence of diagnosis and symptoms, and they can also be used as features to predict future events of interest [3]. One such example is computer-aided diagnosis systems, which can liberate clinicians from analyzing complex, enormous information [10].",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 35,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 37,
                    "end": 39,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 85,
                    "end": 87,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 201,
                    "end": 202,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 414,
                    "end": 415,
                    "mention": "3",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 548,
                    "end": 550,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The abundant Electronic Health Records (EHR) datasets nowadays provide a great information source for medical embedding learning. EHR datasets are often organized by admissions, and contain detailed documentation of patients\u2019 diagnostic and treatment information, including demographic characteristics, symptoms, laboratory test results, diagnoses, and medications. EHR datasets also present unique challenges. On the one hand, missing values are commonly seen [2]. On the other hand, EHR datasets are high-dimensional and have complex structure, which often involves tens of thousands of medical concepts.",
            "cite_spans": [
                {
                    "start": 462,
                    "end": 463,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Prior studies apply different methods for medical feature extraction. Handcrafted feature engineering approaches [23, 25] are labor-intensive and also require extensive clinical expertise. The performances of Knowledge Graph Embedding (KGE) based methods [27] are greatly limited by the unbalance and sparsity of EHR [14]. Homogeneous skip-gram based models [3\u20135] that consider co-occurrence of medical concepts, on the other hand, treat all types of medical concepts equally, and miss the structural information of EHR [10]. Heterogeneous Information Networks (HIN) [8, 24] based models such as HeteroMed [10], though introducing heterogeneity, contain insufficient correlations since they extract edges only from EHR. EHR datasets have limitations. For example, Congestive heart failure and Systolic heart failure are sub-types of Heart failure, which in turn belongs to the Heart Disease hierarchy, but EHR datasets do not contain these relations. Existing methods rely on EHR as their only information source, and thus are unaware of the correlations between medical concepts.",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 116,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 118,
                    "end": 120,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 256,
                    "end": 258,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 318,
                    "end": 320,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 359,
                    "end": 360,
                    "mention": "3",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 361,
                    "end": 362,
                    "mention": "5",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 521,
                    "end": 523,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 568,
                    "end": 569,
                    "mention": "8",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 571,
                    "end": 573,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 607,
                    "end": 609,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To supplement such shortage, we propose a novel multi-information source HIN to model EHR while incorporating external medical knowledge including The International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM) [19] and Medical Subject Headings (MeSH) [20]. We first preprocess data and extract medical concepts from EHR. These concepts, along with patients, are nodes in our HIN. We then add edges between patients and medical concepts based on their co-occurrences in EHR. Besides, we explore ICD-9-CM and MeSH for more edges. Both ICD-9-CM and MeSH contain valuable knowledge, understandings, and insights from medical experts, and reveal correlations between medical concepts. To be more specific, as Congestive heart failure and Systolic heart failure in the above example are closely correlated according to ICD-9-CM, we therefore append an edge between them to capture such correlation. Given the enriched HIN schema, we adopt the commonly-used HIN embedding technique [6] to learn medical concept embeddings.",
            "cite_spans": [
                {
                    "start": 241,
                    "end": 243,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 282,
                    "end": 284,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1006,
                    "end": 1007,
                    "mention": "6",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Our work marks the following contributions:We propose a novel multi-information source HIN that incorporates EHR with abundant external medical knowledge including ICD-9-CM and MeSH. Our design simultaneously preserves structural information lies in EHR and correlations between medical concepts reflected by external medical databases. This work enables the learning of more semantically reflective embeddings, and eventually allows more efficient and effective medical concept analysis.We quantitatively show that the learned embeddings offer significant performance gains over mainstream unsupervised baselines in various medical data mining tasks, including diagnosis, procedure, symptom classification, and clustering.We qualitatively demonstrate by visualization the internal correlations between medical concepts of the same type, as well as across different types.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Our code is publicly available at https://github.com/RingBDStack/MISMV/.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Medical Representation Learning. Pioneer works in medical representation learning that utilize handcrafted features [23, 25] can be traced back to the 2000s. Missing values are commonly seen in EHR, and such incompleteness is one of the leading issues [2]. Besides, feature design is laborious and requires medication expertise [10]. To deal with these, unsupervised approaches [3\u20135] that enlightened by word2vec [17, 18] concatenate medical concepts in admission records to form sequences, and then use the result as corpus. These studies improve and automate medical representation learning. However, they mainly explore co-occurrences and lack consideration of the complex structure of EHR [10]. By contrast, HIN based models [10] preserve the structure of EHR by modeling EHR into a HIN, and then apply heterogeneous skip-gram. Nevertheless, they are unaware of correlations between medical concepts that are absent from EHR.",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 119,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 121,
                    "end": 123,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 253,
                    "end": 254,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 329,
                    "end": 331,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 379,
                    "end": 380,
                    "mention": "3",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 381,
                    "end": 382,
                    "mention": "5",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 414,
                    "end": 416,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 418,
                    "end": 420,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 694,
                    "end": 696,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 730,
                    "end": 732,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Network Embedding. EHR datasets contain structured records that refer to a large set of medical concepts, and can be intuitively represented as networks. Network embedding methods [7, 22, 26] can thus be applied. These methods capture the semantics in the raw networks, and offer natural handling of missing values [10]. Compared to homogeneous ones [22, 26], HIN embedding techniques [6, 9] can jointly model structural and semantic information. This strength comes from the preservation of diverse node types and edge types. Random walks are guided by meaningful metapaths that differentiate nodes\u2019 neighbors by types so that a heterogeneous skip-gram model [6] can then be employed. HIN is therefore adopted by many recent studies [10], including our own. Enriched nodes and edges are essential. Efforts have been devoted to enriching the nodes. [10] properly explored raw text, numerical and categorical data in EHR and fully utilizes information in terms of node extraction. Its edges, however, come only from the EHR. In non-medical domains, it has been shown in [11, 21] that external information sources can reveal correlations between nodes and are worth integrated as edges to enrich the network. In this paper, as we incorporate external medical knowledge including ICD-9-CM and MeSH into network modeling, we extract edges from them for a more informative and semantically rich network.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 182,
                    "mention": "7",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 184,
                    "end": 186,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 188,
                    "end": 190,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 316,
                    "end": 318,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 351,
                    "end": 353,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 355,
                    "end": 357,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 386,
                    "end": 387,
                    "mention": "6",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 389,
                    "end": 390,
                    "mention": "9",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 661,
                    "end": 662,
                    "mention": "6",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 735,
                    "end": 737,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 850,
                    "end": 852,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1070,
                    "end": 1072,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1074,
                    "end": 1076,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "As illustrated in Fig. 1(a), we combine EHR and external knowledge databases, and model them into a HIN. A HIN is defined as a graph \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G = (V, E)$$\\end{document} where V and E stand for collections of nodes (patients and medical concepts) and edges (relations) that are of various types [8]. We also construct a HIN schema, which can be viewed as a meta template of G.",
            "cite_spans": [
                {
                    "start": 555,
                    "end": 556,
                    "mention": "8",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Construction of Multi-information Source HIN ::: The Proposed Framework",
            "ref_spans": [
                {
                    "start": 23,
                    "end": 24,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We model EHR into an initial HIN. EHR datasets are patient-centered, i.e. each record is related to a patient, and refers to a medical concept accompanied by a value [10]. For example, a record in EHR may be abstracted as Hematocrit 42.4% or yeast grew when tested, where the former refers to the medical concept Hematocrit with a value of 42.4%, while the latter refers to medical concept yeast culture with a value of culture-positive. We extract medical concepts from EHR. First, for concepts of categorical values, we either directly grab their values or reduce them into smaller categories based on their similar or identical semantics. Genders are mapped into two nodes. Ages are split into groups using threshold 15, 30 and 64 as suggested in [15]. Ethnicities are reduced into five categories, with rarely seen ones combined as other. Prescriptions are reduced based on constituents, for example, Aspirin and Aspirin (Buffered) are mapped into one. Procedures and diagnoses are mapped into corresponding ICD-9-CM codes. Microbiology tests with culture-positive results are mapped into the names of organisms, for example, yeast grew when tested in the above example is mapped into yeast. Secondly, fields of continuous values, too, are reduced into categories. Laboratory tests are reduced to their codes combined with flags that indicate whether or not the results are within normal ranges, as Hematocrit 42.4% in the above example is mapped into Hematocrit normal. Finally, for raw-text fields, we extract nodes by phrase mining: we conduct phrase matching between notes and vocabularies in MeSH descriptor, and use matched terms as symptoms. We use patients and extracted medical concepts as nodes, and \u201crefer to\u201d relations between them as edges to build the initial HIN, as shown in the upper-left part of Fig. 1(a). We also abstract the types of nodes and edges into an initial HIN schema, as shown in the bottom-left part of Fig. 1(a).",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 169,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 751,
                    "end": 753,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Construction of Multi-information Source HIN ::: The Proposed Framework",
            "ref_spans": [
                {
                    "start": 1823,
                    "end": 1824,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1944,
                    "end": 1945,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We then enrich the initial HIN by exploring selected external knowledge databases for correlations between medical concepts, and integrate these correlations as new edges. Procedures and diagnoses in our HIN are encoded by ICD-9-CM, and symptoms by MeSH. Both descriptors are ordered and of tree structures, which enable us to detect correlations revealed by codes. For each diagnosis, its ICD-9-CM code is comprised of three characters to the left of a decimal point, and one or two digits to its right, where the first three characters indicate which subclass this diagnosis belongs to. For example, 410.0 Acute myocardial infarction of anterolateral wall and 410.2 Acute myocardial infarction of inferolateral wall are both in category 410 Acute myocardial infarction, which is a subclass of 390\u2013459 Disease of the circulatory system. As identical in the first three characters implies similarity, therefore, an edge can be added between them. In this way, we examine all pairs of diagnosis nodes in our HIN and append new edges. Procedure and symptom nodes are examined likewise, except correlations between procedure nodes are based on the identity of the first two digits of ICD-9-CM codes, while symptom nodes are decided by all digits up to the last decimal point in their MeSH codes. The appended edges are highlighted in red in the enriched HIN shown in Fig. 1(a). We also append \u201csimilar to\u201d as a new edge type onto the HIN schema. Figure 2 shows the enriched HIN schema, where the self-loops of symptom, diagnosis, and procedure are made possible by external knowledge extension.\n",
            "cite_spans": [],
            "section": "Construction of Multi-information Source HIN ::: The Proposed Framework",
            "ref_spans": [
                {
                    "start": 1369,
                    "end": 1370,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1450,
                    "end": 1451,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "We derive semantically meaningful metapaths from the HIN schema. A metapath is a path on network schema that defines relations between node types [8], and it carries semantics. For example, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$patient \\rightarrow diagnosis\\leftarrow patient$$\\end{document} implies that two patients are similar because they have the same disease diagnosis. Table 1 lists all metapaths along with their semantics, where the metapaths of length 4 are enabled by exploring external knowledge, and they integrate correlations between nodes of the same type. We use these metapaths to guide heterogeneous random-walks [6], as discussed in detail in Sect. 3.2.\n",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 148,
                    "mention": "8",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 864,
                    "end": 865,
                    "mention": "6",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Construction of Multi-information Source HIN ::: The Proposed Framework",
            "ref_spans": [
                {
                    "start": 613,
                    "end": 614,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Figure 1(b) shows how we learn embeddings from the enriched HIN. We adopt a heterogeneous network embedding technique as proposed in [6]. Note that [6] uses a single metapath, in contrast, we incorporate rich semantics using multiple metapaths as listed in Table 1. In practice, since all our metapaths begin and end with patient, we concatenate them together repeatedly for the random-walks to keep going. Metapaths can have equal or different weights in the concatenation. Figure 1(a) shows an example where we assign both P-D-D-P and P-S-P a weight of 1, and get P-D-D-P-S-P.",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 135,
                    "mention": "6",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 149,
                    "end": 150,
                    "mention": "6",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "HIN Embedding ::: The Proposed Framework",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 482,
                    "end": 483,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 263,
                    "end": 264,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Similar to homogeneous techniques [22, 26] inspired by word2vec [17], our embedding approach is based on local structure prediction, and aim to maximize the probability of seeing the local neighborhood of each node in the network. In addition, we further differentiate the types of neighbors by metapath-guided, heterogeneous random walks. Specifically, after a node is sampled, instead of randomly choosing the next node from its neighbors, we only choose from those of the type designated by the metapath. Figure 1(b) shows a concrete example of random walks guided by metapath P-D-D-P-S-P. Suppose we start from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_1$$\\end{document}, then we can only walk to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}, as the metapath requires the type of the next node to be D. After then, we move on by randomly choosing one from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_2$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_3$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_4$$\\end{document}, as they are neighbors of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}, and also are of type D as required by the metapath. We continue in this manner, and eventually get a metapath instance such as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_1-d_1-d_4-p_3-s_2-p_2$$\\end{document}, which incorporates the semantics of the metapath. Given an embedding function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f:C \\mapsto \\mathbb {R}^m,$$\\end{document} where C denotes the set all medical concepts, the objective of the heterogeneous skip-gram can be formalized as:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\underset{f}{argmax}\\sum _{c \\in C}\\sum _{t \\in T}\\sum _{n_t \\in N_t(c)}\\log P(n_t|f(c)), \\end{aligned}$$\\end{document}where f(c) is the embedding of medical concept c, T stands for the set of all node types, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_t(c)$$\\end{document} stands for c\u2019s neighbors of type t. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P(n_t|f(c))$$\\end{document} can be defined as a softmax function:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} P(n_t|f(c))=\\frac{\\exp (f(c)\\cdot f(n_t))}{\\sum _{v \\in C}\\exp (f(c)\\cdot f(v))} \\end{aligned}$$\\end{document}For efficient computation, we apply negative sampling [18], and (2) becomes:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} P(n_t|f(c))=\\log \\sigma (f(c)\\cdot f(n_t))+\\sum _{m=1}^M\\mathbb {E}_{v^m \\sim P(v)}[\\log \\sigma (-f(c)\\cdot f(v^m))], \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma (x)=\\frac{1}{1+\\exp (-x)} $$\\end{document}, and P(v) is the pre-defined distribution from which we sample M negative nodes. In each training step, we update the embeddings of c, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n_t$$\\end{document} and M sampled negative nodes by Stochastic Gradient Descent (SGD).",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 37,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 39,
                    "end": 41,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 65,
                    "end": 67,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 5073,
                    "end": 5075,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "HIN Embedding ::: The Proposed Framework",
            "ref_spans": [
                {
                    "start": 515,
                    "end": 516,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We use patients and medical concepts contained in Medical Information Mart for Intensive Care III (MIMIC III) [12] as nodes. We use relations between patients and medical concepts in MIMIC III, as well as relations between medical concepts in ICD-9-CM and MeSH as edges. MIMIC III is a large, public EHR dataset that contains de-identified records of more than forty thousand patients. It includes patient-centered clinical records such as demographics, vital sign measurements, caregiver notes, laboratory test results, along with high-level dictionaries of codes and terminologies. Table 2 summarizes our usage of tables and fields in MIMIC III. ICD-9-CM is the official coding system of assigning codes to diagnoses and procedures used by hospitals in the United States [19], where it organizes over 14, 000 diagnoses and 3, 900 procedures into 19 and 18 clinically meaningful classes, respectively. MeSH classifies a comprehensive range of medical concepts into 16 top-level categories, and serves to facilitates article searching [20]. We utilize three top-level categories in the MeSH descriptor hierarchy, i.e. anatomy concepts, organisms, and diseases. The resulting HIN contains 64,740 nodes, including 50,865 patients, 2,007 symptoms, 990 laboratory tests, 309 microbiology tests, 1,952 prescriptions, 2,003 procedures and 6,604 diagnoses. The resulting HIN contains 7,655,615 edges. 7,575,015 are from the initial HIN, including 947,633 between patients and symptoms, 4,281,748 between patients and lab-tests, 46,477 between patients and micro-tests, 1,322,586 between patients and prescriptions, 219,829 between patients and procedures, and 604,147 between patients and diagnoses. In addition, there are 80,600 edges extracted from ICD-9-CM and MeSH, including 46,960 between diagnoses, 31,618 between procedures and 2,022 between symptoms.\n",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 113,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 774,
                    "end": 776,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1036,
                    "end": 1038,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Dataset ::: Experiments",
            "ref_spans": [
                {
                    "start": 590,
                    "end": 591,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "We evaluate our model and compare it to unsupervised baselines through classification, clustering, and visualization. All these are classic tasks that are commonly performed in representation learning studies [3, 7, 26]. All models are trained with window size set to 5, the number of negative samples to 20 and out dimension to 128. The models are as follows:Med2Vec [3]. A word2vec based multilayer neural network for medical concepts and admissions embedding.Word2vec [17]. We concatenate medical concepts referred to by each patient, and use the concatenations to train word2vec model. We experimented on two sets of word2vec embeddings: W2vRaw is trained with the entire corpus, while W2vFiltered ignores medical concepts with frequencies \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${<}10$$\\end{document}.HeteroMed [10]. A HIN based model for medical concept embeddings.MISMV (ours). We train our model in three variations. MISMV-D contains correlations between diagnoses, MISMV-DS further integrates correlations between symptoms, while MISMV-DSP contains correlations between diagnoses, symptoms, and procedures. We use equal weights for all metapaths as we found little difference in task results with different weights.\n",
            "cite_spans": [
                {
                    "start": 210,
                    "end": 211,
                    "mention": "3",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 213,
                    "end": 214,
                    "mention": "7",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 216,
                    "end": 218,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 369,
                    "end": 370,
                    "mention": "3",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 472,
                    "end": 474,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1046,
                    "end": 1048,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Experimental Setup ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "This section evaluates embeddings by multi-class classifications. We use ICD-9-CM [19] and MeSH [20] categories as the ground truths. There are 18 distinct classes for procedures, 46 for symptoms, and 19 for diagnoses. We observe the labels of certain proportions of all nodes, varying from 5\u201390%, and the task is to predict the labels of the rest nodes. We input embeddings to a LogisticRegression classifier, and report Macro-F1 and Micro-F1 scores.\n",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 85,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 97,
                    "end": 99,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Medical Concept Classification ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Result Analysis. Tables 3, 4 and 5 show results for procedure, symptom, and diagnosis classification, respectively. Our models consistently outperform all baselines by large margins in all three categories. Take symptom classification for example, compared to the highest baseline (HeteroMed), MISMV-DSP shows 175%\u2013313% improvements in Macro-F1 and 20%\u201398% gains in Micro-F1 regardless of the variation of training size. A comparison between variations of our models also shows that adding correlations between medical concepts can help improving classification results, as MISMV-DSP, integrates correlations between procedures, shows a >50% higher Macro-F1 and Micro-F1 compared to MISMV-DS and MISMV-D in procedure classification. HeteroMed outperforms W2vRaw in procedure and symptom classifications when the training set becomes large enough (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document}20%), which shows metapath-guided random walks essentially preserve more information about nodes\u2019 correlations. In diagnosis classification, however, HeteroMed does not perform as well. This is because diagnoses in MIMIC III are sparse, as 3,330 out of 6,604 diagnoses are referred by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\le $$\\end{document}5 patients. For diagnosis nodes that are referred by very few patients, MIMIC III alone does not provide enough structural information to fully reveal their relations with other diagnoses in the network. Our models overcome this problem through enriching the structural information, as MISMV-D shows a >100% higher Macro-F1 and a >70% higher Micro-F1 compared to HeteroMed in diagnosis classification despite variation in training size. Moreover, MISMV-DSP shows an additional \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}5% improvement in both metrics when training size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document}50%. This is because MISMV-DSP indirectly introduces more paths between diagnoses into the network by integrating symptoms and procedures correlations. The same thing is true for symptom classification: compared to MISMV-DS, MISMV-DSP on average gives a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}10% higher Macro-F1 and a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}3% higher Micro-F1, because MISMV-DSP indirectly introduces more paths between symptoms by appending links between procedures. As expected, by getting rid of infrequent concepts, W2vFiltered shows better results compared to W2vRaw in all three categories. Med2vec embeddings are tuned for prediction purpose [3], and turned out are not suitable for classification tasks.\n\n",
            "cite_spans": [
                {
                    "start": 3985,
                    "end": 3986,
                    "mention": "3",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Medical Concept Classification ::: Experiments",
            "ref_spans": [
                {
                    "start": 24,
                    "end": 25,
                    "mention": "3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 27,
                    "end": 28,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 33,
                    "end": 34,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "For medical concept clustering, we leverage the k-means algorithm and report normalized mutual information (NMI), purity score, and adjusted rand index (ARI) [16]. We also visualize the embeddings for a direct overview.\n",
            "cite_spans": [
                {
                    "start": 159,
                    "end": 161,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Medical Concept Clustering ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Medical Concept Clustering ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Result Analysis. Table 6 shows the results for procedure, symptom, and diagnosis clustering. Our models act significantly better than baselines. MISMV-DSP shows >35%, >16% and >70% gains in NMI, purity, and ARI, respectively, compared to the best among baselines in all categories. This testifies that incorporating external knowledge can significantly improve clustering performances. A comparison between variations of our models further confirms the validity of our strategy. By integrating direct external knowledge of procedures correlations, MISMV-DSP performs >20% better in all three metrics compared to MISMV-DS and MISMV-D in procedure clustering. Indirect knowledge is also helpful. MISMV-DSP performs better than MISMV-DS in symptom clustering, because it appends edges between procedures, which indirectly creates more paths between symptom nodes. W2vFiltered outperforms other baselines because sparsity was removed. Med2vec embeddings, tailored for prediction tasks [3], did not perform as well in clustering tasks.",
            "cite_spans": [
                {
                    "start": 982,
                    "end": 983,
                    "mention": "3",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Medical Concept Clustering ::: Experiments",
            "ref_spans": [
                {
                    "start": 23,
                    "end": 24,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "Figure 3 shows 2D PCA projections of medical concept embeddings learned by our MISMV-DSP model and W2vFiltered, which is the most competitive baseline in the clustering tasks. In Fig. 3(a), medical concepts of different types fall into clearly separated clusters. This suggests a good capture of the structural information in EHR by MISMV-DS. In Fig. 3(b), however, all concepts are in one large cluster, as W2vFiltered embeddings do not contain the structural information. Figure 3(c) and (d) zoom in on diagnoses embeddings. Figure 3(c) shows that diagnoses from two different classes are clearly separated by MISMV-DSP. This proves that correlations between diagnoses are well preserved by our model. Compared to Fig. 3(c), the separation in Fig. 3(d) is not as clear, since W2vFiltered learns the correlations between medical concepts only from the co-occurrences of them in the EHR.",
            "cite_spans": [],
            "section": "Medical Concept Clustering ::: Experiments",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 184,
                    "end": 185,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 351,
                    "end": 352,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 481,
                    "end": 482,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 534,
                    "end": 535,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 721,
                    "end": 722,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 750,
                    "end": 751,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "We propose a multi-information source HIN that cooperates EHR and external knowledge including ICD-9-CM and MeSH. By integrating various information sources to enrich heterogeneous network schema, our model is well aware of both the structure of EHR and the semantics of as well as the correlations between medical concepts it refers to. The embeddings learned by us are informative and semantically reflective. In experiments, our model significantly outperforms baselines in diagnosis, procedure, symptom classification, and clustering.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Metapaths extracted from network schema.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: MIMIC III usage in our study.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Multi-class procedure classification results.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Multi-class symptom classification results.\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Multi-class diagnosis classification results.\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6.: Medical concept clustering results.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: The Multi-Information Source Medical Vectors (MISMV) framework. S, D, O, E, L, M, T, G, A and P are concept types, and they refer to symptom, diagnosis, procedure, prescription, laboratory test, microbiology test, ethnicity, gender, age and patient, respectively. Lowercase letters with subscripts are concepts, for example, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document} stands for a specific diagnosis such as Systolic heart failure.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Heterogeneous network schema",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Visualization of medical concept embeddings. (a) and (b) are 2D PCA projections of non-diagnosis medical concept embeddings learned by MISMV-DSP and W2vFiltered, respectively. (c) and (d) project the embeddings of all diagnoses in class V01-V91 Mental disorders and 290\u2013319 Supplementary classification learned by MISMV-DSP and W2vFiltered, respectively.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Big data and machine learning in health care",
            "authors": [
                {
                    "first": "AL",
                    "middle": [],
                    "last": "Beam",
                    "suffix": ""
                },
                {
                    "first": "IS",
                    "middle": [],
                    "last": "Kohane",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "JAMA",
            "volume": "319",
            "issn": "13",
            "pages": "1317-1318",
            "other_ids": {
                "DOI": [
                    "10.1001/jama.2017.18391"
                ]
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "MIMIC-III, a freely accessible critical care database",
            "authors": [
                {
                    "first": "AE",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Sci. Data",
            "volume": "3",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/sdata.2016.35"
                ]
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Population analysis of adverse events in different age groups using big clinical trials data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Eldredge",
                    "suffix": ""
                },
                {
                    "first": "CC",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "RA",
                    "middle": [],
                    "last": "Cisler",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "JMIR Med. Inform.",
            "volume": "4",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.2196/medinform.6437"
                ]
            }
        },
        "BIBREF7": {
            "title": "Introduction to information retrieval",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Raghavan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sch\u00fctze",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Nat. Lang. Eng.",
            "volume": "16",
            "issn": "1",
            "pages": "100-103",
            "other_ids": {
                "DOI": [
                    "10.1017/S1351324909005129"
                ]
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Benchmark of deep learning models on large healthcare mimic datasets",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Purushotham",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Che",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J. Biomed. Inform.",
            "volume": "83",
            "issn": "",
            "pages": "112-134",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jbi.2018.04.007"
                ]
            }
        },
        "BIBREF16": {
            "title": "A survey of heterogeneous information network analysis",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "SY",
                    "middle": [],
                    "last": "Philip",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE TKDE",
            "volume": "29",
            "issn": "",
            "pages": "17-37",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Predictive data mining for medical diagnosis: an overview of heart disease prediction",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Soni",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Ansari",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Soni",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IJCA",
            "volume": "17",
            "issn": "",
            "pages": "43-48",
            "other_ids": {
                "DOI": [
                    "10.5120/2237-2860"
                ]
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "EMR-based medical knowledge representation and inference via Markov random fields and distributed representation learning",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Guan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Artif. Intell. Med.",
            "volume": "87",
            "issn": "",
            "pages": "49-59",
            "other_ids": {
                "DOI": [
                    "10.1016/j.artmed.2018.03.005"
                ]
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}