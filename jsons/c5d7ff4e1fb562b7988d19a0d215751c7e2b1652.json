{
    "paper_id": "c5d7ff4e1fb562b7988d19a0d215751c7e2b1652",
    "metadata": {
        "title": "Multi-view Deep Gaussian Process with a Pre-training Acceleration Technique",
        "authors": [
            {
                "first": "Han",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "East China Normal University",
                    "location": {
                        "addrLine": "3663 North Zhongshan Road",
                        "postCode": "200062",
                        "settlement": "Shanghai",
                        "country": "People's Republic of China"
                    }
                },
                "email": ""
            },
            {
                "first": "Jing",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "East China Normal University",
                    "location": {
                        "addrLine": "3663 North Zhongshan Road",
                        "postCode": "200062",
                        "settlement": "Shanghai",
                        "country": "People's Republic of China"
                    }
                },
                "email": ""
            },
            {
                "first": "Shiliang",
                "middle": [],
                "last": "Sun",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "East China Normal University",
                    "location": {
                        "addrLine": "3663 North Zhongshan Road",
                        "postCode": "200062",
                        "settlement": "Shanghai",
                        "country": "People's Republic of China"
                    }
                },
                "email": "slsun@cs.ecnu.edu.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "Deep Gaussian process (DGP) is one of the popular probabilistic modeling methods, which is powerful and widely used for function approximation and uncertainty estimation. However, the traditional DGP lacks consideration for multi-view cases in which data may come from different sources or be constructed by different types of features. In this paper, we propose a generalized multi-view DGP (MvDGP) to capture the characteristics of different views and model data in different views discriminately. In order to make the proposed model more efficient in training, we introduce a pre-training network in MvDGP and incorporate stochastic variational inference for fine-tuning. Experimental results on real-world data sets demonstrate that pre-trained MvDGP outperforms the state-of-the-art DGP models and deep neural networks, achieving higher computational efficiency than other DGP models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Gaussian process (GP) owns a significant ability of modeling representation and can estimate the uncertainty of the prediction effectively [5, 11, 16] . Deep Gaussian process (DGP) is a stack of multi-layer GPs [1, 2, 13] . Benefitting from the hierarchical structure, DGP not only retains the excellent features of GP, but also overcomes the limitations of GP and obtains stronger mapping capability. However, the difficulty in DGP is mainly located on intractable calculations during the training process. The Bayesian training framework based on variational inference for DGP is a classical method but limited by the scale of data [2] . Doubly stochastic variational inference is a state-of-the-art and widely used inference technique, which adopts stochastic optimization and makes it possible for DGP to be applied to large-scale data [13] . Recently, there are some new works focusing on non-Gaussian posterior in the real-world data to develop DGP comprehensively [3, 14] .",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 142,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 143,
                    "end": 146,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 147,
                    "end": 150,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 211,
                    "end": 214,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 215,
                    "end": 217,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 218,
                    "end": 221,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 634,
                    "end": 637,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 840,
                    "end": 844,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 971,
                    "end": 974,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 975,
                    "end": 978,
                    "text": "14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Traditional GP models only focus on modeling data from a single source. As amounts and sources of data are augmented, data integrations from multiple feature sets are referred to as multi-view data [17, 20] . It is improper to treat data from different views equally, and thus multi-view learning flourishes. GP-based models have been extended to multi-view scenarios, in which the multi-view regularized GP [8] and the sparse multimodal GP [9] are the generalizations of shallow GP model. A DGP-based work is also developed [18] , but limited in multi-view unsupervised representation learning, where additional classifiers are needed for classification tasks. Besides, the inference of the unsupervised DGP [18] is based on the Bayesian training framework with strong mean-field and Gaussian assumptions, which underestimates variance and makes the model unable to be applied to large-scale data scenarios. Our goal is to propose a general end-to-end multi-view DGP (MvDGP). We build a scalable model without forcing independence between layers, and apply stochastic variational inference and re-parameterization techniques to improve the ability of modeling on the large-scale data.",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 202,
                    "text": "[17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 203,
                    "end": 206,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 408,
                    "end": 411,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 441,
                    "end": 444,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 525,
                    "end": 529,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 709,
                    "end": 713,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In addition, we expect that the MvDGP model possesses significant superiority in training speed. In the multi-view scenario, we tune the model according to the characteristics of each view, which will inevitably introduce more model parameters and lengthen the training time. Pre-training is a widely used technique [4, 19] , in which a large number of data are taken as training samples to be trained across multiple GPUs. The weights obtained by pre-trained networks are used as the initial weights for new tasks, and then only a few steps of finetuning are needed to get prediction results. In order to make the proposed model more competitive in terms of training speed, we introduce a novel pre-training model for MvDGP. Instead of training with the same model using other data sets, we use the same data set to train with other models. Because the neural network with infinite width has been proved equivalent to GP exactly and the training cost of deep neural network (DNN) is much less than DGP [6, 7, 10] , we pre-train the DNN with a similar structure of MvDGP to analogize the initial training process of MvDGP. Through the DNN pre-training, we aim to get a set of appropriate initial parameters for MvDGP. Since the parameter domains of the DNN and the MvDGP are not the same, the initial parameters of each layer in the MvDGP are obtained by auxiliary optimization of single GP. The optimization efficiency of MvDGP is improved significantly by pre-training.",
            "cite_spans": [
                {
                    "start": 316,
                    "end": 319,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 320,
                    "end": 323,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1003,
                    "end": 1006,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1007,
                    "end": 1009,
                    "text": "7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1010,
                    "end": 1013,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "There are three main contributions in our work:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We propose a generalized and flexible MvDGP, which considers characteristics of different views. Deep structure leads to more powerful abilities of uncertainty estimation and mapping representation compared with shallow models [8, 9] . Furthermore, MvDGP is an end-to-end supervised model, which can take advantage of labels to learn models, and provides stronger robustness and generalization performance than unsupervised multi-view DGP [18] . 2. Scalability: We infer the MvDGP without setting strong mean-field constraints and derive stochastic variational inference. Compared to the model [18] can hardly be applied in large-scale scenarios, our model is capable of it. Meanwhile, our model can be extended to more views easily and can customize the detailed depth of each view according to the view characteristic.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 230,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 231,
                    "end": 233,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 439,
                    "end": 443,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 594,
                    "end": 598,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Generalized Multi-view Deep Gaussian Process (MvDGP):"
        },
        {
            "text": "We obtain appropriate initial parameters by DNN pre-training for MvDGP, which reduces the oscillation and speeds up the training. Experiments demonstrate that the pre-trained MvDGP guarantees higher performance and runs several times faster than unpre-trained methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Efficiency:"
        },
        {
            "text": "Deep Gaussian process (DGP) is a stack of multiple GPs, which possesses a more powerful modeling capability than a GP [2] . For a standard DGP, we review a supervised version as an example. Given a training set, including observed inputs X \u2208 R N \u00d7Q and observed outputs Y \u2208 R N \u00d7D , where N is the number of samples, Q and D are the dimensionality of input and output vector, respectively. For a DGP with L layers of hidden units, we define F = {F 1 , F 2 , ..., F L } as the latent variable set, where F l is the output for layer l and the input for layer l + 1, l = 1, . . . , L \u2212 1. Furthermore, we add additional sets of inducing inputs Z = {Z 1 , Z 2 , ..., Z L } and inducing points U = {U 1 , U 2 , ..., U L } to employ variational inference [15] . The assumption of the model prior is as follows,",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 121,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 749,
                    "end": 753,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Deep Gaussian Process"
        },
        {
            "text": "where m(Z) is the mean function and k(Z, Z) is the kernel function. Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Gaussian Process"
        },
        {
            "text": "We record X as F 0 , and the conditional distribution, corresponding mean and variance are denoted as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Gaussian Process"
        },
        {
            "text": "The likelihood of model is generally set to a Gaussian distribution,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Gaussian Process"
        },
        {
            "text": "where \u03a3 Y is the variance of the observation Y. The joint density of the observed output Y, latent variables F and inducing points U is written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Gaussian Process"
        },
        {
            "text": "Due to the characteristics of multi-view data, the general DGP cannot utilize the rich information in multiple views reasonably. In this section, we propose a new model named multi-view deep Gaussian process (MvDGP), and introduce stochastic variational inference for optimization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-view Deep Gaussian Process"
        },
        {
            "text": "We propose an end-to-end multi-view model and take two views of data and models as an example. For given data {X (1) , X (2) , Y}, X (1) \u2208 R N \u00d7Q1 and X (2) \u2208 R N \u00d7Q2 are observed inputs of the first and the second view respectively and Y \u2208 R N \u00d7D is the observed outputs. For data of each view, there is a deep structure to model it. The latent variables of intermediate layers are recorded as F",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 116,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 121,
                    "end": 124,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 133,
                    "end": 136,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 153,
                    "end": 156,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Multi-view Deep Gaussian Process"
        },
        {
            "text": "The depths of the networks in different views can be determined according to the data characteristics of each view for better mapping. The inducing inputs Z We record F (S) 0 as the transition layer from the separated views F (1) , F (2) to merged view F (S) , and the joint density of MvDGP is written as",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 229,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 234,
                    "end": 237,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Multi-view Deep Gaussian Process"
        },
        {
            "text": "where p(F",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-view Deep Gaussian Process"
        },
        {
            "text": "R ] is the concatenation of the last layers of two views, and \u03a3 (S) 0 represents corresponding unit variance. The joint distribution of latent variables in view v is specifically as ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-view Deep Gaussian Process"
        },
        {
            "text": "0 denote the observed inputs X (1) , X (2) , respectively.",
            "cite_spans": [
                {
                    "start": 31,
                    "end": 34,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 39,
                    "end": 42,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Multi-view Deep Gaussian Process"
        },
        {
            "text": "Directly inferring MvDGP is intractable and complex computationally, we take stochastic variational inference for optimization. The main idea of variational inference is to find an approximate posterior distribution q(F, U) that is as close as possible to the true posterior p(F, U).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Inference"
        },
        {
            "text": "We adopt a factorized form for joint posterior distribution as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Inference"
        },
        {
            "text": "for each view are denoted as Sect. 3.1. We take Gaussian forms for variational dis-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Inference"
        },
        {
            "text": "l ), respectively. Under this setting, the variational posterior can be obtained analytically as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Inference"
        },
        {
            "text": "In order to maintain gradients and update layer-wise parameters in the process of optimization, we introduce the re-parameterization trick and choose Monte Carlo method to estimate variational posterior q(F) [12] . Firstly, draw a noise term (v) l from a standard Gaussian distribution, for view v = 1, 2, S and layer l = 1, . . . , H (v) \u2212 1. Then, iteratively sample latent variableF",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 212,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Variational Inference"
        },
        {
            "text": "can be clearly written a\u015d",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Inference"
        },
        {
            "text": "where \u03bc l and \u03a3 l are mean and covariance functions denoted in (10), (11).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Variational Inference"
        },
        {
            "text": "To minimize the KL divergence of q and p, we maximize the lower bound L of the logarithm marginal likelihood log p(Y), which is formulated as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "By substituting the joint density (7) and posterior distribution (9) to lower bound expression (13) , the term",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 99,
                    "text": "(13)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "l ) in the numerator and denominator can be offset. The variational lower bound of model evidence in MvDGP can be rearranged to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "where KL (v) represents ",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 12,
                    "text": "(v)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "where y i is observed outputs, and F (S)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "Li , F",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "Ri are corresponding latent variables for sample i, i = 1, . . . , N. The addition expression of lower bound allows stochastic optimization to be employed in inference. The samples of minibatch can be regarded as an unbiased estimation of all samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "Model parameters are optimized with the Adam optimizer during training, which include inducing inputs {Z",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": ", and kernel parameters {\u03b8 is set as X (v) * , v = 1, 2. The samples can be obtained according to the re-parameterization Monte Carlo sample steps (12) iteratively.",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 151,
                    "text": "(12)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "If there are more than two views in data, the MvDGP is easily to be generalized to multiple views by adding separated multi-layer GPs structure for new views.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Optimization and Predictions"
        },
        {
            "text": "In order to better model the function approximation of each view, MvDGP introduces more latent variables and model parameters than single-view DGP. The training time of the model with a large number of parameters is not optimistic even with doubly stochastic optimization. Due to the initial parameters of the model have a significant impact on the training efficiency, the training speed of the model with proper initial parameters is faster than the random one. We consider introducing a novel technique of pre-training to MvDGP by training a computational cost-dominant model and getting a suitable set of initial parameters for MvDGP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pre-training Technique for MvDGP"
        },
        {
            "text": "Deep neural network (DNN) is a type of powerful model for representation learning and model mapping. Inspired by the similar characteristics of DNN and DGP [7, 10] , we adopt the DNN with a similar structure to MvDGP to simulate the initial training process of MvDGP. We model the DNN separately for two views and build common network layers whose inputs are the concatenation of the outputs of the separated networks. The number of parameters is related to the number and dimension of hidden units. The number of model parameters in the DNN we used is much smaller than MvDGP, which leads to faster training speed.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 159,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 160,
                    "end": 163,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Pre-training Technique for MvDGP"
        },
        {
            "text": "Since it is not possible to directly use the parameters such as the network weights of the DNN in MvDGP, we use some single-layer GPs as auxiliary pretraining models. We take the values of the adjacent two layers in the DNN as the input and output of the single GP to obtain a set of initial parameters suitable for corresponding layers in MvDGP. Since the training difficulties of DNN and single GP are much lower than that of MvDGP, the pre-training step can be quickly calculated and is reasonable for roughly selecting the initial parameters of MvDGP. Then, taking advantage of powerful uncertainty estimation and robust characteristics of MvDGP, we can perform more precise probability learning in multi-view data. In the processes of training DNN, single GP, as well as MvDGP, stochastic optimization is all adopted to facilitate the generalization of massive data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pre-training Technique for MvDGP"
        },
        {
            "text": "The schematic diagram of pre-trained MvDGP (PreMvDGP) is depicted in Fig. 2 . The basic MvDGP model is framed in orange lines. The gray node in the outermost circle represents the DNN with a similar structure to MvDGP as the first stage of pre-training. The middle layers of the DNN, F",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 69,
                    "end": 75,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Pre-training Technique for MvDGP"
        },
        {
            "text": "l , are used as the observed inputs and observed outputs to train the parameters of each single GP, where v = 1, 2, l = 1, . . . , H (v) , and F (2) . The yellow blocks in the second column of the left and the second column of the right are both single GPs as the second stage of pre-training. The training results of each GP are taken as the initial parameters of the corresponding layer in MvDGP. At last, a precise mapping learning is performed through MvDGP.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 136,
                    "text": "(v)",
                    "ref_id": null
                },
                {
                    "start": 145,
                    "end": 148,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Pre-training Technique for MvDGP"
        },
        {
            "text": "In this section, we evaluate the performance of the proposed model in four realworld data sets. Our concerns about model performance include accuracy and training speed. We analyze experimental results compared with the state-of-theart DGP models and deep neural network. features for each handwritten number ('0'-'9') in MFeat data set 2 . We adopt these features as six views. The data is divided into ten partitions denoted as M-0\u223cM-9, in which partition M-i represents the samples labeled 'i' as positive class and others as negative class samples. 3. Internet Advertisements Data Set (Ads). The Ads data set 3 is composed of the features extracted from five aspects. We consider five features as five views of data. There is a unique label to mark if the sample is an ad. 4. Forest CoverType Data Set (CoverType). The data 4 are composed of quantitative real variables and binary one-hot variables, for which we adopt two views to model. We use samples labeled Spruce-Fir or Lodgepole as positive samples to form two data sets, respectively (marked as partition C-1 and C-2). The total number of samples, dimension of each view, and the sample number of each class for four data sets and partitions are presented detailed in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1230,
                    "end": 1237,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "We conduct a series of experiments on four data sets to verify the performance of our PreMvDGP model. For each experiment, we take 5-fold cross-validation to obtain 80% samples for the train set and 20% samples for the test set. We perform ten repeated experiments to each sample partition and take the average as the final experimental results. We adopt 20 samples as a minibatch and 128 inducing points for every layer in the experiments. The number of hidden layers of different views and the shared layers can be customized by the characteristics of each view data. To illustrate the general characteristics of PreMvDGP, we show the experimental results with L = 1, R = 1, H = 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "To demonstrate the superior performance of our model, we compare with two state-of-the-art DGP methods, including doubly stochastic variational inference DGP (DSVI-DGP) [13] and stochastic gradient Hamilton Monte Carlo DGP (SGHMC-DGP) [3] , and the deep neural network (DNN) which is designed to adapt to multi-view data in this experiments. Since the single-view DGP methods cannot utilize multi-view data directly, we consider separately taking the data of view 1 (V1), view 2 (V2), and the concatenation of two view data (Con) as three types of inputs for WebKB data set to verify the necessity of multi-view modeling.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 173,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 235,
                    "end": 238,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "Experiments using multiple single-source data are redundant and incomplete, so we concatenate the data from all views as the inputs of the other three data sets to make the most of the data. For single-view DGP methods, we abbreviate the methods as DSVI-DGP-Con, SGHMC-DGP-Con. To ensure adequate training and convergence, we use 500 epochs to train DSVI-DGP and SGHMC-DGP, respectively. In the pre-training phase of PreMvDGP, we set the number of hidden units as 64 and the dimension of hidden units as 10 to get a rough set of parameters as quickly as possible. Meanwhile, we set 300 epochs for DNN pre-training, 100 epochs for training single GPs, and 100 epochs for training MvDGP. In practice, the number of iterations set in this way can ensure that each step is completely trained. All parameter settings in our experiments remain fixed in each dataset and comparison method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "The experimental results on the four WebKB data sets, including average classification accuracies, standard deviations, and computational costs, are presented in Table 2 . Experimental results show that the representation with only view 1 is significantly better than the representation with only view 2 in this data set. Concatenating data from two views (Con) has no significant effect on improving accuracy compared to results with view 1 (V1). In Table 2 , the results of (Con) achieves better than (V1) for DSVI-DGP, while the results of (V1) take a bit advantage than (Con) for SGHMC-DGP. Concatenating data from different views causes an increase in the dimensions of the inputs, making the training process more expensive. The experiments prove that PreMvDGP achieves better classification performance than comparison methods, indicating that single-view methods cannot model the data characteristics of different views properly. Since DNN is used as the initializer in our model, we also list the average time required for 300 iterations of DNN and the average classification accuracy only using the DNN optimizer. It can be found that the computational time of the pretrainer takes a small part of the total time, and the training results of the DNN are suitable for the initialization of the MvDGP. PreMvDGP with appropriate initial parameters speeds up the training and learns function approximation more subtly than only using the DNN, resulting in more competitive results.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 162,
                    "end": 169,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 451,
                    "end": 458,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "We model the data from six and five views separately for MFeat and Ads data sets, which means that our approach can be easily generalized to more views instead of using combinations of any two views. The experimental results Table 3 . The average classification accuracies (%), standard deviations, and computational time (s) on multiple data sets and partitions, i.e., MFeat (M-0\u223cM-9), Ads, CoverType (C-1, C-2). including accuracies and computation time in the other three data sets are shown in Table 3 . Our method almost achieves the best accuracy and is dominant in running time in all data sets and partitions, which means that discriminately modeling data of different views is necessary and the pre-training technique plays an important role in optimizing the initial parameters. Significantly, PreMvDGP also works well in the large forest CoverType data set. Stochastic optimization and inducing points help save the computational overhead of our model. Experiments prove that our method is appropriate for multi-view scenarios of large-scale data.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 225,
                    "end": 232,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 498,
                    "end": 505,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Results and Analysis"
        },
        {
            "text": "In this paper, we propose an end-to-end multi-view deep Gaussian process (MvD-GP) model, which is suitable for modeling multi-view data. The inference is based on doubly stochastic optimization and can be applied in large-scale data scenarios. To speed up the training, we introduce a pre-training deep neural network in MvDGP. The initial parameters obtained by the pre-training are proper for MvDGP, and more precise learning is performed by MvDGP. Experimental results demonstrate that pre-trained MvDGP (PreMvDGP) outperforms the state-of-the-art DGP methods in multi-view data modeling, and achieves better performance in training speed. Our work is a generalization of DGP in multiview scenarios, which helps to develop the MvDGP under the trend of large-scale data with its superior computational performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Variational auto-encoded deep Gaussian processes",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Damianou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gonz\u00e1lez",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Lawrence",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1511.06455"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Deep Gaussian processes",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Damianou",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Lawrence",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Artificial Intelligence and Statistics",
            "volume": "",
            "issn": "",
            "pages": "207--215",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Inference in deep Gaussian processes using stochastic gradient hamiltonian monte carlo",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Havasi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Hern\u00e1ndez-Lobato",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Murillo-Fuentes",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "7506--7516",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A better way to pretrain deep boltzmann machines",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "R"
                    ],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2447--2455",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ko",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Fox",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Auton. Robots",
            "volume": "27",
            "issn": "",
            "pages": "75--90",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "A training method using DNN-guided layerwise pretraining for deep Gaussian processes",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Koriyama",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kobayashi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
            "volume": "",
            "issn": "",
            "pages": "2787--2791",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Deep neural networks as Gaussian processes",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bahri",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Novak",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Schoenholz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sohl-Dickstein",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1711.00165"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Multi-view regularized Gaussian processes",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "655--667",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Sparse multimodal Gaussian processes",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Intelligent Science and Big Data Engineering",
            "volume": "",
            "issn": "",
            "pages": "28--40",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Gaussian process behaviour in wide deep neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "G D G"
                    ],
                    "last": "Matthews",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rowland",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hron",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Turner",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ghahramani",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.11271"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Gaussian processes for machine learning toolbox",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "E"
                    ],
                    "last": "Rasmussen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Nickisch",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J. Mach. Learn. Res",
            "volume": "11",
            "issn": "",
            "pages": "3011--3015",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Rezende",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wierstra",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1401.4082"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Doubly stochastic variational inference for deep Gaussian processes",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Salimbeni",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Deisenroth",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "4588--4599",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Deep Gaussian processes with importance-weighted variational inference",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Salimbeni",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dutordoir",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hensman",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "P"
                    ],
                    "last": "Deisenroth",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.05435"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Sparse Gaussian processes using pseudo-inputs",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Snelson",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ghahramani",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1257--1264",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Practical Bayesian optimization of machine learning algorithms",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Snoek",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Larochelle",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Adams",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2951--2959",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A survey of multi-view machine learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Neural Comput. Appl",
            "volume": "23",
            "issn": "",
            "pages": "2031--2038",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Multi-view deep Gaussian processes",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Neural Information Processing",
            "volume": "",
            "issn": "",
            "pages": "130--139",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Discriminative pretraining of deep neural networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "T B"
                    ],
                    "last": "Seide",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Multi-view learning overview: recent progress and new challenges",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Inf. Fusion",
            "volume": "38",
            "issn": "",
            "pages": "43--54",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Sect. 2. In addition to the separated GP layers for each view, there are also common layers that share information for both views, in which variables and model parameters are denoted as F . . . , H (S) . The graphical model of MvDGP is illustrated in Fig. 1, and the depth for each view is marked as H (1) = L, H (2) = R, H (S) = H.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The graphical model for multi-view deep Gaussian process.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The depth for each view is H (1) = L, H (2) = R, H (S) = H and the symbols ofF",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": ") in the variational lower bound can be written in the form of additions for samples as follows,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "2, S. Stochastic optimization and unbiased minibatch samples ensure the scalability of MvDGP. Our model can be easily generalized to large-scale data. For predictions, we take the mean of multiple samples of F (S) * H as the predict outputs Y * for test inputs X * = {X (1) * , X (2) * }, and q(F 1 , U H ), where K is the number of samples, and the value ofF",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "The schematic diagram of pre-trained MvDGP.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "1 is composed of four universities, Cornell, Texas, Washington, and Wisconsin, in which data are captured from two views, words in web pages and hyperlinks. The web page can be divided into five categories, where we denote the category of the largest number of samples as positive class and the rest as negative class. 2. Multiple Feature Data Set (MFeat). There are 200 samples as well as six",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Detailed data set information.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The average classification accuracies (%), standard deviations, and computational time(s) of comparison methods and PreMvDGP on the WebKB data sets.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Data set DSVI-DGP-ConSGHMC-DGP-Con DNN PreMvDGP",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgments. The corresponding author Jing Zhao would like to thank supports from the National Natural Science Foundation of China under Projects 61673179, Shanghai Knowledge Service Platform Project (No. ZF1213) and Shanghai Sailing Program 17YF1404600.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}