{
    "paper_id": "992c791664998635bfe6fb0a3042cdb09f737383",
    "metadata": {
        "title": "HIN: Hierarchical Inference Network for Document-Level Relation Extraction",
        "authors": [
            {
                "first": "Hengzhu",
                "middle": [],
                "last": "Tang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "tanghengzhu@iie.ac.cn"
            },
            {
                "first": "Yanan",
                "middle": [],
                "last": "Cao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "caoyanan@iie.ac.cn"
            },
            {
                "first": "Zhenyu",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "zhangzhenyu1996@iie.ac.cn"
            },
            {
                "first": "Jiangxia",
                "middle": [],
                "last": "Cao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "caojiangxia@iie.ac.cn"
            },
            {
                "first": "Fang",
                "middle": [],
                "last": "Fang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "fangfang0703@iie.ac.cn"
            },
            {
                "first": "Shi",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "wangshi@ict.ac.cn"
            },
            {
                "first": "Pengfei",
                "middle": [],
                "last": "Yin",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "yinpengfei@iie.ac.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "Document-level RE requires reading, inferring and aggregating over multiple sentences. From our point of view, it is necessary for document-level RE to take advantage of multi-granularity inference information: entity level, sentence level and document level. Thus, how to obtain and aggregate the inference information with different granularity is challenging for document-level RE, which has not been considered by previous work. In this paper, we propose a Hierarchical Inference Network (HIN) to make full use of the abundant information from entity level, sentence level and document level. Translation constraint and bilinear transformation are applied to target entity pair in multiple subspaces to get entity-level inference information. Next, we model the inference between entity-level information and sentence representation to achieve sentence-level inference information. Finally, a hierarchical aggregation approach is adopted to obtain the document-level inference information. In this way, our model can effectively aggregate inference information from these three different granularities. Experimental results show that our method achieves state-of-the-art performance on the largescale DocRED dataset. We also demonstrate that using BERT representations can further substantially boost the performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Relation extraction (RE) aims to detect the semantic relation between entities in plain text, which plays an important role in knowledge base population and natural language understanding. Most previous work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, deep learning models have been widely applied to sentence-level RE and achieved remarkable success [4, 16] . Despite the great success of previous work, sentence-level RE suffers from a serious restriction in practice: a large amount of relational facts are expressed in multiple sentences. Taking Fig. 1 as an example, in order to identify the relational fact (Chris Carter, creator, Fox Mulder ), one should first identify the fact \"Nisei\" is an episode of the American science fiction television series from sentence 1, then identify the facts that Fox Mulder is a character in \"Nisei\" and Chris Carter is one of the writers of \"Nisei\" from sentence 8 and 3 respectively. To extract these relational facts, it is necessary to infer and aggregate over multiple sentences. Obviously, most traditional sentence-level RE models often fail to generalize extraction to this situation. To move RE forward from sentence level to document level, many efforts have been made [13, 15] , but most previous methods used only entity-level information and this is not adequate. Thus, there are still some deep-seated problems unsolved in document-level RE.",
            "cite_spans": [
                {
                    "start": 412,
                    "end": 415,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 416,
                    "end": 419,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 611,
                    "end": 617,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "To predict the relation between two entities, we argue that the documentlevel RE model requires taking advantage of multi-granularity inference information: entity level, sentence level and document level. Let's go back to the former example, entity-level inference information is derived from the semantic of all mentions of Chris Carter and Fox Mulder in the document, sentence-level inference information represents the information related to relational facts in each sentence, document-level inference information aggregates all the necessary information in supporting sentences (sentence 1, 3 and 8) and discards information in noise sentences. Technically, it is clear that document-level RE faces two main challenges: (1) How to obtain the inference information with different granularity; (2) How to aggregate these different granularity inference information and make the final prediction.",
            "cite_spans": [
                {
                    "start": 725,
                    "end": 728,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we propose a new neural architecture, Hierarchical Inference Network (HIN), to tackle above challenges. Specifically, inspired by translation constraint [1] , which models a relational fact r(e h , e t ) with e h + r \u2248 e t , we apply this translation constraint to target entity pair. Besides, a bi-affine layer is also used to obtain bilinear representation for the target entity pair. To jointly attend to information from different representation subspaces, we implement the above two transformations in multiple subspaces in parallel, and acquire entitylevel inference information. To obtain the sentence-level inference information, we first apply vanilla attention mechanism to calculate the vector representation for each sentence, which enables our model to pay more attention to the informative words. Then we adopt the semantic matching method [2] which is widely used in natural language inference (NLI) domain to compare the entity-level inference information with each sentence vector. Furthermore, in order to calculate the document-level inference information, we apply a hierarchical BiLSTM and again use attention mechanism to distinguish crucial sentence-level inference information for overall document-level inference representation. Finally, we aggregate inference information of different granularity, the entity-level and document-level inference representations are combined into a fixed-length vector, which is further fed into a classification layer for prediction.",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 171,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 869,
                    "end": 872,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To summarize, we make the following contributions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "1. We propose a Hierarchical Inference Network (HIN) for document-level RE, which is capable of aggregating inference information from entity level to sentence level and then to document level. 2. We conduct thorough evaluation on DocRED dataset. Results show that our model achieves the state-of-the-art performance. We further demonstrate that using BERT representations further substantially boosts the performance. 3. We analyze the effectiveness of our model on different number of supporting sentences and experimental results show that our model performs much better than previous work when the number of supporting sentences is large.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "For document-level RE, the input is a document with annotated entities, as well as multiple occurrences of each entity, i.e., entity mentions, the goal is to identify all the related entity pairs in the document. Following [15] , we transform RE into a classification problem. We use upper case letters to represent entities (E 1 , \u00b7 \u00b7 \u00b7 , E m ) and lower case letters to represent mentions (e 1 , \u00b7 \u00b7 \u00b7 , e m ). The RE model is given a relation candidate (E a , E b , D) and expected to output the relations between E a and E b , where E a and E b are entities in the document D. Figure 2 gives an illustration of our model. We describe the details of different components in the following sections.",
            "cite_spans": [
                {
                    "start": 223,
                    "end": 227,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 581,
                    "end": 589,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Task Description"
        },
        {
            "text": "-Word Embeddings. In order to capture the meaningful semantic information of words, we map each word into a low-dimensional word embedding vector. The dimension of word embeddings is d w . -Entity Type Embeddings. We utilize the entity type information to enrich the representation of the input. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) into a vector. The dimension of entity type embeddings is d t . -Coreference Embeddings. Usually each entity may be mentioned many times in a document. Following previous work, we assign entity mentions corresponding to the same entity with the same entity id, which is determined by the order in which entities appear in the document. Then entity ids are embedded into vectors. The dimension of coreference embeddings is d c .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input Layer"
        },
        {
            "text": "We concatenate all three embeddings together for each word w i , and a document is transformed into a matrix X = [w 1 , w 2 , . . . , w n ], where each word vector w i \u2208 R dw+dt+dc and n is the length of the document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input Layer"
        },
        {
            "text": "In this section, we compute the entity-level inference information for target entity pair. To represent each word in its context, we encode the document",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "where h i \u2208 R d is a contextualized representation of w i , summarizing the context information centered around w i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "Considering that an entity may be mentioned many times in a document and a mention may also contain more than one word, we represent each entity and mention with the average of the embeddings of different elements. Correspondingly, the mention representation is formed as the average of the words that the mention contains, the entity representation is computed as the average of the mention representations associated with the entity:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "We claim that it is beneficial to allow the model to jointly attend to information from different representation subspaces, thus, we use different learnable projection matrices to project entities into K subspaces:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "where E k a \u2208 R k corresponds to the representation of E a in the k-th latent space, W (0) k \u2208 R d\u00d7d and W (1) k \u2208 R d\u00d7k are the learnable projection matrices corresponding to the k-th subspace. For each of these projected versions, we perform the entity-level inference in parallel. These are concatenated and once again projected, resulting in the final entity-level inference information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "Inspired by TransE [1] which modelled a triple r(e h , e t ) with e h + r \u2248 e t , we argue that (E b \u2212 E a ) could represent the relation between E a and E b in the document to some extent. In addition, a bilinear representation can be obtained by a bi-affine layer to enhance the expression ability of model. We define the following formula as entity-level inference representation in the k-th latent space:",
            "cite_spans": [
                {
                    "start": 19,
                    "end": 22,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "where R k \u2208 R k\u00d7k\u00d7k is a learned bi-affine tensor, Concat denotes concatenation. Moreover, we believe that the relative distances between two target entities can help us better judge the relations. Empirically, we use the relative distances between the first mentions of the two entities as the relative distances between two target entities. Finally, all entity-level inference representations in different latent space and the relative distance embeddings are fed into a feed-forward neural network (FFNN) to form the final entity-level inference information:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "here G e is a FFNN with ReLU activation function, M is an embedding matrix, d ab and d ba are the relative distances between E a and E b in the document. I e \u2208 R d describes relation features between E a and E b at entity level.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity-Level Inference Module"
        },
        {
            "text": "In this section, we propose a hierarchical inference mechanism, inference information is aggregated from entity level to sentence level and then to document level.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "In this way, our model can aggregate all useful information of the document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "Sentence-Level Inference. Assume that a document contains L sentences, and w jt represent the t-th word in the j-th sentence. Given the j-th sentence S j , to represent words in its context, the sentence is fed into a BiLSTM encoder:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "(6) Since different words in a sentence are differentially informative, inspired by [14] , we introduce the vanilla attention mechanism to enable our model to selectively assign higher weights for the informative words and lower weights for the other words. Then we aggregate the representations of those informative words to form a sentence vector. Specifically, \u03b1jt = u w tanh (Wwhjt + bw)",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 88,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "where u w , b w \u2208 R d and W w \u2208 R d\u00d7d are learnable parameters. Word hidden state h jt \u2208 R d is first fed through a one-layer MLP, then we obtain weights of words by measuring \"which words are more related to the target entities\". Finally, we compute the sentence vector S j as a weighted sum of the word hidden states. For obtaining the sentence-level inference information, we adopt a semantic matching method which is used in previous NLI model [2] . Through comparing sentence vector S j with entity-level inference representation I e , we can derive sentence-level inference representation I sj for the j-th sentence:",
            "cite_spans": [
                {
                    "start": 448,
                    "end": 451,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "where G s is FFNN with ReLU function, a matching trick with elementwise subtraction and multiplication is used for building better matching representations [10] . I sj represents the inference information derived from the j-th sentence.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 160,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "Document-Level Inference. In order to distinguish crucial sentence-level inference information for overall document-level inference representation, vanilla attention mechanism is again used. We build a BiLSTM followed by the attention network on top of the sentence-level inference vectors (I s ) to aggregate all essential evidence information scattered in different sentences:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "\u03b1j = u s tanh (Wscsj + bs) (12) ",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "text": "(12)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "here u s , b s \u2208 R d and W s \u2208 R d\u00d7d are learnable parameters, I d \u2208 R d is the document-level inference representation which represents all the inference information that we can obtain from the document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hierarchical Document-Level Inference Module"
        },
        {
            "text": "To better integrate inference information of different granularity, we concatenate entity-level inference representation I e and document-level inference representation I d together to form the final inference representation. Since there are often multiple relations holding between an entity pair, we use a FFNN with the sigmoid function to calculate the probability of each relation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prediction Layer"
        },
        {
            "text": "where W r , b r are the weight matrix and bias for the linear transformation. A binary label vector y is set to indicate the set of true relations holding between the entity pair, where 1 means an relation is in the set, and 0 otherwise. In our experiments, we use the binary cross entropy (BCE) as training loss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prediction Layer"
        },
        {
            "text": "where y r \u2208 {0, 1} is the true value on label r and l is the number of relations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prediction Layer"
        },
        {
            "text": "Given a document, we rank the predicted results by their confidence and traverse this list from top to bottom by F1 score on dev set, the probability value corresponding to the maximum F1 is picked as threshold \u03b4. This threshold is used to control the number of extracted relational facts on test set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Prediction Layer"
        },
        {
            "text": "To evaluate the effectiveness of our model, we use the DocRED dataset [15] , which is the largest human-annotated document-level RE dataset constructed from Wikidata and Wikipedia. DocRED contains over 5,053 documents, 40,276 sentences, 132,375 entities and 96 frequent relation types. Entity types in DocRED are annotated. It is also introduced by the author of DocRED that about 40.7% of relational facts can only be extracted from multiple sentences and 61.1% relational instances require a variety of reasoning.",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 74,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "We compare our model against the following document-level RE baselines:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison Models and Evaluation Metrics"
        },
        {
            "text": "CNN/LSTM/BiLSTM-RE: They first encode a document into a hidden state vector sequence with CNN/LSTM/BiLSTM as encoder, and then predict relations for each entity pair by feeding them into a bilinear function [15] . Context-Aware: It uses an LSTM-based encoder to jointly learn representations for all relations in the context, and then combines other context relations with target relation to make the final prediction [12] .",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 418,
                    "end": 422,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Comparison Models and Evaluation Metrics"
        },
        {
            "text": "It uses BERT to encode the document, entities are represented by their average word embedding. A BiLinear layer is applied to predict the relation between entity pairs [13] .",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 172,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "BERT-RE:"
        },
        {
            "text": "Step: Based on BERT-RE, it models the document-level RE through a two-step process. The first step is to predict whether or not two entities have a relation, the second step is to predict the specific relation [13] .",
            "cite_spans": [
                {
                    "start": 210,
                    "end": 214,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "BERT-Two-"
        },
        {
            "text": "This is the main model of this paper. Multi-granularity inference information is used to better model complex interactions between entities. The widely used metric F1 is used in our experiments. Moreover, since some relational facts present in both training and dev/test sets, we also report the F1 excluding those relational facts and denote it as Ign F1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "HIN:"
        },
        {
            "text": "We try two embedding methods in our experiments: 100-dimensional GloVe [11] embeddings and BERT representations [3] . For the BERT representations, the base uncased English model with dimension 768 is used, we map word representations into 100 dimensional vectors by a linear projection layer. Once the word representations are initialized, they are fixed during training. The embedding dimensions of coreference, distance and entity type are all set to be 20. For LSTM encoder, the dimension of the hidden units is 128. The number of latent space is 2. Furthermore, we regularize our network using dropout and the dropout ratio is 0.2. We optimized our model using Adam [5] , with learning rate of 10 \u22124 , \u03b2 1 = 0.9, \u03b2 2 = 0.999. The batch size is set to be 12 and the value of threshold \u03b4 is determined by the performance on the dev set.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 112,
                    "end": 115,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 671,
                    "end": 674,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Implementation Details"
        },
        {
            "text": "Overall Performance. Experimental results are shown in Table 1 . From the results, we can observe that: (1) Compared with BiLSTM-RE, the state-of-theart model without BERT, our HIN-GloVe achieves significant improvements of 2.24% in F1, we claim that it is mainly due to the reasoning mechanism and hierarchical aggregation structure in HIN, which will be further discussed in ablation study. (2) Even though BERT based models provides strong prediction power, HIN-BERT consistently improves over them, which further proves the effectiveness of our hierarchical inference network. (3) Although Context-Aware model combines context relations with the target relation, it can't use the evidence information in document as effectively as HIN. Hence our model also outperforms it by 2.60% in F1. (4) BERT representations further boost the performance of our model, the HIN-BERT approach outperforms all these previous methods, which indicates the importance of prior knowledge. Ablation Study. To study the contribution of each component in HIN-BERT, we run an ablation study on DocRED dev set (see Table 2 ). From these ablations, we find that: (1) When we remove the translation mechanism and bilinear transformation, F1 score drops by 1.21% and 2.02% respectively, which indicates that these two transformations can enhance the expression ability of HIN at the entity level. (2) Removing the multi-space projection hurts the result by 1.72%, which proves that it is beneficial to allow the model to jointly attend to information from different representation subspaces. (3) F1 drops by 1.25% when we remove the sentence-level inference mechanism, i.e., replacing the sentence-level inference vector with sentence vector. (4) F1 drops by 2.81% when we discard the hierarchical aggregation approach. Instead, we run BiLSTM followed by meanpooling layer over the whole document to get the document vector. (5) We also observe that F1 drops by 4.21% when we discard the above all factors together.",
            "cite_spans": [
                {
                    "start": 1902,
                    "end": 1905,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 55,
                    "end": 62,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1095,
                    "end": 1102,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Results and Analyses"
        },
        {
            "text": "In summary, all components play an important role in our model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results and Analyses"
        },
        {
            "text": "As we discussed before, it is challenging for document-level RE to reason from multiple sentences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis by the Number of Supporting Sentences."
        },
        {
            "text": "To further prove the effectiveness of HIN, we analyze the recall on relational facts with different number of supporting sentences here. 1 As shown in Fig. 3 , we find that our model always performs better than other baselines, especially when the number of supporting sentences increases gradually. More specifically, HIN-GloVe even outperforms BERT-RE when the number of supporting sentences exceeds 4, which fully proves the superiority of HIN. Note that when the number of supporting sentences exceeds 7, HIN-GloVe and other baselines behave the same. We think this is because there are very few samples with more than 7 supporting sentences in dev set. We believe when the number of relational facts with more supporting sentences increase our model will achieve better results. Case Study. We compare our model with BERT-RE on some cases from dev set, as shown in Table 3 . (1) Example 1 represents the situation that logical reasoning is required. Specifically, in order to identify the relational fact, we have to first identify the fact that Galaxy S series is a line of Samsung from sentence 0 and 2, then identify the fact Samsung Galaxy S9 is the latest smartphones in the Galaxy S series from sentence 4. We explain that our model uses a hierarchical aggregation approach to collect inference information from multiple sentences, so that it can better deal with this complex inter-sentence relationship. (2) Example 2 represents the case of coreference reasoning. In this situation, we claim that the attention and reasoning mechanisms in sentence-level inference module can help us to identify that \"He\" refers to Robert Kingsbury Huntington in sentence 3. In the end, our model can identify the right relation while BERT-RE mistakenly assumes that Los Angeles is the place where Robert Kingsbury Huntington died. (3) Example 3 is a case that needs to combine context information with commonsense knowledge. Through some external common-sense knowledge, we might know that South America is a continent and S\u00e3o Paulo is a city, which is the useful information to help judge their relation. We think the problem can be solved by adding some external knowledge and we leave it as our future work.",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 138,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 151,
                    "end": 157,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 870,
                    "end": 877,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Analysis by the Number of Supporting Sentences."
        },
        {
            "text": "In recent years, more and more neural models have been applied to RE. Zeng el al. [17] employed a one-dimensional CNN with additional lexical features to encode relations. Miwa et al. [9] used LSTM with tree structures for RE. Zhou el al. [18] showed that combining CNN/RNN with attention mechanism can further improve performance. And the emergence of various optimization algorithms [6] [7] [8] makes these neural models more effective. Most existing RE work focuses on modeling within a single sentence. However, usually documents provide more information than sentences. Moving research from sentence level to document level is necessary. Recently, there has been increasing interest in document-level RE. Yao et al. [15] proposed a large-scale human-annotated document-level RE dataset, DocRED, and first compute the representations for all entities then predict relations for each entity pair by feeding them into a bilinear function. Wang et al. [13] used BERT to encode the document, it also used bilinear layer to predict the relation between entity pairs, but it modelled the document-level RE through a two-step process. Most previous methods used only entity-level information and this is not adequate. In this paper, we propose to effectively aggregate the inference information of different granularity.",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 86,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 184,
                    "end": 187,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 385,
                    "end": 388,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 389,
                    "end": 392,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 393,
                    "end": 396,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 721,
                    "end": 725,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 953,
                    "end": 957,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this paper, we proposed a Hierarchical Inference Network (HIN) for documentlevel RE. It uses a hierarchical inference method to aggregate the inference information of different granularity: entity level, sentence level and document level. We show that our method achieves state-of-the-art performance on the largest human-annotated DocRED dataset. Experimental analysis shows that both the inference mechanism and hierarchical aggregation approach in our model play an important role. In the future, we plan to incorporate external knowledge to further improve the proposed model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Translating embeddings for modeling multi-relational data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bordes",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Usunier",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garcia-Duran",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Yakhnenko",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Enhanced LSTM for natural language inference",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ling",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Inkpen",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1609.06038"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Bert: pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Hierarchical relation extraction with coarse-to-fine grained attention",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Adam: a method for stochastic optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.6980"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Lingo: linearized Grassmannian optimization for nuclear norm minimization",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Riemannian submanifold tracking on low-rank algebraic variety",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Learning robust low-rank approximation for crowdsourcing on Riemannian manifold",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "End-to-end relation extraction using LSTMS on sequences and tree structures",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Miwa",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bansal",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1601.00770"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Natural language inference by tree-based convolution and heuristic matching",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mou",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Glove: global vectors for word representation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Context-aware representations for knowledge base relation extraction",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sorokin",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Gurevych",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Fine-tune Bert for Docred with two-step process",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Focke",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sylvester",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Mishra",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1909.11898"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Hierarchical attention networks for document classification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dyer",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Hovy",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Docred: a large-scale document-level relation extraction dataset",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Beyond word attention: using segment attention in neural relation extraction",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Relation classification via convolutional deep neural network",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Attention-based bidirectional long short-term memory networks for relation classification",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An example from DocRED. Each document in DocRED is annotated with named entity mentions, coreference information, relations, and supporting sentences.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The overall architecture of the Hierarchical Inference Network(HIN)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Recall of models on relational facts with different number of supporting sentences. Numbers in parentheses represent the number of relational facts with different number of supporting sentences in dev set.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Performance of different models on DocRED (%).",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Results of ablation study (%).",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The results predicted by BERT-RE and HIN-BERT. The reasoning type of each example is different and the first row for each example is the input document. The head , tail , relation and supporting sentences are colored accordingly.The Galaxy S series is a line of Samsung Electronics, a division of Samsung [2] Galaxy S line has ... being Samsung 's flagship smartphones. [4] the latest smartphones in Galaxy S series are the Samsung Galaxy S9 ... Lable: manufacturer BERT-RE: None HIN-BERT: manufacturer Robert Kingsbury Huntington, was a naval aircrewman and member of Torpedo Squadron 8. [2] ... Huntington was shot down during the Battle of Midway ... [3] He was born in Los Angeles, California ... Lable: birth place BERT-RE: death place HIN-BERT: birth place IBM Research -Brazil is one of twelve research laboratories comprising IBM Research, its first in South America. [1] It was established in June 2010, with locations in S\u00e3o Paulo and Rio de Janeiro ...",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}