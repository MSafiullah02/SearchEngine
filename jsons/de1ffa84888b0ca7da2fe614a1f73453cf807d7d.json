{
    "paper_id": "de1ffa84888b0ca7da2fe614a1f73453cf807d7d",
    "metadata": {
        "title": "BERT for Evidence Retrieval and Claim Verification",
        "authors": [
            {
                "first": "Amir",
                "middle": [],
                "last": "Soleimani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Amsterdam",
                    "location": {
                        "settlement": "Amsterdam",
                        "country": "The Netherlands"
                    }
                },
                "email": "a.soleimani@uva.nl"
            },
            {
                "first": "Christof",
                "middle": [],
                "last": "Monz",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Amsterdam",
                    "location": {
                        "settlement": "Amsterdam",
                        "country": "The Netherlands"
                    }
                },
                "email": "c.monz@uva.nl"
            },
            {
                "first": "Marcel",
                "middle": [],
                "last": "Worring",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Amsterdam",
                    "location": {
                        "settlement": "Amsterdam",
                        "country": "The Netherlands"
                    }
                },
                "email": "m.worring@uva.nl"
            }
        ]
    },
    "abstract": [
        {
            "text": "We investigate BERT in an evidence retrieval and claim verification pipeline for the task of evidence-based claim verification. To this end, we propose to use two BERT models, one for retrieving evidence sentences supporting or rejecting claims, and another for verifying claims based on the retrieved evidence sentences. To train the BERT retrieval system, we use pointwise and pairwise loss functions and examine the effect of hard negative mining. Our system achieves a new state of the art recall of 87.1 for retrieving evidence sentences out of the FEVER dataset 50K Wikipedia pages, and scores second in the leaderboard with the FEVER score of 69.7.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The constantly growing online textual information has been accompanied by an increasing spread of false claims. Therefore, there is a need for automatic claim verification and fact-checking. The Fact Extraction and VERification (FEVER) shared task [14] introduces a benchmark for evidence-based claim verification, making it possible to integrate information retrieval and natural language inference components. FEVER consists of 185 K claims labelled as 'Supported', 'Refuted' or 'NotEnoughInfo' ('NEI') based on a 50K Wikipedia pages dump. The task is to classify the claims and extract the corresponding evidence sentences (see Fig. 1 ). To evaluate the retrieval and verification performance together, FEVER score is defined as label accuracy conditioned on providing evidence sentence(s) unless the label is 'NEI'.",
            "cite_spans": [
                {
                    "start": 248,
                    "end": 252,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 631,
                    "end": 637,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Verifying a claim based on 50K pages is a computational challenge and can be alleviated by a multi-step pipeline. Most work [7] [8] [9] 16] on FEVER has adopted a three-step pipeline. (1) Document Retrieval: a set of documents, which possibly contain relevant information to support or reject a claim, are retrieved; (2) Sentence Retrieval: five sentences are extracted out of the retrieved documents;",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 127,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 128,
                    "end": 131,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 132,
                    "end": 135,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 136,
                    "end": 139,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 317,
                    "end": 320,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(3) Claim Verification: the claim is verified against the retrieved sentences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Pre-trained language models, particularly Bidirectional Encoder Representations from Transformers (BERT) [6] has significantly advanced the performance This research was partly supported by VIVAT. in a wide variety of information retrieval and natural language processing tasks including passage re-ranking [2] , question answering [1, 6] , and question retrieval [12] .",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 108,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 307,
                    "end": 310,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 332,
                    "end": 335,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 336,
                    "end": 338,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 364,
                    "end": 368,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we examine BERT for evidence-based claim verification in a three-step pipeline. A first BERT model is trained to retrieve evidence sentences. We compare pointwise cross entropy loss and pairwise Hinge loss and Ranknet loss [3] for the BERT evidence retrieval. We also investigate the effect of Hard Negative Mining (HNM), which means training on harder negative samples. Next, we train another BERT model to verify claims against the retrieved evidence. The code is available online 1 .",
            "cite_spans": [
                {
                    "start": 238,
                    "end": 241,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In summary, our contributions are as follows: (1) To the best of our knowledge, we are the first to use BERT for evidence retrieval and claim verification;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "(2) We compare pointwise and pairwise loss functions for training the BERT sentence retrieval; (3) We investigate and employ HNM to improve the retrieval performance; (4) We achieve second rank in the FEVER leaderboard.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Thorne et al. [14] shortlists the k-nearest documents based on TF-IDF features similar to DrQA [4] . UCL [16] detects the pages titles in the claims and rank pages by logistic regression. UKP-Athene [7] , the highest document retrieval scoring system, uses MediaWiki API 2 to search Wikipedia for the claims noun phrases.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 18,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 95,
                    "end": 98,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 105,
                    "end": 109,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 199,
                    "end": 202,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "To extract evidence sentences, Thorne et al. [14] use a TF-IDF approach similar to their document retrieval. UCL [16] trains a logistic regression model on a heuristically set of features. Enhanced Sequential Inference Model (ESIM) [5] has been used in [7, 9] . ESIM uses two BiLSTMs and the co-attention mechanism to classify a hypothesis based on a premise.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 113,
                    "end": 117,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 232,
                    "end": 235,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 253,
                    "end": 256,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 257,
                    "end": 259,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Decomposable attention [10] is used by Thorne et al. [14] for claim verification, which compares and aggregates soft-aligned words in sentences. In [8] , transformer networks pre-trained on language generation [11] are employed.",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 27,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 53,
                    "end": 57,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 148,
                    "end": 151,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 210,
                    "end": 214,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "ESIM has been widely used for this step [7, 9, 16] . UNC [9] , proposes a modified ESIM that takes the concatenation of the retrieved sentences and claim along with ELMo embedding. Very recently, Dream [17] published the state of the art FEVER score using a graph reasoning module. It uses pre-trained XLNet [15] to just calculate contextual word embedding without fine-tuning. However, we show that using BERT as retrieval and verification components without any additional modules achieves comparable results.",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 43,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 44,
                    "end": 46,
                    "text": "9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 47,
                    "end": 50,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 57,
                    "end": 60,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 202,
                    "end": 206,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 308,
                    "end": 312,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The goal is to classify the claim c l for l = 1, ... , N C = 145K as 'Supported', 'Refuted', or 'NEI'. For a prediction to be considered correct, a complete set of ground-truth evidence must be retrieved for the claim c l . The 'NEI' labels do not have an evidence set. We explain the proposed system in the three-step pipeline: document retrieval, sentence retrieval, and claim verification. Figure 2 demonstrates the proposed BERT architectures for the second and third steps. We adopt the pre-trained BERT model and fine-tune using two different pointwise and pairwise approaches. By default, we use the BERT base (12 layers) in all the experiments. In order to compensate for the missed co-reference pronouns in the sentences [8] , we add the page titles at the beginning of sentences. We use a batch size of 32, a learning rate of 2e \u2212 5, and one epoch of training.",
            "cite_spans": [
                {
                    "start": 730,
                    "end": 733,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 393,
                    "end": 401,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "FEVER provides"
        },
        {
            "text": "In the pointwise approach, we use cross entropy loss, and every single input is classified as evidence or non-evidence. At testing time, S c l top sentences are selected by their probability values. Alternatively, a threshold can also be used on the values to filter out uncertain results and trade-off the recall against the precision.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "FEVER provides"
        },
        {
            "text": "In the pairwise approach, a pair of positive and negative samples are compared against each other (Fig. 2 (right) ) using the Ranknet loss function [3] . We do not force the positive and negative samples to be selected from the same claims because the number of sentences per claim is significantly different and this results in oversampling sentences from the claims with limited sentences. In addition, we experiment with the modified Hinge loss functions like [7] . At testing time, for both pairwise loss functions, the top five sentences S c l top are selected based on their output probability values.",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 151,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 463,
                    "end": 466,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 98,
                    "end": 113,
                    "text": "(Fig. 2 (right)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "FEVER provides"
        },
        {
            "text": "The ratio of negative to positive sentences is high, thus it is not reasonable to train on all the negative samples. Random sampling limits the number of negative samples, however, this leads to training on trivial samples. Similar to [13] , we focus on online HNM. We fix the positive samples batch size of 16 but heuristically increase negative batch from 16 to 64 and train on the positive samples and only the 16 negative samples with the highest loss values. In the case of pairwise retrieval, HNM selects the 32 hardest pairs out of 128 pairs. Loss values are computed in the no-gradient mode, and thus there is no need for more GPUs than normal training without HNM. In the claim verification step, each claim c l is compared against S c l top and the final claim classification label is determined by aggregating the five individual decisions. Like [8] , the default label is 'NEI' unless there is any supporting evidence to predict the claim label as 'Supported'. If there is at least one rejecting evidence and no supporting fact, the label is 'Refuted'. We train a pre-trained BERT as a three-class classifier (Fig. 2(left) ). We train the model on 722K evidence-claim pairs provided by the first two steps. We use the batch size of 32, the learning rate of 2e\u22125, and two epochs of training. Table 1 compares the development set performance of different retrieval variants. It indicates that both pointwise and pairwise BERT sentence retrieval improve the recall. Note that although precision and F1 are of value, as discussed by UNC [9] , recall is the most important factor because the retrieval predictions are the samples that the verification system is trained on, and low recall leaves many claims with no probable evidence. Additionally, recall is weighted by the FEVER score requiring evidence for 'Supported' and 'Refuted' claims. A threshold can also regulate the recall and precision and shows that our method can achieve the best precision and F1 too. We opt to focus more on recall and train the claim verification model on the predictions with the maximum recall. Although the pairwise Ranknet with HNM marginally has the best recall, we cannot conclude that it is necessarily better for this task. This is more clear by a trade-off between the precision and recall displayed in Fig. 3 . The pointwise methods surpass the pairwise methods in terms of recall-precision performance. It also shows that HNM enhances both Ranknet and Hinge pairwise and preserves the pointwise performance. Table 2 compares the development set results of the previous methods with the BERT models. The BERT claim verification even if it is trained on the UKP-Athene sentence retrieval predictions, the previous method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT retrieval predictions significantly enhances the verification because while it improves the FEVER score by providing more correct evidence, it provides a better training set for the verification system. It also shows that pointwise retrieval leads to more accurate claim verification. The large BERTs are only trained on the best retrieval systems, and as expected significantly improve the performance. Finally, we report the blind test set results in Table 3 using the FEVER leaderboard 3 . Our best model ranks at the second place that indicates the importance of using pre-trained language models for both sentence retrieval and claim verification.",
            "cite_spans": [
                {
                    "start": 235,
                    "end": 239,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 857,
                    "end": 860,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 1121,
                    "end": 1134,
                    "text": "(Fig. 2(left)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1303,
                    "end": 1310,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 2304,
                    "end": 2310,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 2511,
                    "end": 2518,
                    "text": "Table 2",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 3273,
                    "end": 3280,
                    "text": "Table 3",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "FEVER provides"
        },
        {
            "text": "We demonstrated the BERT promising performance for the sentence retrieval and claim verification pipeline. In the retrieval step, we compared the pointwise and pairwise approaches and concluded that although the pairwise Ranknet approach achieved the highest recall, pairwise approaches are not necessarily superior to the pointwise approach particularly if precision is taken into account. Our results showed that training BERT on the pointwise retrieved sentences results in a better performance. We also examined HNM for training the retrieval systems and showed that it improves the retrieval and verification performance. We suspect that HNM can also make the training faster and leave the investigation for future work. Furthermore, using BERT as an end-to-end framework for the entire evidence-based claim verification pipeline can be investigated in the future.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A BERT baseline for the natural questions",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Alberti",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Collins",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1901.08634"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "MS MARCO: a human generated machine reading comprehension dataset",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bajaj",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.09268"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Learning to rank using gradient descent",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Burges",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of the 22nd International Conference on Machine learning",
            "volume": "",
            "issn": "",
            "pages": "89--96",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Reading Wikipedia to answer opendomain questions",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fisch",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bordes",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
            "volume": "1",
            "issn": "",
            "pages": "1870--1879",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Enhancing and combining sequential and tree LSTM for natural language inference",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ling",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1609.06038"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.04805"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "UKP-Athene: multi-sentence textual entailment for claim verification",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hanselowski",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)",
            "volume": "",
            "issn": "",
            "pages": "103--108",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/W18-5516"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Team Papelo: transformer networks at FEVER",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Malon",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)",
            "volume": "",
            "issn": "",
            "pages": "109--113",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/W18-5517"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Combining fact extraction and verification with neural semantic matching networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bansal",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "33",
            "issn": "",
            "pages": "6859--6866",
            "other_ids": {
                "DOI": [
                    "10.1609/aaai.v33i01.33016859"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A decomposable attention model for natural language inference",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Parikh",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "T\u00e4ckstr\u00f6m",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1606.01933"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Improving language understanding by generative pre-training",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Narasimhan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Salimans",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "FAQ retrieval using queryquestion similarity and BERT-based query-answer relevance",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Sakata",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Shibata",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tanaka",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kurohashi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "SIGIR 2019: 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "1113--1116",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "FaceNet: a unified embedding for face recognition and clustering",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schroff",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kalenichenko",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Philbin",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "815--823",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "FEVER: a large-scale dataset for fact extraction and verification",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Thorne",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vlachos",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Christodoulopoulos",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1803.05355"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "XLNet: generalized autoregressive pretraining for language understanding",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Carbonell",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.08237"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "UCL machine reading group: four factor framework for fact finding (HexaF)",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Yoneda",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Welbl",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Stenetorp",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Riedel",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)",
            "volume": "",
            "issn": "",
            "pages": "97--102",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/W18-5515"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Reasoning over semantic-level graph for fact checking",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1909.03745"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Pointwise sentence retrieval and claim verification (left), Pairwise sentence retrieval (right). Orange boxes indicate the last hidden state of the [CLS] token. (Color figure online)Following the UKP-Athene promising document retrieval component[7] (MediaWiki API), which results in more than 93% document recall, we use their method to collect a set of top documents D c l top for the claim c l . We use all the retrieved documents as D c l top . The sentence retrieval step extracts the top five evidence sentences S c l top . The training set consists of claims and the sentences from D c l top corresponding to c l (S c l all = {S di |d i \u2208 D c l top }). BERT is a multi-layer transformer pre-trained on next sentence prediction and masked word prediction using extremely large datasets. BERT takes the input with a special classification embedding ([CLS]) followed by the tokens representations of the first and second sentences separated by another specific token ([SEP]). To use BERT for classification, a softmax is added on the last hidden state of the classification token ([CLS]) and trained together with the pre-trained layers.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Recall and precision results on the development set.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Claim: Roman Atwood is a content creator. (Supported) Evidence: [wiki/Roman_Atwood] He is best known for his vlogs, where he posts updates about his life on a daily basis. Claim: Furia is adapted from a short story by Anna Politkovskaya. (Refuted) Evidence: [wiki/Furia_(film)] Furia is a 1999 French romantic drama film directed by Alexandre Aja, ..., adapted from the science fiction short story Graffiti by Afghanistan is the source of the Kushan dynasty. (NotEnoughInfo) Fig. 1. Three examples from the FEVER dataset[14].",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Development set sentence retrieval performance. For * we calculated the scores using the official code, and for ** we used the F1 formula to calculate the score.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Development set verification scores.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Results on the test set (October 2019).",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}