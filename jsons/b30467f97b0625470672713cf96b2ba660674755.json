{
    "paper_id": "b30467f97b0625470672713cf96b2ba660674755",
    "metadata": {
        "title": "Clustering volatility regimes for dynamic trading strategies",
        "authors": [
            {
                "first": "Gilad",
                "middle": [],
                "last": "Francis",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "NSW",
                    "location": {
                        "country": "Australia"
                    }
                },
                "email": ""
            },
            {
                "first": "Nick",
                "middle": [],
                "last": "James",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "NSW",
                    "location": {
                        "country": "Australia"
                    }
                },
                "email": ""
            },
            {
                "first": "Max",
                "middle": [],
                "last": "Menzies",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Tsinghua University",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Arjun",
                "middle": [],
                "last": "Prakash",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "NSW",
                    "location": {
                        "country": "Australia"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We develop a new method to find the number of volatility regimes in a non-stationary financial time series. We use change point detection to partition a time series into locally stationary segments, then estimate the distributions of each piece. The distributions are clustered into a learned number of discrete volatility regimes via an optimisation routine. Using this method, we investigate and determine a clustering structure for indices, large cap equities and exchangetraded funds. Finally, we create and validate a dynamic portfolio allocation strategy that learns the optimal match between the current distribution of a time series with its past regimes, thereby making online risk-avoidance decisions in the present.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Modelling the volatility of a financial time series is an important task for traders and economists. Financial markets are not only important in their own right, but also have immense flow on effects on the rest of society, as seen during the global financial crisis, US-China trade war or current COVID-19 pandemic. Volatility may be modelled from an individual stock level to an index level; the latter can represent the uncertainty of an entire sector or economy. It has been surmised that financial time series exhibit regime switching patterns, switching between periods of heightened volatility and ease [1, 2, 3] .",
            "cite_spans": [
                {
                    "start": 610,
                    "end": 613,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 614,
                    "end": 616,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 617,
                    "end": 619,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Statistical methods for volatility modelling have long been popular in the literature [4] . Long standing parametric methods such as ARCH and GARCH [5, 6] model the volatility of individual stocks, designing models in order to obey assumptions such as stylized facts [7] , and appropriately choosing parameters to best fit past data. This allows traders to model future returns, assuming these assumptions continue to hold. Regime switching models [1, 8] have been developed to model the patterns of volatility switching; these are also generally parametric, building on ARCH and GARCH, and must a priori estimate the number of regimes. In this paper, we introduce a new non-parametric method to analyse the number and switching behaviour of volatility regimes, making no assumptions about the data or underlying context.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 89,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 148,
                    "end": 151,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 152,
                    "end": 154,
                    "text": "6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 267,
                    "end": 270,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 448,
                    "end": 451,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 452,
                    "end": 454,
                    "text": "8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We begin by decomposing our data into locally stationary segments, via change point detection. Developed by Hawkins et al. [9, 10] , change point algorithms seek to determine breaks in a time series at which the stochastic properties of the underlying random variables change, and have become instrumental in time series analysis. Specifically, in order to analyse volatility, we use the Mood test for variance, Section 4 of [11] .",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 126,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 127,
                    "end": 130,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 425,
                    "end": 429,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Having determined structural breaks, we associate to each segment of the partitioned time series a distribution. Then, we use the Wasserstein metric to compute the distances between these distributions and use spectral clustering to allocate these segments into specific classes of volatility regimes. The precise number of regimes is carefully chosen. Thus, our method can determine the number of regimes to use in any candidate regime switching model in advance. We then draw on our findings and use additional learning procedures to design a dynamic trading strategy. We show that it provides superior risk-adjusted returns to the S&P 500 index in various market conditions. This improves on existing strategies in two ways detailed below. Our contributions are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "1. We introduce a non-parametric method for detection and classification of volatility regimes, including the number of regimes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "2. We include validation scores for our methodology, and demonstrate good results for synthetic data and real data across a variety of asset classes that match well with known periods of higher volatility.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "3. We develop a new dynamic trading strategy that is able to identify volatile time periods and allocate capital in real-time. By learning the past volatility structure of the S&P 500, we determine whether the present time period is volatile based on the minimal distance to other past distributions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "4. This improves on existing methods in two ways. First, it is more reliable than simply switching at a detected change point [12] , as a change point may not indicate a change in volatility regime. Secondly, we optimise the time period of how long to look back; a change point algorithm also has a detection delay, but it cannot be controlled.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 130,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In Section 2, we outline all steps of our methodology in technical detail. In Section 3, we validate our methodology on synthetic data, and then show a reasonable clustering structure can be determined for major indices, stocks, and popular ETFs. In Section 4, we develop our dynamic portfolio allocation trading strategy, incorporating our insights and additional learning procedures. Section 5 concludes the body of the paper. In Appendix A and Appendix B respectively, we provide a description of the change point algorithm used and provide additional figures from our experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In mathematical statistics, a time series (X t ) is a sequence of random variables -measurable functions from a probability space to the real numbersindexed by time. In finance, one generally conflates the random variable with the observed data point at each point in time. As such, a financial time series is a sequence of price data. In this paper, we will examine the time series of adjusted closing prices (p t ) t\u22650 at time t, and the log returns (R t ) t\u22651 = log( pt pt\u22121 ). In subsequent sections, we describe our method in detail. We begin by assuming our non-stationary time series are generated from Dahlhaus locally stationary processes [13] and proceed to partition the time series into stationary segments; specifically, we detect changes in the volatility of a time series via the Mood test change point method. We then estimate the distribution of each segment via kernel density estimation, and use the Wasserstein metric to quantify distance between these distributions. We determine an allocation into an appropriate number of clusters by an optimisation routine that combines spectral clustering and silhouette scoring. Thus, we classify our segments of volatility into discrete classes in a non-parametric way. We record the number of clusters and their structure, together with the silhouette score as a means of validating the allocation into these volatility regimes.",
            "cite_spans": [
                {
                    "start": 648,
                    "end": 652,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Mathematical model"
        },
        {
            "text": "The precise method, applicable to volatility clustering, that we describe below, is not exhaustive. As long as there is consistency between the regime characteristic of interest, the change point algorithm (and its test statistic if applicable), and the distance metric between distributions, the method below could easily be reworked for detection and classification of regimes of alternative characteristics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mathematical model"
        },
        {
            "text": "Given time series price data, begin by forming the log return time series (R t ) t=1,...,T over a particular time interval. It is generally appropriate to assume the log returns are independent random variables, but not appropriate to assume they have any particular distribution. With this in mind, we apply the nonparametric Mood test, performed in the CPM package of Ross [14] , to detect changes in the volatility of a time series. Although this is commonly known as a median test, it is also appropriate for detecting change in the variance between two distributions, as described in Section 4 of [11] . More details on the change point framework and implementation can be found in Appendix A. This yields a collection of change points \u03c4 1 , ..., \u03c4 m\u22121 . For notational convenience, set \u03c4 0 = 1, \u03c4 m = T . The stationary segments according to this partition are then",
            "cite_spans": [
                {
                    "start": 375,
                    "end": 379,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 602,
                    "end": 606,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Partition of the time series"
        },
        {
            "text": "This yields m stationary segments. Now let (Y (j) ) be the restricted time series whose entries are taken from the time interval [\u03c4 j\u22121 , \u03c4 j ]. That is, (Y (j) t ) consists of the values R t where t ranges from \u03c4 j\u22121 to \u03c4 j . Each (Y (j) ) has been determined by the algorithm to be sampled from a consistent distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Partition of the time series"
        },
        {
            "text": "Next, for each stationary segment (Y (j) ), we perform kernel density estimation to estimate the probability density function of the underlying distribution. In general, given data points (x 1 , x 2 , ..., x n ) drawn from some arbitrary data generating process, the KDE is given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernel density estimation"
        },
        {
            "text": "K is a kernel function, h is a smoothing parameter. We use a Gaussian kernel for K, [15] and the Silverman rule of thumb [16] to choose h.",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 88,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 121,
                    "end": 125,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Kernel density estimation"
        },
        {
            "text": "With this procedure, we associate to each restricted time series (Y (j) ) a kernel density function f (j) , j = 1, ..., m.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kernel density estimation"
        },
        {
            "text": "Next, we compute the Wasserstein distance between these kernel density functions f (j) . The Wasserstein metric, also known as the earth mover's distance, is the minimal work to move the mass of one probability distribution into another. Given probability measures \u00b5, \u03bd on Euclidean space R d , define",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Wasserstein distance"
        },
        {
            "text": "This infimum is taken over all joint probability measures \u03b3 on R d \u00d7 R d with marginal probability measures \u00b5 and \u03bd. In our case, d = 1. To each kernel density estimate function f (j) , we form the associated Radon-Nikodym measure [17] \u00b5 j = f (j) (x)dx, where dx is Lebesgue measure. This allows us to form a m \u00d7 m distance matrix of Wasserstein distances.",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 235,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Wasserstein distance"
        },
        {
            "text": "Henceforth, set p = 1. Concretely, W p (f (x)dx, g(x)dx) may be computed [18] as",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 77,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Wasserstein distance"
        },
        {
            "text": "where F, G are the cumulative density functions associated to probability density functions f and g respectively. Thus, we produce an m \u00d7 m distance matrix D between the m distributions of each locally stationary segment of the log return time series. The Wasserstein metric is continuous with respect to small perturbations in the probability density functions, so small changes to the kernel density estimates through choices of h and K above affect the distances only slightly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Wasserstein distance"
        },
        {
            "text": "To the distance matrix D we associate an affinity matrix A by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral clustering"
        },
        {
            "text": "where \u03c3 is a parameter to be chosen. One then forms the Laplacian L and normalized Laplacian L sym following [19] . First, form the diagonal degree matrix given by Deg ii = j A ij . Then form",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Spectral clustering"
        },
        {
            "text": "Note L, L sym are m \u00d7 m symmetric matrices, and hence are diagonalizeable with all real eigenvalues. By the definition of L and the normalization L sym , all their eigenvalues are non-negative, 0 = \u03bb 1 \u2264 ... \u2264 \u03bb m . Spectral clustering proceeds as follows. For some fixed choice of k, compute the normalized eigenvectors u 1 , ..., u k corresponding to the k smallest eigenvalues of L sym . Form the matrix U \u2208 R m\u00d7k whose columns are u 1 , ..., u k . Let v i \u2208 R k be the rows of U , i = 1, ..., m. Cluster these rows into clusters C 1 , ..., C k according to k-means. Finally, output clusters A l = {i : v i \u2208 C l }, l = 1, ..., k to assign the original m elements, in this case segment KDEs, into the corresponding clusters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Spectral clustering"
        },
        {
            "text": "Spectral clustering has a uniquely determined output (in the absence of degeneracy) given a fixed k, but the choice of optimal k is a problem with no definitive answer. We introduce the concept of silhouette scoring [20] . Suppose m data nodes indexed 1, 2, ..., m have been sorted into k clusters C l , l = 1, ..., k. Following the notation of the previous sections, let D ij be the distances between these nodes. For a node i in cluster C, define an internal cluster distance by",
            "cite_spans": [
                {
                    "start": 216,
                    "end": 220,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Choice of k and silhouette scoring"
        },
        {
            "text": "Next, define b(i) as the minimal dissimilarity between node i \u2208 C and any different cluster C ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of k and silhouette scoring"
        },
        {
            "text": "The silhouette score for the point i is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of k and silhouette scoring"
        },
        {
            "text": "Finally, the overall silhouette score of the clustering is simply the average s = 1 m m i=1 s(i). Note each individual s(i) ranges from \u22121 to 1. The closer it is to 1, the better matched the node i to its constituent cluster C. s(i) = 0 is poor and s(i) = \u22121 is abysmal, so the value should be as close to 1 as possible. Therefore, the final s is an overall score for the quality of the clustering. Table 1 records the interpretation of these values, as in [21] .",
            "cite_spans": [
                {
                    "start": 457,
                    "end": 461,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 399,
                    "end": 406,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Choice of k and silhouette scoring"
        },
        {
            "text": "In order to select k, we combine two methods. We begin with a standard choice of \u03c3 = 1. With this parameter set, we use the elbow method [19] within our spectral clustering implementation in order to select a first choice k 0 . We then identify the respective clusters identified relative to this value k 0 . Then, we use this initial estimate as the starting point for an optimisation routine. We vary k and \u03c3 simultaneously, at each point recording the respective cluster outputs, and calculating the total silhouette score for that clustering. We vary our parameters in the range 2 \u2264 k \u2264 5, 0.1 \u2264 \u03c3 \u2264 10, in order to optimise the silhouette score among the determined cluster outputs.",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 141,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Choice of k and silhouette scoring"
        },
        {
            "text": "Our entire method concludes with outputting the clusters as well as the silhouette score as a means of validation. Therefore, we have partitioned a time series into segments according to changes in their volatility, clustered those segments that are similar in distribution with respect to the Wasserstein metric, and included a validation metric for the quality of the clustering.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of k and silhouette scoring"
        },
        {
            "text": "Silhouette Score Interpretation 0.71 -1.00 A strong structure has been found 0.51 -0.70 A reasonable structure has been found 0.26 -0.50 A weak and possibly artificial structure has been found \u2264 0. 25 No substantial structure has been found ",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 200,
                    "text": "25",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Choice of k and silhouette scoring"
        },
        {
            "text": "In this section, we validate our method on a synthetic time series. We generate this time series, with artificially pronounced breaks in volatility, by concatenating different segments, each randomly drawn from two data generating processes and randomly chosen between 150 and 200 in length:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Synthetic data"
        },
        {
            "text": "i are added random noise to ensure none of the data generating processes are identical. This time series, together with the change point partition described in Section 2.1, is displayed in Figure 1a . In this case, the delay between change point and detection time, described in detail in Appendix A, is not visible.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 189,
                    "end": 198,
                    "text": "Figure 1a",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Synthetic data"
        },
        {
            "text": "Subsequently, we form the kernel density estimate functions, compute the Wasserstein distance, and perform the clustering of the resulting distributions. Figure 1b shows the KDEs on one plot; they have been clustered into two clusters and coloured accordingly. Figure 1c shows the final clustering of the segments of the synthetic time series. Note this whole procedure correctly identifies the change in variance, as well as the existence of two regimes (clusters) of volatility. The final silhouette score in this synthetic example is an excellent 0.91.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 154,
                    "end": 163,
                    "text": "Figure 1b",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 261,
                    "end": 270,
                    "text": "Figure 1c",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Synthetic data"
        },
        {
            "text": "In this section, we apply our method to the S&P 500, and analyse the volatility clustering in detail. We draw adjusted closing price data from Yahoo! Finance, https://finance.yahoo.com, from 1 October 2009 to 1 October 2019, and immediately calculate the log returns. We begin by forming the change point segmentation of the time series as in the previous section, and clustering the KDEs, displayed in Figures 2a and 2b respectively. Only two clusters are found, one of lower volatility, one of greater volatility. The silhouette score for this clustering is 0.63, indicating a reasonable structure has been found.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 403,
                    "end": 420,
                    "text": "Figures 2a and 2b",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "S&P 500"
        },
        {
            "text": "The blue periods of higher volatility correspond to the 2010 flash crash, the European sovereign debt crisis of August 2011, the 2015 August flash crash and the US/China trade war in 2018. In Figure 2b , these correspond to the blue kernel density estimate functions. Note these KDEs are more spread out, indicating that their corresponding distributions have much higher variance than the other cluster. Also, note that a change point is detected between the fifth and sixth segment of Figure 2c , and yet there was no regime change in volatility at this time. This can occur when the distributions are different, but not different enough to warrant an entire regime change. Understanding and being able to predict the volatility of the S&P 500 is the basis of our dynamic trading strategy, which will be described in more detail in Section 4.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 192,
                    "end": 201,
                    "text": "Figure 2b",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 487,
                    "end": 496,
                    "text": "Figure 2c",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "S&P 500"
        },
        {
            "text": "In this section, we outline the results of our methodology across various asset classes: stocks, currencies, ETFs, and indices. Once again we pull the adjusted closing price data from Yahoo! Finance from 1 October 2009 to 1 October 2019 and calculate the log returns. For each time series, the main result is the number of segments and clusters. This provides the number of discretised volatility regimes. The main evaluation metric is the silhouette score, to two significant figures. We also include the cluster sizes for completeness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Empirical results on various asset classes"
        },
        {
            "text": "We display these results in Tables 2, 3 , 4, 5. In Table 2 , the MSCI index is a weighted composite of the 1655 most valuable companies from around the world. Table 3 . In a testament to the reliability of the clustering algorithm, the choice of \u03c3 did not affect any of the cluster outputs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 28,
                    "end": 39,
                    "text": "Tables 2, 3",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 51,
                    "end": 58,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 159,
                    "end": 166,
                    "text": "Table 3",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Empirical results on various asset classes"
        },
        {
            "text": "Since the silhouette score is only a function of the cluster outputs, it was not affected by the parameter \u03c3 either. Although our methodology described in Section 2 can output the final value of \u03c3, we have omitted it from the tables below. For further reference, all plots of clustered distributions and the time series partitioned into volatility regimes can be found in Appendix B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Empirical results on various asset classes"
        },
        {
            "text": "According to [21] , any silhouette score above 0.5 indicates that a reasonable clustering structure has been found. Hence, the results are promising for indices, large equities and ETFs, which have average silhouette scores of 0.62, 0.63 and 0.60 respectively, with each individual time series among them scoring over 0.5. The results for the currency pairs are slightly weaker, with an average silhouette score of 0.49, but still, three out of five tested pairs have scores of at least 0.5. Remarkably, all time series examined have only two volatility regimes. As an aside, this is by no means inevitable; indeed, by selecting contrived values for the parameter \u03c3 such as 0.001, three volatility regimes could be identified. As in Section 3.2, note a change point does not necessarily indicate a change in volatility regime.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Though the count of distributions differs within each cluster, it is still possible to find similarities between related assets. For example, the S&P 500 and the Dow Jones both have volatile periods around March 2010, April 2011, and late 2018. In fact, all five of the listed firms registered a volatile period associated with the US/China trade war of late 2018. In contrast, ETFs do not share many volatile periods, as they are composites of different asset classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Regime switching models usually a priori assume the number of regimes, for which they are often criticised. When misspecified, they may perform badly, such as modelling three piecewise autoregressive processes with a 2-regime switching model. Interestingly, our model, which estimates the number of data generating processes flexibly, identifies a manageable number of regimes in most scenariosusually 2. This finding suggests that regime switching models may have their place in statistical modelling for financial time series, if there is a thoughtful way of estimating the number of regimes based on the data. These findings support the work of [1, 3] .",
            "cite_spans": [
                {
                    "start": 648,
                    "end": 651,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 652,
                    "end": 654,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Note that our methodology may be combined with other regime switching modelling methods. As our methodology suitably determines the number of volatility regimes, this number can then be used in any other regime switching method, which often require the number of clusters to begin with. As a further application, we show in the next section how these results can be used to make decisions about asset allocation in a dynamic trading strategy. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "In recent times, passive investing has gathered more asset inflows than active investment management. In particular, index funds and ETFs that track major indices such as the S&P 500 are a popular way of attaining broad market exposure for investors. We apply our analysis of the cluster structure of the S&P 500 index volatility to determine a dynamic trading strategy that can simultaneously benefit from the index's appreciation while minimising risk. In Section 3.2, we determined that the S&P 500 has two distinct volatility regimes, captured in two distinct clusters of volatility periods. Our contrived trading strategy is to buy and hold SPY, a tracker of the S&P 500, in low volatility periods, and then flee to the safe haven of GLD, a gold bullion tracker, in high volatility periods. Note that if our trading strategy were applied among a collection of less efficient assets, such as the index's underlying equity constituents, the trading strategy may attain greater expected returns and higher risk-adjusted return ratios. We improve on the previous work of [12] , who uses a live implementation of the rank test to move away from the S&P 500. This method has two drawbacks: first, as noted in Section 3, a change point does not necessarily indicate a change in regime; secondly, their method has an unpredictable delay in registering the change point, as discussed in Appendix A.",
            "cite_spans": [
                {
                    "start": 1072,
                    "end": 1076,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Application of results: trading strategy"
        },
        {
            "text": "Instead, we implement a dynamic procedure with a 4-year sliding window. Model parameters are learned within the prior window, and then applied to the proceeding four years of data. Suppose our algorithm begins with years 0 : 4. First, analyse the S&P 500 over the prior 4-year period, years -4 : 0. Determine the cluster structure of the distribution segments of the S&P 500 over this prior period. To make investment decisions in the current period of 0 : 4 years, we try to match the present distribution with the most similar distribution in the prior window. Specifically, we examine the present local distribution of the last n days, where n is a learned parameter, and determine the minimal distance between the local distribution and the kernel density estimate distributions of the prior 4-year period. If this closest point lies in the most volatile class of past distributions, characterised by widest kernel density estimate functions, we determine that the local distribution is volatile, and allocate all capital toward gold. This method works even if greater than 2 volatility clusters are found during the previous window.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application of results: trading strategy"
        },
        {
            "text": "The parameter n is optimised relative to the -4 : 0 year window. Specifically, having determined the cluster structure, n is chosen to optimise the Sharpe ratio, a well-established measure of risk-adjusted returns, when testing over that window. We optimise n over a range 10 \u2264 n \u2264 30, that is, 2 to 6 trading weeks. Thus, n is learned in this prior window and then used in the algorithm in the subsequent window. The window is then successively slid forward four years, and the process repeats. That is, model parameters estimated on years 0 : 4 are used to forecast in years 4 : 8, and so on. This 4-year period is chosen as the literature suggests that equity markets follow four year cycles, associated with the cyclicality of Kitchin cycles [22] and the US presidential election [23] . This adaptive sliding window technique allows us to convincingly validate the long-run performance of our trading strategy.",
            "cite_spans": [
                {
                    "start": 746,
                    "end": 750,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 784,
                    "end": 788,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Application of results: trading strategy"
        },
        {
            "text": "We analyse the strategy's performance in a period from immediately prior to the global financial crisis (GFC), up to the present day. Accordingly, our initial backtest period of -4 : 0 is 2004-2008, while our first period of trading, years 0 : 4, is 2008-2012. We compare the performance of our dynamic trading strategy with three other strategies: holding SPY, holding GLD, and a baseline strategy holding an equal split between the two. We use six common validation metrics to evaluate and compare our trading strategy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application of results: trading strategy"
        },
        {
            "text": "1. Annualised return (AR): the total return a strategy yields relative to the time the strategy has been in place.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application of results: trading strategy"
        },
        {
            "text": "2. The overall standard deviation (SD) of the portfolio.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application of results: trading strategy"
        },
        {
            "text": "3. Sharpe ratio (SR): a common measure of risk-adjusted return. Unfortunately, this penalises both upside and downside volatility. Some strategies with strong annualised returns may have lower Sharpe ratios due to erratic, yet positive return profiles.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Application of results: trading strategy"
        },
        {
            "text": ": an alternative penalty function capturing the maximum peak to trough trading loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maximum drawdown (MD)"
        },
        {
            "text": ": an alternative measure of risk-adjusted return that only penalises downside deviation in the denominator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sortino ratio (SoR)"
        },
        {
            "text": "6. Calmar ratio (CR): a measure of risk-adjusted returns that penalises the maximum realised drawdown over some candidate investment period.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sortino ratio (SoR)"
        },
        {
            "text": "Implementing our trading strategy between January 2008 and April 2020 would have been highly successful for both risk-averse and risk-on investors. Seen in Table 6 and Figure 3 , the strategy consistently outperformed the S&P 500 index, and overall generated annualised returns of 11%. The S&P 500 returned 5.4% while the static baseline strategy returned 6.3%. The strategy clearly generates alpha by its dynamic nature, automatically detecting market regimes and allocating capital successfully. This entire period can broadly be characterised as a bull market, and yet features several severe market shocks; the strategy's consistent performance demonstrates its robustness to varied market dynamics. Figure 4 Of the four strategies compared, our dynamic trading strategy has the best annualized returns, Sharpe ratio, Sortino ratio and Calmar ratio, and lowest drawdown. It has the second lowest standard deviation of 0.16, close to the baseline static strategy's 0.14. The most significant component of the Sharpe ratio's performance comes from strong annualised returns; the increased upside volatility is the main contributor to the standard deviation. Indeed, our strategy's Sortino ratio is 3 times greater than that of the S&P 500; this confirms that a significant degree of the penalty in the standard deviation and Sharpe ratio is generated from upside returns. That is, the strong annualised returns of our trading strategy are generated in a relatively volatile manner. This is unsurprising, given that the strategy generates performance due to market timing.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 156,
                    "end": 163,
                    "text": "Table 6",
                    "ref_id": null
                },
                {
                    "start": 168,
                    "end": 176,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 704,
                    "end": 712,
                    "text": "Figure 4",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Model performance: 2008-2020"
        },
        {
            "text": "In this section, we describe the performance in detail over various time periods, particularly during market crises. Note: while we have reported our findings over one period 2008-2020, in fact four separate learning and evaluation procedures have been performed. All four periods were successful for our strategy, visible in Figure 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 326,
                    "end": 334,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Detailed analysis of performance over time"
        },
        {
            "text": "First, the strategy performs well during the GFC. Our strategy generates the second best returns during the GFC, surpassed only by gold. During the GFC, gold provided extraordinary returns for investors who invested prior to or during the crisis. After incurring a sharp drawdown, our strategy reallocates capital from S&P 500 into gold and consequently outperforms equity markets until late 2011.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detailed analysis of performance over time"
        },
        {
            "text": "Next, the market experienced significant drawdown in December 2018. Given the brevity of this drawdown, our trading strategy is unable to reallocate capital away from the S&P 500 into gold fast enough to meaningfully reduce the strategy's drawdown. After all, our strategy is predicated on identifying regimes, and allocating capital when new data are identified as similar to past phenomena. It reflects the delicate balance in the look back length n. If it were too long, trading decisions would be made too slowly; if it were too short, trading decisions would be made too frivolously.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detailed analysis of performance over time"
        },
        {
            "text": "The final significant market crisis during our window of analysis is the market turbulence associated with COVID-19. Our strategy performs extremely well during this period. Although the strategy does experience losses in late January and February 2020, capital is reallocated toward gold and strategy returns recover quickly. In fact, the cumulative returns of the strategy are no lower than the previous high, prior to the COVID-19 crisis. In comparison, the S&P 500 and our static baseline strategy suffered more significant drawdowns, and have failed to return to prior high watermark levels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Detailed analysis of performance over time"
        },
        {
            "text": "During the four 4-year windows that make up the 2008-2020 experiment, the optimal look back length n changes as follows. For the four windows, the optimal chosen n is 13, 13, 18 and 16 for 2004-2008, 2008-2012, 2012-2016 and 2016-2020 respectively. This suggests that continually updating the look back length is important, due to the dynamic nature of markets. Note that the longest look back length is during 2012-2016, a bull market period with the greatest consistency and least volatility in the return profile. This suggests that regimes were more persistent and possibly easier to identify during the 2012-2016 period.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 234,
                    "text": "13, 13, 18 and 16 for 2004-2008, 2008-2012, 2012-2016 and 2016-2020",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Detailed analysis of performance over time"
        },
        {
            "text": "We have shown that our new methodology for clustering volatility regimes is a useful tool for making inferences on financial time series and for designing trading strategies. Results on both synthetic and real data are promising, with good validation scores and significant simplification of the time series. These findings help support the work by [1, 3] who contributed to the idea of discrete changes in volatility regimes. Moreover, while these models generally select the number of regimes to begin with, we have have determined the number of clusters, and showed this is overwhelmingly 2 in practice. And yet, our method is flexible enough to detect greater numbers of regimes if clearly present, such as piecewise autoregressive models. Our method fits well with others in the literature, as our determined number of volatility regimes can then be used in an alternative regime switching model, which generally requires this number to be set a priori.",
            "cite_spans": [
                {
                    "start": 349,
                    "end": 352,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 353,
                    "end": 355,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Our dynamic trading strategy performs well at avoiding periods of significant volatility and drawdown, and performs substantially better than the S&P 500, in various market conditions. Our method continually updates its distributions and parameters, reflecting the need for ongoing learning of market conditions and volatility structure. Our method is also flexible, with several natural alternatives one could adopt. For instance, one could switch from SPY to cash as an alternative safe haven asset, replacing gold. Our methodology could also be combined with other statistical or machine learning methods in the literature. For example, instead of a static safe haven class to which the strategy flees in times of volatility, one could use a learned allocation of low beta assets as an evolving safe haven.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Appendix A.1. General change point detection framework First, we outline the change point detection framework in greatest generality. A sequence of observations x 1 , x 2 , ..., x n are drawn from random variables X 1 , X 2 , ..., X n . We wish to determine points \u03c4 1 , ..., \u03c4 m at which the distributions change. One always assumes that the underlying random variables are independent and identically distributed between change points. One can summarize this with the following notation, following Ross [14] :",
            "cite_spans": [
                {
                    "start": 505,
                    "end": 509,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Appendix A. Details of change point detection algorithm"
        },
        {
            "text": "That is, one assumes X i is a random sampling of a different distribution over each time period [\u03c4 i , \u03c4 i+1 ]. In order to meet the apparently restrictive assumption of independence of the data, one must usually perform an appropriate transformation of the data. The log quotient transformation, which yields the log returns from the closing price data, is one such transformation [24] .",
            "cite_spans": [
                {
                    "start": 382,
                    "end": 386,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Appendix A. Details of change point detection algorithm"
        },
        {
            "text": "Ross [25] points out the fact that log returns often exhibit heavy tailed behaviour. As a result, a non-parametric test is needed to detect change points that do not a priori assume the distribution of the data. The rank test is one such test. Suppose there are two samples of observations from unknown distributions A = {r 1,1 =, r 1,2 , ..., r 1,m } and B = {r 2,1 =, r 2,2 , ..., r 2,n }. Define the rank of an observation r \u2208 A \u222a B as follows: rank(r) = m j 1 (r\u2265r1,j ) + n j 1 (r\u2265r2,j ) = #{s \u2208 A \u222a B : r \u2265 s} A larger rank indicates a higher positioning in the ordering of the elements of A and B. If both sets of samples have the same distribution, the median rank among {rank(r) : r \u2208 A \u222a B} is 1 2 (n + m + 1). In this case, one would assume that both sets have a near equal split of the ranks.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Appendix A.2. Rank of observations and Mood Test"
        },
        {
            "text": "The Mood test determines the extent that each observation's rank differs from the median rank, thereby detecting differences in the distributions' variance. If the samples have different variances, then one set of samples would have more extreme values than the other, which means the ranks would not be even between the two sets. Specifically, the test statistic is as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A.2. Rank of observations and Mood Test"
        },
        {
            "text": "This is appropriately normalized:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A.2. Rank of observations and Mood Test"
        },
        {
            "text": "If M mn is greater than some threshold h, we reject the null hypothesis that the distributions have the same variance, and conclude they have different variances. As depicted in Appendix B, the log return time series are tail heavy but strongly mean and median centred. Thus, the Mood test reliably detects changes in the variance without being affected by changes in the median. Compare Sections 4 and 5 of [11] for this distinction.",
            "cite_spans": [
                {
                    "start": 408,
                    "end": 412,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Appendix A.2. Rank of observations and Mood Test"
        },
        {
            "text": "Ross' CPM algorithm [14] works by feeding in one data point at a time. When a change point \u03c4 is detected, the algorithm restarts and proceeds from that point, so it suffices to describe how the algorithm determines its very first change point. Suppose x 1 , ..., x N is a sequence for which no change point has been detected. For each m = 1, 2, ..., N define n = N \u2212 m, mirroring the notation of Appendix A.2, and compute the Mood test statistic M m,n . If the maximum among these, M N = max m+n=N M m,n , exceeds a threshold parameter h N , we declare a change point in the variance has occurred at\u03c4 = argmax m M m,n . If the maximum such test statistic does not exceed the threshold parameter, feed in the next data point x N +1 and continue. Note if a change point\u03c4 = m is detected at time N , there has been a delay of n units in its detection. This delay is necessary for the algorithm to examine data points on each side of the change point. The algorithm then restarts from the change point\u03c4 .",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 24,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Appendix A.3. CPM algorithm"
        },
        {
            "text": "In our implementation of the algorithm, we always read in at least 30 values before looking for another change point, so that all stationary periods have length at least 30. We choose our parameters h in order to manage the number of false positives (Type I errors). Given an acceptability threshold \u03b1, the following equations specify that this error should remain constant over time:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A.3. CPM algorithm"
        },
        {
            "text": "In the event that no change point exists, a false positive will nonetheless be detected at time 1/\u03b1 on average. This quantity is the average run length parameter ARL 0 that is passed to CPM, which in term calculates the appropriate choice of h t . In this case ARL 0 is set to 10,000. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix A.3. CPM algorithm"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A new approach to the economic analysis of nonstationary time series and the business cycle",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Hamilton",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Econometrica",
            "volume": "57",
            "issn": "",
            "pages": "357--384",
            "other_ids": {
                "DOI": [
                    "10.2307/1912559"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Adaptive detection of multiple change-points in asset price volatility",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lavielle",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Teyssi\u00e8re",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Long Memory in Economics",
            "volume": "",
            "issn": "",
            "pages": "129--156",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-540-34625-8_5"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Persistence in variance, structural change, and the GARCH model",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "G"
                    ],
                    "last": "Lamoureux",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "D"
                    ],
                    "last": "Lastrapes",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Journal of Business & Economic Statistics",
            "volume": "8",
            "issn": "",
            "pages": "225--234",
            "other_ids": {
                "DOI": [
                    "10.2307/1391985"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Stock market analysis: A review and taxonomy of prediction techniques",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Isah",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zulkernine",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Journal of Financial Studies",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3390/ijfs7020026"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "F"
                    ],
                    "last": "Engle",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Econometrica",
            "volume": "50",
            "issn": "",
            "pages": "987--1007",
            "other_ids": {
                "DOI": [
                    "10.2307/1912773"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Generalized autoregressive conditional heteroskedasticity",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Bollerslev",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "Journal of Econometrics",
            "volume": "31",
            "issn": "86",
            "pages": "90063--90064",
            "other_ids": {
                "DOI": [
                    "10.1016/0304-4076(86)90063-1"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "From the bird's eye to the microscope: A survey of new stylized facts of the intra-daily foreign exchange markets",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Guillaume",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Dacorogna",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "R"
                    ],
                    "last": "Dav\u00e9",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [
                        "A"
                    ],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "B"
                    ],
                    "last": "Olsen",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [
                        "V"
                    ],
                    "last": "Pictet",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Finance and Stochastics",
            "volume": "1",
            "issn": "",
            "pages": "95--129",
            "other_ids": {
                "DOI": [
                    "10.1007/s007800050018"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Improving GARCH volatility forecasts with regimeswitching GARCH",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Klaassen",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Empirical Economics",
            "volume": "27",
            "issn": "",
            "pages": "363--394",
            "other_ids": {
                "DOI": [
                    "10.1007/s001810100100"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Testing a sequence of observations for a shift in location",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Hawkins",
                    "suffix": ""
                }
            ],
            "year": 1977,
            "venue": "Journal of the American Statistical Association",
            "volume": "72",
            "issn": "",
            "pages": "180--186",
            "other_ids": {
                "DOI": [
                    "10.1080/01621459.1977.10479935"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A change-point model for a shift in variance",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Hawkins",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Zamba",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Journal of Quality Technology",
            "volume": "37",
            "issn": "",
            "pages": "21--31",
            "other_ids": {
                "DOI": [
                    "10.1080/00224065.2005.11980297"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "On the asymptotic efficiency of certain nonparametric twosample tests",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Mood",
                    "suffix": ""
                }
            ],
            "year": 1954,
            "venue": "The Annals of Mathematical Statistics",
            "volume": "25",
            "issn": "",
            "pages": "514--522",
            "other_ids": {
                "DOI": [
                    "10.1214/aoms/1177728719"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Detecting change points in VIX and S&P 500: A new approach to dynamic asset allocation",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nystrup",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "W"
                    ],
                    "last": "Hansen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Madsen",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Lindstr\u00f6m",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Journal of Asset Management",
            "volume": "17",
            "issn": "",
            "pages": "361--374",
            "other_ids": {
                "DOI": [
                    "10.1057/jam.2016.12"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Fitting time series models to nonstationary processes",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dahlhaus",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "The Annals of Statistics",
            "volume": "25",
            "issn": "",
            "pages": "1--37",
            "other_ids": {
                "DOI": [
                    "10.1214/aos/1034276620"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Parametric and nonparametric sequential change detection in R: The cpm package",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Ross",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of Statistical Software",
            "volume": "66",
            "issn": "",
            "pages": "1--20",
            "other_ids": {
                "DOI": [
                    "10.18637/jss.v066.i03"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "The distribution of realized stock return volatility",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "G"
                    ],
                    "last": "Andersen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Bollerslev",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "X"
                    ],
                    "last": "Diebold",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ebens",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Journal of Financial Economics",
            "volume": "61",
            "issn": "",
            "pages": "43--76",
            "other_ids": {
                "DOI": [
                    "10.1016/s0304-405x(01)00055-1"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Density Estimation for Statistics and Data Analysis",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "W"
                    ],
                    "last": "Silverman",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1201/9781315140919"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Real Analysis: Measure Theory, Integration, and Hilbert Spaces",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "M"
                    ],
                    "last": "Stein",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Shakarchi",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1017/s0025557200181343"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Central limit theorems for the wasserstein distance between the empirical and the true distributions",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Barrio",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gin\u00e9",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Matr\u00e1n",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "The Annals of Probability",
            "volume": "27",
            "issn": "",
            "pages": "1009--1071",
            "other_ids": {
                "DOI": [
                    "10.1214/aop/1022677394"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A tutorial on spectral clustering",
            "authors": [
                {
                    "first": "U",
                    "middle": [],
                    "last": "Luxburg",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Statistics and Computing",
            "volume": "17",
            "issn": "",
            "pages": "395--416",
            "other_ids": {
                "DOI": [
                    "10.1007/s11222-007-9033-z"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Rousseeuw",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "Journal of Computational and Applied Mathematics",
            "volume": "20",
            "issn": "",
            "pages": "90125--90132",
            "other_ids": {
                "DOI": [
                    "10.1016/0377-0427(87)90125-7"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Finding Groups in Data",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kaufman",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Rousseeuw",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1002/9780470316801"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A spectral analysis of world GDP dynamics: Kondratieff waves, Kuznets swings, Juglar and Kitchin cycles in global economic development, and the 2008-2009 economic crisis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Korotayev",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tsirel",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Structure and Dynamics : e-Journal of Anthropological and Related Sciences",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Is there an election cycle in American stock returns?",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "G\u00e4rtner",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "W"
                    ],
                    "last": "Wellershoff",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "International Review of Economics & Finance",
            "volume": "4",
            "issn": "",
            "pages": "90036--90041",
            "other_ids": {
                "DOI": [
                    "10.1016/1059-0560(95)90036-5"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Adaptive Filtering and Change Detection",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Gustafsson",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1002/0470841613"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Modelling financial volatility in the presence of abrupt changes",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "J"
                    ],
                    "last": "Ross",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Physica A: Statistical Mechanics and its Applications",
            "volume": "392",
            "issn": "",
            "pages": "350--360",
            "other_ids": {
                "DOI": [
                    "10.1016/j.physa.2012.08.015"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "displays results for large firms: MSFT (Microsoft), APPL (Apple), AMZN (Amazon), GOOG (Alphabet), BRK-",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Synthetic",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "S&P 500 time series data Class A).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Cumulative",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Positions held by dynamic strategy: January 2008 -April 2020",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Figure B.5: Dow Jones results",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Figure B.9: Microsoft",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Figure B.10: Apple",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Figure B.11: Amazon",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Figure B.12: Alphabet",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Berkshire",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "RYT",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "GLD",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Figure B.16: XLF: Financial Select Sector SPDR Fund",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Figure B.17: IJS: iShares SP Small-Cap 600",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "Figure B.18: DRGW: WisdomTree U.S. Quality Dividend Growth Fund",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Figure B.19: USD/JPY",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Figure B.20: AUD/USD",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Figure B.21: EUR/USD",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Figure B.22: GBP/USD",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "NZD/AUD",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Silhouette score interpretation",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "displays results for ETFs: RYT (Invesco S&P 500 Equal Weight Technology ETF), GLD (SPDR Gold Shares), XLF (Financial Select Sector SPDR Fund), IJS (iShares SP Small-Cap 600), DGRW (WisdomTree U.S. Quality Dividend Growth Fund)",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Major indices",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Large firms by market capitalisation",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Popular ETFs",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "Currency pairs",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Many thanks to Alex Judge for helpful comments and suggestions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        },
        {
            "text": "What follows are plots pertaining to the body of the paper. First, the complete set of results for Section 3 can be found below. Figures B.5 Note all distribution plots are strongly centred in mean and median about zero. This is an important technical point for the Mood test to work correctly to detect changes in variance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 129,
                    "end": 140,
                    "text": "Figures B.5",
                    "ref_id": null
                }
            ],
            "section": "Appendix B. Plots"
        }
    ]
}