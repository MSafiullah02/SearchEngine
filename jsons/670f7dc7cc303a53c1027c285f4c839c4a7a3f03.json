{
    "paper_id": "670f7dc7cc303a53c1027c285f4c839c4a7a3f03",
    "metadata": {
        "title": "Chinese Sentence Semantic Matching Based on Multi-Granularity Fusion Model",
        "authors": [
            {
                "first": "Xu",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Qilu University of Technology (Shandong Academy of Sciences)",
                    "location": {
                        "settlement": "Jinan",
                        "country": "China"
                    }
                },
                "email": "xuzhang.p@foxmail.com"
            },
            {
                "first": "Wenpeng",
                "middle": [],
                "last": "Lu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Qilu University of Technology (Shandong Academy of Sciences)",
                    "location": {
                        "settlement": "Jinan",
                        "country": "China"
                    }
                },
                "email": "wenpeng.lu@qlu.edu.cn"
            },
            {
                "first": "Guoqiang",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Technology Sydney",
                    "location": {
                        "settlement": "Sydney",
                        "country": "Australia"
                    }
                },
                "email": "guoqiang.zhang@uts.edu.au"
            },
            {
                "first": "Fangfang",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {},
                "email": "fangfang.li@oohmedia.com.au"
            },
            {
                "first": "Shoujin",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Macquarie University",
                    "location": {
                        "settlement": "Sydney",
                        "country": "Australia"
                    }
                },
                "email": "shoujin.wang@mq.edu.au"
            }
        ]
    },
    "abstract": [
        {
            "text": "Sentence semantic matching is the cornerstone of many natural language processing tasks, including Chinese language processing. It is well known that Chinese sentences with different polysemous words or word order may have totally different semantic meanings. Thus, to represent and match the sentence semantic meaning accurately, one challenge that must be solved is how to capture the semantic features from the multi-granularity perspective, e.g., characters and words. To address the above challenge, we propose a novel sentence semantic matching model which is based on the fusion of semantic features from charactergranularity and word-granularity, respectively. Particularly, the multigranularity fusion intends to extract more semantic features to better optimize the downstream sentence semantic matching. In addition, we propose the equilibrium cross-entropy, a novel loss function, by setting mean square error (MSE) as an equilibrium factor of cross-entropy. The experimental results conducted on Chinese open data set demonstrate that our proposed model combined with binary equilibrium cross-entropy loss function is superior to the existing state-of-the-art sentence semantic matching models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Sentence semantic matching plays a key role in many natural language processing tasks such as question answering (QA), natural language inference (NLI), machine translation (MT), etc. The key of sentence semantic matching is to calculate the semantic similarity between given sentences from multiple text segmentation granularity such as character, word and phrase. Currently, the commonly used text segmentation is in word granularity only, especially for Chinese. However, many researchers have realized that a text can be viewed from not only word granularity but also the others.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In word granularity, many deep learning based sentence semantic matching models have been proposed, such as DeepMatch tree [18] , ARC-II [5] , Match-Pyramid [12] , Match-SRNN [16] , etc. However, these word-granularity models are unable to fully capture the semantic features embedded in sentences, sometimes even produce noise and thus hurt the performance of sentence matching. Eventually, more and more researchers turn to design semantic matching strategy combing word and phrase granularity, such as MultiGranCNN [24] , MV-LSTM [15] , MPCM [22] , BiMPM [21] , DIIN [3] . These models somehow overcome the word-granularity modelling limitations, however, they still cannot thoroughly solve the issue of semantic loss in the process of sentence encoding, especially for Chinese corpus which are usually with rich semantic features.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 127,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 137,
                    "end": 140,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 157,
                    "end": 161,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 175,
                    "end": 179,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 518,
                    "end": 522,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 533,
                    "end": 537,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 545,
                    "end": 549,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 558,
                    "end": 562,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 570,
                    "end": 573,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Similarly for Chinese sentence semantic matching task, many researchers attempt to mix words and characters together into a simple sequence. For example, multi-granularity Chinese word embedding [23] and lattice CNNs for QA [7] have achieved great performance. However, most Chinese characters cannot be treated as independent words or phrases as these works did. This is because the simple combining of characters or words together, or encoding characters according to character lattice may easily lose the meaning that is embedded in the corresponding character.",
            "cite_spans": [
                {
                    "start": 195,
                    "end": 199,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 224,
                    "end": 227,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In order to capture the sentence features from both character and word perspectives more deeply and comprehensively, we propose a new sentence semantic matching model with multi-granularity fusion. The semantic features of the text are obtained from the character and word perspectives respectively, and the more critical semantic information in the text is captured through the superposition effect of the two features. Our model significantly improves the representation of textual features. Moreover, for most existing deep learning applications, crossentropy is a commonly used loss function to train the models. We design a novel loss function, which utilizes mean square error (MSE) as an equilibrium parameter to strengthen and enhance cross-entropy with the ability to distinguish the fuzzy classification boundary, which greatly improves the performance of our model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our contributions are summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We propose a novel sentence encoding method named multi-granularity fusion model to better capture semantic features via the integration of multigranularity encoding. -We propose a novel deep neural architecture for sentence semantic matching task, which includes embedding layer, multi-granularity fusion encoding layer, matching layer and prediction layer.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We propose a new loss function integrating equilibrium parameter into crossentropy function. MSE is introduced as the equilibrium parameter to construct the binary equilibrium cross-entropy loss. -Our source code is publicly available 1 . Our work may provide a reference for researchers in NLP community.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest of the paper is structured as follows. We introduce the related work about sentence semantic matching in Sect. 2, and propose multi-granularity fusion model in Sect. 3. Section 4 demonstrates the empirical experimental results, followed by the conclusion in Sect. 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Semantic matching in short text is the basis of natural language understanding tasks. Its improvement will help advance the progress of natural language understanding tasks. A lot of work has put great efforts into the semantic matching in short texts [3, 10, 16, 20, 21, 25] .",
            "cite_spans": [
                {
                    "start": 252,
                    "end": 255,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 256,
                    "end": 259,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 260,
                    "end": 263,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 264,
                    "end": 267,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 268,
                    "end": 271,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 272,
                    "end": 275,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "With the continuous development of deep learning, it is difficult to further obtain the text semantic information only depending on designing the models with more complex and deep architecture. The researchers then begin to consider obtaining more semantic features from texts on different granularity. In the matching process, both the sentence and the word, phrase perspectives are considered. The results of multi-faceted feature matching are combined to get better results [1, 15, 19, 21, 23, 24] . Yin et al. propose MultiGranCNN to first obtain text features on different granularity such as words, phrases, and sentences, and then concatenate these text features and calculate the similarity between the two sentences [24] . Wan et al. propose MV-LSTM method similar to MultiGranCNN, which can capture long-distance and short-distance dependencies simultaneously [15] . MIX is a multi-channel convolutional neural network model for text matching, with additional attention mechanisms on sentences and semantic features [1] . MIX compares text fragments on varied granularity to form a series of multi-channel similarity matrices, which are then crossed with another set of carefully designed attention matrices to expose the rich structure of sentences to a deep neural network. Though all the above methods perform feature representation for the same text on word, phrase and sentence granularity simultaneously, they still ignore the influence of features on other granularity, such as character. In order to solve this problem in Chinese language, we generate corresponding text vectors, extracting the character-granularity and the corresponding word-granularity features separately. The feature on each granularity is captured from the corresponding text sequence.",
            "cite_spans": [
                {
                    "start": 477,
                    "end": 480,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 481,
                    "end": 484,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 485,
                    "end": 488,
                    "text": "19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 489,
                    "end": 492,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 493,
                    "end": 496,
                    "text": "23,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 497,
                    "end": 500,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 725,
                    "end": 729,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 870,
                    "end": 874,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1026,
                    "end": 1029,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Most tasks in natural language processing field can be considered as classification problems. For classification tasks, the most commonly used loss function in deep learning methods is cross-entropy. In view of the related tasks in computer vision, a series of loss functions based on optimization have been proposed to improve face recognition [2, 8, 17] , image segmentation [11, 13, 14] and other tasks. Compared with computer vision, there is few related work on reconstructing loss function for a specific task in natural language processing field. Kriz et al. present a customized loss function to replace the standard cross-entropy during training, which takes the complexity of content words into account [6] . They propose a metric that modifies cross-entropy loss to up weight simple words and down weight more complex words for sentence simplification. Besides, Hsu et al. introduce the inconsistency loss function to replace cross-entropy loss in text extraction and summarization [4] . To better distinguish the classification results, Zhang et al. modify the cross-entropy loss function and apply it on the text matching task [25] . Inspired by the work, we propose a new loss function, where MSE is used as the balance factor to enhance the cross-entropy loss function. It can strengthen the ability to distinguish the fuzzy classification boundary in the training process and improve classification accuracy. As shown in Fig. 1 , our proposed model architecture includes a multi-granularity embedding layer, a multi-granularity fusion encoding layer, a matching layer and a prediction layer. First, we embed the input sentences from both character and word perspectives through the multi-granularity embedding layer. Then, the output of multi-granularity embedding layer is transmitted to the multigranularity fusion encoding layer to extract two streams of semantic features on the character and word granularity, respectively. When the semantic feature extraction is complete, the semantic feature is fed to the matching layer to generate a final matching representation of the input sentences, which is further transferred to a Sigmoid function to judge their matching degree in the prediction layer.",
            "cite_spans": [
                {
                    "start": 345,
                    "end": 348,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 349,
                    "end": 351,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 352,
                    "end": 355,
                    "text": "17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 377,
                    "end": 381,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 382,
                    "end": 385,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 386,
                    "end": 389,
                    "text": "14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 713,
                    "end": 716,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 993,
                    "end": 996,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [
                {
                    "start": 1437,
                    "end": 1443,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Related Work"
        },
        {
            "text": "For Chinese text, after sentence segmentation from character and word perspectives, we obtain two sentence sequences based on character granularity and word granularity. By the multi-granularity embedding layer, the original sentence sequences are converted to the corresponding vector representations, respectively. In this embedding layer, we utilize the pre-trained embeddings, which are trained with Word2Vec on the target data set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Granularity Embedding Layer"
        },
        {
            "text": "In this subsection, we introduce our key contribution module which named multigranularity fusion encoding layer to improve the semantic encoding performance. This model integrates and considers the word vector and character vector comprehensively, which are depended on its own text sequence respectively. As shown in Fig. 2 , for the input sentence, we use different encoding methods to generate the character-granularity sentence vectors and the word-granularity sentence vectors. Aiming at the word-granularity sentence vector, we use two LSTMs for sequential encoding, then introduce the attention mechanism on deep feature extraction. Meanwhile, aiming at the character-granularity sentence vector, we use the same encoding method, which is similar with the word-granularity sentence vector. Moreover, for the character-granularity sentence vectors, we supplement a single layer of LSTM for encoding and then use the attention mechanism for deep feature extraction. For the above two encoding results on character granularity, we add them together to obtain more accurate semantic representation information on the character granularity.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 318,
                    "end": 324,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Multi-Granularity Fusion Encoding Layer"
        },
        {
            "text": "As shown in Fig. 2 , by the above operations on character-granularity and word-granularity sentence vectors, we can obtain semantic feature information on two perspectives. In order to capture more semantic features and understand the sentence semantic meaning more deeply, we add the sentence vectors from two perspectives together.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Multi-Granularity Fusion Encoding Layer"
        },
        {
            "text": "With this multi-granularity fusion encoding layer, the complex semantic features of the sentences are captured from the character and word perspectives respectively, and the more critical and important semantic information in the sentences are obtained through the superposition effect of the two features. This model can significantly improves the representation of sentence features. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Granularity Fusion Encoding Layer"
        },
        {
            "text": "The multi-granularity fusion encoding layer outputs the semantic feature vectors (Q1 Feature and Q2 Feature) for the sentences Q1 and Q2, which are transferred to interaction matching layer, as shown in Fig. 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 203,
                    "end": 209,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Fig. 3. Interaction Matching"
        },
        {
            "text": "In the interaction matching layer, we utilize multiple calculation methods to hierarchically compare the similarity of the semantic feature vectors for sentences Q1 and Q2. The initial operations are described as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 3. Interaction Matching"
        },
        {
            "text": "As shown in Fig. 3 , the sentence features are hierarchically matched. The input Q1 and Q2 features are handled by a full connected dense layer to generate the Q1 and Q2 features, which are processed and matched further with Eq. (5) and Eq. (6), whose outputs are concatenated together with Eq. (7).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Fig. 3. Interaction Matching"
        },
        {
            "text": "The feature representation \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 Concatenate obtained with Eq. (4) is further extracted using two dense layers, whose dimensions are 300 and 600 respectively. Then, we add this transformed representation and another feature representation \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Concatenate obtained with Eq. (7) together to generate a combined representation, followed by a dense layer whose dimension is 1. Finally, the output of the last dense layer is added to \u2212 \u2192 C3 ij obtained with Eq. (3) to generate the final matching representation of input sentences, which is further sent to the Sigmoid function to judge their matching degree in the prediction layer.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fig. 3. Interaction Matching"
        },
        {
            "text": "In most classification tasks, the cross-entropy loss function shown in Eq. (8) , is usually the first choice. In our work, aiming to solve the difficulty of crossentropy loss function on the fuzzy classification boundary, we try to make some modifications on cross-entropy so as to make the classification more effectively. we propose equilibrium cross-entropy by setting MSE as an equilibrium factor of cross-entropy. It can improve the accuracy when the classification boundary is fuzzy.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 78,
                    "text": "(8)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Equilibrium Cross-Entropy Loss Function"
        },
        {
            "text": "As shown in Eq. (9), We use MSE as the equilibrium factor.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equilibrium Cross-Entropy Loss Function"
        },
        {
            "text": "By using MSE as equilibrium factor in the equilibrium loss function shown in Eq. (10), the loss function can strengthen its ability to distinguish the fuzzy boundary and eliminate the blurring phenomenon in classification tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equilibrium Cross-Entropy Loss Function"
        },
        {
            "text": "(L mse * y true log y pred + (1 \u2212 L mse ) * (1 \u2212 y true ) log(1 \u2212 y pred )) (10)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equilibrium Cross-Entropy Loss Function"
        },
        {
            "text": "Our methods are compared with the-state-of-art methods on the public dataset, i.e., LCQMC. It's a large-scale Chinese question matching corpus released by Liu et al. [9] , which focuses on intent matching rather than paragraph matching. We use the same proportion ratio to split the dataset into training, validation and test parts, as mentioned in [9, 25] . We choose a set of examples from LCQMC to introduce the text semantic matching task, shown in Table 1 . From the examples, we can learn that if two sentences are matched, they should be similar in intention. ",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 169,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 349,
                    "end": 352,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 353,
                    "end": 356,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [
                {
                    "start": 453,
                    "end": 460,
                    "text": "Table 1",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Dataset"
        },
        {
            "text": "We implement our multi-granularity fusion model architecture for sentence semantic matching with Python based on Keras and Tensorflow framework. All the experiments are performed in a ThinkStation P910 Workstation with 192GB memory and one 2080Ti GPU. After testing a variety number of multigranularity embedding layer, we empirically set its dimensionality to 300. The number of units in multi-granularity fusion encoding layer is set to 300. In the Interaction matching layer, the widths of the dense layers are shown in Fig. 3 . In addition, the last dense layer utilizes sigmoid as the activation function and the other dense layers use relu. And in the multi-granularity fusion layer, we set dropout rate to 0.5. In the optimization, the epochs number is 200 and batch size is 512. We set up the early stopping mechanism. After 10 epochs, if the accuracy is not improved on the validation set, the training process will automatically stop and verify the model's performance on the test set.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 523,
                    "end": 529,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Experimental Setting"
        },
        {
            "text": "On LCQMC dataset, Liu et al. [9] and Zhang et al. [25] have realized nine relevant and representative state-of-the-art methods, which are used as the baselines to evaluate our model.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 32,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 50,
                    "end": 54,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Baseline Methods"
        },
        {
            "text": "-Unsupervised Methods: Some unsupervised matching methods based on word mover distance (WMD), word overlap (C wo ), n-gram overlap (C ngram ), edit distance (D edt ) and cosine similarity respectively (S cos ) [9] . -Supervised Methods: Some unsupervised matching methods based on convolutional neural network (CNN), bi-directional long short term memory (BiLSTM), bilateral multi-Perspective matching (BiMPM) [9, 21] and deep feature fusion model (DFF) [25] .",
            "cite_spans": [
                {
                    "start": 210,
                    "end": 213,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 410,
                    "end": 413,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 414,
                    "end": 417,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 454,
                    "end": 458,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Baseline Methods"
        },
        {
            "text": "A comparison of our work with the baseline methods, is shown in 15 .53%. We can see that the improvement of our proposed model is very prominent. Compared with the unsupervised method, the proposed MGF model is a supervised one, which can use the error between the real label and the prediction to carry out backpropagation to correct and optimize the massive parameters in neural network. Besides, MGF can obtain more feature expressions through deep feature encoding. These properties gives MGF the abilities to surpass the unsupervised methods greatly.",
            "cite_spans": [
                {
                    "start": 64,
                    "end": 66,
                    "text": "15",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "Compared with the basic neural network methods, i.e., CBOW char , CBOW word , CNN char , CNN word , BiLSTM char , BiLSTM word , our model MGF improves the precision metric by 14.89%, 13.49%, 14.29%, 12.99%, 13.99%, 10.7%, recall by 10.1%, 3%, 7.3%, 8.3%, 1.9%, 3.6%, F 1 -score by 12.92%, 9.32%, 11.52%, 11.02%, 9.22%, 7.8%, and accuracy by 15.23%, 12.13%, 14.03%, 13.03%, 12.33%, 9.73%. Though MGF is constructed by these basic neural network methods, it is equipped with a deeper network structure. Therefore, richer and deeper semantic features can be extracted to make the performance of our model more prominent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "Compared with the advanced neural network methods, i.e., BiMPM char , BiMPM word , DFF char , DFF word , our model MGF improves the precision metric by 3.79%, 3.69%, 2.81%, 3.7%, recall by \u2212 1%, \u2212 0.6%, \u2212 0.98%, \u2212 1.18%, F 1 -score by 1.72%, 1.82%, 1.21%, 1.66% and accuracy by 2.43%, 2.53%, 1.68%, 2.3%. BiMPM is a bilateral multi-perspective matching model, which utilizes BiLSTM to learn the sentence representation and implements four strategies to match the sentences from different perspectives [21] . DFF is a deep feature fusion model for sentence representation, which is integrated into the popular deep architecture for SSM task [25] . Compared with BiMPM and DFF, MGF realizes multi-granularity fusion encoding, which considers both character and word perspectives for the whole text. MGF can capture more comprehensive and complicated features, which leads to a better performance than the others.",
            "cite_spans": [
                {
                    "start": 501,
                    "end": 505,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 640,
                    "end": 644,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Performance Evaluation"
        },
        {
            "text": "To better address the Chinese sentence matching problem better, we put forward a new sentence matching model, i.e., multi-granularity fusion model, which takes both Chinese word-granularity and character-granularity into account. Specifically, we integrate word and character embedding representations together, and capture more hierarchical matching features between sentences. In addition, to solve the fuzzy boundary problem in the classification process, we use MSE as an equilibrium factor to improve the cross-entropy loss function. Extensive experiments on the real-world data set, i.e., LCQMC, have clearly shown that our model outperforms the existing state-of-the-art methods. In future, we will introduce more features on different granularity. i.e., n-grams and phrases, etc., to encode and represent the sentences more comprehensively, and try to further improve semantic matching performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Mix: multi-channel information crossing for text matching",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
            "volume": "",
            "issn": "",
            "pages": "110--119",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Arcface: additive angular margin loss for deep face recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Xue",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zafeiriou",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "4690--4699",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Natural language inference over interaction space",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1709.04348"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A unified model for extractive and abstractive summarization using inconsistency loss",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "T"
                    ],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K"
                    ],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Min",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "132--141",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Convolutional neural network architectures for matching natural language sentences",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2042--2050",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Complexity-weighted loss and diverse reranking for sentence simplification",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kriz",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "",
            "issn": "",
            "pages": "3137--3147",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Lattice CNNS for matching based Chinese question answering 33",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "6634--6641",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Focal loss for dense object detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "2980--2988",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "LCQMC: a large-scale Chinese question matching corpus",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1952--1962",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Siamese recurrent architectures for learning sentence similarity",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Mueller",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Thyagarajan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "2786--2792",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Gated CRF loss for weakly supervised semantic image segmentation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Obukhov",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Georgoulis",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Gool",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.04651"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Text matching as image recognition",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "2793--2799",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Normalized cut loss for weakly-supervised CNN segmentation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Djelouah",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Perazzi",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Boykov",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Schroers",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "1818--1827",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "On regularized losses for weakly-supervised CNN segmentation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Perazzi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Djelouah",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Ben Ayed",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Schroers",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Boykov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "volume": "",
            "issn": "",
            "pages": "507--522",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A deep architecture for semantic matching with multiple positional sentence representations",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "2835--2841",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Match-SRNN: modeling the recursive matching structure with spatial RNN",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "2922--2928",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Additive margin softmax for face verification",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Signal Process. Lett",
            "volume": "25",
            "issn": "7",
            "pages": "926--930",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Syntax-based deep matching of short texts",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "1354--1361",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Inferring implicit rules by learning explicit and hidden item dependency",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Syst. Man Cybern. Syst",
            "volume": "1",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Sequential recommender systems: challenges, progress and prospects",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "Z"
                    ],
                    "last": "Sheng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Orgun",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "6332--6338",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Bilateral multi-perspective matching for natural language sentences",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hamza",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Florian",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "4144--4150",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Multi-perspective context matching for machine comprehension",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mi",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hamza",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Florian",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1612.04211"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Multi-granularity Chinese word embedding",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "981--986",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "MultiGranCNN: an architecture for general matching of text chunks on multiple levels of granularity",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sch\u00fctze",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
            "volume": "1",
            "issn": "",
            "pages": "63--73",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Deep feature fusion model for sentence semantic matching",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Comput. Mater. Continua",
            "volume": "61",
            "issn": "",
            "pages": "601--616",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Model architecture of sentence matching",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Multi-Granularity Fusion Encoding",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "Examples in LCQMC Corpus.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Compared with the unsupervised methods, i.e., WMD char , WMD word , C wo , C ngram , D edt , S cos , our model MGF improves the precision metric by 14.39%, 16.99%, 20.29%, 29.09%, 34.89%, 21.29%, recall by 11.7%, 14.3%, 9.3%, 3.6%, 6.5%, 4.2%, F 1 -score by 13.32%, 15.92%, 16.12%, 20.72%, 26.22%, 15.12% and accuracy by 15.23%, 25.83%, 15.13%, 24.63%, 33.53%,",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Experiments on LCQMC. char means embeddings are character-based and word means word-based.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}