{
    "paper_id": "PMC7148094",
    "metadata": {
        "title": "Influence of Random Walk Parametrization on Graph Embeddings",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Fabian",
                "middle": [],
                "last": "Schliski",
                "suffix": "",
                "email": "schliski@fim.uni-passau.de",
                "affiliation": {}
            },
            {
                "first": "J\u00f6rg",
                "middle": [],
                "last": "Schl\u00f6tterer",
                "suffix": "",
                "email": "joerg.schloetterer@uni-passau.de",
                "affiliation": {}
            },
            {
                "first": "Michael",
                "middle": [],
                "last": "Granitzer",
                "suffix": "",
                "email": "michael.granitzer@uni-passau.de",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Network analysis involves methods to predict over nodes and edges, such as node classification [5], link prediction [12], clustering [8], and visualization [13].",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 97,
                    "mention": "5",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 117,
                    "end": 119,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 134,
                    "end": 135,
                    "mention": "8",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 157,
                    "end": 159,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Node classification aims at predicting the labels of unlabeled nodes based on a set of different labeled nodes and the network topology. An example is to predict the interests of a user in a social network based on other users with overlapping characteristics. Link prediction is used to predict missing or future links between nodes in the network. In a social network, it can be used to recommend new friends based on the current ones. Clustering attempts to identify similarities between nodes in the network and groups them into same-labeled clusters. This can be used to detect communities with similar interests in a social network. Visualization helps to gain quick insights about the structure of the network.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Graph embeddings are feature vector representations of the nodes and edges of a network and are used as input to the methods above. node2vec [10] uses a random-walk based approach to create those embeddings and introduces two additional parameters to guide the random walk, aiming at preserving both community structure and structural roles. Community structure in a graph is based on proximity, i.e., nodes that are close together belong to a community. Structural roles can be nodes that act as bridges between sub-networks, or hubs, which are the main exchange point between many nodes. The two parameters guiding the random walk in node2vec [10] are:Return parameter p, controlling the likeliness of immediately revisiting a node.In-out parameter q, controlling how far outward the random walk should progress from the starting node.\n",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 144,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 646,
                    "end": 648,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "These parameters allow to resemble depth-first (DFS) or breadth-first (BFS) search-like bevahiour in the most extreme setting, as well as a smooth interpolation between DFS and BFS. Grover and Leskovec suggest \u201cthat BFS and DFS strategies represent extreme ends on the spectrum of embedding nodes based on the principles of homophily (i.e., network communities) and structural equivalence (i.e., structural roles of nodes)\u201d [10]. In a case study, they demonstrate that subject to the parameter settings, the resulting embeddings can capture homophily or structural equivalence. In this paper1, wetry to reproduce the case study illustrating homophily and structural equivalence.try to reproduce node2vec\u2019s node classification result.introduce two additional modifications to the random walk strategy and evaluate and compare them on the node classification task. The additional strategies comprise hub attention and jump probalities, where the latter can be seen as noise.\n",
            "cite_spans": [
                {
                    "start": 425,
                    "end": 427,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Algorithms to create graph embeddings can be divided into three categories: Factorization based, deep learning based, and random walk based [9].",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 142,
                    "mention": "9",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Factorization based algorithms represent the graph as a matrix and apply methods such as eigenvalue decomposition or gradient descent to obtain node embeddings [9]. Examples are LLE [19], Laplacian Eigenmaps [4], GraRep [6], and HOPE [16].",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 162,
                    "mention": "9",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 183,
                    "end": 185,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 209,
                    "end": 210,
                    "mention": "4",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 221,
                    "end": 222,
                    "mention": "6",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 235,
                    "end": 237,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The deep learning based methods try to improve the performance of the factorization algorithms by computing non-linear functions on the graph. Examples are SDNE [20] (auto-encoder to reduce dimensionality), DNGR [7] (deep neural networks), and GCN [11] (graph convolutional networks).",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 164,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 213,
                    "end": 214,
                    "mention": "7",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 249,
                    "end": 251,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Random walk based approaches create embeddings by processing sets of random walks through the graph. First in this line was DeepWalk [18], which samples purely random walks. These walks are then treated as sentence equivalents (where every node in the sequence corresponds to a word) and fed to a skip-gram model, a model variant of the word embeddings introduced by Mikolov et al. [14], which became famous under the term word2vec. node2vec [10] follows this approach, but provides means to control the random walk behavior.",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 136,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 383,
                    "end": 385,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 443,
                    "end": 445,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "For further methods and additional details of the methods mentioned above consult a survey by Goyal and Ferrara [9].",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 114,
                    "mention": "9",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "We introduce the parameter j to modify the random walk during walking. It controls the probability of jumping to a random node in the graph at any given time. Intuitively, j ranges from 0 to 1, where with 0 no jumps to a random node occur, and with 1 every walking step is a random jump. The latter allows to create a truly random \u201cwalk\u201d through the graph, without drawing any attention to the structure of the graph and edges with their respective weights. We are sampling truncated random walks, i.e., we start walks with a fixed length from every node in the graph. Therefore, the jump probability can be seen as noise in the truncated walks, opposed to jump probability in a single (huge) walk (as used by PageRank [17] for example).",
            "cite_spans": [
                {
                    "start": 720,
                    "end": 722,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Jump Probability ::: Additional Random Walk Modifications",
            "ref_spans": []
        },
        {
            "text": "Hubs are nodes in a graph with a degree that greatly exceeds the average [2]. The threshold \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varepsilon $$\\end{document} is the sum of the mean node degree and the standard deviation of all degrees:\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varepsilon = \\overline{deg} + \\sigma _{deg} $$\\end{document}Based on that, we define the subset of hubs H of the nodes N as every node with a higher degree than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varepsilon $$\\end{document}:\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ H=\\{n \\in N \\mid deg(n) > \\varepsilon \\}\\subset {N} $$\\end{document}We are now modifying the random walk during sampling. Similar to node2vec [10], we change the unnormalized transition probability \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\pi _{vx}$$\\end{document} of a node v to its neighbors x on edges (v, x) to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\pi _{vx}=\\alpha _{h}(x)*w_{vx}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_{vx}$$\\end{document} is the weight of the edge, and\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _{h}(x)= \\left\\{ \\begin{array}{lr} \\frac{1}{h} &{} if\\ x \\in H \\\\ 1 &{} else \\end{array} \\right. $$\\end{document}The parameter h introduced here controls the random walk tendency towards and away from hubs. If this parameter is set to a high value (>1), the probability of directly revisiting hubs is reduced, and the less frequented nodes are explored. On the other hand, if h is small (<1), it increases the likelihood of traversing to hubs, so the walk is more focused on the areas around hubs in the graph.",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 75,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1610,
                    "end": 1612,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Hub Attention ::: Additional Random Walk Modifications",
            "ref_spans": []
        },
        {
            "text": "To create the visualizations, we first learn the embeddings of the Les Mis\u00e9rables dataset using the respective random-walk algorithm. We then cluster these embeddings using k-means and assign the nodes of the graph colors based on their cluster. These embeddings are then visualized as a graph with Gephi [3]. For the common parameters, we used the values reported by Grover and Leskovec: embedding dimension \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d=16$$\\end{document}, walk length \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l=80$$\\end{document}, number of walks \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n=10$$\\end{document} and context window size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w=10$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 306,
                    "end": 307,
                    "mention": "3",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Reproduction of Les Mis\u00e9rables Case Study ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "The upper graph in the original paper [10, figure 3] reflects community structure and the values \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q=0.5$$\\end{document} are reported for walk parametrization. Our result in Fig. 1a also represents the community structure of the network, comparable to that in the original paper.\n",
            "cite_spans": [
                {
                    "start": 39,
                    "end": 41,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Reproduction of Les Mis\u00e9rables Case Study ::: Evaluation",
            "ref_spans": [
                {
                    "start": 739,
                    "end": 740,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "For the lower graph in the original paper [10, figure 3], which as per description resembles structural equivalence, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q=2$$\\end{document} are specified. Even with grid-search over these and further parameters (l, n and w), no result close to the original could be produced. The graph never represented structural equivalence, but community structure as well (with 3 instead of 6 clusters), as shown in Fig. 1b.",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 45,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Reproduction of Les Mis\u00e9rables Case Study ::: Evaluation",
            "ref_spans": [
                {
                    "start": 968,
                    "end": 969,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We are running a multi-label node classification task on the BlogCatalog dataset. Same as in the original paper [10], we use a one-vs-rest logistic regression classifier with L2 regularization. The dataset is split into training and test data, with a training fraction of 50%, and the scores are averaged over 10 random splits. Again, we use the common parameters as reported by Grover and Leskovec [10]: embedding dimension \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d=128$$\\end{document}, walk length \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l=80$$\\end{document}, number of walks \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n=10$$\\end{document} and window size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w=10$$\\end{document}. All results of the following sections are reported in Table 1. The underlined values are the best per walk strategy, and the bold one is the best value overall.\n",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 115,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 400,
                    "end": 402,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Node Classification ::: Evaluation",
            "ref_spans": [
                {
                    "start": 1693,
                    "end": 1694,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Reproduction of Results. We used the values \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=q=0.25$$\\end{document} as reported by node2vec [10] and also included results with parameters \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=q=1$$\\end{document}, eliminating the influence of the parameters on the random walk, resembling a \u201cpure\u201d random walk like DeepWalk [18]. We were not able to reproduce the results within a single iteration of the skip-gram model, but used five iterations instead. We suspect the difference to be due to an unrecognized change of the default hyper-parameters in gensim2, the word2vec implementation used by node2vec. Our reproduced score is close to the reported one and even slightly better, which can be explained by the random factor of the experiment. From this, we can conclude that our values are consistent with those in the original paper. Furthermore, node2vec [10] performed better than non-parameterized random walks like DeepWalk, at least on this dataset and parameter settings.",
            "cite_spans": [
                {
                    "start": 363,
                    "end": 365,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 812,
                    "end": 814,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1349,
                    "end": 1351,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Node Classification ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "Jump Probability. We set j to each of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{1, 0.25, 0.1\\}$$\\end{document}. Expectably, the higher the jump probability is, the consistently worse the results get. However, at \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j=0.1$$\\end{document}, i.e. 10% noise, the performance is close to DeepWalk. This indicates, that a small amount of noise in the walks does not drastically harm the performance of the resulting embeddings on the node classification task.",
            "cite_spans": [],
            "section": "Node Classification ::: Evaluation",
            "ref_spans": []
        },
        {
            "text": "Hub Attention. We set h to each of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{0.5, 0.75, 4, 8, 10\\}$$\\end{document}. As shown in Table 1, a value of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h=4$$\\end{document} allows us to achieve the best value across the different strategies. When focussing on hubs (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h=0.5,h=0.75$$\\end{document}), performance even drops below the score of a jump probability of 25% (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j=0.25$$\\end{document}), i.e. jumping to a random node in every 4th step on average. Conversely, we gain performance if we put more attention to otherwise less-frequently visited nodes (i.e. if we increase h), at least up to a certain point where the results stabilize.",
            "cite_spans": [],
            "section": "Node Classification ::: Evaluation",
            "ref_spans": [
                {
                    "start": 363,
                    "end": 364,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We attribute our inability to reproduce the case study in terms of structural equivalence to the skip-gram model. Its objective is to predict neighboring nodes and hence, nodes with similar neighbors are represented closeby in embedding space. Another factor is the context window size of the skip-gram model: No matter how far out a walk traverses, only nodes within this window will be considered as context. This also means, that the walks which start at a particular node are not that relevant to this particular node, but its embedding is determined by all the walks traversing through this node. With optimal parameter settings and taking the standard deviation into account, the performance difference between the walk strategies on the node classification task is negligible. In addition, Perozzi et al. report a macro-F1 score of 27.3 for DeepWalk [18], doing shorter, but more walks. They report performance to increase constantly with the number of walks until it finally stabilizes.",
            "cite_spans": [
                {
                    "start": 858,
                    "end": 860,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Discussion and Conclusion",
            "ref_spans": []
        },
        {
            "text": "We conclude, that adapting the walk strategy can improve the embedding performance, if the number of sampled walks is insufficient. However, instead of tuning hyper-parameters of particular walk strategies, we can also increase the number of sampled walks per node instead. The nature of the skip-gram model and our inability to reproduce the structural equivalence case study point to the embeddings always representing homophily.",
            "cite_spans": [],
            "section": "Discussion and Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Macro f1 scores and standard deviation (\u00b1) of the node classification task on the BlogCatalog dataset using different parameters for each algorithm.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Les Mis\u00e9rables network with nodes colored corresponding to their cluster in embeddings created by different walk strategies. (Color figure online)",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "The link-prediction problem for social networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liben-Nowell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J. Am. Soc. Inf. Sci. Technol.",
            "volume": "58",
            "issn": "7",
            "pages": "1019-1031",
            "other_ids": {
                "DOI": [
                    "10.1002/asi.20591"
                ]
            }
        },
        "BIBREF4": {
            "title": "Visualizing data using t-SNE",
            "authors": [
                {
                    "first": "LVD",
                    "middle": [],
                    "last": "Maaten",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Mach. Learn. Res.",
            "volume": "9",
            "issn": "Nov",
            "pages": "2579-2605",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "Nonlinear dimensionality reduction by locally linear embedding",
            "authors": [
                {
                    "first": "ST",
                    "middle": [],
                    "last": "Roweis",
                    "suffix": ""
                },
                {
                    "first": "LK",
                    "middle": [],
                    "last": "Saul",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Science",
            "volume": "290",
            "issn": "5500",
            "pages": "2323-2326",
            "other_ids": {
                "DOI": [
                    "10.1126/science.290.5500.2323"
                ]
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [
                {
                    "first": "AL",
                    "middle": [],
                    "last": "Barab\u00e1si",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Network Science",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "Graph embedding techniques, applications, and performance: a survey",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Ferrara",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Knowl.-Based Syst.",
            "volume": "151",
            "issn": "",
            "pages": "78-94",
            "other_ids": {
                "DOI": [
                    "10.1016/j.knosys.2018.03.022"
                ]
            }
        }
    }
}