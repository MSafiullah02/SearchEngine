{
    "paper_id": "PMC7148082",
    "metadata": {
        "title": "On the Replicability of Combining Word Embeddings and Retrieval Models",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Luca",
                "middle": [],
                "last": "Papariello",
                "suffix": "",
                "email": "luca.papariello@researchstudio.at",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Bampoulidis",
                "suffix": "",
                "email": "alexandros.bampoulidis@researchstudio.at",
                "affiliation": {}
            },
            {
                "first": "Mihai",
                "middle": [],
                "last": "Lupu",
                "suffix": "",
                "email": "mihai.lupu@researchstudio.at",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "The last 5 years have seen proof that neural network-based word embedding models provide term representations that are a useful information source for a variety of tasks in natural language processing. In information retrieval (IR), \u201ctraditional\u201d models remain a high baseline to beat, particularly when considering efficiency in addition to effectiveness [6]. Combining the word embedding models with the traditional IR models is therefore very attractive and several papers have attempted to improve the baseline by adding in, in a more or less ad-hoc fashion, word-embedding information. Onal et al. [10] summarized the various developments of the last half-decade in the field of neural IR and group the methods in two categories: aggregate and learn. The first one, also known as compositional distributional semantics, starts from term representations and uses some function to combine them into a document representation (a simple example is a weighted sum). The second method uses the word embedding as a first layer of another neural network to output a document representation.",
            "cite_spans": [
                {
                    "start": 357,
                    "end": 358,
                    "mention": "6",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 604,
                    "end": 606,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The advantage of the first type of methods is that they often distill down to a linear combination (perhaps via a kernel), from which an explanation about the representation of the document is easier to induce than from the neural network layers built on top of a word embedding. Recently, the issue of explainability in IR and recommendation is generating a renewed interest [15].",
            "cite_spans": [
                {
                    "start": 377,
                    "end": 379,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this sense, Zhang et al. [14] introduced a new model for combining high-dimensional vectors, using a mixture model of von Mises-Fisher (VMF) instead of Gaussian distributions previously suggested by Clinchant and Perronnin [3]. This is an attractive hypothesis because the Gaussian Mixture Model (GMM) works on Euclidean distance, while the mixture of von Mises-Fisher (moVMF) model works on cosine distances\u2014the typical distance function in IR.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 31,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 227,
                    "end": 228,
                    "mention": "3",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In the following sections, we set up to replicate the experiments described by Zhang et al. [14]. They are grouped in three sets: classification, clustering, and information retrieval, and compare \u201cstandard\u201d embedding methods with the novel moVMF representation.",
            "cite_spans": [
                {
                    "start": 93,
                    "end": 95,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "All experiments are conducted on publicly available datasets and are briefly described here below.",
            "cite_spans": [],
            "section": "Datasets ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "\nClassification. Two subsets of the movie review dataset: (i) the subjectivity dataset (subj) [11]; and (ii) the sentence polarity dataset (sent) [12].Clustering. The 20 Newsgroups dataset1 was used in the original paper, but the concrete version was not specified. We selected the \u201cbydate\u201d version, because it is, according to its creators, the most commonly used in the literature. It is also the version directly load-able in scikit-learn2, making it therefore more likely that the authors had used this version.Retrieval. The TREC Robust04 collection [13].\n",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 97,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 147,
                    "end": 149,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 556,
                    "end": 558,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Datasets ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "The methods used to generate vectors for terms and documents are:TF-IDF. The basic term frequency - inverse document frequency method [5].Implemented in the scikit-learn library3.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 136,
                    "mention": "5",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Models ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "\nLSI. Latent Semantic Indexing [4].LDA. Latent Dirichlet Allocation [2].cBoW. Word2vec [9] in the Continuous Bag-of-Word (cBow) architecture.PV-DBOW/DM. Paragraph vector (PV) is a document embedding algorithm that builds on Word2vec. We use here both its implementations: Distributed Bag-of-Words (PV-DBOW) and Distributed Memory (PV-DM) [7].The LSI, LDA, cBoW, and PV implementations are available in the gensim library4.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 33,
                    "mention": "4",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 69,
                    "end": 70,
                    "mention": "2",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 88,
                    "end": 89,
                    "mention": "9",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 339,
                    "end": 340,
                    "mention": "7",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Models ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "\nFisher Kernel (FK). The FK framework offers the option to aggregate word embeddings to obtain fixed-length representations of documents. We use Fisher vectors (FV) based on (i) a Gaussian mixture model (FV-GMM) and (ii) a mixture of von Mises-Fisher distributions (FV-moVMF) [1].\n",
            "cite_spans": [
                {
                    "start": 277,
                    "end": 278,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Models ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "We first fit (i) a GMM and (ii) a moVMF model on previously learnt continuous word embeddings. The fixed-length representation of a document X containing T words \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_i$$\\end{document}\u2014expressed as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X = \\{ E_{w_1}, \\ldots , E_{w_T} \\}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_{w_i}$$\\end{document} is the word vector representation of word \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_i$$\\end{document}\u2014is then given by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {G}^X = [\\mathcal {G}_1^X, \\ldots , \\mathcal {G}_K^X]$$\\end{document}, where K is the number of mixture components. The vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {G}_i^X$$\\end{document}, having the dimension (d) of the word vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_{w_i}$$\\end{document}, are explicitly given by [3, 14]:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\text {(i)} \\quad \\mathcal {G}_i^X = \\frac{1}{\\sqrt{\\omega _i}} \\sum _{t=1}^T \\gamma _t (i) \\frac{x_t - \\mu _i}{\\sigma _i} , \\quad \\text {and} \\quad \\text {(ii)} \\quad \\mathcal {G}_i^X = \\sum _{t=1}^T \\frac{\\gamma _t (i) x_t d}{\\omega _i \\kappa _i} , \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\omega _i$$\\end{document} are the mixture weights, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma _t (i) = p(i | x_t )$$\\end{document} is the soft assignment of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_t$$\\end{document} to (i) Gaussian and (ii) VMF distribution i, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _i^2 = \\text {diag}(\\varSigma _i)$$\\end{document}, with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varSigma _i$$\\end{document} the covariance matrix of Gaussian i. In (i), \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _i$$\\end{document} refers to the mean vector; in (ii) it indicates the mean direction and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa _i$$\\end{document} is the concentration parameter.",
            "cite_spans": [
                {
                    "start": 2506,
                    "end": 2507,
                    "mention": "3",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 2509,
                    "end": 2511,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Models ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "We implement the FK-based algorithms by ourselves, with the help of the scikit-learn library for fitting a mixture of Gaussian models and of the Spherecluster package5 for fitting a mixture of von Mises-Fisher distributions to our data. The implementation details of each algorithm are described in what follows.",
            "cite_spans": [],
            "section": "Models ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "Logistic regression is used for classification in Zhang et al., and therefore also used here. The results of our experiments, for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d = 50$$\\end{document} and 100-dimensional feature vectors, are summarised in Table 1. For all the methods, we perform a parameter scan of the (inverse) regularisation strength of the logistic regression classifier, as shown in Fig. 1(a) and (b). Additionally, the learning algorithms are trained for a different number of epochs and the resulting classification accuracy assessed, cf. Fig. 1(c) and (d).\n",
            "cite_spans": [],
            "section": "Classification ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 632,
                    "end": 633,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 790,
                    "end": 791,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 483,
                    "end": 484,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Classification ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "Figure 1(a) indicates that cBow, FV-GMM, FV-moVMF, and the simple TF-IDF, when properly tuned, exhibit a very similar accuracy on subj\u2014the given confidence intervals do not indeed allow us to identify a single, best model. Surprisingly, TF-IDF outperforms all the others on the sent dataset (Fig. 1(b)). Increasing the dimensionality of the feature vectors, from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d = 50$$\\end{document} to 100, has the effect of reducing the gap between TF-IDF and the rest of the models on the sent dataset (see Table 1).",
            "cite_spans": [],
            "section": "Classification ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 297,
                    "end": 298,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 771,
                    "end": 772,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "For clustering experiments, the obtained feature vectors are passed to the k-means algorithm. The results of our experiments, measured in terms of Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI), are summarised in Table 2. We used both \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d = 20$$\\end{document} and 50-dimensional feature vectors. Note that the evaluation of the clustering algorithms is based on the knowledge of the ground truth class assignments, available in the 20 Newsgroups dataset.",
            "cite_spans": [],
            "section": "Clustering ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 238,
                    "end": 239,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "As opposed to classification, clustering experiments show a generous imbalance in performance and firmly speak in favour of PV-DBOW. Interestingly, TF-IDF, FV-GMM, and FV-moVMF, all providing high-dimensional document representations, have a low clustering effectiveness.\n\n",
            "cite_spans": [],
            "section": "Clustering ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "For these experiments, we extracted from every document of the test collection all the raw text, and preprocessed it as described in the beginning of this section. The documents were indexed and retrieved for BM25 with the Lucene 8.2 search engine. We experimented with three topic processing ways: (1) title only, (2) description only, and (3) title and description. The third way produces the best results and closest to the ones reported by Zhang et al. [14], and hence are the only ones reported here.",
            "cite_spans": [
                {
                    "start": 458,
                    "end": 460,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Document Retrieval ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "An important aspect of BM25 is the fact that the variation of its parameters \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textit{k}_{1}$$\\end{document} and b could bring significant improvement in performance, as reported by Lipani et al. [8]. Therefore, we performed a parameter scan for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k_1 \\in [0, 3]$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b \\in [0, 1]$$\\end{document} with a 0.05 step size for both parameters. For every TREC topic, the scores of the top 1000 documents retrieved from BM25 were normalised to [0,1] with the min-max normalisation method, and were used in calculating the scores of the documents for the combined models [14].",
            "cite_spans": [
                {
                    "start": 465,
                    "end": 466,
                    "mention": "8",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1384,
                    "end": 1386,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Document Retrieval ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "The original results, those of our replication experiments with standard (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k_1=1.2$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b=0.75$$\\end{document}) and best BM25 parameter values\u2014measured in terms of Mean Average Precision (MAP) and Precision at 20 (P@20)\u2014are outlined in Table 3.",
            "cite_spans": [],
            "section": "Document Retrieval ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 794,
                    "end": 795,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "We replicated previously reported experiments that presented evidence that a new mixture model, based on von Mises-Fisher distributions, outperformed a series of other models in three tasks (classification, clustering, and retrieval\u2014when combined with standard retrieval models).",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "Since the source code was not released in the original paper, important implementation and formulation details were omitted, and the authors never replied to our request for information, a significant effort has been devoted to reverse engineer the experiments. In general, for none of the tasks were we able to confirm the conclusions of the previous experiments: we do not have enough evidence to conclude that FV-moVMF outperforms the other methods. The situation is rather different when considering the effectiveness of these document representations for clustering purposes: we find indeed that the FV-moVMF significantly underperforms, contradicting previous conclusions. In the case of retrieval, although Zhang et al.\u2019s proposed method (FV-moVMF) indeed boosts BM25, it does not outperform most of the other models it was compared to.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Results of classification experiments on the subj and sent datasets. Shown are the mean accuracy and standard deviation, under 10-fold cross-validation, for optimally chosen hyperparameters (i.e. top values in Fig. 1).\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Results of clustering experiments (mean performance and standard deviation over 10 runs) in terms of Adjusted Rand Index (ARI) and Normalised Mutual Information (NMI).\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Results of retrieval experiments with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$95\\%$$\\end{document} confidence intervals.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Results of classification experiments, for 50-dimensional feature vectors, on the subj dataset [top panels, (a) and (c)] and sent dataset [bottom panels, (b) and (d)]. LSI and LDA achieve low accuracy (see Table 1) and are omitted here for visibility. The left panels [(a) and (b)] show the effect of (inverse) regularisation of the logistic regression classifier on the accuracy, while the right panels [(c) and (d)] display the effect of training for the learning algorithms. The two symbols on the right axis in panels (a) and (b) indicate the best (FV-moVMF) results reported in [14].",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Clustering on the unit hypersphere using von Mises-Fisher distributions",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "IS",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sra",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "J. Mach. Learn. Res.",
            "volume": "6",
            "issn": "",
            "pages": "1345-1382",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Neural information retrieval: at the end of the early years",
            "authors": [
                {
                    "first": "KD",
                    "middle": [],
                    "last": "Onal",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Inf. Retrieval J.",
            "volume": "21",
            "issn": "2",
            "pages": "111-182",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-017-9321-y"
                ]
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "The TREC robust retrieval track",
            "authors": [
                {
                    "first": "EM",
                    "middle": [],
                    "last": "Voorhees",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "SIGIR Forum",
            "volume": "39",
            "issn": "1",
            "pages": "11-20",
            "other_ids": {
                "DOI": [
                    "10.1145/1067268.1067272"
                ]
            }
        },
        "BIBREF5": {
            "title": "Aggregating neural word embeddings for document representation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "303-315",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Latent Dirichlet allocation",
            "authors": [
                {
                    "first": "DM",
                    "middle": [],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "AY",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "MI",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res.",
            "volume": "3",
            "issn": "",
            "pages": "993-1022",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "Indexing by latent semantic analysis",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Deerwester",
                    "suffix": ""
                },
                {
                    "first": "ST",
                    "middle": [],
                    "last": "Dumais",
                    "suffix": ""
                },
                {
                    "first": "GW",
                    "middle": [],
                    "last": "Furnas",
                    "suffix": ""
                },
                {
                    "first": "TK",
                    "middle": [],
                    "last": "Landauer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Harshman",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "J. Am. Soc. Inf. Sci.",
            "volume": "41",
            "issn": "6",
            "pages": "391-407",
            "other_ids": {
                "DOI": [
                    "10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9"
                ]
            }
        },
        "BIBREF10": {
            "title": "Distributional structure",
            "authors": [
                {
                    "first": "ZS",
                    "middle": [],
                    "last": "Harris",
                    "suffix": ""
                }
            ],
            "year": 1954,
            "venue": "Word",
            "volume": "10",
            "issn": "2\u20133",
            "pages": "146-162",
            "other_ids": {
                "DOI": [
                    "10.1080/00437956.1954.11659520"
                ]
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}