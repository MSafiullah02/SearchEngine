{
    "paper_id": "PMC7206263",
    "metadata": {
        "title": "Self-supervised Learning for Semi-supervised Time Series Classification",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Shayan",
                "middle": [],
                "last": "Jawed",
                "suffix": "",
                "email": "shayan@ismll.uni-hildesheim.de",
                "affiliation": {}
            },
            {
                "first": "Josif",
                "middle": [],
                "last": "Grabocka",
                "suffix": "",
                "email": "josif@ismll.uni-hildesheim.de",
                "affiliation": {}
            },
            {
                "first": "Lars",
                "middle": [],
                "last": "Schmidt-Thieme",
                "suffix": "",
                "email": "schmidt-thieme@ismll.uni-hildesheim.de",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Modern deep learning architectures have taken the fields of Computer Vision, Natural Language Processing and Recommender Systems by storm. Time series Classification is no stranger to Recurrent Neural Networks and Convolutional Neural Networks (ConvNets) too [6, 19]. Although proven to learn high level features across a broad domain of time series classification problems, the success of ConvNets hinges on the availability of large amounts of labeled training data. In reality, however, there is a high cost associated in acquiring such labeled data. As a result, there have been efforts to utilize semi-supervised learning algorithms catered especially for time series classification [2, 9, 12, 17, 21, 22]. The idea behind semi-supervised learning is to exploit unlabeled data for training purpose in the presence of only few labeled instances. The applicability of this learning paradigm naturally extends to time series data as plentiful of it can be acquired trivially. For example, a single polysomnography (sleep study) can generate up to 40,000 heartbeats but it takes the time and expertise of a cardiologist to annotate individual heartbeats [2]. Hence, effective methods for semi-supervised learning can lead to mining vast amounts of time series data for which only comparatively few labels might be available.",
            "cite_spans": [
                {
                    "start": 260,
                    "end": 261,
                    "mention": "6",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 263,
                    "end": 265,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 689,
                    "end": 690,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 692,
                    "end": 693,
                    "mention": "9",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 695,
                    "end": 697,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 699,
                    "end": 701,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 703,
                    "end": 705,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 707,
                    "end": 709,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1156,
                    "end": 1157,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A related stream of works has been dedicated to learning high level ConvNets based representations that do not require any manual annotation of data. Self-supervised learning has emerged as a prominent learning paradigm among such, where the idea is to define an annotation-free pretext task that is inherent in the data itself. The task stands to provide a surrogate supervision signal for feature learning. Example tasks include classifying image rotations [7], colorizing images [23] solving Jigsaw puzzles [15] to learn transferable representations for high-level tasks such as object detection and semantic segmentation. Until so far, applications have been limited to the Computer Vision domain.",
            "cite_spans": [
                {
                    "start": 460,
                    "end": 461,
                    "mention": "7",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 483,
                    "end": 485,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 511,
                    "end": 513,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In the same spirit of learning generalizable representations, we now introduce Multi-task learning. Multi-task learning is an important paradigm in machine learning which builds upon the idea of sharing knowledge between different tasks [1]. A set of tasks is learned in parallel, aiming to improve performance over each task compared with learning one of these tasks in isolation. A multi-task learning problem can also be formulated with respect to main and auxiliary tasks. Auxiliary tasks are motivated by the intuition that for most problem settings, performance over one particular task is of primary importance. However, in order to still reap the benefits of multi-task learning, related tasks could be modeled as auxiliary tasks [16]. These exist solely for the purpose of learning an enriched representation that could increase prediction accuracy over the main tasks.",
            "cite_spans": [
                {
                    "start": 238,
                    "end": 239,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 739,
                    "end": 741,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In our work, we bring together ideas from these high-impact research ideas of self-supervised learning and multi-task learning to propose an auxiliary forecasting task that is inherent in labeled and unlabeled time series data both. This auxiliary task stands to provide a strong surrogate supervision signal for feature learning which when learned in parallel with the main task of classification of time series boosts the performance of the classifier especially in semi-supervised setting. More specifically, we first define a sliding window function parametrized by hyper-parameters of stride and horizon to be forecasted. Next, we augment the training set with generated samples for the forecasting task by providing labeled and unlabeled samples as input to this function. The ConvNet model is trained jointly to classify the labeled samples and forecast future series values. This exploitation of the unlabeled samples leads to learning representations that help boost the classification accuracy. The intuition is that these unlabeled samples come from the same distribution and if the model learns the complex task of forecasting series values accurately, then the same latent representations could be leveraged for classification. In our experiments we show that is indeed the case and our proposed method excels in semi-supervised setting where only a few labeled instances might be available for the model to learn from.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To recap, our contributions are:A novel self-supervised task that is intuitive, requires close to no changes in the base network structure and provides a strong surrogate supervisory signal for feature learning in the realm of time series classification.A multi-task network which enables the forecasting and classification task to share latent representations and learns high-order interactions automatically.Extensive experimental evaluation of our self-supervised method in the domain of semi-supervised learning for time series classification and show that it outperforms state-of-the-art baselines.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The problem of learning with both labeled and unlabeled data is of central importance in machine learning [25]. We specifically review works that have focused on time series classification. We note the seminal work in the field from Wei et al. [21]. They proposed a self-training approach based on a nearest neighbor classifier. The work from [2] later improved the method significantly by proposing a new meta-feature based distance. In [14] a clustering approach was proposed combined with self-training. Another SSL algorithm in [12] also is in essence a clustering based method. The authors of [22] proposed a graph theoretic SSL algorithm that constructs graphs relating all samples based on different distance functions and consequently propagates labels. The current state-of-the-art method in the field [17] is based on shapelet learning [8] on both labeled and unlabeled time series data.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 109,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 245,
                    "end": 247,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 344,
                    "end": 345,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 439,
                    "end": 441,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 533,
                    "end": 535,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 599,
                    "end": 601,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 812,
                    "end": 814,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 847,
                    "end": 848,
                    "mention": "8",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "On the other hand, we note recent works [4, 5, 7, 15, 23] which showed that strong supervision could be leveraged by describing a task that is inherent in the data itself (requires no manual annotation). We consider the pioneering work by [4] which leveraged spatial context in an image for self-supervised learning by predicting relative location of one sampled patch to another. Similar self-supervised tasks were image colorization [23], solving jigsaw puzzles [15] and classifying image rotations [7]. More closely related to our work is a multi-task self-supervised network [5]. The work firstly tries to compare how the representations learned from recent proposed self-supervised approaches like above compare with each other, and then shows that combining these tasks even in a bare-bones multi-task network without catering for any controlled parameter sharing lifted the accuracy compared with the single-task networks compared before. Moreover, we also note the works that cater for temporal structure. Such temporal structure is inherent in video data, work in [13] proposed a sequential verification task to determine whether a sequence of frames was in correct order. It was shown that with this simple but intuitive task, the ConvNet captures temporally varying information such as human poses and ultimately lifted the accuracy on benchmark action recognition datasets. Another closely related example is [20] where the task was to recognize whether the video is playing forwards or backwards.",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 42,
                    "mention": "4",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 44,
                    "end": 45,
                    "mention": "5",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 47,
                    "end": 48,
                    "mention": "7",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 50,
                    "end": 52,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 54,
                    "end": 56,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 240,
                    "end": 241,
                    "mention": "4",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 436,
                    "end": 438,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 465,
                    "end": 467,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 502,
                    "end": 503,
                    "mention": "7",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 580,
                    "end": 581,
                    "mention": "5",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1074,
                    "end": 1076,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1422,
                    "end": 1424,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "With motivations behind our method set from the literature review, we now draw the following insights: firstly there exist semi-supervised learning approaches similar to ours that learn from unlabeled data, most notably current state-of-the-art shapelet learning approach [17] if we consider shapelets to be similar to convolutional filters. However, with our work we exploit deep learning based methods which solve an auxiliary self-supervised task of forecasting which forces the network to learn filters to solve this particular complex task. Secondly, there have been a plethora of works that proposed novel self-supervised tasks, however to the best of our knowledge, there are no examples for the time series domain and neither that cast a self-supervised task as an auxiliary task.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 275,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The core intuition to model forecasting as an auxiliary task is to force the ConvNet model to learn a set of rich hidden state representations from unlabeled but structured data. In the case of only few labeled instances being available as in semi-supervised setting, a fully-supervised approach can overfit on the training instances by learning a poor set of features which can hardly distinguish different classes. However, since training proceeds, by using the same set of features repeatedly the model can be more assuming of its predictions which would decrease the training loss in turn. In order to avoid this, a self-supervised task on unlabeled data could be leveraged that can learn comparatively more discriminative features for training and ultimately lead to a significant lift in accuracy on unseen data.",
            "cite_spans": [],
            "section": "Forecasting as a Self-supervised Task ::: Method",
            "ref_spans": []
        },
        {
            "text": "Additionally, forecasting is well-studied and easily formulated, but at the same time is complex enough which does not open any doors for cheating, as there are no trivial shortcuts for the model to exploit for solving the task [7]. Moreover, the task allows us flexibility in terms of data generation. By configuring the different values of the horizon and stride, h and s respectively, one could control the number of samples needed to configure an optimal balance between the classification and forecasting task samples.",
            "cite_spans": [
                {
                    "start": 229,
                    "end": 230,
                    "mention": "7",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Forecasting as a Self-supervised Task ::: Method",
            "ref_spans": []
        },
        {
            "text": "Central to the theory of multi-task learning is the leveraging of hidden state representations from multiple tasks simultaneously in order to create a more robust model. This begs the question as to whether there exist tasks that could mutually benefit each other by sharing parameters between. Naturally, forecasting fits well with a classification task in a multi-task model as both tasks share the same input space. Moreover, the learned feature spaces are expected to be correlated in turn also [1].",
            "cite_spans": [
                {
                    "start": 500,
                    "end": 501,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Multi-task Learning Approach ::: Method",
            "ref_spans": []
        },
        {
            "text": "However, designing a multi-task learning network poses two key challenges. Firstly, how to divide the feature space in shared and task-specific feature sets. Secondly, how to balance the weights between the different loss functions so as to distinguish between the main and auxiliary tasks. We rely on the hard-parameter sharing scheme, in which the learning parameters are all shared between the tasks up to the final fully connected layer in a layered architecture. From thereon, task-specific final layers output predictions for each task. This is illustrated in the Fig. 1 where we indicate shared parameters between the two tasks with same colored space. On the other hand, by adopting task specific weights we aim to cast the forecasting as an auxiliary task. We formulate the multi-task learning approach as an optimization process over the weighted sum of the two loss functions.3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{MTL}(X^F,\\theta _f,X^L,\\theta _c)=L_c (X^L, \\theta _c)+\\lambda L_f (X^F, \\theta _f) \\end{aligned}$$\\end{document}\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda $$\\end{document} is a hyper-parameter that controls parameter updates of the network relative to forecasting loss. It is thus crucial to tune for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda $$\\end{document} as too high of a value could bias the network weights for the forecasting task. On the other hand, if it is set too low, then the network would not learn for the forecasting task at all [1, 10].",
            "cite_spans": [
                {
                    "start": 2193,
                    "end": 2194,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 2196,
                    "end": 2198,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Multi-task Learning Approach ::: Method",
            "ref_spans": [
                {
                    "start": 575,
                    "end": 576,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "So far we have not drawn a link between the two feature sets of the tasks. In order to do so, consider that in a multi-task setting, the model is able to accurately forecast an unlabeled sample. The intuition is, if this unlabeled sample belongs to the same class as the very labeled sample the model is now trying to classify, and hence both are similar, then the latent features that were activated for the unlabeled sample could be leveraged to classify. Additionally, since the model is trained end-to-end, we also hypothesize that the model automatically learns to share latent representations between tasks and their corresponding high-order interactions based on this latent space.",
            "cite_spans": [],
            "section": "Multi-task Learning Approach ::: Method",
            "ref_spans": []
        },
        {
            "text": "Wei\u2019s method [21] is based on self-training through which the classifier iteratively augments the labeled set by adding a sample from the unlabeled set. The choice as to which sample to add is based on the (nearest neighbour) classifier\u2019s prediction of which sample was the closest to any of its labeled counterpart in euclidean distance. The newly added sample is then given the same class as its closest neighbour.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 16,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "DTW-D [2] is a meta-feature based distance. This distance was defined as the ratio of DTW to the euclidean distance. The intuition is to exploit the difference between the two distance\u2019s performance mainly the benefit of choosing DTW over the euclidean distance. Self-training is then carried out based on this distance.",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "SUCCESS [12] does constrained hierarchical clustering of the complete set of training samples, irrespective of labels. The distance metric utilized is DTW and all unlabeled samples are given the top-level seed\u2019s label.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 11,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Xu\u2019s method [22] is a graph theoretic SSL algorithm that constructs graphs relating all samples based on different distance functions such as DTW or Wavelet Transform. A probabilistic method optimally combines these various graphs after which a well studied harmonic Gaussian field based method [24] is adopted for label propagation.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 15,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 296,
                    "end": 298,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Bag-of-words [18] leverages a sliding window procedure to generate local segments from time series data. These local segments are used to create histograms to train an SVM model for classification. It is worth noting that this method differs from above as it uses only labeled samples.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 16,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "SSSL [17] is the current state-of-the-art method in the field. It uses shapelets to classify unlabeled samples thereby producing pseudo-labels. A coordinate descent solver wraps the optimization process by iteratively solving for the classification of labeled samples, pseudo-labels and shapelets respectively.",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 8,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Base [19] is a single-task variant of our proposed method that is only trained on the labeled samples to do classification.",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 8,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {\\Pi }$$\\end{document}-Model [11] is well-known semi-supervised learning method for image classification task. The basic idea is rooted in incorporating stronger regularization via ensembling. The method relies on dropout and asks the network being trained to output consistent labels for the same input. The input albeit goes through different dropout conditions leading to stochastic outputs. This makes it a well-defined task to exploit especially for unlabeled data. As the training proceeds, it is expected for the network\u2019s self-ensembled predictions to converge to the same labels for both labeled and unlabeled data. We sandwiched dropout layers after the batch-normalization layers and trained with dropout values of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$20\\%$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$40\\%$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 307,
                    "end": 309,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Transfer Learning is common in the regime of self-supervised learning based methods [4, 7]. Following these works, we train a non-linear classifier on top of each of a network\u2019s layers trained particularly for forecasting the datasets under consideration. The forecasting network in question is composed of stacking 6 convolutional layers in successive order with filter numbers 8, 16, 32, 64, 128, 256 respectively. Moreover, we sandwich maxpooling layers between halving the input in temporal dimension after each convolutional layer. Next, flattening and training 2 non-linear fully connected layers with dimensions 200 and 100. The very final layer\u2019s dimensionality corresponds to the horizon. This network is the result of an extensive grid search over multiple forecasting tasks from concurrent work. As we motivated, this network is geared towards forecasting in sharp contrast to the network in Fig. 1 adopted for the classification task. We trained this network with a grid search in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s \\times h$$\\end{document} where, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s \\in \\{0.05, 0.1, 0.2\\}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h \\in \\{0.1, 0.2\\}$$\\end{document}3, and used the configuration resulting with the least loss in Eq. 1.",
            "cite_spans": [
                {
                    "start": 85,
                    "end": 86,
                    "mention": "4",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 88,
                    "end": 89,
                    "mention": "7",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": [
                {
                    "start": 908,
                    "end": 909,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "This baseline serves to evaluate the self-supervised learned features from the forecasting task, by measuring classification accuracy that they achieve when we train a classifier on top of them without any fine-tuning [7]. This classifier has two non-linear layers corresponding to dimensions of 200 and 100 respectively. We hypothesize that if the features do correlate between the classification and forecasting task, then this non-linear classifier is expected to perform well.",
            "cite_spans": [
                {
                    "start": 219,
                    "end": 220,
                    "mention": "7",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We proposed a novel semi-supervised learning algorithm for time series classification based on a self-supervised feature learning task. We trained a ConvNet model that jointly classified and did auxiliary forecasting by sharing latent representations and learning high-order interactions end-to-end. As a result of exploiting the unlabeled data more effectively, our method was able to outperform state-of-the-art baselines. Future work includes extending our method to multivariate time series and researching additional ways to incorporate consistency regularization, which might yield better performance.",
            "cite_spans": [],
            "section": "Conclusion ::: Results",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Summary statistics of 13 real-world datasets from [3, 17].\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: The proposed method vs. baselines.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: This table reports maximum accuracy for our proposed approach when marginalizing out horizon and stride from all runs and possible \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda $$\\end{document} values.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: The proposed multi-task model for joint forecasting and classification of time series. We adopt this architecture from [19] where it was shown to outperform variety of baselines on a majority of datasets. We reuse the same parameters for f(.) up-to the last convolutional block, from where a dedicated linearly fully connected layer denoted by FC outputs for the horizon.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: We plot here the qualitative results for the forecasting task. We observe that the network is able to model the underlying distribution, albeit not perfectly.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Multitask learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Caruana",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Mach. Learn.",
            "volume": "28",
            "issn": "1",
            "pages": "41-75",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1007379606734"
                ]
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "SUCCESS: a new approach for semi-supervised classification of time-series",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Marussy",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Buza",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Artificial Intelligence and Soft Computing",
            "volume": "",
            "issn": "",
            "pages": "437-447",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "Shuffle and learn: unsupervised learning using temporal order verification",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Misra",
                    "suffix": ""
                },
                {
                    "first": "CL",
                    "middle": [],
                    "last": "Zitnick",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hebert",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision \u2013 ECCV 2016",
            "volume": "",
            "issn": "",
            "pages": "527-544",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Noroozi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Favaro",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision \u2013 ECCV 2016",
            "volume": "",
            "issn": "",
            "pages": "69-84",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Time series feature learning with labeled and unlabeled data",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Pattern Recogn.",
            "volume": "89",
            "issn": "",
            "pages": "55-66",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2018.12.026"
                ]
            }
        },
        "BIBREF9": {
            "title": "Bag-of-words representation for biomedical time series classification",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "MF",
                    "middle": [],
                    "last": "She",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Nahavandi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kouzani",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Biomed. Signal Process. Control",
            "volume": "8",
            "issn": "6",
            "pages": "634-644",
            "other_ids": {
                "DOI": [
                    "10.1016/j.bspc.2013.06.004"
                ]
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Colorful image colorization",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Isola",
                    "suffix": ""
                },
                {
                    "first": "AA",
                    "middle": [],
                    "last": "Efros",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision \u2013 ECCV 2016",
            "volume": "",
            "issn": "",
            "pages": "649-666",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Deep learning for time series classification: a review",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ismail Fawaz",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Forestier",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weber",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Idoumghar",
                    "suffix": ""
                },
                {
                    "first": "P-A",
                    "middle": [],
                    "last": "Muller",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Data Min. Knowl. Disc.",
            "volume": "33",
            "issn": "4",
            "pages": "917-963",
            "other_ids": {
                "DOI": [
                    "10.1007/s10618-019-00619-1"
                ]
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Invariant time-series factorization",
            "authors": [
                {
                    "first": "Josif",
                    "middle": [],
                    "last": "Grabocka",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "Schmidt-Thieme",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Data Min. Knowl. Disc.",
            "volume": "28",
            "issn": "5",
            "pages": "1455-1479",
            "other_ids": {
                "DOI": [
                    "10.1007/s10618-014-0364-z"
                ]
            }
        }
    }
}