{
    "paper_id": "a837f2b2af5adfb580eb68d661f1d30d629adffb",
    "metadata": {
        "title": "Solving constrained optimization problems without Lagrange multipliers",
        "authors": [
            {
                "first": "Cyril",
                "middle": [],
                "last": "Cayron",
                "suffix": "",
                "affiliation": {
                    "laboratory": "PX-Group chair",
                    "institution": "Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)",
                    "location": {}
                },
                "email": "cyril.cayron@epfl.ch"
            }
        ]
    },
    "abstract": [
        {
            "text": "Constrained optimization problems exist in all the domain of science, such as thermodynamics, mechanics, economics etc. For more than two centuries, these problems have been solved with the help of the Lagrange multipliers and the Lagrangian function. This method has however the disadvantage that it adds the number of constraint equations to the dimensionality of the function. Here, we propose a method that solves constrained optimization problems without Lagrange multiplier and without increasing the dimensionality of the problem. In addition, we show that thank to the constraint submatrices the function to be optimized can be developed in a Taylor series of any of its variable with coefficients algebraically determined by an iterative process of partial derivation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Constrained optimization problems exist in all the domains of science. In mechanics, the trajectory of a particle is that one that optimizes the action (the difference between the kinetics energy and the potential energy) along the path with the constraint that the total energy (the sum of the kinetics energy and the potential energy) is constant [1] [2] . In thermodynamics, the engineer wants to extract the maximum external work of a system to run a mechanical machine. The optimum is obtained by reducing at the minimum the irreversible entropy created during heat exchange between the system and the machine with the constraint that the total energy (system and machine) is constant [3] . Inversely, if the system is free to spontaneously change, no work can be extracted and the system's entropy increases to its maximum. In economics, one would like a system in which the exchanges (goods, information etc.) are such that they maximize the effective work useful for the society while minimizing the financial transactions with their inevitable irreversible losses in tax havens, with the constraints that the amount of goods is limited, and the amount of money is controlled by the central banks. These problems are in general very complex because of the high number of parameters [4] [5] . During a pandemic crisis, such as that of Coronavirus we are facing at the moment, trying to maintain at the minimum the mortality while keeping the economy running is also an optimization problem with a high number of intricate parameters. Optimization problems are actually everywhere.",
            "cite_spans": [
                {
                    "start": 353,
                    "end": 356,
                    "text": "[2]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 690,
                    "end": 693,
                    "text": "[3]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1290,
                    "end": 1293,
                    "text": "[4]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "text": "[5]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "For more than two centuries, the unique method to solve constrained optimization problems has consisted in using the Lagrange multipliers [6] [7] . Lagrange multiplier theorem states that for a real function in a N-dimension space of parameters 1 , \u2026 , that follows M constrains ( 1 , \u2026 , ) = for \u2208 [1, ], the stationary points of (maximum, minima or inflexion) are such that in these points the gradient of is a linear combination of the gradients of the constrained function . The Lagrange multipliers are the coefficients of this linear relation:",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 141,
                    "text": "[6]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 142,
                    "end": 145,
                    "text": "[7]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2207 ( 1 , \u2026 , ) = \u2211 \u2207 ( 1 , \u2026 , ) =1 (1)",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 34,
                    "text": "=1",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "where \u2207 and \u2207 are the gradients of the functions and . They are -dimension vectors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Let us illustrate the Lagrange theorem in 2D with a function ( , ). The function can represented by contour curves ( , ) = . At the optimum points obtained for some specific values of , the curve becomes tangent to that of the constraint ( , ) = C which implies than the gradient vectors \u2207 and \u2207 are parallel. Two graphical examples are shown in Figure 1 with (1a) ( , ) = 2 + 2 2 with the constraint ( , ) = \u2212 2 = 1 (1b) ( , ) = 2 + 2 2 \u2212 2 2 with the constraint ( , ) = + 2 = 1 with the contour curves of ( , ) represented in blue-to-red thermal colors, and the contour curve of the constraint ( , ) in red. In the example (a), there is one solution ( 1 , 1 ) = (1,0) for which ( 1 , 1 ) = 1 enlightened by the blue curve. In the example (b), there are three solutions ( 1 , 1 ) =",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 346,
                    "end": 354,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "(1,0), ( 2 , 2 ) = ( ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The points of optimum are the solutions of a system of + equations with + unknown \u2207\u2112( 1 , \u2026 , , 1 , \u2026 , ) = 0 and \u2200 \u2208 [1, ], ( 1 , \u2026 , ) = C , or more explicitly",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Consequently, the use of the Lagrange multipliers in optimization problems \"artificially\" increases the space dimension from to + . In addition, with this method, the number of constrains seems to be decorrelated from the dimensionality of the problem . This is unfortunate because we know that we need \u2212 1 constraint equations that are non-linearly linked to find the points of optimization. We will show that the problem can be solved in the initial -dimension space without the Lagrange multipliers, without increasing the dimensionality of the problem. The method we propose results from an intuitive, simple and effective equation. The fact that \u2212 1 constraints are required to get the solution will appear as an immediate consequence of the equation. = 0 (4) In other words, the optimum condition is replaced by a \"constraint-like\" equation, without Lagrange multiplier, without increasing the dimensionality of the initial problem.",
            "cite_spans": [
                {
                    "start": 761,
                    "end": 764,
                    "text": "(4)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The generalization is direct. Let us consider a space of dimension with a real function ( 1 , \u2026 , ) that we would like to optimize while respecting \u2212 1 constraints ( 1 , \u2026 , ) = C for \u2208 [1, \u2212 1]. We use ( , 1 , \u2026 \u22121 ) the -vector constituted by the functions and all the to define ( , ) to introduce the Jacobian of the problem: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "The optimum points ( 1 , \u2026 , ) on which is optimum with the constraints are those that check",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "with a non-null infinitesimal vector ( 1 ,\u2026, ). Consequently, the \"constraint-like\" equation that substitutes the optimum condition on to complete the \u2212 1 initial constraints is simply ( , ) = 0 (6) One can note that equation . Writing the gradients as column vectors, it is clear that such linearity condition is equivalent to the condition on the transpose of ( , ) :",
            "cite_spans": [
                {
                    "start": 195,
                    "end": 198,
                    "text": "(6)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "( ( , ) ) = 0. The equation ( ) = 0 is now obvious, but to our knowledge, it has never been reported. This is surprising because it makes any optimization problem of dimension equivalent to a solving a problem of equations. With the classical method based on the Lagrange multipliers and its associated Lagrangian function, one would have to solve 2 \u2212 1 equations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "The system (5) of equations allows us to determine the optima of the function . If the expression of and are complex, the solutions can be obtained only numerically. However, we will show that the function can be developed in Taylor series of any of the variable with coefficients algebraically determined thanks the constraint submatrices. These submatrices are also useful to determine the boundaries of the variables ( 1 , \u2026 , ) imposed by the constraints . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "The ( \u2212 1) matrix in this equation is the Jacobian ( ) of the vectorial function ( 1 , \u2026 \u22121 ) , the system is 1-underdetermined, which means that any is a linear combination of the other with \u2260 . The practical way to establish the linear relation is simple. Let us explain it with 1 . Equation (7) can be written",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "It is a 2 square submatrix of the Jacobian ( \u2212 1) matrix ( ) . If at the optimum point ( 1 , \u2026 , ), the matrix 1 is invertible, then all the other infinitesimals with \u2260 1 are proportional to 1 by the set of equalities written in the vectorial form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "The same reasoning can be applied to any infinitesimal . We call the submatrix of ( ) obtained by removing the k-line and the k-column:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "Equation (7) becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "If is invertible, the infinitesimal \u2200 \u2260 are proportional to by ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "Consequently, for any fixed k, \u2200 \u2260 ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "The equation was written here in a compact form, but , are actually functions of the N variables, and one should read for them , ( 1 , \u2026 , ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalization"
        },
        {
            "text": "The set of constraint equations ( 1 , \u2026 , ) = C define the constraint curve ( ) . This curve is bounded on the ( 1 , \u2026 , ) space. For any fixed k, the extrema values on the -axis verify the condition = 0, with \u2260 0 for at least one \u2260 . If the vector . They are thus often used to evaluate the rate of change of the optima due a relaxation of a given constraint, and check whether this value is a maximum or a minimum of this constraint. However, it is also possible to describe the \"nature\" of optimum due a relaxation of the initial parameters 1 , \u2026 , ; and this can be done without the Lagrange multipliers, by using the constraint submatrices that links the infinitesimal. As the constraint equations are not linearly linked, there is at least one matrix that is invertible, which means that the nature of the optimality of can be unambiguously defined at least along one direction . We will show that the function can be developed in a Taylor series of at any optimum point, and more generally at any point of the constraint curve ( ) . To simplify the rest of the section, let us consider the case where det ( ) \u2260 0, \u2200 \u2208 [1, ].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The constraint submatrices and the extrema of the g-constraint curve"
        },
        {
            "text": "Using the infinitesimal in mathematics is however always \"risky\". We tried to use the linear relations of the infinitesimal to calculate the second derivative 2 ( 1 , \u2026 , ) = \u2211 \u2211 2 ( 1 ,\u2026, ) =1 =1 , by changing by the product of by . We realized on the simple examples (1a) and (1b) that such a na\u00efve approach leads to incorrect results. We interpret this failure by the fact that the notations in the first derivative and in the second derivative are misleading. One should actually understand them as new variables, and one should write ( 1 , \u2026 , , \u2026 \u2026 ), i.e. as a function a 2 variables, and 2 ( 1 , \u2026 , , \u2026 \u2026 ) , i.e. as a function of function of + 2 variables. Tensor notations \uf0c4 would be more appropriate to describe the variables of multiderivative, and more generally to describe the differentials, as clearly pointed in Ref. [8] .",
            "cite_spans": [
                {
                    "start": 835,
                    "end": 838,
                    "text": "[8]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "The constraint submatrices and the extrema of the g-constraint curve"
        },
        {
            "text": "It is however possible to use the linear relations between the infinitesimals to determine the value of ( 1 , \u2026 , , ) , 2 ( 1 , \u2026 , ), \u2026 ( 1 , \u2026 , ) \u2026, and consequently establish the Taylor's series of along any variable at the -optimum points determined in section 2, and more generally at any point of the constraint curve ( ) . Let us come back to the usual partial derivative formula ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The constraint submatrices and the extrema of the g-constraint curve"
        },
        {
            "text": "The same process could be repeated -times to calculate the derivative of at the order ( ) ( 1 , \u2026 , ). Here, we limit the development to the second order. The calculation of the other terms of the Taylor series ( ) ( 1 , \u2026 , ) is possible by repeating the operation. Equation (15) can be useful to evaluate the behavior of the function around its optimum points. It is important to note that the algebraic expression ( ) ( 1 , \u2026 , ) can be determined by repeating the process of derivation and variable change described above, which means that the Taylor series of as a function of can be determined algebraically to any order , even if the forms of ( 1 , \u2026 , ) and ( 1 , \u2026 , ) are too complex to be solved algebraically and cannot be expressed as an algebraic function of .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The constraint submatrices and the extrema of the g-constraint curve"
        },
        {
            "text": "The examples we propose are simple in order to explain the resolution method without too long calculations. The Taylor series of can be calculated in any point, but they will be given only around the optimum points. Indeed, it is often of interest to evaluate the influence of the variables around the optima to get an idea of the accuracy required to reach them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Examples"
        },
        {
            "text": "Let us use the example of Figure 1a to compare our method with the usual Lagrange multiplier method. We want to optimize ( , ) = 2 + 2 2 with the constraint ( , ) = \u2212 2 = 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 26,
                    "end": 35,
                    "text": "Figure 1a",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Example 1a"
        },
        {
            "text": "The Lagrangian function is \u2112( , , ) = 2 + 2 2 + ( \u2212 2 \u2212 1 ). Its gradient is \u2207\u2112 is null when ). Only the first one is real and appears in Figure 1a .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 138,
                    "end": 147,
                    "text": "Figure 1a",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Example 1a"
        },
        {
            "text": "With the method we propose, the Jacobian is gives the boundaries of ( ) the constraint curve of the constraints. The -boundary points are obtained for = 0 and \u2260 0, which imposes that = 0, the point is thus (1,0) . This point is also the point of optimality of . The -boundary points are obtained for = 0 and \u2260 0, which is impossible, and means that the curve is not bounded along the -axis. This result is obvious is one considers the parabolic shape formed by the points of ( , ) = \u2212 2 = 1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 206,
                    "end": 211,
                    "text": "(1,0)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Example 1a"
        },
        {
            "text": "For the -direction, = 2 + 4 = 2 + 2 = 2( + 1) . Thus, \u2032 = = 2( + 1), and \u2032\u2032 = 2. There is no other higher order terms ( ) . Consequently, at the real optimal point (1,0), is exactly For the y-direction, = 2 + 4 = 4 + 4 . Thus, \u2032 = = 4 ( + 1). With the partial derivative \u2032 = 4 + 4( + 1) = (8 2 + 4 + 4) . Thus, \u2032\u2032 = 8 2 + 4 + 4. We conclude that at the optimal point (1,0), can be approximated by The same result could be obtained by writing the Taylor series of ( ) = 2 2 + (1 + 2 ) 2 up to the second order. The main advantage of the method we propose is that it works even when it is not possible to change the variables to write ( , ) as a function of uniquely, or as a function of uniquely, which can occur if the constraint equation contains complex intricate terms, for example 5 terms.",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 121,
                    "text": "( )",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Example 1a"
        },
        {
            "text": "In the example of Figure 1b we look for the optima of ( , ) = 2 + 2 2 \u2212 2 2 with the constraint ( , ) = + 2 = 1",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 27,
                    "text": "Figure 1b",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Example 1b"
        },
        {
            "text": "The Jacobian of the problem is ( , ) = ( 2 \u2212 2 2 4 \u2212 4 1 2 ). With the equation ( , ) = 0, the optimum problem becomes the solving problem The parabolic surfaces of \u0303( ) and \u0303( ) are represented in Figure 3 for the optimum points A and B. Those of C are deduced by symmetry \u2194 \u2212 . a b ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 198,
                    "end": 206,
                    "text": "Figure 3",
                    "ref_id": "FIGREF17"
                }
            ],
            "section": "Example 1b"
        },
        {
            "text": "Let us propose now an example in 3D. We look for the optima of ( , , ) = 2 \u2212 2 + 3 with the two constraints 1 ( , , ) = 2 + + = 1 and 2 ( , , ) = \u2212 2 = \u22121.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example 2"
        },
        {
            "text": "The contour surfaces of ( , , ) = are illustrated for different values of in Figure 4 . The contour surface of 1 ( , , ) = 1 and that of 2 ( , , ) = \u22121, with their intersection are shown in Figure 5 . The graph of ( ) and its Taylor series around these points are shown in Figure 8 . The graph of ( ) and its Taylor series around these points are shown in Figure 9 . The graph of ( ) and its Taylor series around these points are shown in Figure 10 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 77,
                    "end": 85,
                    "text": "Figure 4",
                    "ref_id": "FIGREF18"
                },
                {
                    "start": 190,
                    "end": 198,
                    "text": "Figure 5",
                    "ref_id": null
                },
                {
                    "start": 273,
                    "end": 281,
                    "text": "Figure 8",
                    "ref_id": "FIGREF21"
                },
                {
                    "start": 356,
                    "end": 364,
                    "text": "Figure 9",
                    "ref_id": "FIGREF22"
                },
                {
                    "start": 439,
                    "end": 448,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Example 2"
        },
        {
            "text": "The problem of optimizing a -dimension function with constraint equations has been treated until now with the Lagrange multipliers and the associated Lagrange function. Generally, the problem is solvable if = \u2212 1; finding the optima is equivalent to solve a system of 2 \u2212 1 equations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Here, we show that the problem that consists in optimizing a function of N variables 1 , \u2026 , with N-1 constraints of type ( 1 , \u2026 , ) = , \u2208 [1, \u2212 1] can be solved without the Lagrange multipliers and without increasing the dimensionality of . The optimality is obtained when In addition, for any \u2208 [1, ] we introduced the submatrices of the matrix ( ) , the Jacobian of the constraint functions. They are obtained by removing the k-line and the k-column of ( ) . The . Thanks to these relations and the partial derivative formula, one can determine by an iterative process for any -axis the algebraic expression of ( ) ( ), the -derivative of along . Consequently, for any point ( 1 , \u2026 , ) , can be developed in a Taylor series of around ( 1 , \u2026 , ) . The method works even if the algebraic forms of ( 1 , \u2026 , ) and ( 1 , \u2026 , ) = are too complex to get the algebraic expression of ( ) by substitution. Some simple 2D and 3D constrained optimization problems are proposed to illustrate to show practically how to determine the optima of , the boundaries of the constraint curve ( ) , and the parabolic Taylor series of around the optimum points along the main axes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We think that the method we propose will simplify the resolution of optimization problems. We would like to investigate its possible implications in physics, notably to clarify the connection between the second principle of thermodynamics and the least action principle of mechanics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "The Feynman Lectures on Physics",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Feynman",
                    "suffix": ""
                }
            ],
            "year": 1964,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "The Elementary Principles in Statistical Mechanics",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "J"
                    ],
                    "last": "Gibbs",
                    "suffix": ""
                }
            ],
            "year": 1902,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "New Optimality Principles for Economic Efficiency and Equilibrium",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "G"
                    ],
                    "last": "Luenberger",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Journal of Optimization Theory and Applications",
            "volume": "75",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Lagrange Multiplier Problems in Economics",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "V"
                    ],
                    "last": "Baxley",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Moorhouse",
                    "suffix": ""
                }
            ],
            "year": 1984,
            "venue": "The American Mathematical Monthly",
            "volume": "91",
            "issn": "",
            "pages": "404--412",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Mani\u00e8re plus simple et plus g\u00e9n\u00e9rale de faire usage de la formule de l'\u00e9quilibre donn\u00e9e dans la section deuxi\u00e8me",
            "authors": [
                {
                    "first": "J.-L",
                    "middle": [],
                    "last": "Lagrange",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "77--112",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Practical Optimization, Algorithms and Engineering Applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Antoniou",
                    "suffix": ""
                },
                {
                    "first": "W.-S",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Differential calculus, tensor products and the importance of notation",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Manton",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "for which the values of ( , ) are respectively 1 (curve in cyan), 2/3 twice (curves in blue). The details of the calculation will be given in section 4. Two 2D examples representing the function ( , ) by its contour curves ( , ) = . The optimum points are obtained for specifc values of when the contour curv of becomes tangent to the contour curve of ( , ) = . At these points, marked by the black spots, the gradients and are parallel and linearly linked by the Lagrange multipliers. (a) ( , ) = 2 + 2 2 with the constraint ( , ) = \u2212 2 = 1, (b) ( , ) = 2 + 2 2 \u2212 2 2 with the constraint ( , ) = + 2 = 1. The usual optimization method then consists in introducing the Lagrangian function \u2112 \u2112( 1 , \u2026 , , 1 , \u2026 , ) = ( 1 , \u2026 , ) \u2212 \u2211 ( 1 , \u2026 , ) =1",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Making the optimization problem a solving problem of same dimension 2.1 Intuitive approach Let us consider a 2D problem. We want to optimize ( , ) with the constraint ( , ) = C. Instead of representing the problem in 2D we temporarily use a third dimension to plot = ( , ). In this space the constraint ( , ) = C appears as a vertical cylinder along . The intersection of the two surfaces formed by ( , ) and ( , ) is a curve in the ( , , ) space, as illustrated in the examples (1a) and (1b) in Figure 2. a b",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "3D surface ( , , ) representation the 2D problems previously shown by iso-contours in Figure 1. The intersection of the surfaces represeting the function = ( , ) with the cylinder representing constraint ( , ) = is a curve enlightned in green. Its extrema are the solution of the constrained optimum problem.The optimum points of this 3D curve are the optimum points of ( , ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "( , ) = 0 leads to an additional equation if and only if the \u2212 1 initial constrain equations are not linearly linked. It is also clear that if the number of constraint equations were strictly lower than \u2212 1 the problem could not be solved because of its under-determination; the solutions could not be expressed as a finite set of points. Now, let us consider again the Lagrange theorem that tells that the optima are obtained when the gradient of is a linear combination of the gradients of the constraints , i.e. \u2207 ( 1 , \u2026 ,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "The constraint submatrices, the g-constraint domain and the Taylor series of f 3.1 The constraint submatrices and the infinitesimal variables Let us consider only the constraints. The infinitesimal entities are linearly linked each other by the derivative of the constraint equations",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Consequently, the extrema of the constraint curve ( ) along the -axis are the points ( 1 , \u2026 , ) that are the solutions of the system of \u2212 1 constraint submatrices and the Taylor's series of f The Lagrange multipliers are the partial derivatives of the Lagrangian on the constraint value of",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "that now we get the derivative of along as a function of 1 , \u2026 , :",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "At any point P ( 1 , \u2026 , ), the behavior of along the -axis with the other variables obeying the constraints ( 1 , . . . , ) = C \u2200 \u2260 , i.e. \u0303( ) = ( ) / ( 1 ,\u2026, )=C can be approximate to the second order by the polynomial form",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Even if it is here trivial, let us illustrate the role of the submatrices to determine the nature of the optimum in(1,0). Here, the Jacobian of the constraint is ( ) = (1, \u22122 ), so the submatrices are1 ( , ) = [\u22122 ] and 2 ( , ) = [1].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "exactly the equation that could be obtained by substituting the expression of y extracted from ( , ) inside the expression of ( , ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "To determine the nature of the optima, we use the Jacobian of the constraint ( ) = (1,2 ).The submatrices are 1 ( , ) = [2 ] and 2 ( , ) = [1]. . Of course this relation could have been found directly by derivation of ( , ). The boundaries of the constraint domain ( ) are the same as in the previous example and reduce to a unique point =",
            "latex": null,
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Polynomial Taylor approximation functions \u0303( ) and \u0303( ) in red and yellow, respectively, of the function around its g-constraint optimum points, (a) A and (b) B.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Contour surfaces of ( , , ) = 2 \u2212 2 + 3 = for \u2208 {\u221214, \u221212, \u221210, \u22129, \u22125,0,10} from blue to red.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Coutour surfaces of 1 ( , , ) = 2 + + = 1 and 2 ( , , ) = \u2212 2 = \u22121, with the intersection curve enlightened in green.In general, the solution is found by numerical methods, but here the problem can be solved algebraically (we used Mathematica). There are four real solutions and two irrational ones.The optimum points are positioned at the intersection of the three surfaces ( , , ) = ( ) with the two constraints 1 ( , , ) = 1 ( ) = 1 and 2 ( , , ) = 2 ( ) = \u22121, as shown in Contour surfaces the two constraints 1 ( , , ) = 1 ( ) = 1 (in green) and 2 ( , , ) = 2 ( ) = \u22121 (in orange), with ( , , ) = ( ) (in blue) at the optimum points = (a) , (b) , and (c) and . The values of at these points are (a) -14, (b) 1, (c) The linear relations between the infinitesimals , , are given by the submatrices of ( The boundary points of the constraint domain ( 1 , 2 ) along , and are determined by solving the initial constraints conditions of 1 and 2 with the additional condition that \u2192 {(0,3, \u22122), (0,0,1), (\u2212\u221a2, \u22121,0), (\u221a2, \u22121,0)} \u2192 {(0,3, \u22122), (0,0,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "The six points that bound the contraints domain.The submatrices also allow us to create the linear links between the infinitesimals:These relationships can now be used to determine the Taylor series expression of around its optimum points A, B, C, D. The derivative of is determined from the partial derivativesAlong the -axis, thanks to the relation (\u211b ), we get Along the -axis, thanks to the relation (\u211b ), with the same method, we get Along the -axis, thanks to the relation (\u211b ), with the same method, we get \u2032 = ( , , ) = 3( \u2212 2) \u2212 1 \u27f9 \u2032\u2032 ( , , ) = 6( \u2212 1)The Taylor series along the -axis is (the details are skipped)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "Taylor series of along the -axis. (a) Graph of ( ). Here, the function ( , , ) with the two contraints 1 ( , , ) and 2 ( , , ) can be transformed into two functions ( ) by substitution, in blue and orange. (b) Parabolic Taylor series of ( ) around A and B, and (c) around D (C is the same by symmetry \u2194 \u2212 ).The Taylor series along the -axis is:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "Taylor series of along the -axis. (a) Graph of ( ). Here, the function ( , , ) with the two contraints 1 ( , , ) and 2 ( , , ) can be transformed into two functions ( ) by substitution, in blue and orange. (b) Parabolic Taylor series of ( ) around A, (c) around B, and (d) around C and D.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Taylor series of along the -axis. (a) Graph of ( ). Here, the function ( , , ) with the two contraints 1 ( , , ) and 2 ( , , ) can be transformed into one function ( ) by substitution. (b) Parabolic Taylor series of ( ) around A, (c) around B, and (d) around C and D.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "where( , )  is the Jacobian of the vectorial function ( , 1 , \u2026 , \u22121 ). The optimization problem is thus directly transformed into a solving problem of N equations:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "the constraint curve ( ) along the -axis are the points ( 1 , \u2026 , ) that are the solutions of the system of \u2212",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}