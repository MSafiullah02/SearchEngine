{
    "paper_id": "PMC7148013",
    "metadata": {
        "title": "The Effect of Content-Equivalent Near-Duplicates on the Evaluation of Search Engines",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Maik",
                "middle": [],
                "last": "Fr\u00f6be",
                "suffix": "",
                "email": "maik.froebe@informatik.unihalle.de",
                "affiliation": {}
            },
            {
                "first": "Jan",
                "middle": [
                    "Philipp"
                ],
                "last": "Bittner",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Martin",
                "middle": [],
                "last": "Potthast",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Matthias",
                "middle": [],
                "last": "Hagen",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Web crawls contain pages that are duplicates or near-duplicate of other pages [15]. Although there can be legitimate reasons for a web publisher to host pages that duplicate other publishers\u2019 pages, the user of a web search engine gains nothing from viewing basically the same search result twice or more while browsing search results. Therefore, web search engines typically identify duplicates, either at crawl time, at indexing time, or at retrieval time, in order to remove all but one of them from their search results, showing only the \u201cbest\u201d version of a piece of content according to some selection criteria.",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 81,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The fact that duplicate results are not \u201cuseful\u201d to the users of a web search engine has not been overlooked: Treating results as irrelevant when they are found to be content-equivalent to a document the user has already seen, Bernstein and Zobel [4] applied this so-called \u201cnovelty principle\u201d during their analysis of content-equivalent documents within the GOV collections. With respect to the TREC 2004 Terabyte Track, their research shows (1) that 16.6% of all relevant documents in submitted runs are content-equivalent, and (2) that the application of the novelty principle causes MAP scores to decrease by 20% on average. This situation is an obstacle to progress, since doing the right thing and filtering duplicates is penalized. Fifteen years have passed since the report by Bernstein and Zobel, and we are curious as to whether anything has changed: Are there still duplicate documents in commonly used benchmarks, and if so, how do they affect the evaluation of retrieval systems?",
            "cite_spans": [
                {
                    "start": 248,
                    "end": 249,
                    "mention": "4",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The two contributions of this paper are (1) a reproduction, and (2) an extension and generalization of the work of Bernstein and Zobel on the effects of content-equivalent documents on search engine evaluation. We independently confirm their findings on the TREC 2004 Terabyte Track, using our own reimplementation of their approach.1 Thus validated, we go on to apply it to a selection of ad hoc retrieval tracks succeeding the one originally studied by Bernstein and Zobel: the Terabyte Track (2004\u20132006) [6, 7, 12], the Web Track (2009\u20132014) [8\u201311, 13, 14], and the recent Common Core Track (2017\u20132018) [1, 2]. Applying the novelty principle causes changes in the evaluation scores of all shared tasks under consideration. These changes do not uniformly spread across participants. A participant who applies the novelty principle independently of others can loose up to 53 positions.",
            "cite_spans": [
                {
                    "start": 508,
                    "end": 509,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 511,
                    "end": 512,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 514,
                    "end": 516,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 546,
                    "end": 547,
                    "mention": "8",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 548,
                    "end": 550,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 552,
                    "end": 554,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 556,
                    "end": 558,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 607,
                    "end": 608,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 610,
                    "end": 611,
                    "mention": "2",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Following Bernstein and Zobel, we first analyzed the corpora employed in the shared tasks under consideration by grouping their documents into retrieval equivalence classes. Retrieval equivalence is determined using a fingerprint function based on selected elements of a standard indexing pipeline: the document string is lowercased, all HTML tags, punctuation, and stop words are replaced by a blank, all remaining words are stemmed, and all white space sequences are collapsed. The resulting string is fed to a cryptographic hash function, and the hash value is used as the document\u2019s fingerprint. We reimplement these steps using widespread open-source tools (e.g., Anserini [17]). Figure 1 shows the results of our analysis.\n",
            "cite_spans": [
                {
                    "start": 679,
                    "end": 681,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Corpus Analysis: Retrieval-Equivalent Documents",
            "ref_spans": [
                {
                    "start": 692,
                    "end": 693,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Terabyte Track. The Terabyte Track employed the GOV2 corpus, a crawl of web sites hosted under the .gov domain that took place in 2004 [7]. Bernstein and Zobel also analyzed its predecessor, the GOV1 corpus, which is why we do so as well. Although efforts were made to remove duplicates during crawling, this did not include retrieval-equivalent documents. As shown in the table in Fig. 1, 23.39% of the GOV2 documents are equivalent to at least one other document for a total of 794,889 equivalence classes. Our results for both GOV1 and GOV2 deviate by only about 1% from those reported by Bernstein and Zobel (22,870 and 865,362 classes, respectively), which, given the corpus sizes, is sufficiently close to say that their experiment can be successfully reproduced. This is further corroborated by the fact that the plot of the distribution of class sizes for GOV2, by visual inspection, has the same characteristics as that of Bernstein and Zobel. The observed differences are due to the lacking descriptions in the original paper of the normalization steps of the fingerprint function. Asking the authors for details was deemed unnecessary given the closeness of fit. Although the organizers of this track took note of the work of Bernstein and Zobel, they did not report on any activities to reduce the impact of near-duplicates [6, 12].",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 137,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1337,
                    "end": 1338,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1340,
                    "end": 1342,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Corpus Analysis: Retrieval-Equivalent Documents",
            "ref_spans": [
                {
                    "start": 387,
                    "end": 388,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Web Track. The Web Track employed the unrestricted web crawls ClueWeb09 and, as of 2013, ClueWeb12. Although the track organizers reported the removal of some duplicate documents up front, our analysis shows that both corpora still contain a large proportion of retrieval-equivalent documents (7.74% and 14.71%, respectively). The corpora are about 40 times larger than the GOV2 corpus, and so are the numbers of equivalence classes.",
            "cite_spans": [],
            "section": "Corpus Analysis: Retrieval-Equivalent Documents",
            "ref_spans": []
        },
        {
            "text": "Core Track. The Core Track employed the New York Times Annotated Corpus (NYT-AC) in 2017, and the Washington Post (WaPo) corpus in 2018. One of this track\u2019s goals was to revisit the methodology of constructing evaluation corpora, implementing new ideas to avoid shortcomings of previous ones. The generation of relevance judgments for both corpora has been carefully carried out and documented. Unfortunately, we identify a proportion of retrieval-equivalent documents in the WaPo corpus similar to that of the ClueWeb12.",
            "cite_spans": [],
            "section": "Corpus Analysis: Retrieval-Equivalent Documents",
            "ref_spans": []
        },
        {
            "text": "Altogether, the GOV2 corpus has the largest proportion of retrieval-equivalent documents, followed by ClueWeb12 and WaPo, ClueWeb09 and GOV1, and last the NYT-AC corpus with the least duplicates. Since each track has at least one corpus with significant amounts of duplication, this merits further investigation.",
            "cite_spans": [],
            "section": "Corpus Analysis: Retrieval-Equivalent Documents",
            "ref_spans": []
        },
        {
            "text": "Bernstein and Zobel consider two documents to be content-equivalent if they convey the same information. To quantify content equivalence, they employ a similarity measure that first fingerprints each document as a set of word 8-grams, and then divides the number of overlapping 8-grams between both fingerprints by the mean of their sizes (previously introduced as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_3$$\\end{document} score in [3], and similar to the set-based resemblance measure of Broder [5]). An \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_3$$\\end{document} score of 0 indicates no syntactic overlap, and 1 retrieval-equivalence. From a user study, Bernstein and Zobel obtain an \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_3$$\\end{document} threshold of 0.58 above which content-equivalent pairs of GOV2 documents are identified with a precision of 0.95.",
            "cite_spans": [
                {
                    "start": 664,
                    "end": 665,
                    "mention": "3",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 728,
                    "end": 729,
                    "mention": "5",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Assessment of Content-Equivalent Documents",
            "ref_spans": []
        },
        {
            "text": "We repeated the user study for the GOV2 documents, as well as for the generic ClueWeb web pages and the news articles employed by the other tracks. We sampled 100 document pairs per track at random and judged their content equivalence, ensuring that the sample uniformly covers the range of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_3$$\\end{document} scores between 0.4 and 1. For the threshold reported by Bernstein and Zobel, we achieved only a precision of 87% on our sample under our interpretation of content equivalence. We hence chose the threshold 0.68 for GOV2 documents, 0.84 for generic ClueWeb pages, and 0.68 for news articles to obtain a precision of 0.95 for all three text genres. Given the threshold difference we obtained for the GOV2 documents, we compared all 50 topics of the Terabyte Track 2004 in detail with the results reported by Bernstein and Zobel and found only two with discrepancies; a reasonable result given the difficulty of repeating a user study.",
            "cite_spans": [],
            "section": "Assessment of Content-Equivalent Documents",
            "ref_spans": []
        },
        {
            "text": "Following Bernstein and Zobel, we implemented the SPEX algorithm [3] to identify all pairs of documents for which relevance judgments have been collected during one of the eleven editions of the three tracks, which exceed the aforementioned \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_3$$\\end{document} thresholds, respectively. Figure 2 shows box plots for each of the tracks, contrasting retrieval- and content-equivalent documents, when regarding only documents judged as relevant. Each box plot indicates the range of numbers of equivalent documents across the topics of its corresponding track. Except for the Core Track 2017, all tracks have topics with a high number of equivalent documents. Particularly striking is Topic 194 in the Web Track 2012 for the query designer dog breeds: among 47 relevant documents, there are 40 content-equivalent ones derived from the same Wikipedia article.\n",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 67,
                    "mention": "3",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Assessment of Content-Equivalent Documents",
            "ref_spans": [
                {
                    "start": 563,
                    "end": 564,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "The novelty principle states that a document, though relevant in isolation, is irrelevant if it is content-equivalent to a document the user has already seen in the result list. We quantify the effect of the novelty principle on the shared tasks under consideration. Like Bernstein and Zobel, we removed poorly performing runs\u2014keeping the best 75%\u2014to discount the effect of those runs.",
            "cite_spans": [],
            "section": "Impact of the Novelty Principle on Retrieval Evaluation",
            "ref_spans": []
        },
        {
            "text": "We experiment with two strategies to model the novelty principle: local judgment manipulation as per Bernstein and Zobel, and a global judgment manipulation of our own design. In local judgment manipulation, judgments are manipulated for each run independently, so that a document that is content-equivalent to another document is judged irrelevant if the latter appears above in a search results list. Bernstein and Zobel employ only local judgment manipulation, which does not consistently implement the novelty principle.\n",
            "cite_spans": [],
            "section": "Impact of the Novelty Principle on Retrieval Evaluation",
            "ref_spans": []
        },
        {
            "text": "Consider a ranking that does not contain any document from a given class of relevant, content-equivalent documents. In local judgment manipulation, all documents of that class are considered relevant for that ranking, whereas this is not the case for an alternative ranking that contains all documents of that class, where only one of them would be relevant. To resolve this contradiction, we propose a global judgment manipulation in which all documents of the same equivalence class\u2014except for a representative document\u2014are marked as irrelevant for a query. If a ranking contains documents of a relevant equivalence class, we apply the local judgment manipulation to choose the representative document. Otherwise, we chose a representative at random. In all our experiments, local manipulation amplified score changes compared to global manipulation.",
            "cite_spans": [],
            "section": "Impact of the Novelty Principle on Retrieval Evaluation",
            "ref_spans": []
        },
        {
            "text": "Bernstein and Zobel find that many content-equivalent documents have inconsistent relevance judgments, i.e., one being judged relevant, but not an equivalent one. We confirm this observation for all considered tracks. The minimum of 33 inconsistent classes was found in the Core Track 2018, and the maximum of 604 in the Terabyte Track 2004. The inconsistencies are fixed by assigning the entire equivalence class the most frequently occurring judgment.",
            "cite_spans": [],
            "section": "Impact of the Novelty Principle on Retrieval Evaluation",
            "ref_spans": []
        },
        {
            "text": "In their analysis, Bernstein and Zobel examine the impact of the novelty principle on Mean Average Precision (MAP) scores. Meanwhile, the use of MAP has been discouraged [16]. We hence also analyze the novelty principle\u2019s impact on nDCG scores, observing much greater changes in MAP scores than for nDCG. Due to space limitations, we only discuss the novelty principle\u2019s impact on nDCG scores under global manipulation; a reproduction of the MAP-based analysis is included in our accompanying repository (see link above).",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 173,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Impact of the Novelty Principle on Retrieval Evaluation",
            "ref_spans": []
        },
        {
            "text": "Table 1 shows the impact of the novelty principle on all considered tracks. Regarding the Terabyte Track 2004 (column group \u201cEquiv. irrelevant\u201d), we observe a reduction \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta _{\\mathrm {nDCG}}$$\\end{document} of the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {avg}_{\\mathrm {nDCG}}$$\\end{document} by 5.1% from 0.425 to 0.403. This reduction may be ignored if it were rank-preserving with respect to the ranking of the participating systems. The original ranking of systems correlates to the cleansed one with 0.96 Kendall\u2019s \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tau $$\\end{document}. This seems acceptable, but we measure only 0.8 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tau {}@5$$\\end{document}, regarding only the top five ranks; the best-performing systems are affected more strongly. We inspected how many ranks an \u201cideal system\u201d I would drop, if it were the only one to remove content-equivalent documents from its rankings: In the median, it would drop 9 ranks (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {med}_I$$\\end{document}) and in the worst case 19 (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\max _I$$\\end{document}). By contrast, if all systems had removed content-equivalent documents from their rankings/runs (column group \u201cEquiv. removed\u201d), the novelty principle\u2019s impact on the Terabyte Track 2004 would have been negligible. No ranking changes among the best-performing five systems occur, and the rank correlation of all systems increases to 0.98. Interestingly, even the average \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {nDCG}$$\\end{document} would have increased by 0.3%, caused by the fact that each run is only expected to return one document of a class of relevant content-equivalent documents under global judgment manipulation.",
            "cite_spans": [],
            "section": "Impact of the Novelty Principle on Retrieval Evaluation",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Similarly, Table 1 shows the novelty principle\u2019s impact on all other tracks under consideration. A complete discussion as exemplified for the Terabyte Track 2004 is beyond the space limitations; just a few more highlights: It turns out that the novelty principle has a strong impact on the Web Tracks of 2010 and 2012 in terms of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta _{\\mathrm {nDCG}}$$\\end{document}. But we observe the maximum drop of ranks (53, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\max _I$$\\end{document}) in the Terabyte Track 2006. Unlike for the Terabyte Track 2004, for the Core Track 2018, if all participants were to remove duplicates (\u201cEquiv. removed\u201d), we still observe a large difference in Kendall\u2019s \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tau {}@5$$\\end{document} of 0.73.",
            "cite_spans": [],
            "section": "Impact of the Novelty Principle on Retrieval Evaluation",
            "ref_spans": [
                {
                    "start": 17,
                    "end": 18,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We successfully reproduced the work of Bernstein and Zobel [4], confirming their findings on the impact of near-duplicate and content-equivalent documents on the evaluation of the TREC Terabyte Track 2004. In addition, we extended their analysis to all Terabyte Tracks, the Web Tracks, and the two recent Core Tracks, and we improved upon their original implementation of the novelty principle. With the exception of the Core Track 2017, all of the tracks under consideration are (strongly) affected by the presence of content-equivalent duplicates.",
            "cite_spans": [
                {
                    "start": 60,
                    "end": 61,
                    "mention": "4",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "Our findings are alarming. Not only are the evaluations carried out thus far invalidated to some extent, they also subdue newcomers: In practice, filtering duplicates from search results is done as a matter of course and without a second thought, and diligent participants may thus never learn that their retrieval systems would have actually outperformed the state of the art. One cannot expect anyone to realize that abstaining from filtering duplicates may result in better performance at TREC\u2014a conclusion one can only draw from an in-depth run analysis. This is a call to action to all track organizers to henceforth take duplicates into account.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: The impact of the novelty principle on the ranking of retrieval-systems under the scenarios that: (1)  content-equivalent documents are marked as irrelevant, (2)  content-equivalent documents are removed by the search engine. We report the average \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {nDCG}$$\\end{document} (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {avg}_{\\mathrm {nDCG}}$$\\end{document}), the median (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {med}_{I}$$\\end{document}) and maximum (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\max _{I}$$\\end{document}) ranking changes of the ideal participation model, changes in the average \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {nDCG}$$\\end{document} (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta _{\\mathrm {nDCG}}$$\\end{document}), as well as Kendall\u2019s \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tau $$\\end{document}, and Kendall\u2019s \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tau $$\\end{document} of the top-5 systems (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tau $$\\end{document}@5).\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Overview of the studied corpora (left), and the size of equivalence classes (right).",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Each box plots indicates the range of equivalent documents among the relevant documents across the topics of its respective track.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Some common mistakes in IR evaluation, and how they can be avoided",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Fuhr",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "SIGIR Forum",
            "volume": "51",
            "issn": "3",
            "pages": "32-41",
            "other_ids": {
                "DOI": [
                    "10.1145/3190580.3190586"
                ]
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "A scalable system for identifying co-derivative documents",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bernstein",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zobel",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "String Processing and Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "55-67",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}