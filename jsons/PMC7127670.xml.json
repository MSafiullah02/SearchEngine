{
    "paper_id": "PMC7127670",
    "metadata": {
        "title": "Mining association rules with multiple minimum supports: a new mining algorithm and a support tuning mechanism",
        "authors": [
            {
                "first": "Ya-Han",
                "middle": [],
                "last": "Hu",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Yen-Liang",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "email": "ylchen@mgt.ncu.edu.tw",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Data mining has recently attracted considerable attention from database practitioners and researchers because of its applicability in many areas such as decision support, market strategy and financial forecasts. Many approaches have been proposed to find out useful and invaluable information from huge databases [2], [7]. One of the most important approaches is mining association rules, which was first introduced in Ref. [1] and can be stated as follows.",
            "cite_spans": [
                {
                    "start": 313,
                    "end": 316,
                    "mention": "[2]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 318,
                    "end": 321,
                    "mention": "[7]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 424,
                    "end": 427,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Let I={i\n1, i\n2,\u2026, i\nm} be a set of items and D be a set of transactions, where each transaction T (a data case) is a set of items so that T\u2286I. An association rule is an implication of the form, X\u2192Y, where X\u2282I, Y\u2282I and X\u2229Y=\u03d5. The rule X\u2192Y holds in the transaction set T with confidence c, if c% of transactions in T that support X also support Y. The rule has support s in T if s% of the transactions in T contains X\u222aY. Given a set of transactions D (the database), the problem of mining association rules is to discover all association rules that have support and confidence greater than the user-specified minimum support (called minsup) and minimum confidence (called minconf).",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The key element that makes association-rule mining practical is minsup. It is used to prune the search space and to limit the number of rules generated. However, using only a single minsup implicitly assumes that all items in the database are of the same nature or of similar frequencies in the database. This is often not the case in real-life applications [10], [14]. In the retailing business, customers buy some items very frequently but other items very rarely. Usually, the necessities, consumables and low-price products are bought frequently, while the luxury goods, electric appliance and high-price products infrequently. In such a situation, if we set minsup too high, all the discovered patterns are concerned with those low-price products, which only contribute a small portion of the profit to the business. On the other hand, if we set minsup too low, we will generate too many meaningless frequent patterns and they will overload the decision makers, who may find it difficult to understand the patterns generated by data mining algorithms.",
            "cite_spans": [
                {
                    "start": 358,
                    "end": 362,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 364,
                    "end": 368,
                    "mention": "[14]",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The same difficulty may occur when we are about to mine medical records. Mining medical records is a very important issue in real-life application and it can reveal which symptoms are related to which disease. However, many important symptoms and diseases are infrequent in medical records. For example, flu occurs much more frequent than severe acute respiratory syndrome (SARS), and both have symptoms of fever and persistent cough. If the value of minsup is set high, though the rule \u201cflu\u2192fever, cough\u201d can be found, we would never find the rule \u201cSARS\u2192fever, cough.\u201d To find this SARS rule, we need to set the value of minsup very low. However, this will cause lots of meaningless rules to be found at the same time.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The dilemma faced in the two applications above is called the rare item problem\n[11]. In view of this, researchers either (A) split the data into a few blocks according to the frequencies of the items and then mine association rules in each block with a different minsup\n[9], or (B) group a number of related rare items together into an abstract item so that this abstract item is more frequent [6], [9]. The first approach is not satisfactory because rules that involve items across different blocks are difficult to find. Similarly, the second approach is unable to find rules that involve individual rare items and the more frequent items. Clearly, both approaches are ad hoc and \u201capproximate\u201d [9].",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 84,
                    "mention": "[11]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 271,
                    "end": 274,
                    "mention": "[9]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 395,
                    "end": 398,
                    "mention": "[6]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 400,
                    "end": 403,
                    "mention": "[9]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 697,
                    "end": 700,
                    "mention": "[9]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To solve this problem, Liu et al. [10] have extended the existing association rule model to allow the user to specify multiple minimum supports to reflect different natures and frequencies of items. Specifically, the user can specify a different minimum item support for each item. Thus, different rules may need to satisfy different minimum supports depending on what items are in the rules. This new model enables users to produce rare item rules without causing frequent items to generate too many meaningless rules. However, the proposed algorithm in Liu et al. [10], named the MSapriori algorithm, adopts an Apriori-like candidate set generation-and-test approach and it is always costly and time-consuming, especially when there exist long patterns. In this study, we propose a novel multiple item support tree (MIS-tree for short) structure, which extends the FP-tree structure [8] for storing compressed and crucial information about frequent patterns, and we develop an efficient MIS-tree-based mining method, the CFP-growth algorithm, for mining the complete set of frequent patterns with multiple minimum supports. The experimental result shows that the CFP-growth algorithm is efficient and scalable on both synthetic data and real-life data, and that it is about an order of magnitude faster than the MSapriori algorithm.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 38,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 566,
                    "end": 570,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 885,
                    "end": 888,
                    "mention": "[8]",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In real-life applications, users cannot find applicable support value at once and always tune its support value constantly. To do this, every time when users change the items' minsup, they must rescan database and then execute the mining algorithm once again. It is very time-consuming and costly. Thus, it is attractive to consider the possibility of designing a maintenance algorithm for tuning minimum supports (MS for short). In the past, although there were few researches dealing with this problem [3] for single MS scenario, most of previous researches are concerned with how to maintain the knowledge in correctness after the database is updated [4], [5], [12], [13].",
            "cite_spans": [
                {
                    "start": 504,
                    "end": 507,
                    "mention": "[3]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 654,
                    "end": 657,
                    "mention": "[4]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 659,
                    "end": 662,
                    "mention": "[5]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 664,
                    "end": 668,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 670,
                    "end": 674,
                    "mention": "[13]",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The problem addressed above will become even more serious for frequent pattern mining with multiple MS, because previously users only need to tune a single MS threshold but now they need to tune many MS thresholds. Thus, it is even more demanding to have a maintenance algorithm for MS tuning. This paper proposes, therefore, a maintenance algorithm to keep our MIS-tree in correct status after tuning MS. The experimental evaluation shows that our MIS-tree maintenance method can react almost instantaneously when tuning MS.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The remaining of the paper is organized as follows. In Section 2, we briefly review the Apriori algorithm [1], the MSapriori algorithm [10] and the FP-growth algorithm [8]. Some of those concepts will be used in developing our algorithm. Section 3 introduces the MIS-tree structure and its construction method. Then, we develop a MIS-tree-based frequent pattern mining algorithm, the CFP-growth algorithm, in Section 4. In Section 5, we propose the maintenance algorithm for MS tuning. The performance evaluation is done in Section 6. Finally, the conclusion is drawn in Section 7.",
            "cite_spans": [
                {
                    "start": 106,
                    "end": 109,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 135,
                    "end": 139,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 168,
                    "end": 171,
                    "mention": "[8]",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The Apriori algorithm [1] discovers frequent itemsets from databases by iteration. Basically, iteration i computes the set of frequent i-itemsets (frequent patterns with i items.) In the first iteration, the set of candidate 1-itemsets contains all items in the database. Then, the algorithm counts their supports by scanning the database, and those 1-itemsets whose supports satisfy the MS threshold are selected as frequent 1-itemsets.",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 25,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "The Apriori algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "In the kth (k\u22652) iteration, the algorithm consists of two steps. First, the set of frequent itemsets L\nk\u22121 found in the (k\u22121)th iteration is used to generate the set of candidate itemsets C\nk. Next, we compute the supports of candidate itemsets in C\nk by scanning the database and then we obtain the set L\nk of frequent k-itemsets.",
            "cite_spans": [],
            "section": "The Apriori algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "The iteration will be repeatedly executed until no candidate patterns can be found.",
            "cite_spans": [],
            "section": "The Apriori algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "The MSapriori algorithm [10] can find rare item rules without producing a huge number of meaningless rules. In this model, the definition of the minimum support is changed. Each item in the database can have its minsup, which is expressed in terms of minimum item support (MIS). In other words, users can specify different MIS values for different items. By assigning different MIS values to different items, we can reflect the natures of the items and their varied frequencies in the database.Definition 1Let I={a\n1, a\n2,\u2026, a\nm} be a set of items and MIS(a\ni) denote the MIS value of item a\ni. Then the MIS value of itemset A={a\n1, a\n2,\u2026, a\nk} (1\u2264k\u2264m) is equal to:Min[MIS(a1),MIS(a2),\u2026,MIS(ak)]\n\nExample 1Consider the following items in a database, bread, shoes and clothes. The user-specified MIS values are as follows:MIS(bread)=2%,MIS(shoes)=0.1%,MIS(clothes)=0.2%\nIf the support of itemset{clothes, bread} is 0.15%, then itemset{clothes, bread} is infrequent because the MIS value of itemset{clothes, bread} is equal to min[MIS(clothes), MIS(bread)]=0.2%, which is larger than 0.15%.The task of mining association rules is usually decomposed into two steps:(1)Frequent itemset generation: to find all frequent itemsets with supports exceeding minsup.(2)Rule generation: to construct from the set of frequent itemsets all association rules with confidences exceeding the minimum confidence.Note that, in order to generate the association rules from a frequent itemset, not only we need to know the support of this itemset, but the supports of all its subsets must also be known. Otherwise, it would be impossible to compute the confidences of all related rules.When there is only one single MS, the above two steps satisfy the downward closure property. That is, if an itemset is frequent, then all its subsets are also frequent. Therefore, after applying the Apriori algorithm we can find the support values of all subsets of frequent itemset{A, B, C, D} and all related rules as well. On the contrary, when there are multiple MS, the downward closure property no longer holds. That is, some subsets of a frequent itemset may not be frequent and their supports will be missing.\nExample 2Consider four items A, B, C and D in a database. Their MIS values are:\n\n",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 28,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "The MSapriori algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "If the support of itemset{B, C} is 13% and that of itemset{B, D} is 14%, then both itemsets {B, C} and {B, D} are infrequent; for they do not satisfy their MIS values (MIS(B, C)=min[MIS(B), MIS(C)]=15% and MIS(B, D)=min[MIS(B), MIS(D)]=15%). Suppose the support of itemset{A, B, C, D} is 8%. Then itemset{A, B, C, D} is frequent because MIS(A) is only 5%.",
            "cite_spans": [],
            "section": "The MSapriori algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "The above example indicates that a subset of a frequent itemset may be not frequent. Thus, the fact that the support of a frequent itemset is known does not necessarily imply that the supports of all its subsets are known. As a result, knowing the supports of all frequent itemsets is not enough to generate association rules.",
            "cite_spans": [],
            "section": "The MSapriori algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "The MSapriori algorithm aims to find all frequent itemsets by modifying the well-known Apriori algorithm. These modifications include presorting all the items according to their MIS values and modifying the candidate set generation procedure. After the application of the MSapriori algorithm, all frequent itemsets are found but the supports of some subsets may be still unknown. Thus, if we intend to generate association rules, we need a post-processing phase to find the supports of all subsets of frequent itemsets. This procedure is time-consuming because we need to scan the database again and compute the supports of all subsets of frequent itemsets.",
            "cite_spans": [],
            "section": "The MSapriori algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "An FP-tree is an extended prefix-tree structure for storing compressed and crucial information about frequent patterns, while the FP-growth algorithm uses the FP-tree structure to find the complete set of frequent patterns [8].",
            "cite_spans": [
                {
                    "start": 223,
                    "end": 226,
                    "mention": "[8]",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "The FP-growth algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "An FP-tree consists of one root labeled as \u201cnull\u201d, a set of item prefix subtrees as the children of the root and a frequent-item header table. Each node in the prefix subtree consists of three fields: item-name, count and node-link. The count of a node records the number of transactions in the database that share the prefix represented by the node, and node-link links to the next node in the FP-tree carrying the same item-name. Each entry in the frequent-item header table consists of two fields: item-name and head of node-link, which points to the first node in the FP-tree carrying the item-name. Besides, the FP-tree assumes that the items are sorted in decreasing order of their support counts, and only frequent items are included.",
            "cite_spans": [],
            "section": "The FP-growth algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "After the FP-tree is built, the FP-growth algorithm recursively builds conditional pattern base and conditional FP-tree for each frequent item from the FP-tree and then uses them to generate all frequent itemsets.",
            "cite_spans": [],
            "section": "The FP-growth algorithm ::: Related work",
            "ref_spans": []
        },
        {
            "text": "In this section, a new tree structure, named the MIS-tree, is proposed for mining frequent pattern with multiple MS. It is an extended version of the FP-tree structure.",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": []
        },
        {
            "text": "According to Definition 1, let DB={T\n1, T\n2,\u2026, T\nn} be a transaction database, where T\nj (j\u2208[1\u2026n]) is a transaction containing a set of items in I. The support of an itemset A is the percentage of transactions containing A in DB. If itemset A's support is no less than MIS(A), then pattern A is a frequent pattern.",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": []
        },
        {
            "text": "Let MIN denote the smallest MIS value of all items (MIN=min[MIS(a\n1), MIS(a\n2),\u2026, MIS(a\nm)]), and let the set of MIN_frequent items F denote the set of those items with supports no less than MIN.Example 3In Example 2, we have four items as well as their MIS values. The value of MIN is equal to min[MIS(A), MIS(B), MIS(C), MIS(D)]=5%. If A.support=3%, B.support=20%, C.support=25%, D.support=50%, then the set of MIN_frequent items F={B, C, D}\nLemma 1\nLet L\nk\nbe the set of frequent k-itemsets. Then each item in L\nk\n(k>1) must be in F.\n\n",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": []
        },
        {
            "text": "There is a very important difference between the FP-tree and the MIS-tree: the FP-tree only contains frequent items, but the MIS-tree consists of not only all frequent items but also those infrequent items with supports no less than MIN. Based on Lemma 1, each item in L\nk must belong to F. We must retain those infrequent items which belong to F because their supersets may be frequent itemsets.Example 4In Example 3, we know that A.support=3%, B.support=20%, C.support=25%, D.support=50%, and the set of MIN_frequent items F={B, C, D}. Consider the infrequent item C, where the support of item C=25% and MIS(C)=30%. We must retain the infrequent item C because the itemset{B, C} may be frequent. However, if the support of infrequent item C is less than MIN (not belonging to F), we can discard item C immediately.\nDefinition 2MIS-treeA multiple item support tree is a tree structure defined as follows.(1)It consists of one root labeled as \u201cnull\u201d, a set of item prefix subtrees as the children of the root, and a MIN_frequent item header table which contains all items in F.(2)Each node in the item prefix subtree consists of three fields: item-name, count and node-link, where item-name registers which item this node presents, count registers the number of transactions represented by the portion of the path reaching this node, and node-link links to the next node in the MIS-tree carrying the same item-name, or null if there is none.(3)Each entry in the MIN_frequent item header table consists of three fields: item-name, item's minsup MIS(a\ni) and head of node-link which points to the first node in the MIS-tree carrying the item-name.(4)All the items in the table are sorted in non-increasing order in terms of their MIS values.\nAccording to Definition 2, we have the following MIS-tree construction algorithm and each function used in Algorithm 1 is shown in Fig. 1\n.\n",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": [
                {
                    "start": 1871,
                    "end": 1877,
                    "mention": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We use the following example to illustrate the MIS-tree construction process.Example 5The construction of MIS-treeLet us consider the transaction database DB shown in Table 1\n. The MIS value of each item is shown in Table 2\n. According to Algorithm 1, the order of the items in the MIS-tree is arranged according to their MIS values in non-increasing order. For ease of discussion, the rightmost column of Table 1 lists all the items in each transaction following this order.\n",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": [
                {
                    "start": 167,
                    "end": 174,
                    "mention": "Table 1",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 216,
                    "end": 223,
                    "mention": "Table 2",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 406,
                    "end": 413,
                    "mention": "Table 1",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "To create the MIS-tree, we first create the root of the tree, labeled as \u201cnull\u201d. The scan of the first transaction leads to the construction of the first branch of the MIS-tree: ((a:1), (c:1), (d:1), (f:1)). Notice that all items in the transaction would be inserted into the tree according to their MIS values in non-increasing order. The second transaction (a, c, e, f, g) shares the same prefix (a, c) with the existing path (a, c, d, f). So, the count of each node along the prefix is increased by 1 and the remaining item list (e, f, g) in the second transaction would be created as the new nodes. The new node (e:1) is linked as a child of (c:2); node (f:1) as a child of (e:1); node (g:1) as a child of (f:1). For the third transaction (a, b, c, f, h), it shares only the node (a). Thus, a's count is increased by 1, and the remaining item list (b, c, f, h) in the third transaction would be created just like the second transaction. The remaining transactions in DB can be done in the same way.",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": []
        },
        {
            "text": "To facilitate tree traversal, a MIN_frequent item header table is built in which each item points to its occurrences in the tree via the head of node-link. Nodes with the same item-name are linked in sequence via such node-links. After all the transactions are scanned, the tree with the associated node-links is shown in Fig. 2\n.",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": [
                {
                    "start": 322,
                    "end": 328,
                    "mention": "Fig. 2",
                    "ref_id": "FIGREF11"
                }
            ]
        },
        {
            "text": "After scanning all the transactions, we will get the count of each item as (a:3, b:3, c:4, d:1, e:1, f:4, g:2, h:1) and the initial MIS-tree shown in Fig. 2. According to Lemma 1, we only need to retain those items with supports no less than MIN=2 (all items in F) in our MIS-tree. So, we remove the nodes with item-name=(d, e, h) and the result is shown in Fig. 3\n.",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": [
                {
                    "start": 150,
                    "end": 156,
                    "mention": "Fig. 2",
                    "ref_id": "FIGREF11"
                },
                {
                    "start": 358,
                    "end": 364,
                    "mention": "Fig. 3",
                    "ref_id": "FIGREF20"
                }
            ]
        },
        {
            "text": "After these nodes are removed, the remaining nodes in the MIS-tree may contain child nodes carrying the same item-name. For the sake of compactness, we traverse the MIS-tree and find that node (c:2) has two child nodes carrying the same item-name f. We merge these two nodes into a single node with item-name=f, and its count is set as the sum of counts of these two nodes (shown in Fig. 4\n). At last, the complete and compact MIS-tree is shown in Fig. 5\n.",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": [
                {
                    "start": 383,
                    "end": 389,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF21"
                },
                {
                    "start": 448,
                    "end": 454,
                    "mention": "Fig. 5",
                    "ref_id": "FIGREF22"
                }
            ]
        },
        {
            "text": "To construct the MIS-tree, our algorithm only needs one scan of the transaction database. This scan happens when we insert every transaction into the tree. After insertion, we delete those superfluous items from MIS-tree and merge nodes for compactness. Next, we will show that the MIS-tree contains the complete information for frequent pattern mining with multiple MS.Lemma 2\nGiven a transaction database DB and a support threshold MIS(a\ni\n) of each item a\ni\n, the constructed MIS-tree contains the complete information about frequent patterns in DB.\n\n",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": []
        },
        {
            "text": "\nRationale: In the MIS-tree's construction process, each transaction in DB is mapped to one path in the MIS-tree. And all MIN_frequent items information in each transaction is completely stored in the MIS-tree. Notice that we retained those infrequent items with supports no less than MIN in our MIS-tree because Lemma 1 indicates that these items' supersets may be frequent.",
            "cite_spans": [],
            "section": "Multiple Item Support tree (MIS-tree): design and construction",
            "ref_spans": []
        },
        {
            "text": "In this section, we will propose the CFP-growth method for mining the complete set of frequent patterns. Before presenting the algorithm, we observe some interesting properties of the MIS-tree structure.Definition 3Conditional patternA pattern x is called a\ni's conditional pattern if a\ni is in x and satisfies MIS(x)=MIS(a\ni).\nExample 6In Example 2, itemset{A, B, C, D} is an A's conditional pattern because MIS(A)=MIS({A, B, C, D}).\nDefinition 4Conditional frequent patternA frequent pattern x is called a\ni's conditional frequent pattern if a\ni is in x and satisfies MIS(x)=MIS(a\ni).\nExample 7In Example 2, itemset{A, B, C, D} is an A's conditional frequent pattern because itemset{A, B, C, D} is frequent and MIS(A)=MIS({A, B, C, D}).\nProperty 1Node-link propertyFor any frequent item a\ni, all the possible a\ni's conditional frequent patterns can be obtained by following a\ni's node-link, starting from a\ni's head in the MIS-tree header.\n",
            "cite_spans": [],
            "section": "The CFP-growth algorithm",
            "ref_spans": []
        },
        {
            "text": "This property is directly based on the construction process of the MIS-tree. Through the a\ni's node-link, all the transactions (built in the MIS-tree) related to a\ni would be traversed. Hence, it will find all the pattern information related to a\ni by following a\ni's node-link, and then all the a\ni's conditional frequent patterns can be obtained.Property 2Prefix path propertyTo calculate the a\ni's conditional frequent patterns in a path P, only the prefix subpath of node a\ni in P needs to be accumulated, and the frequency count of every node in the prefix path should carry the same count as node a\ni.\n",
            "cite_spans": [],
            "section": "The CFP-growth algorithm",
            "ref_spans": []
        },
        {
            "text": "\nRationale: Let the nodes along the path P be labeled as a\n1, a\n2,\u2026, a\nn in such an order that a\n1 is the root of the prefix subtree, a\nn is the leaf of the subtree in P, and a\ni (1\u2264i\u2264n) is the node being referenced. Based on the process of constructing MIS-tree presented in Algorithm 1, for each prefix node a\nk(1\u2264k<i), the prefix subpath of the node a\ni in P occurs together with a\nk exactly a\ni.count times. Thus, every such prefix node should carry the same count as node a\ni. Notice that a postfix node a\nm (i<m\u2264n) along the same path also co-occurs with node a\ni. However, the patterns with a\nm will be generated at the examination of the postfix node a\nm, and enclosing them here will lead to redundant generation of the patterns that would have been generated for a\nm.",
            "cite_spans": [],
            "section": "The CFP-growth algorithm",
            "ref_spans": []
        },
        {
            "text": "The MIS-tree itself does not give the frequent itemsets directly. Nevertheless, the CFP-growth algorithm recursively builds \u201cconditional MIS-trees\u201d, from the MIS-tree, which results in the set of all frequent itemsets. Let us illustrate the procedure by an example.Example 8According to Property 1, we collect all the patterns that a node a\ni participates in by starting from a\ni's head (in the MIN_frequent header table) and following a\ni's node-link. We examine the CFP-growth algorithm by starting from the bottom of the header table.For the MIS-tree in Fig. 5, let us consider how to build a conditional pattern base and conditional MIS-tree for item g. First, the node-link of item g is followed. Each such path in the MIS-tree ends at a node \u201cg\u201d. However, we exclude the node \u201cg\u201d itself and add it to the conditional pattern base and the conditional MIS-tree for item g. Counter of each node in the path is set to that of the node \u201cg\u201d itself. In this example, following the node-link for g, we get two paths in the MIS-tree: (a:3, c:2, f:2, g:1) and (b:2, f:1, g:1). To build the conditional pattern base and conditional MIS-tree for g, we exclude the node g in these two paths, (a:1, c:1, f:1) and (b:1, f:1). Notice that counters of the nodes in these two paths are all set to 1, because the counter values of both nodes g in the paths (a:3, c:2, f:2, g:1) and (b:2, f:1, g:1) are 1. After adding these two paths, the conditional MIS-tree for item g is shown in Fig. 6(a). Whether an item is frequent in the g's conditional MIS-tree is checked by following the node-link of each item, summing up the counts along the link and seeing whether it exceeds the MIS value of item g. In the conditional MIS-tree for g, the support count of a is 1, that of b is 1, that of c is 1 and that of f is 2. Since the MIS value of item g is 2, only item f is frequent in the g's conditional MIS-tree here. So we find g's conditional frequent pattern (fg:2). It is important that the CFP-growth method would not terminate here. After finding all the g's conditional patterns (ag, bg, cg, fg) at level 2, it will build ag, bg, cg and fg's conditional pattern base and conditional MIS-tree, respectively. For ag and bg's conditional pattern bases, they contain no items and would be terminated. For cg and fg's conditional pattern bases and conditional MIS-trees, we will find all the g's conditional patterns (acg, afg, cfg, bfg) at level 3 and then try to construct their conditional pattern bases, respectively. The CFP-growth method for item g will not be terminated until all g's conditional pattern bases contain no items. Repeatedly doing this for all items in the header table, we can get the whole conditional pattern base in Table 3\nand all conditional patterns in Table 4\n.\nFig. 7\nshows the detailed steps of the CFP-growth algorithm. In the following theorem, we show that the CFP-growth algorithm is both correct and complete. Here, \u201ccorrect\u201d means every pattern output by the algorithm is correct and \u201ccomplete\u201d means that every correct pattern will be output by the algorithm. However, to streamline the presentation we move the proof to Appendix A.\nTheorem 1\nThe CFP-growth algorithm is correct and complete.\n\n",
            "cite_spans": [],
            "section": "The CFP-growth algorithm",
            "ref_spans": [
                {
                    "start": 557,
                    "end": 563,
                    "mention": "Fig. 5",
                    "ref_id": "FIGREF22"
                },
                {
                    "start": 1470,
                    "end": 1476,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF23"
                },
                {
                    "start": 2774,
                    "end": 2780,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF24"
                },
                {
                    "start": 2724,
                    "end": 2731,
                    "mention": "Table 3",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 2764,
                    "end": 2771,
                    "mention": "Table 4",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "Note that, when we have multiple MS, knowing the support of a frequent itemset does not imply that the supports of all its subsets are known. Thus, the MIS-tree differs from the FP-tree in that the FP-tree only contains frequent items in the tree while MIS-tree may contain infrequent items. If we only want to find all frequent patterns without considering the problem of rule generation, we can discard those infrequent items in the MIS-tree. However, in our CFP-growth method, we do the pattern growth for each item in the MIS-tree, so that not only frequent patterns but also the support values of all their subsets are found. Doing this enables us to obtain the support values of all conditional patterns.",
            "cite_spans": [],
            "section": "The CFP-growth algorithm",
            "ref_spans": []
        },
        {
            "text": "The primary challenge of devising an effective maintenance algorithm for association rules is how to reuse the original frequent itemsets and avoid the possibility of rescanning the original database DB [4]. In this study, we focus on the maintenance of the MIS-tree, so that every time after we tune the items' supports, we can keep our MIS-tree in correct status without rescanning DB. The maintenance process can be stated as follows.",
            "cite_spans": [
                {
                    "start": 203,
                    "end": 206,
                    "mention": "[4]",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Tuning MS",
            "ref_spans": []
        },
        {
            "text": "First, after the user tunes the items' supports, we will get the new item order list in the MIS-tree. We need to determine which items should be moved up so that items in the MIS-tree can match the new item order. Notice that the MIN value is unchangeable during the support tuning process, and the MS of an item is not allowed to become either greater than MIN when it is smaller than MIN, or smaller than MIN when it is greater than MIN. In other words, all the items in the MIS-tree must be kept the same after the tuning process.",
            "cite_spans": [],
            "section": "Tuning MS",
            "ref_spans": []
        },
        {
            "text": "We add this restriction for two reasons. First, with this restriction we do not need to access the database again when we change the minimum supports, because all the data needed to find frequent patterns are kept in the MIS-tree. This can greatly improve the performance of the support tuning mechanism. Second, this restriction does not present any real problem to the maintenance algorithm, because none of the important patterns would be missing if we use a low MIN value. (This restriction does not harm the applicability of the tuning algorithm, because by setting a low value of MIN the items' supports can be tuned in a wide range).",
            "cite_spans": [],
            "section": "Tuning MS",
            "ref_spans": []
        },
        {
            "text": "In Table 5\n, we scan the items from the smallest old order to the largest one. If we find an item whose new order is smaller than its preceding items, then this item should be moved up. Continuously doing this, we can find all items that should be moved up. In Table 5, items c and f are two such items. As to item c, we see that the new order is 1. The items preceding c are items a and b, and their new orders are 2 and 4. So we find that the new order of item c is smaller than that of item b and should be moved up. As to item f, we see that the new order of item f is 3. The items preceding f are items c, a and b, and their new orders are 1, 2 and 4. Since the new order of item f is smaller than item b, it should be moved up. After this scanning, we know we must do the move-up operation twice: firstly, to move-up item c, and, secondly, to move-up item f. After the first move-up operation, its order becomes the one shown in the first column of the right table in Table 5. Finally, the second move-up makes it become the one shown in the last column in Table 5.",
            "cite_spans": [],
            "section": "Tuning MS",
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "mention": "Table 5",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 261,
                    "end": 268,
                    "mention": "Table 5",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 974,
                    "end": 981,
                    "mention": "Table 5",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 1063,
                    "end": 1070,
                    "mention": "Table 5",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "Note that an item may occur several times in the MIS-tree, where all of them are linked together through its node-link. When we decide to move up an item a\ni, we first find the entry with item-name=a\ni in the MIN_frequent item header table and the head of its node-link. By traversing the node-link, we can visit all the nodes carrying the same item-name. In each visit, we move up the node of this item to the correct position. Let the node we are currently visiting be node i, and let node f be the parent node of node i and node gf the grandparent of node i. If the new order of node f is smaller than that of node i, then the work is over. On the contrary, node i should be moved up above node f. Here, if f.support=i.support, then we can directly swap these two nodes without any other modifications. However, if f.support>i.support, then we split node f into two nodes, node f\n1 and node f\n2, where f\n1.support=i.support and f\n2.support=f.support\u2212i.support. As for node f\n1, we make node i as its only child node, and then we swap node f\n1 and node i; as for node f\n2, we make all child nodes of node f except node i as its child nodes; as for node gf, we make f\n1 and f\n2 as his children. This ascending process will be run repeatedly until the new order of the parent node is smaller than the currently visited node or until the parent node is the root. Fig. 8\nshows how we finish the two move-up operations above.",
            "cite_spans": [],
            "section": "Tuning MS",
            "ref_spans": [
                {
                    "start": 1362,
                    "end": 1368,
                    "mention": "Fig. 8",
                    "ref_id": "FIGREF25"
                }
            ]
        },
        {
            "text": "After moving up these nodes, the nodes in MIS-tree may contain child nodes carrying the same item-name. For the sake of compactness, we use MIS_merge method to merge those nodes. Following the example in Fig. 8, the MIS_merge method can be illustrated in Fig. 9\n.",
            "cite_spans": [],
            "section": "Tuning MS",
            "ref_spans": [
                {
                    "start": 204,
                    "end": 210,
                    "mention": "Fig. 8",
                    "ref_id": "FIGREF25"
                },
                {
                    "start": 255,
                    "end": 261,
                    "mention": "Fig. 9",
                    "ref_id": "FIGREF26"
                }
            ]
        },
        {
            "text": "In our experiments, we use the method proposed in Ref. [10] to assign MIS values to items. We use the actual frequencies of the items in the DB as the basis for MIS assignments. The formula can be stated as follows:\nMIS(ai)={M(ai)M(ai)>M\u2062I\u2062NM\u2062I\u2062NotherwiseM(ai)=\u03c3\u00d7f(ai)\n\n",
            "cite_spans": [
                {
                    "start": 55,
                    "end": 59,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Experimental evaluation on four algorithms ::: Experimental evaluation",
            "ref_spans": []
        },
        {
            "text": "M(a\ni) is the actual frequency of item a\ni in the DB. MIN denotes the smallest MIS value of all items. \u03c3(0\u2264\u03c3\u22641) is a parameter that controls how the MIS value for items should be related to their frequencies. If \u03c3=0, we have only one MS, MIN, which is the same as the traditional association rule mining.",
            "cite_spans": [],
            "section": "Experimental evaluation on four algorithms ::: Experimental evaluation",
            "ref_spans": []
        },
        {
            "text": "In the experiments, dataset N1000-T10-I4-D0200K is used in Fig. 10, Fig. 11, Fig. 12\n, N1000-T10-I4-D0400K used in Fig. 13, Fig. 14, Fig. 15\n, and N1000-T10-I4-D0600K used in Fig. 16, Fig. 17, Fig. 18\n. These figures compare the run times of the four algorithms with respect to \u03c3. In addition, the two real datasets are compared in Fig. 19, Fig. 20, Fig. 21, Fig. 22, Fig. 23, Fig. 24\n. From all these figures, we see that our CFP-growth algorithm is about an order of magnitude faster than the MSapriori algorithm in all datasets.",
            "cite_spans": [],
            "section": "Experimental evaluation on four algorithms ::: Experimental evaluation",
            "ref_spans": [
                {
                    "start": 59,
                    "end": 66,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 68,
                    "end": 75,
                    "mention": "Fig. 11",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 77,
                    "end": 84,
                    "mention": "Fig. 12",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 115,
                    "end": 122,
                    "mention": "Fig. 13",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 124,
                    "end": 131,
                    "mention": "Fig. 14",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 133,
                    "end": 140,
                    "mention": "Fig. 15",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 175,
                    "end": 182,
                    "mention": "Fig. 16",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 184,
                    "end": 191,
                    "mention": "Fig. 17",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 193,
                    "end": 200,
                    "mention": "Fig. 18",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 332,
                    "end": 339,
                    "mention": "Fig. 19",
                    "ref_id": "FIGREF10"
                },
                {
                    "start": 341,
                    "end": 348,
                    "mention": "Fig. 20",
                    "ref_id": "FIGREF12"
                },
                {
                    "start": 350,
                    "end": 357,
                    "mention": "Fig. 21",
                    "ref_id": "FIGREF13"
                },
                {
                    "start": 359,
                    "end": 366,
                    "mention": "Fig. 22",
                    "ref_id": "FIGREF14"
                },
                {
                    "start": 368,
                    "end": 375,
                    "mention": "Fig. 23",
                    "ref_id": "FIGREF15"
                },
                {
                    "start": 377,
                    "end": 384,
                    "mention": "Fig. 24",
                    "ref_id": "FIGREF16"
                }
            ]
        },
        {
            "text": "To test the scalability with the number of transactions, we used the N1000-T10-I4-D0200K, N1000-T10-I4-D0400K and N1000-T10-I4-D0600K for our experiments. The MIN value is set to 1% in Fig. 25(a), 0.75% in Fig. 25(b) and 0.5% in Fig. 25(c). The reported run time is the average of the 20 tests for \u03c3 from 0.05 to 1 (0.05, 0.1, 0.15,\u2026, 0.95, 1). The experiments show that the run times of these four algorithms (Apriori, FP-tree, MSapriori, MIS-tree) grow linearly when the number of transactions is increased form 200K to 600K. However, as the number of transactions is increased, the difference between (Apriori, MSapriori) and (FP-tree, MIS-tree) gets larger.",
            "cite_spans": [],
            "section": "Experimental evaluation on four algorithms ::: Experimental evaluation",
            "ref_spans": [
                {
                    "start": 185,
                    "end": 192,
                    "mention": "Fig. 25",
                    "ref_id": "FIGREF17"
                },
                {
                    "start": 206,
                    "end": 213,
                    "mention": "Fig. 25",
                    "ref_id": "FIGREF17"
                },
                {
                    "start": 229,
                    "end": 236,
                    "mention": "Fig. 25",
                    "ref_id": "FIGREF17"
                }
            ]
        },
        {
            "text": "To test the scalability with MIN, we used the N1000-T10-I4-D0200K, N1000-T10-I4-D0400K and N1000-T10-I4-D0600K for our experiments. As in the preceding paragraph, the reported run time is the average value for 20 cases. Fig. 25(d\u2013f) shows that the FP-growth and the CFP-growth algorithms have good scalability with respect to MIN. Besides, the FP-growth and CFP-growth algorithms perform much better than the Apriori and MSapriori algorithm in scalability. This is because as we decrease the support threshold, the number of frequent itemsets increases dramatically; in turn, this makes the set of candidate itemsets used in the Apriori algorithm and the MSapriori algorithm become extremely large. So, the time increases rapidly as well.",
            "cite_spans": [],
            "section": "Experimental evaluation on four algorithms ::: Experimental evaluation",
            "ref_spans": [
                {
                    "start": 220,
                    "end": 227,
                    "mention": "Fig. 25",
                    "ref_id": "FIGREF17"
                }
            ]
        },
        {
            "text": "All the experiments show that the CFP-growth algorithm is only a little slower than the FP-growth method. This result is quite encouraging, for our algorithm does two more things than the FP-growth algorithm does\u2014(1) we find frequent itemsets with multiple MS and (2) we find not only the supports of frequent itemsets but also the supports of all subsets of frequent itemsets.",
            "cite_spans": [],
            "section": "Experimental evaluation on four algorithms ::: Experimental evaluation",
            "ref_spans": []
        },
        {
            "text": "To test the performance of our tree maintenance algorithm when tuning MS, we compare it with the new construction of the MIS-tree. The reported run time is the average of the times spent in both synthetic and real-life datasets. The MIN value and \u03c3 are set to 0.5% and 0.5, respectively. We randomly choose items in F with probability varied from 5% to 80%. The new MIS value of each chosen item will be set by randomly selecting a value from the range [old\u00d7(1\u22120.05), old\u00d7(1+0.05)], where old denotes the original MIS value. The results in Fig. 26\nshow that in average using our MIS-tree maintenance method is able to save more than 70% run time of re-constructing the MIS-tree. Fig. 27\nshows the scalability with MS tuning process. All experiments show that the saving is very significant in practice.",
            "cite_spans": [],
            "section": "Experiments for MS tuning ::: Experimental evaluation",
            "ref_spans": [
                {
                    "start": 540,
                    "end": 547,
                    "mention": "Fig. 26",
                    "ref_id": "FIGREF18"
                },
                {
                    "start": 679,
                    "end": 686,
                    "mention": "Fig. 27",
                    "ref_id": "FIGREF19"
                }
            ]
        },
        {
            "text": "In this paper, we have developed an efficient algorithm for mining association rules with multiple MS and presented a maintenance mechanism for MS tuning without rescanning database. We have implemented the CFP-growth method and studied its performance in comparison with several frequent pattern mining algorithms. The results indicate that in all cases our algorithm is faster than the MSapriori algorithm. Besides, we also examined our maintenance algorithm for MS tuning. Experimental results show that our method is faster than the method of reconstructing the MIS-tree.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "In short, the paper has three main results. First, we have developed an efficient algorithm for mining frequent patterns with multiple MS. Second, we solve the problem occurred in the MSapriori algorithm that it cannot generate association rules unless a post-processing phase is executed. Our method finds not only all frequent itemsets but also all subsets needed for generating association rules. Finally, we develop an efficient maintenance algorithm for updating the MIS-tree when the user tunes items' MIS values.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "The paper can be extended in several ways. First, we only consider the MIS-tree maintenance problem after the minimum supports are changed. Since the database is subject to update in practice, an interesting problem arising immediately is how to maintain the MIS-tree after the database is updated. In addition, we may consider how to mine other kinds of knowledge under the constraint of multiple MS rather than setting a single MS threshold for all items. Because many kinds of knowledge that can be discovered from databases contain multiple items, all these types of knowledge can be extended naturally by setting different support thresholds for different items.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF1": {
            "text": "Table 1: A transaction database DB\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 2: The MIS value of each item in DB\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 3: Conditional pattern base\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 4: All conditional patterns and conditional frequent patterns\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 5: The old and the new item order in the MIS-tree\n",
            "type": "table"
        },
        "TABREF6": {
            "text": "Table 6: Parameter settings for synthetic data generation\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1: MIS-tree construction algorithm.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 10: N1000-T10-I4-D0200K (MIN=0.01).",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 11: N1000-T10-I4-D0200K (MIN=0.0075).",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 12: N1000-T10-I4-D0200K (MIN=0.005).",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig. 13: N1000-T10-I4-D0400K (MIN=0.01).",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Fig. 14: N1000-T10-I4-D0400K (MIN=0.0075).",
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Fig. 15: N1000-T10-I4-D0400K (MIN=0.005).",
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Fig. 16: N1000-T10-I4-D0600K (MIN=0.01).",
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Fig. 17: N1000-T10-I4-D0600K (MIN=0.0075).",
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Fig. 18: N1000-T10-I4-D0600K (MIN=0.005).",
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Fig. 19: BMS-POS dataset (MIN=0.01).",
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Fig. 2: The incompact MIS-tree (before MIS_Pruning and MIS_Merge).",
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Fig. 20: BMS-POS dataset (MIN=0.0075).",
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Fig. 21: BMS-POS dataset (MIN=0.005).",
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Fig. 22: BMS-Webview1 dataset (MIN=0.01).",
            "type": "figure"
        },
        "FIGREF15": {
            "text": "Fig. 23: BMS-Webview1 dataset (MIN=0.0075).",
            "type": "figure"
        },
        "FIGREF16": {
            "text": "Fig. 24: BMS-Webview1 dataset (MIN=0.005).",
            "type": "figure"
        },
        "FIGREF17": {
            "text": "Fig. 25: Scalability with number of transactions and support thresholds.",
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Fig. 26: Experimental results with MS tuning process.",
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Fig. 27: Scalability with MS tuning process.",
            "type": "figure"
        },
        "FIGREF20": {
            "text": "Fig. 3: MIS_Pruning process of MIS-tree.",
            "type": "figure"
        },
        "FIGREF21": {
            "text": "Fig. 4: MIS_Merge process of MIS-tree.",
            "type": "figure"
        },
        "FIGREF22": {
            "text": "Fig. 5: The complete and compact MIS-tree.",
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Fig. 6: g's conditional MIS-tree.",
            "type": "figure"
        },
        "FIGREF24": {
            "text": "Fig. 7: The CFP-growth algorithm.",
            "type": "figure"
        },
        "FIGREF25": {
            "text": "Fig. 8: Move-up method in MS tuning process.",
            "type": "figure"
        },
        "FIGREF26": {
            "text": "Fig. 9: MIS_merge method in MS tuning process.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Srikant",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Proceedings of the 20th Very Large DataBases Conference (VLDB'94), Santiago de Chile, Chile",
            "volume": "",
            "issn": "",
            "pages": "487-499",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [
                {
                    "first": "B.",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Mannila",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Quantifying the utility of the past in mining large databases",
            "authors": [
                {
                    "first": "V.",
                    "middle": [],
                    "last": "Pudi",
                    "suffix": ""
                },
                {
                    "first": "J.R.",
                    "middle": [],
                    "last": "Haritsa",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Information Systems",
            "volume": "25",
            "issn": "5",
            "pages": "323-343",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Bodagala",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Alsabti",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Ranka",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [
                {
                    "first": "M.C.",
                    "middle": [],
                    "last": "Tseng",
                    "suffix": ""
                },
                {
                    "first": "W.Y.",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "International Conference on Data Warehousing and Knowledge Discovery (DaWaK'01), Munich, Germany",
            "volume": "",
            "issn": "",
            "pages": "11-20",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [
                {
                    "first": "Z.",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Kohavi",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Mason",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "401-406",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Data mining: an overview from a database perspective",
            "authors": [
                {
                    "first": "M.S.",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "P.S.",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "volume": "8",
            "issn": "",
            "pages": "866-883",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Cheung",
                    "suffix": ""
                },
                {
                    "first": "O.R.",
                    "middle": [],
                    "last": "Zaiane",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proceedings of the 7th International Database Engineering and Applications Symposium (IDEAS'03), Hong Kong",
            "volume": "",
            "issn": "",
            "pages": "111-116",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Cheung",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "V.",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "C.Y.",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Proceedings of International Conference on Data Engineering (ICDE'96), New Orleans, LA, USA",
            "volume": "",
            "issn": "",
            "pages": "106-114",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Feldman",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Aumann",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Amir",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Mannila",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Proceedings of SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery (DMKD'97), Tucson, AZ, USA",
            "volume": "",
            "issn": "",
            "pages": "59-66",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Fu",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Proceedings of the 21th Very Large DataBases Conference (VLDB'95), Zurich, Switzerland",
            "volume": "",
            "issn": "",
            "pages": "420-431",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Kamber",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Pei",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "S.J.",
                    "middle": [],
                    "last": "Stolfo",
                    "suffix": ""
                },
                {
                    "first": "K.W.",
                    "middle": [],
                    "last": "Mok",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}