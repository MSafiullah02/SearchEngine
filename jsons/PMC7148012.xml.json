{
    "paper_id": "PMC7148012",
    "metadata": {
        "title": "Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-Shot Learning",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Sean",
                "middle": [],
                "last": "MacAvaney",
                "suffix": "",
                "email": "sean@ir.cs.georgetown.edu",
                "affiliation": {}
            },
            {
                "first": "Luca",
                "middle": [],
                "last": "Soldaini",
                "suffix": "",
                "email": "lssoldai@amazon.com",
                "affiliation": {}
            },
            {
                "first": "Nazli",
                "middle": [],
                "last": "Goharian",
                "suffix": "",
                "email": "nazli@ir.cs.georgetown.edu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Every day, billions of non-English speaking users [22] interact with search engines; however, commercial retrieval systems have been traditionally tailored to English queries, causing an information access divide between those who can and those who cannot speak this language [39]. Non-English search applications have been equally under-studied by most information retrieval researchers. Historically, ad-hoc retrieval systems have been primarily designed, trained, and evaluated on English corpora (e.g., [1, 5, 6, 23]). More recently, a new wave of supervised state-of-the-art ranking models have been proposed by researchers [11, 14, 21, 24, 26, 35, 37]; these models rely on neural architectures to rerank the head of search results retrieved using a traditional unsupervised ranking algorithm, such as BM25. Like previous ad-hoc ranking algorithms, these methods are almost exclusively trained and evaluated on English queries and documents.",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 53,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 277,
                    "end": 279,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 508,
                    "end": 509,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 511,
                    "end": 512,
                    "mention": "5",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 514,
                    "end": 515,
                    "mention": "6",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 517,
                    "end": 519,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 630,
                    "end": 632,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 634,
                    "end": 636,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 638,
                    "end": 640,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 642,
                    "end": 644,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 646,
                    "end": 648,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 650,
                    "end": 652,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 654,
                    "end": 656,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The absence of rankers designed to operate on languages other than English can largely be attributed to a lack of suitable publicly available data sets. This aspect particularly limits supervised ranking methods, as they require samples for training and validation. For English, previous research relied on English collections such as TREC Robust 2004 [32], the 2009\u20132014 TREC Web Track [7], and MS MARCO [2]. No datasets of similar size exist for other languages.",
            "cite_spans": [
                {
                    "start": 353,
                    "end": 355,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 388,
                    "end": 389,
                    "mention": "7",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 406,
                    "end": 407,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "While most of recent approaches have focused on ad hoc retrieval for English, some researchers have studied the problem of cross-lingual information retrieval. Under this setting, document collections are typically in English, while queries get translated to several languages; sometimes, the opposite setup is used. Throughout the years, several cross lingual tracks were included as part of TREC. TREC 6, 7, 8 [4] offer queries in English, German, Dutch, Spanish, French, and Italian. For all three years, the document collection was kept in English. CLEF also hosted multiple cross-lingual ad-hoc retrieval tasks from 2000 to 2009 [3]. Early systems for these tasks leveraged dictionary and statistical translation approaches, as well as other indexing optimizations [27]. More recently, approaches that rely on cross-lingual semantic representations (such as multilingual word embeddings) have been explored. For example, Vuli\u0107 and Moens [34] proposed BWESG, an algorithm to learn word embeddings on aligned documents that can be used to calculate document-query similarity. Sasaki et al. [28] leveraged a data set of Wikipedia pages in 25 languages to train a learning to rank algorithm for Japanese-English and Swahili-English cross-language retrieval. Litschko et al. [20] proposed an unsupervised framework that relies on aligned word embeddings. Ultimately, while related, these approaches are only beneficial to users who can understand documents in two or more languages instead of directly tackling non-English document retrieval.",
            "cite_spans": [
                {
                    "start": 413,
                    "end": 414,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 635,
                    "end": 636,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 771,
                    "end": 773,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 943,
                    "end": 945,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1094,
                    "end": 1096,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1276,
                    "end": 1278,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A few monolingual ad-hoc data sets exist, but most are too small to train a supervised ranking method. For example, TREC produced several non-English test collections: Spanish [12], Chinese Mandarin [31], and Arabic [25]. Other languages were explored, but the document collections are no longer available. The CLEF initiative includes some non-English monolingual datasets, though these are primarily focused on European languages [3]. Recently, Zheng et al. [40] introduced Sogou-QCL, a large query log dataset in Mandarin. Such datasets are only available for languages that already have large, established search engines.",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 179,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 200,
                    "end": 202,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 217,
                    "end": 219,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 433,
                    "end": 434,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 461,
                    "end": 463,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Inspired by the success of neural retrieval methods, this work focuses on studying the problem of monolingual ad-hoc retrieval on non English languages using supervised neural approaches. In particular, to circumvent the lack of training data, we leverage transfer learning techniques to train Arabic, Mandarin, and Spanish retrieval models using English training data. In the past few years, transfer learning between languages has been proven to be a remarkably effective approach for low-resource multilingual tasks (e.g. [16, 17, 29, 38]). Our model leverages a pre-trained multi-language transformer model to obtain an encoding for queries and documents in different languages; at train time, this encoding is used to predict relevance of query document pairs in English. We evaluate our models in a zero-shot setting; that is, we use them to predict relevance scores for query document pairs in languages never seen during training. By leveraging a pre-trained multilingual language model, which can be easily trained from abundant aligned [19] or unaligned [8] web text, we achieve competitive retrieval performance without having to rely on language specific relevance judgements. During the peer review of this article, a preprint [30] was published with similar observations as ours. In summary, our contributions are:We study zero shot transfer learning for IR in non-English languages.We propose a simple yet effective technique that leverages contextualized word embedding as multilingual encoder for query and document terms. Our approach outperforms several baselines on multiple non-English collections.We show that including additional in-language training samples may help further improve ranking performance.We release our code for pre-processing, initial retrieval, training, and evaluation of non-English datasets.1 We hope that this encourages others to consider cross-lingual modeling implications in future work.\n",
            "cite_spans": [
                {
                    "start": 526,
                    "end": 528,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 530,
                    "end": 532,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 534,
                    "end": 536,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 538,
                    "end": 540,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 1047,
                    "end": 1049,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1065,
                    "end": 1066,
                    "mention": "8",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 1241,
                    "end": 1243,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Zero-Shot Multi-lingual Ranking. Because large-scale relevance judgments are largely absent in languages other than English, we propose a new setting to evaluate learning-to-rank approaches: zero-shot cross-lingual ranking. This setting makes use of relevance data from one language that has a considerable amount of training data (e.g., English) for model training and validation, and applies the trained model to a different language for testing.",
            "cite_spans": [],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "More formally, let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {S}$$\\end{document} be a collection of relevance tuples in the source language, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {T}$$\\end{document} be a collection of relevance judgments from another language. Each relevance tuple \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\langle \\mathbf {q},\\mathbf {d},r\\rangle $$\\end{document} consists of a query, document, and relevance score, respectively. In typical evaluation environments, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {S}$$\\end{document} is segmented into multiple splits for training (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {S}_{train}$$\\end{document}) and testing (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {S}_{test}$$\\end{document}), such that there is no overlap of queries between the two splits. A ranking algorithm is tuned on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {S}_{train}$$\\end{document} to define the ranking function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\mathcal {S}_{train}}(\\mathbf {q},\\mathbf {d})\\in \\mathbb {R}$$\\end{document}, which is subsequently tested on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {S}_{test}$$\\end{document}. We propose instead tuning a model on all data from the source language (i.e., training \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{\\mathcal {S}}(\\cdot )$$\\end{document}), and testing on a collection from the second language (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {T}$$\\end{document}).",
            "cite_spans": [],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "Datasets. We evaluate on monolingual newswire datasets from three languages: Arabic, Mandarin, and Spanish. The Arabic document collection contains 384k documents (LDC2001T55), and we use topics/relevance information from the 2001\u201302 TREC Multilingual track (25 and 50 topics, respectively). For Mandarin, we use 130k news articles from LDC2000T52. Mandarin topics and relevance judgments are utilized from TREC 5 and 6 (26 and 28 topics, respectively). Finally, the Spanish collection contains 58k articles from LDC2000T51, and we use topics from TREC 3 and 4 (25 topics each). We use the topics, rather than the query descriptions, in all cases except TREC Spanish 4, in which only descriptions are provided. The topics more closely resemble real user queries than descriptions.2 We test on these collections because they are the only document collections available from TREC at this time.3\n",
            "cite_spans": [],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "We index the text content of each document using a modified version of Anserini with support for the languages we investigate [36]. Specifically, we add Anserini support for Lucene\u2019s Arabic and Spanish light stemming and stop word list (via SpanishAnalyzer and ArabicAnalyzer). We treat each character in Mandarin text as a single token.",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 129,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                }
            ],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "Modeling. We explore the following ranking models:Unsupervised baselines. We use the Anserini [36] implementation of BM25, RM3 query expansion, and the Sequential Dependency Model (SDM) as unsupervised baselines. In the spirit of the zero-shot setting, we use the default parameters from Anserini (i.e., assuming no data of the target language).PACRR [14] models n-gram relationships in the text using learned 2D convolutions and max pooling atop a query-document similarity matrix.KNRM [35] uses learned Gaussian kernel pooling functions over the query-document similarity matrix to rank documents.Vanilla BERT [21] uses the BERT [10] transformer model, with a dense layer atop the classification token to compute a ranking score. To support multiple languages, we use the base-multilingual-cased pretrained weights. These weights were trained on Wikipedia text from 104 languages.\n",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 97,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 352,
                    "end": 354,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 488,
                    "end": 490,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 613,
                    "end": 615,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 632,
                    "end": 634,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "We use the embedding layer output from base-multilingual-cased model for PACRR and KNRM. In pilot studies, we investigated using cross-lingual MUSE vectors [8] and the output representations from BERT, but found the BERT embeddings to be more effective.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 158,
                    "mention": "8",
                    "ref_id": "BIBREF38"
                }
            ],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "Experimental Setup. We train and validate models using TREC Robust 2004 collection [32]. TREC Robust 2004 contains 249 topics, 528k documents, and 311k relevance judgments in English (folds 1\u20134 from [15] for training, fold 5 for validation). Thus, the model is only exposed to English text in the training and validation stages (though the embedding and contextualized language models are trained on large amounts of unlabeled text in the languages). The validation dataset is used for parameter tuning and for the selection of the optimal training epoch (via nDCG@20). We train using pairwise softmax loss with Adam [18].",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 86,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 200,
                    "end": 202,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 618,
                    "end": 620,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "We evaluate the performance of the trained models by re-ranking the top 100 documents retrieved with BM25. We report MAP, Precision@20, and nDCG@20 to gauge the overall performance of our approach, and the percentage of judged documents in the top 20 ranked documents (judged@20) to evaluate how suitable the datasets are to approaches that did not contribute to the original judgments.\n",
            "cite_spans": [],
            "section": "Methodology",
            "ref_spans": []
        },
        {
            "text": "We present the ranking results in Table 1. We first point out that there is considerable variability in the performance of the unsupervised baselines; in some cases, RM3 and SDM outperform BM25, whereas in other cases they under-perform. Similarly, the PACRR and KNRM neural models also vary in effectiveness, though more frequently perform much worse than BM25. This makes sense because these models capture matching characteristics that are specific to English. For instance, n-gram patterns captured by PACRR for English do not necessarily transfer well to languages with different constituent order, such as Arabic (VSO instead of SVO). An interesting observation is that the Vanilla BERT model (which recall is only tuned on English text) generally outperforms a variety of approaches across three test languages. This is particularly remarkable because it is a single trained model that is effective across all three languages, without any difference in parameters. The exceptions are the Arabic 2001 dataset, in which it performs only comparably to BM25 and the MAP results for Spanish. For Spanish, RM3 is able to substantially improve recall (as evidenced by MAP), and since Vanilla BERT acts as a re-ranker atop BM25, it is unable to take advantage of this improved recall, despite significantly improving the precision-focused metrics. In all cases, Vanilla BERT exhibits judged@20 above 85%, indicating that these test collections are still valuable for evaluation.\n",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 40,
                    "end": 41,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "To test whether a small amount of in-language training data can further improve BERT ranking performance, we conduct an experiment that uses the other collection for each language as additional training data. The in-language samples are interleaved into the English training samples. Results for this few-shot setting are shown in Table 2. We find that the added topics for Arabic 2001 (+50) and Spanish 4 (+25) significantly improve the performance. This results in a model significantly better than BM25 for Arabic 2001, which suggests that there may be substantial distributional differences in the English TREC 2004 training and Arabic 2001 test collections. We further back this up by training an \u201coracle\u201d BERT model (training on the test data) for Arabic 2001, which yields a model substantially better (P@20 = 0.7340, nDCG@20 = 0.8093, MAP = 0.4250).",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 337,
                    "end": 338,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "We introduced a zero-shot multilingual setting for evaluation of neural ranking methods. This is an important setting due to the lack of training data available in many languages. We found that contextualized languages models (namely, BERT) have a big upper-hand, and are generally more suitable for cross-lingual performance than prior models (which may rely more heavily on phenomena exclusive to English). We also found that additional in-language training data may improve the performance, though not necessarily. By releasing our code and models, we hope that cross-lingual evaluation will become more commonplace.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Zero-shot multi-lingual results for various baseline and neural methods. Significant improvements and reductions in performance compared with BM25 are indicated with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow $$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\downarrow $$\\end{document}, respectively (paired t-test by query, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.05$$\\end{document}).\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Zero-Shot (ZS) and Few-Shot (FS) comparison for Vanilla BERT (multilingual) on each dataset. Within each metric and dataset, the top result is listed in bold. Significant increases from using FS are indicated with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow $$\\end{document} (paired t-test, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p<0.05$$\\end{document}).\n",
            "type": "table"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Probabilistic models of information retrieval based on measuring the divergence from randomness",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Amati",
                    "suffix": ""
                },
                {
                    "first": "CJ",
                    "middle": [],
                    "last": "Van Rijsbergen",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "ACM Trans. Inf. Syst. (TOIS)",
            "volume": "20",
            "issn": "4",
            "pages": "357-389",
            "other_ids": {
                "DOI": [
                    "10.1145/582415.582416"
                ]
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Google\u2019s multilingual neural machine translation system: enabling zero-shot translation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Trans. Assoc. Comput. Linguis.",
            "volume": "5",
            "issn": "",
            "pages": "339-351",
            "other_ids": {
                "DOI": [
                    "10.1162/tacl_a_00065"
                ]
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "An introduction to neural information retrieval",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Craswell",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Found. Trends\u00ae Inf. Retrieval",
            "volume": "13",
            "issn": "1",
            "pages": "1-126",
            "other_ids": {
                "DOI": [
                    "10.1561/1500000061"
                ]
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "Neural information retrieval: at the end of the early years",
            "authors": [
                {
                    "first": "KD",
                    "middle": [],
                    "last": "Onal",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Inf. Retrieval J.",
            "volume": "21",
            "issn": "2\u20133",
            "pages": "111-182",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-017-9321-y"
                ]
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Peters",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Braschler",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Clough",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Multilingual Information Retrieval: From Research to Practice",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "CLEF 2003 \u2013 overview of results",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Braschler",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Comparative Evaluation of Multilingual Information Access Systems",
            "volume": "",
            "issn": "",
            "pages": "44-63",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "Anserini: reproducible ranking baselines using Lucene",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J. Data Inf. Qual.",
            "volume": "10",
            "issn": "",
            "pages": "16:1-16:20",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "A survey of automatic query expansion in information retrieval",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Carpineto",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Romano",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ACM Comput. Surv. (CSUR)",
            "volume": "44",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1145/2071389.2071390"
                ]
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}