{
    "paper_id": "PMC7148208",
    "metadata": {
        "title": "Learning to Rank Images with Cross-Modal Graph Convolutions",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Thibault",
                "middle": [],
                "last": "Formal",
                "suffix": "",
                "email": "thibault.formal@naverlabs.com",
                "affiliation": {}
            },
            {
                "first": "St\u00e9phane",
                "middle": [],
                "last": "Clinchant",
                "suffix": "",
                "email": "stephane.clinchant@naverlabs.com",
                "affiliation": {}
            },
            {
                "first": "Jean-Michel",
                "middle": [],
                "last": "Renders",
                "suffix": "",
                "email": "jean-michel.renders@naverlabs.com",
                "affiliation": {}
            },
            {
                "first": "Sooyeol",
                "middle": [],
                "last": "Lee",
                "suffix": "",
                "email": "sooyeol.lee@navercorp.com",
                "affiliation": {}
            },
            {
                "first": "Geun",
                "middle": [
                    "Hee"
                ],
                "last": "Cho",
                "suffix": "",
                "email": "geunhee.cho@navercorp.com",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "This paper considers the typical image search scenario, where a user enters a text query, and the system returns a set of ranked images. More specifically, we are interested in re-ranking a subset of candidate images retrieved from the whole image collection by an efficient base ranker, following standard multi-stage ranking architectures in search engines [36]. Directly including visual features in the ranking process is actually not straightforward due to the semantic gap between text and images: this is why the problem has initially been addressed using standard text-based retrieval, relying for instance on text crawled from the image\u2019s webpage (e.g. surrounding text, title of the page etc.). In order to exploit visual information, and therefore improve the quality of the results \u2013especially because this text is generally noisy, and hardly describes the image semantic\u2013, many techniques have been developed since. For instance, some works have focused on building similarity measures by fusing mono-modal similarities, using either simple combination rules, or more complex propagation mechanisms in similarity graphs. More recently, techniques have emerged from the computer vision community, where text and images are embedded in the same latent space (a.k.a. joint embedding), allowing to directly match text queries to images. The latter are currently considered as state-of-the-art techniques for the cross-modal retrieval task. However, they are generally evaluated on artificial retrieval scenarios (e.g. on MSCOCO dataset [34]), and rarely considered in a re-ranking scenario, where mechanisms like pseudo-relevance feedback (PRF) [31] are highly effective.",
            "cite_spans": [
                {
                    "start": 360,
                    "end": 362,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1546,
                    "end": 1548,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1655,
                    "end": 1657,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We propose to revisit the problem of cross-modal retrieval in the context of re-ranking. Our first contribution is to derive a general formulation of a differentiable architecture, drawing inspiration from cross-modal retrieval, learning to rank, neural information retrieval and graph neural networks. Compared to joint embedding approaches, we tackle the problem in a different view: instead of learning new (joint) embeddings, we focus on designing a model that learns to combine information from different modalities. Finally, we validate our approach on two datasets, using simple instances of our general formulation, and show that the approach is not only able to reproduce PRF, but actually outperform it.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Cross-Modal Retrieval. In the literature, two main lines of work can be distinguished regarding cross-modal retrieval: the first one focuses on designing effective cross-modal similarity measures (e.g. [2, 10]), while the second seeks to learn how to map images and text into a shared latent space (e.g. [15, 18, 19, 54]).",
            "cite_spans": [
                {
                    "start": 203,
                    "end": 204,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 206,
                    "end": 208,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 305,
                    "end": 307,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 309,
                    "end": 311,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 313,
                    "end": 315,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 317,
                    "end": 319,
                    "mention": "54",
                    "ref_id": "BIBREF49"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The first set of approaches simply combines different mono-media similarity signals, relying either on simple aggregation rules, or on unsupervised cross-modal PRF mechanisms, that depend on the choice of a few but critical hyper-parameters [2, 10, 11, 45]. As it will be discussed in the next section, the latter can be formulated as a two-step PRF propagation process in a graph, where nodes represent multi-modal objects and edges encode their visual similarities. It has been later extended to more general propagation processes based on random walks [28].",
            "cite_spans": [
                {
                    "start": 242,
                    "end": 243,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 245,
                    "end": 247,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 249,
                    "end": 251,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 253,
                    "end": 255,
                    "mention": "45",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 556,
                    "end": 558,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Alternatively, joint embedding techniques aim at learning a mapping between textual and visual representations [15, 18, 19, 23, 52\u201355, 61]. Canonical Correlation Analysis (CCA) [17] and its deep variants [5, 27, 58], as well as bi-directional ranking losses [8, 9, 52, 53, 55, 61] (or triplet losses) ensure that, in the new latent space, an image and its corresponding text are correlated or close enough w.r.t. to the other images and pieces of text in the training collection. Other objective functions utilize metric learning losses [35], machine translation-based measures [44] or even adversarial losses [51].",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 114,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 116,
                    "end": 118,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 120,
                    "end": 122,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 124,
                    "end": 126,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 128,
                    "end": 130,
                    "mention": "52",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 131,
                    "end": 133,
                    "mention": "55",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 135,
                    "end": 137,
                    "mention": "61",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 178,
                    "end": 180,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 205,
                    "end": 206,
                    "mention": "5",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 208,
                    "end": 210,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 212,
                    "end": 214,
                    "mention": "58",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 259,
                    "end": 260,
                    "mention": "8",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 262,
                    "end": 263,
                    "mention": "9",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 265,
                    "end": 267,
                    "mention": "52",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 269,
                    "end": 271,
                    "mention": "53",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 273,
                    "end": 275,
                    "mention": "55",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 277,
                    "end": 279,
                    "mention": "61",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 538,
                    "end": 540,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 579,
                    "end": 581,
                    "mention": "44",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 611,
                    "end": 613,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "These approaches suffer from several limitations [61]: they are sensitive to the triplet sampling strategy as well as the choice of appropriate margins in the ranking losses. Moreover, constituting a training set that ensures good learning and generalization is not an easy task: the text associated to an image should describe its visual content (e.g. \u201ca man speaking in front of a camera in a park\u201d), and nothing else (e.g. \u201cthe President of the US, the 10th of March\u201d, \u201cJohn Doe\u201d, \u201cjoy and happiness\u201d).",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 52,
                    "mention": "61",
                    "ref_id": "BIBREF57"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Building a universal training collection of paired (image, text) instances, where text describes faithfully the content of the image in terms of elementary objects and their relationships, would be too expensive and time-consuming in practice. Consequently, image search engines rely on such pairs crawled from the Web, where the link between image and text (e.g. image caption, surrounding sentences etc.) is tenuous and noisy.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "To circumvent this problem, query logs could be used but, unfortunately \u2013and this is our second argument regarding the limitations\u2013, real queries are never expressed in the same way as the ones considered when evaluating joint embedding methods (e.g. artificial retrieval setting on MSCOCO [34] or Flickr-30K [43] datasets, where the query is the full canonical textual description of the image). In practice, queries are characterised by very large intent gaps: they do not really describe the content of the image but, most of the time, contain only a few words, and are far from expressing the true visual needs. What does it mean to impose close representations for all images representing \u201cParis\u201d (e.g. \u201cthe Eiffel Tower\u201d, \u201cLouvre Museum\u201d), even if they can be associated to the same textual unit?",
            "cite_spans": [
                {
                    "start": 291,
                    "end": 293,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 310,
                    "end": 312,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Neural Information Retrieval. Neural networks, such as RankNet and LambdaRank, have been intensively used in IR to address the learning to rank task [7]. More recently, there has been a growing interest in designing effective IR models with neural models [1, 12, 13, 20, 25, 26, 37, 38, 41, 56], by learning the features useful for the ranking task directly from text.",
            "cite_spans": [
                {
                    "start": 150,
                    "end": 151,
                    "mention": "7",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 256,
                    "end": 257,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 259,
                    "end": 261,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 263,
                    "end": 265,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 267,
                    "end": 269,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 271,
                    "end": 273,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 275,
                    "end": 277,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 279,
                    "end": 281,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 283,
                    "end": 285,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 287,
                    "end": 289,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 291,
                    "end": 293,
                    "mention": "56",
                    "ref_id": "BIBREF51"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "While standard strategies focus on learning a global ranking function that considers each query-document pair in isolation, they tend to ignore the difference in distribution in the feature space for different queries [4]. Hence, some recent works have been focusing on designing models that exploit the context induced by the re-ranking paradigm, either by explicitly designing differentiable PRF models [32, 40], or by encoding the ranking context \u2013the set of elements to re-rank\u2013, using either RNNs [4] or attention mechanisms [42, 62]. Consequently, the score for a document takes into account all the other documents in the candidate list. Because of their resemblance with structured problems, this type of approaches could benefit from the recent body of work around graph neural networks, which operate on graphs by learning how to propagate information to neighboring nodes.",
            "cite_spans": [
                {
                    "start": 219,
                    "end": 220,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 406,
                    "end": 408,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 410,
                    "end": 412,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 503,
                    "end": 504,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 531,
                    "end": 533,
                    "mention": "42",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 535,
                    "end": 537,
                    "mention": "62",
                    "ref_id": "BIBREF58"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Graph Neural Networks. Graph Neural Networks (GNNs) are extensions of neural networks that deal with structured data encoded as a graph. Recently, Graph Convolutional Networks (GCNs) [30] have been proposed for semi-supervised classification of nodes in a graph. Each layer of a GCN can generally be decomposed as: (i) node features are first transformed (e.g. linear mapping), (ii) node features are convolved, meaning that for each node, a differentiable, permutation-invariant operation (e.g. sum, mean, or max) of its neighbouring node features is computed, before applying some non-linearity, (iii) finally, we obtain a new representation for each node in the graph, which is then fed to the next layer. Many extensions of GCNs have been proposed (e.g. GraphSAGE [21], Graph Attention Network [50], Graph Isomorphism Network [57]), some of them directly tackling the recommendation task (e.g. PinSAGE [59]). But to the best of our knowledge, there is no prior work on using graph convolutions for the (re-)ranking task.",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 186,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 769,
                    "end": 771,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 799,
                    "end": 801,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 831,
                    "end": 833,
                    "mention": "57",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 907,
                    "end": 909,
                    "mention": "59",
                    "ref_id": "BIBREF54"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Our goal is to extend and generalize simple yet effective unsupervised approaches which have been proposed for the task [2, 3, 10, 11, 45], that can be seen as an extension of pseudo-relevance feedback methods for multi-modal objects. Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d \\in D$$\\end{document} denote a document to re-rank, composed of text and image. We denote by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_V(.,.)$$\\end{document} a normalized similarity measure between two images, and by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_T(q,d)$$\\end{document} the textual relevance score of document d w.r.t. query q. The cross-modal similarity score is given by:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\forall d \\in D, s_{CM}(q,d)&= \\sum \\limits _{d_i \\in NN_T^K(q)} s_T(q,d_i) s_V(d_i,d) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$NN_T^K(q)$$\\end{document} denotes the set of K most relevant documents w.r.t. q, based on text, i.e. on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_T(q,.)$$\\end{document}. The model can be understood very simply: similarly to PRF methods in standard information retrieval, the goal is to boost images that are visually similar to top images (from a text point of view), i.e. images that are likely to be relevant to the query but were initially badly ranked (which is likely to happen in the web scenario, where text is crawled from source page and can be very noisy).",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 122,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 124,
                    "end": 125,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 127,
                    "end": 129,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 131,
                    "end": 133,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 135,
                    "end": 137,
                    "mention": "45",
                    "ref_id": "BIBREF39"
                }
            ],
            "section": "Cross-Modal Similarity Measure ::: Learning to Rank Images",
            "ref_spans": []
        },
        {
            "text": "Despite showing good empirical results, cross-modal similarities are fully unsupervised, and lack some dynamic behaviour, like being able to adapt to different queries. Moreover, they rely on a single relevance score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_T(q,.)$$\\end{document}, while it could actually be beneficial to learn how to use a larger set of features such as the ones employed in learning to rank models.",
            "cite_spans": [],
            "section": "Cross-Modal Similarity Measure ::: Learning to Rank Images",
            "ref_spans": []
        },
        {
            "text": "In [3], the authors made a parallel between the cross-modal similarity from Eq. (1) and random walks in graphs: it can be seen as a kind of multimodal label propagation in a graph. This motivates us to tackle the task using graph convolutions. We therefore represent each query \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q \\in \\mathcal {Q}$$\\end{document} as a graph \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {G}_q$$\\end{document}, as follows:The set of nodes is the set of candidate documents \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_i$$\\end{document} to be re-ranked for this query: typically from a few to hundreds of documents, depending on the query.Each node i is described by a set of n learning to rank features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{q,d_i} \\in \\mathbb {R}^n$$\\end{document}.\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i \\in \\mathbb {R}^d$$\\end{document} denotes the (normalized) visual embedding for document \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_i$$\\end{document}.As we do not have an explicit graph structure, we consider edges given by a k\u2013nearest neighbor graph, based on a similarity between the embeddings \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i$$\\end{document}1.We denote by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {N}_i$$\\end{document} the neighborhood of node i, i.e. the set of nodes j such that there exists an edge from j to i.We consider edge weights, given by a similarity function between the visual features of its two extremity nodes \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_{ij}=\\varvec{g}(v_i,v_j)$$\\end{document}.\n",
            "cite_spans": [
                {
                    "start": 4,
                    "end": 5,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Cross-Modal Graph Convolution ::: Learning to Rank Images",
            "ref_spans": []
        },
        {
            "text": "Our goal is to learn how to propagate features in the above graph. Generalizing convolution operations to graphs can generally be expressed as a message passing scheme [16]:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} h_i^{(l+1)} = \\varvec{\\gamma } (h_i^{(l)}, \\sum _{j \\in \\mathcal {N}_i} \\varvec{\\phi }(h_i^{(l)},h_j^{(l)}, f_{ij})) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{\\gamma }$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{\\phi }$$\\end{document} denote differentiable functions, e.g. MLPs (Multi Layer Perceptron). By choosing \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{\\phi }(h_i^{(l)}, h_j^{(l)}, f_{ij})=\\varvec{\\tau }(h_i^{(l)}) \\varvec{g}(v_{i},v_{j})$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{\\gamma }(x,y)=y$$\\end{document}, Eq. (2) reduces to:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} h_i^{(l+1)} = \\sum \\limits _{j \\in \\mathcal {N}_i} \\varvec{\\tau }(h_j^{(l)}) \\varvec{g}(v_{i},v_{j}) \\end{aligned}$$\\end{document}This graph convolution can be reduced to the cross-modal similarity in Eq. (1). Indeed, assuming that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_i:=s_T(q,d_i)$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{\\tau }(.)$$\\end{document} is a top-k filtering function, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{g}(v_{i},v_{j}):=s_V(d_i,d)$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {N}_i :=\\mathcal {N}$$\\end{document} is the whole set of candidates to re-rank, then:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\sum \\limits _{j \\in \\mathcal {N}} \\tau _k(s_T(q,d_i) ) \\varvec{g}(v_{i},v_{j}) = \\sum \\limits _{d_i \\in NN_T^K(q)} s_T(q,d_i) s_V(d_i,d) \\end{aligned}$$\\end{document}In other words, one layer defined with Eq. (3) includes the standard cross-modal relevance feedback as a special case. Equation (3) is more general, and can easily be used as a building block in a differentiable ranking architecture.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 171,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Cross-Modal Graph Convolution ::: Learning to Rank Images",
            "ref_spans": []
        },
        {
            "text": "In the following, we derive a simple convolution layer from Eq. (3), and we introduce the complete architecture \u2013called DCMM for Differentiable Cross-Modal Model\u2013, summarized in Fig. 1. Learning to rank features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{q,d_i}$$\\end{document} are first encoded with an MLP(.;\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{\\theta }$$\\end{document}) with ReLU activations, in order to obtain node features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h^0_i$$\\end{document}. Then, the network splits into two branches:The first branch simply projects linearly each \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h^{(0)}_i$$\\end{document} to a real-valued score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_T(q,d_i)=\\varvec{w_0}^Th^{(0)}_i$$\\end{document}, that acts as a pure text-based score2.The second branch is built upon one or several layer(s) of cross-modal convolution, simply defined as: 5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} h_i^{(l+1)} = \\text {ReLU}(\\sum \\limits _{j \\in \\mathcal {N}_i} \\varvec{W}^{(l)}h_j^{(l)} \\varvec{g}(v_i,v_j)) \\end{aligned}$$\\end{document}\n\n",
            "cite_spans": [],
            "section": "Learning to Rank with Cross-Modal Graph Convolutions ::: Learning to Rank Images",
            "ref_spans": [
                {
                    "start": 183,
                    "end": 184,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "For the edge function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{g}$$\\end{document}, we consider two cases: the cosine similarity \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$g_{cos}(v_i,v_j)=\\cos (v_i,v_j)$$\\end{document}, defining the first model (referred as DCMM-cos), and a simple learned similarity measure parametrized by a vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{a}$$\\end{document} such that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{g}_{edge}(v_i,v_j)= v_i^T diag(\\varvec{a})v_j$$\\end{document}, defining our second model (referred as DCMM-edge).",
            "cite_spans": [],
            "section": "Learning to Rank with Cross-Modal Graph Convolutions ::: Learning to Rank Images",
            "ref_spans": []
        },
        {
            "text": "After the convolution(s), the final embedding for each node \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_i^{(L)}$$\\end{document} is projected to a real-valued score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{conv}(q,d_i)$$\\end{document}, using either a linear layer (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{conv}(q,d_i)=\\varvec{w_L}^Th^{(L)}_i$$\\end{document}) or an MLP (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{conv}(q,d_i)= MLP(h^{(L)}_i,\\varvec{\\omega })$$\\end{document}). Finally, the two scores are combined to obtain the final ranking score:\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} s(q,d_i) =\\varvec{w_0}^Th^{(0)}_i + s_{conv}(q,d_i) \\end{aligned}$$\\end{document}\n\n",
            "cite_spans": [],
            "section": "Learning to Rank with Cross-Modal Graph Convolutions ::: Learning to Rank Images",
            "ref_spans": []
        },
        {
            "text": "The model is trained using backpropagation and any standard learning to rank loss: pointwise, pairwise or listwise. It is worth to remark that, by extending PRF mechanisms for cross-modal re-ranking, our model is actually closer to listwise context-based models introduced in Sect. 2 than current state-of-the-art cross-modal retrieval models. It is listwise by design3: an example in a batch is not a single image in isolation, but all the candidate images for a given query, encoded as a graph, that we aim to re-rank together in a one shot manner. In our experiments, we used the pairwise BPR loss [46], from which we obtained the best results4. Let\u2019s consider a graph (i.e. the set of candidate documents for query q) in the batch, and all the feasible pairs of documents \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_q^{+,-}$$\\end{document} for this query (by feasible, we mean all the pairs that can be made from positive and negative examples in the graph). Then the loss is defined:\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathcal {L}(\\varvec{\\theta },\\varvec{w_0},\\varvec{\\gamma }_{\\text {conv}},\\varvec{\\omega }) = - \\sum \\limits _{d^+,d^- \\in D_q^{+,-}} \\log \\sigma (s(q,d^{+}) - s(q,d^-)) \\end{aligned}$$\\end{document}Note that contrary to previous works on listwise context modeling, we consider a set of objects to re-rank, and not a sequence (for instance in [4], a RNN encoder is learned for re-ranking). In other words, we discard the rank information of the first ranker into the re-ranking process: we claim that the role of the first retriever is to be recall-oriented, and not precision-oriented. Thus, using initial order might be a too strong prior, and add noise information. Moreover, in the case of implicit feedback (clicks used as weak relevance signals), using rank information raises the issue of biased learning to rank (sensitivity to position and trust biases). It is also worth to emphasize that, contrary to most of the works around graph convolution models, our graph structure is somehow implicit: while edges between nodes generally indicate a certain relationship between nodes (for instance, connection between two users in a social network), in our case a connection represents the visual similarity between two nodes.",
            "cite_spans": [
                {
                    "start": 602,
                    "end": 604,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 1845,
                    "end": 1846,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                }
            ],
            "section": "Learning to Rank with Cross-Modal Graph Convolutions ::: Learning to Rank Images",
            "ref_spans": []
        },
        {
            "text": "MediaEval. We first conduct experiments on the dataset from the \u201cMediaEval17, Retrieving Diverse Social Images Task\u201d challenge7. While this challenge also had a focus on diversity aspects, we solely consider the standard relevance ranking task. The dataset is composed of a ranked list of images (up to 300) for each query, retrieved from Flickr using its default ranking algorithm. The queries are general-purpose queries (e.g. q = autumn color), and each image has been annotated by expert annotators (binary label, i.e. relevant or not). The goal is to refine the results from the base ranking. The training set contains 110 queries for 33340 images, while the test set contains 84 queries for 24986 images.",
            "cite_spans": [],
            "section": "Datasets ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "While we could consider any number of learning to rank features as input for our model, we choose to restrict ourselves to a very narrow set of weak relevance signals, in order to remain comparable to its unsupervised counterpart, and ensure that the gain does not come from the addition of richer features. Hence, we solely rely on four relevance scores, namely tf-idf, BM25, Dirichlet smoothed LM [60] and DESM score [39], between the query and each image\u2019s text component (the concatenation of the image title and tags). We use an Inception-ResNet model [48] pre-trained on ImageNet to get the image embeddings (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d=1536$$\\end{document}).",
            "cite_spans": [
                {
                    "start": 400,
                    "end": 402,
                    "mention": "60",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 420,
                    "end": 422,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 558,
                    "end": 560,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                }
            ],
            "section": "Datasets ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "WebQ. In order to validate our approach on a real world dataset, we sample a set of 1000 queries8 from the image search logs of Naver. All images appearing in the top-50 candidates for these queries within a period of time of two weeks have been labeled by three annotators in terms of relevance to the query (binary label). Because of different query characteristics (in terms of frequency, difficulty etc.), and given the fact that new images are continuously added to/removed from the index, the number of images per query in our sample is variable (from around ten to few hundreds). Note that, while we actually have access to a much larger amount of click logs, we choose to restrict the experiments to this small sample in order keep the evaluations simple. Our goal here is to show that we are able to learn and reproduce some PRF mechanisms, without relying on large amount of data. Moreover, in this setting, it is easier to understand model\u2019s behaviour, as we avoid to deal with click noise and position bias. After removing queries without relevant images (according to majority voting among the three annotators), our sample includes 952 queries, and 43064 images, indexed through various text fields (title of the page, image caption etc.). We select seven of such fields, that might contain relevant pieces of information, and for which we compute two simple relevance features w.r.t. query q: BM25 and DESM [39] (using embeddings trained on a large query corpus from an anterior period). We also add an additional feature, which is a mixture of the two above, on the concatenation of all the fields. Image embeddings (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d=2048$$\\end{document}) are obtained using a ResNet-152 model [22] pre-trained on ImageNet.",
            "cite_spans": [
                {
                    "start": 1423,
                    "end": 1425,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1965,
                    "end": 1967,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Datasets ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Given the limited number of queries in both collections, we conducted 5-fold cross-validation, by randomly splitting the queries into five folds. The model is trained on 4 folds (with 1 fold kept for validation, as we use early stopping on nDCG), and evaluated on the remaining one; this procedure is repeated 5 times. Then, the average validation nDCG is used to select the best model configuration. Note that for the MediaEval dataset, we have access to a separate test set, so we modify slightly the evaluation methodology: we do the above 5-fold cross-validation on the training set, without using a validation fold (hence, we do not use early stopping, and the number of epochs is a hyperparameter to tune). Once the best model has been selected with the above strategy, we re-train it on the full training set, and give the final performance on the test set. We report the nDCG, MAP, P@20, and nDCG@20 for both datasets.",
            "cite_spans": [],
            "section": "Evaluation Methodology ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We train the models using stochastic gradient descent with the Adam optimizer [29]. We set the batch size (i.e. number of graphs per batch) to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$bs=\\{5\\},\\{32\\}$$\\end{document} for respectively MediaEval and WebQ, so that training fits on a single NVIDIA Tesla P100 GPU. The hyper-parameters we tune for each dataset are: (1) the learning rate \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{1e{-}3,1e{-}4,5e{-}5\\}$$\\end{document}, (2) the number of layers \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{2,3\\}$$\\end{document} for the input MLP, as well as the number of hidden units \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{4,8,16,32\\},\\{8,16,32,64\\}$$\\end{document}, (2) the dropout rate [47] in the MLP layers \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{0,0.2\\}$$\\end{document}, (4) the number of graph convolutions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{1,2,3,4\\}$$\\end{document} as well as the number of hidden units \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{4,8,16\\},\\{8,16,32\\}$$\\end{document}, (5) the dropout rate of the convolution layers \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{0,0.2,0.5\\}$$\\end{document} and (6) the number of visual neighbors to consider when building the input graph, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{1,3,5,10, 20,50,80,100,120,|\\mathcal {G}|-1\\}$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{1,3,5,10,15,20,30,|\\mathcal {G}|-1\\}$$\\end{document} for respectively MediaEval and WebQ. For MediaEval, we also tune the number of epochs \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\in \\{50,100,200,300,500\\}$$\\end{document}, while for WebQ, we set it to 500, and use early stopping with patience set to 80. All node features are query-level normalized (mean-std normalization). The models are implemented using PyTorch and PyTorch geometric9 [14] for the message passing components.",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 81,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1646,
                    "end": 1648,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 4368,
                    "end": 4370,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Evaluation Methodology ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In order to be fair, we want to compare methods with somewhat similar feature sets. Obviously, for the supervised methods, results can be improved by either adding richer/more features, or increasing models\u2019 capacity. For both datasets, we compare our DCMM model to the following baselines:A learning to rank model only based on textual features (LTR).The cross-modal similarity introduced in Sect. 3.1 [2, 3, 10, 11, 45] (CM).The above LTR model with the cross-modal similarity as additional input feature (LTR+CM), to verify that it is actually beneficial to learn the cross-modal propagation in DCMM in a end-to-end manner.\n",
            "cite_spans": [
                {
                    "start": 404,
                    "end": 405,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 407,
                    "end": 408,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 410,
                    "end": 412,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 414,
                    "end": 416,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 418,
                    "end": 420,
                    "mention": "45",
                    "ref_id": "BIBREF39"
                }
            ],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "For the cross-modal similarity, we use as proxy for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_T(q,.)$$\\end{document} a simple mixture of term-based relevance score (Dirichlet-smoothed LM and BM25 for respectively MediaEval and WebQ) and DESM score, on a concatenation of all text fields. From our experiments, we observe that it is actually beneficial to recombine the cross-modal similarity with the initial relevance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_T(q,.)$$\\end{document}, using a simple mixture. Hence, three parameters are tuned (the two mixture parameters, and the number of neighbors for the query), following the evaluation methodology introduced in Sect. 4.210. The LTR models are standard MLPs: they correspond to the upper part of architecture Fig. 1 (text branch), and are tuned following the same strategy.",
            "cite_spans": [],
            "section": "Baselines ::: Experiments",
            "ref_spans": [
                {
                    "start": 1227,
                    "end": 1228,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We do not compare our models with joint embedding approaches on those datasets for the reasons mentioned in Sect. 2, but also due to our initial experiments on Medieval which gave poor results. For the sake of illustration, on MediaEval, 64% of the queries have no lemmas in common with training queries (and 35% for WebQ): given the relatively small size of these datasets, the models cannot generalize to unseen queries. This illustrates an \u201cextreme\u201d example of the generalization issues \u2013especially on tail queries\u2013 of joint embedding techniques. In the meantime, as our model is fed with learning to rank features, especially term-based relevance scores like BM25, it could be less sensitive to generalization issues, for instance on new named entities. However, we want to emphasize that both approaches are not antagonist, but can actually be complementary. As our model can be seen as an extension of listwise learning to rank for bi-modal objects (if edges are removed, the model reduces to a standard MLP-based learning to rank), it can take as input node features matching scores from joint embeddings models. The model being an extension of PRF, we actually see the approaches at different stages of ranking.\n",
            "cite_spans": [],
            "section": "Baselines ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Table 1 gathers the main results of our study. Without too much surprise, going from pure text ranker to a model using both media types improves the results by a large margin (all the models are significantly better than the text-based LTR model, so we do not include these tests on Table 1 for clarity). Moreover, results indicate that combining initial features with the unsupervised cross-modal similarity in a LTR model allows to slightly improve results over the latter (not significantly though) for the MediaEval dataset, while it has no effect on WebQ: this is likely due to the fact that features are somehow redundant in our setting, because of how \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_T(q,.)$$\\end{document} is computed for the cross-modal similarity; the same would not hold if we would consider a richer set of features for the LTR models. Furthermore, the DCMM-cos model outperforms all the baselines, with larger margins for MediaEval than for WebQ; the only significant result (p-value\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$<0.05$$\\end{document}) is obtained for the MAP on MediaEval. Nevertheless, it shows that this simple architecture \u2013the most straightforward extension of cross-modal similarity introduced in Sect. 3.1\u2013, with a handful of parameters (see Table 1) and trained on small datasets, is able to reproduce PRF mechanisms. Interestingly, results tend to drop as we increase the number of layers (best results are obtained with a single convolution layer), no matter the number of neighbors chosen to define the visual graph. While it might be related to the relative simplicity of the model, it actually echoes common observations in PRF models (e.g. [3]): if we propagate too much, we also tend to diffuse information too much. Similarly, we can also make a parallel with over-smoothing in GNNs [33], which might be more critical for PRF, especially considering the simplicity of this model.",
            "cite_spans": [
                {
                    "start": 2147,
                    "end": 2148,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 2292,
                    "end": 2294,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Results and Analysis ::: Experiments",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 289,
                    "end": 290,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1747,
                    "end": 1748,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "The DCMM-edge shows interesting results: on WebQ, we manage to improve results significantly w.r.t. to CM sim, while on MediaEval, results are slightly worse than DCMM-cos (except for the MAP). It might be due to the fact that images in the latter are more alike to the ones used to train image signatures, compared to the (noisy) web images in WebQ; hence, learning a new metric between images has less impact. Interestingly, for both datasets, best results are obtained with more than a single layer; we hypothesize that the edge function plays the role of a simple filter for edges, allowing to propagate information from useful nodes across more layers. Note that the number of layers needed for the task is tied with how we define our input graph: the less neighbors we consider for each node, the more layers might be needed, in order for each node to gather information from useful nodes. In Fig. 2, we observe that if the number of neighbors is too small (e.g. 3 or 5), then the model needs more layers to improve performance. On the other side, when considering too many neighbors (e.g. 20 or all), the nodes already have access to all the useful neighbors, hence adding layers only reduces performances. We need to find the right balance between the number of neighbors and the number of convolution layers, so that the model can learn to propagate relevant signals (e.g. 10 neighbors and 3 layers for WebQ).\n",
            "cite_spans": [],
            "section": "Results and Analysis ::: Experiments",
            "ref_spans": [
                {
                    "start": 904,
                    "end": 905,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "In this paper, we have proposed a reformulation of unsupervised cross-modal PRF mechanisms for image search as a differentiable architecture relying on graph convolutions. Compared to its unsupervised counterpart, our novel approach can integrate any set of features, while providing a high flexibility in the design of the architecture. Experiments on two datasets showed that a simple model derived from our formulation achieved comparable \u2013or better\u2013 performance compared to cross-modal PRF.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "There are many extensions and possible directions stemming from the relatively simple model we have studied. Given enough training data (e.g. large amount of click logs), we could for instance learn to dynamically filter the visual similarity by using an attention mechanism to choose which nodes to attend, similarly to Graph Attention Networks [50] and Transformer model [49], discarding the need to set the number of neighbors in the input graph. Finally, our approach directly addressed the cross-modal retrieval task, but its application to the more general PRF problem in IR remains possible.",
            "cite_spans": [
                {
                    "start": 347,
                    "end": 349,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 374,
                    "end": 376,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                }
            ],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Comparison of the methods on both datasets (test metrics). Significant improvement w.r.t. the cross-modal similarity (CM sim) is indicated with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$*$$\\end{document} (p-value\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$<0.05$$\\end{document}). The number of trained parameters are indicated for the convolution models: ranging from few hundreds to few thousands, i.e. orders of magnitude less than joint embeddings models.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Differentiable Cross-Modal Model architecture - schematic view. Note that the upper part is a standard MLP-based LTR model.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Impact of the number of convolutions layers and top-k neighbors for WebQ.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Trans-media pseudo-relevance feedback methods in multimedia retrieval",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Clinchant",
                    "suffix": ""
                },
                {
                    "first": "J-M",
                    "middle": [],
                    "last": "Renders",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Csurka",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Advances in Multilingual and Multimodal Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "569-576",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Isard",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lazebnik",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Int. J. Comput. Vis.",
            "volume": "106",
            "issn": "2",
            "pages": "210-233",
            "other_ids": {
                "DOI": [
                    "10.1007/s11263-013-0658-4"
                ]
            }
        },
        "BIBREF9": {
            "title": "Improving image-sentence embeddings using large weakly annotated photo collections",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hodosh",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hockenmaier",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lazebnik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Computer Vision \u2013 ECCV 2014",
            "volume": "",
            "issn": "",
            "pages": "529-545",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Unsupervised visual and textual information fusion in CBMIR using graph-based methods",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ah-Pine",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Csurka",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Clinchant",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ACM Trans. Inf. Syst.",
            "volume": "33",
            "issn": "2",
            "pages": "9:1-9:31",
            "other_ids": {
                "DOI": [
                    "10.1145/2699668"
                ]
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "Deep coupled metric learning for cross-modal matching",
            "authors": [
                {
                    "first": "VE",
                    "middle": [],
                    "last": "Liong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "YP",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Multimedia",
            "volume": "19",
            "issn": "6",
            "pages": "1234-1244",
            "other_ids": {
                "DOI": [
                    "10.1109/TMM.2016.2646180"
                ]
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "J. Mach. Learn. Res.",
            "volume": "15",
            "issn": "",
            "pages": "1929-1958",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF43": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "Learning two-branch neural networks for image-text matching tasks",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lazebnik",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "41",
            "issn": "2",
            "pages": "394-407",
            "other_ids": {
                "DOI": [
                    "10.1109/TPAMI.2018.2797921"
                ]
            }
        },
        "BIBREF48": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF49": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF50": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF51": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF52": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF53": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF54": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF55": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF56": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF57": {
            "title": "Deep cross-modal projection learning for image-text matching",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Computer Vision \u2013 ECCV 2018",
            "volume": "",
            "issn": "",
            "pages": "707-723",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF58": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF59": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF60": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF61": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}