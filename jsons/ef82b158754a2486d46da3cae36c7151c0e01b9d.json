{
    "paper_id": "ef82b158754a2486d46da3cae36c7151c0e01b9d",
    "metadata": {
        "title": "Attention-Based Aggregation Graph Networks for Knowledge Graph Information Transfer",
        "authors": [
            {
                "first": "Ming",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shanghai Jiao Tong University",
                    "location": {
                        "settlement": "Shanghai",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Weijia",
                "middle": [],
                "last": "Jia",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shanghai Jiao Tong University",
                    "location": {
                        "settlement": "Shanghai",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Yusheng",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shanghai Jiao Tong University",
                    "location": {
                        "settlement": "Shanghai",
                        "country": "China"
                    }
                },
                "email": "huangyusheng@sjtu.edu.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "Knowledge graph completion (KGC) aims to predict missing information in a knowledge graph. Many existing embedding-based KGC models solve the Out-of-knowledge-graph (OOKG) entity problem (also known as zero-shot entity problem) by utilizing textual information resources such as descriptions and types. However, few works utilize the extra structural information to generate embeddings. In this paper, we propose a new zero-shot scenario: how to acquire the embedding vector of a relation that is not observed at training time. Our work uses a convolutional transition and attention-based aggregation graph neural network to solve both the OOKG entity problem and the new OOKG relation problem without retraining, regarding the structural neighbors as the auxiliary information. The experimental results show the effectiveness of our proposed models in solving the OOKG relation problem. For the OOKG entity problem, our model performs better than the previous GNN-based model by 23.9% in NELL-995-Tail dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Knowledge graphs (KGs) have been used in many applications such as information retrieval, question answering and text understanding. Standard KGs are collections of triplet (h, r, t), where h and t represent head entity and tail entity respectively, and r stands for the relation from h to t. Nowadays, large-scale KGs such as Freebase [2] , WordNet [9] and DBpedia [1] have been constructed and maintained for many practical tasks. Although a knowledge graph contains millions of triplets, there are incompleteness problems, which are divided into two types: sparsity and poor scalability. Therefore knowledge graph completion (KGC) has become the most important task in knowledge graph constructions.",
            "cite_spans": [
                {
                    "start": 336,
                    "end": 339,
                    "text": "[2]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 350,
                    "end": 353,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 366,
                    "end": 369,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "One of the significant issues for knowledge graph constructions is the poor scalability of KGs. Some KGE models are proposed to extend KGs automatically with OOKG entities (also called zero-shot scenario), proposed by [17] .",
            "cite_spans": [
                {
                    "start": 218,
                    "end": 222,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The zero-shot scenario is summarized as that every testing triplet contains at least one OOKG entity when doing KGC tasks. These zero-shot KGE models use additional textual information, such as descriptions and types, to generate the embedding vectors for OOKG entities. Some KGE models [6, 15] utilize additional structural information rather than textual information at testing time to solve certain OOKG entity. They generate its embedding vector based on some triplets (called auxiliary triples), and each of them contains one new entity, another in-KG entity and in-KG relation. However, the Graph Neural Network (GNN) based model [6] is effective merely on datasets that contain a small number of relations such as WordNet11 and Freebase13. Besides, this model can only handle the problem of OOKG entities but cannot deal with the problem of new relations, which is a new zero-shot scenario for OOKG relations firstly proposed in our work.",
            "cite_spans": [
                {
                    "start": 287,
                    "end": 290,
                    "text": "[6,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 291,
                    "end": 294,
                    "text": "15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 636,
                    "end": 639,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This new scenario is important because the OOKG relation is likely to be added to the knowledge graph to expand the scale and strengthen the connections between entities. As shown in Fig. 1 , we want to define the new relation \"was-teammate\" between basketball players. And we know the extra information (\"Chris Bosh\",\"was-teammate\",\"Dwyane Wade\") (auxiliary triplet). Then we want to infer whether there exists the relation \"was-teammate\" between \"Dwyane Wade\" and \"LeBron James\". And it should help us to estimate that the answer is yes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 183,
                    "end": 189,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "To handle the two zero-shot scenarios only using structural information (auxiliary triplets), we propose a convolutional transition and attention-based aggregation graph neural network structure. Our proposed model is inspired by GNN [6] and Graph Attention Networks [14] . We will demonstrate the whole framework in Sect. 3.",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 237,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 267,
                    "end": 271,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our main contributions can be summarized as follows: (1) We propose a new approach for generating embedding vectors of Out-of-KG relations; (2) We develop a Convolutional Transition Function to transfer information for Outof-KG entities and Out-of-KG relations in parallel; (3) We propose a Graph Attention-based Aggregation Function to merge the embeddings effectively; and (4) We verify the effectiveness of our approaches in several different datasets with different experiment settings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Knowledge graph embedding (KGE) aims to represent entities and relations into embedding vectors. Typical KGE models include TransE [3] , ConvE [5] , TransD [7] and etc. There are also some methods utilizing relation paths, such as TransE-NMM [10] , ProjE [11] . Some other methods utilize extra textual information to represent entities and relations, for example, DKRL [17] , Open-world KGC [12] . And MetaR [4] concentrate on few-shot link prediction in knowledge graph. In this problem, few triplets are given at training time, while in OOKG entities and relations problems, auxiliary triplets are given at testing time.",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 134,
                    "text": "[3]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 143,
                    "end": 146,
                    "text": "[5]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 156,
                    "end": 159,
                    "text": "[7]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 242,
                    "end": 246,
                    "text": "[10]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 255,
                    "end": 259,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 370,
                    "end": 374,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 392,
                    "end": 396,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 409,
                    "end": 412,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Recently, some KGE models have involved GNNs for the representation of OOKG entities, and they are directly related with our work. Hamaguchi et al. [6] employ GNN to build embeddings based on the knowledge transfer between neighbor entities. But Basic-GNN [6] defines two transition networks for each relation. For a knowledge graph with many relations, the triplets are not enough to fit the transition functions for the relation with few instances. The work proposed by [15] provides a logic attention network as the aggregation function in GNN. They mainly compare this LAN aggregation function with average method and LSTM method. We mainly utilize the graph attention based method as the aggregation function. In the meantime, our proposed models can deal with the OOKG scenario for relations.",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 151,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 256,
                    "end": 259,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 472,
                    "end": 476,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Firstly, we introduce some notations that are used in this paper. Let E be the set of entities and R be the set of relations. And a typical knowledge graph is denoted by G = {(h, r, t)} \u2286 E \u00d7 R \u00d7 E, where (h, r, t) means the fact or the relation triplet. Let G gold be a set of facts. Any triplet in G gold is called positive triplets. Otherwise, it is a negative triplet. Generally, we can see that G \u2282 G gold .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Notations and Problem Formulation"
        },
        {
            "text": "Triplet Classification is a typical KGC task [13] and has become a standard benchmark for KGE methods. Let H = (E \u00d7 R \u00d7 E)\\G be the set of facts that are not in the existing knowledge graph. For each triplet f \u2208 H, it is either a positive triplet (i.e., f \u2208 G gold ), or it is a negative triplet (i.e., f / \u2208 G gold ). A standard triplet classification limits that E and R only appear in G.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Notations and Problem Formulation"
        },
        {
            "text": "One of the new tasks is called the OOKG (out-of-knowledge-graph) entity TC. In addition to the knowledge graph G, new triplets G aux e are given at test time. Each triplet in G aux e contains exactly one OOKG entity from E OOKG = E(G aux e )\\E(G), one entity from E(G), where E(G) = {h|(h, r, t) \u2208 G} \u222a {t|(h, r, t) \u2208 G} and no new relations are involved. G aux e gives the relationship between OOKG entities and old entities in E(G). In this setting, the task is to correctly identify the test triplets that involve the OOKG entities E OOKG given the training set G and auxiliary set G aux e . The other new task is called OOKG relation TC. Similar to OOKG entity problem, new triplets G aux r are given at test time. However, each triplet in G aux r contains exactly one OOKG relation and two old entities from E(G). We denote the OOKG relations as R OOKG = R(G aux r )\\R(G), where R(G) = {r|(h, r, t) \u2208 G}. In this new zero-shot scenario, our task is to correctly identify triplets that involve the OOKG relations R OOKG , given the training set G and auxiliary set G aux r .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Notations and Problem Formulation"
        },
        {
            "text": "We propose a framework that can solve the above two zero-shot scenarios. The designed models can transfer information from E(G) and R(G) to E OOKG and R OOKG . This framework consists of two models, the propagation model and the output model. The next two subsections demonstrate how we design the propagation model to satisfy the information transfer in KGs. The output model defines an objective function according to given tasks using the embeddings of entities and relations. Combining the propagation models and the output model, we can get the complete GNN model. At training time, the propagation models for entities and relations gather the information from entities' and relations' corresponding triplets, to calculate the embeddings of h,r,t in (h, r, t). Then the output model takes the embeddings and calculates the absolute-margin objective function. The parameters and embeddings are trained using stochastic gradient descent with backpropagation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Framework of Our Proposed Model"
        },
        {
            "text": "At testing time, for OOKG entity e, we initialize its embeddings randomly. Then we can calculate the embedding of e through the propagation model for entities using auxiliary triplets. The auxiliary triplets define the neighbors of entity e, and the embeddings of neighbors and relations are trained already so they can be used in transition functions. For attention-based aggregation, we can calculate the normalized attention value using the embeddings of neighbors, relations and the randomly initialized embedding of e. For OOKG relation r, we can also calculate the embedding of r through the propagation model for relations, following the same procedure as OOKG entities. That is to say, we can calculate the embeddings of OOKG entities and relations through extra structure information without retraining.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Framework of Our Proposed Model"
        },
        {
            "text": "Let e \u2208 E(G) be an entity, and v e \u2208 R d be the d-dimensional representation vector of e. We define the propagation model by the following equation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "where head neighbors N head (e) = {(h, r, e)|(h, r, e) \u2208 G} and tail neighbors N tail (e) = {(e, r, t)|(e, r, t) \u2208 G}. The T head and T tail are the transition function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "The transition function is used to transform the vector of a node into the vector of its neighbors, and the transition function's parameters depend on the specific node pair and the edge between them. S head (e) contains the embeddings transformed from e's neighbors N head (e). And S tail (e) contains the embeddings transformed from e's neighbors N tail (e). The Eq.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "(3) represents the information aggregation function of the head set and tail set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "Transition Function. The purpose of the transition function is to define how to transfer information between the current node and its neighbors. In Hamaguchi's model, they design 2n independent transition functions if there are n relations in a KG. For the knowledge graph with many relations, such as NELL-995, the triplets are not enough to fit their transition functions for the relation with few instances. So we define the following two kinds of transition functions to solve this problem:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "where dir = head if e i is a head entity or dir = tail if e i is a tail entity. Given a triplet (h, r, e), T conv head and T fcl head can transform the v h and v r together into a temporary vector v h,r\u2192e . Similarly, given a triplet (e, r, t), the T conv tail and T fcl tail can transform the v t and v r into a temporary vector v t,r\u2192e . The difference between functions proposed by Hamaguchi et al. [6] and (4) (5) is that we substitute the 2n fully-connected layers with 2 designed convolutional 2D layers or 2 fully-connected layers. Besides, the calculation complexity is lower for (5), but the feature extraction capability is higher for (4), which is tested through experiments.",
            "cite_spans": [
                {
                    "start": 402,
                    "end": 405,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "The F CL head function is a fully-connected layer similar to the functions in [6] . Conv head and Conv tail are two convolutional 2D layers. Conv head takes v h and v h as inputs. Firstly, we concatenate them along the first dimension into a n \u00d7 2 matrix. Then we pad zeros to the matrix along the n-dimension side to get a (n + 1) \u00d7 2 matrix. After that, we use a 2 \u00d7 2 filter to extract the feature between v h and v r . Then the output of Conv head is an n-dimensional vector. Similarly, Conv tail has the same structure with Conv tail except that it takes the vectors of t and r as inputs. After extracting features through convolutional 2D layers or fullyconnected layers, we apply batch normalization and ReLU to the output. Aggregation Function. Aggregation function P mentioned in Eq. (3) maps a set of vectors to a vector. The purpose of P is to merge information together from a set of vectors.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 81,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": ", some simple pooling methods are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "where max is the element-wise max function. We apply these three methods separately to our models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "In addition to the simple pooling methods, we also propose an attentionbased aggregation network to define the weight value for each temporary information vector inspired by the Graph Attention Network [14] .",
            "cite_spans": [
                {
                    "start": 202,
                    "end": 206,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "Each information transfer edge attention value is defined as Eq. (6) . Entity e is the target entity. Entity set E neighbor (e) is the neighbor entities set of e. For each e i \u2208 E neighbor (e), we denote the relation between e i and e as r i , and e can be head entity or tail entity. W 1 denotes the linear transformation matrix from 3d dimensions to d dimensions. W 2 denotes the linear transformation matrix from d dimensions to 1 dimension. || denotes the concatenation operation. The neural network mechanism used in [15] takes transformed vectors as inputs but we take the vectors of entities and relations as inputs",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 68,
                    "text": "(6)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 522,
                    "end": 526,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "For each entity e, there are several neighbor entities and corresponding value. And we use the softmax function to modify these values as Eq. (7).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "where (e, r i , e i ) \u2208 N head (e) \u222a N tail (e) or (e i , r i , e) \u2208 N head (e) \u222a N tail (e). The attention-based aggregation function is defined as Eq. (8). The v trans (e i ) is the vector calculated by the transition function. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "Using the above transition functions and aggregation functions, we construct the information propagation model for entities as shown in Eq. (1)-(3). We gather the entity e's neighbors and extract their features along with specific relation r. Then we merge the information using different aggregation functions to get the embedding of e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Entities"
        },
        {
            "text": "The second propagation model is designed for relation's information transfer. We can transfer the information in G into R OOKG by this model. Intuitively, we have a basic assumption that the relation's embedding vector is only related to the triplets where it shows. That is to say, given the triplets containing relation r and their corresponding entities embedding, we can represent r's embedding. We propose a propagation function based on the above assumption:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Relations"
        },
        {
            "text": "where S rel (r) contains the embeddings that are transformed from r's corresponding triplets N rel (r). And N rel (r) stands for the set of triplets that contains relation r. The function P is the simple pooling function mentioned in Aggregation Function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Relations"
        },
        {
            "text": "The T conv rel transition function: R d \u00d7R d \u00d7E(G)\u00d7R(G)\u00d7E(G) \u2192 R d transforms the embeddings of entity h and t into a temporary embedding of relation r. The specific form is as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Relations"
        },
        {
            "text": "where Conv rel is a convolutional 2D layer similar to Conv head . The inputs are v h and v t and the parameters are independent of Conv head . For a specific relation r in R(G), we first collect the triplets that contain r. Then we gather information from the corresponding entity pairs set {(h i , t i )}, using the transition function T conv rel . Then we mix together these temporary embeddings to get the embedding v r of r.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Propagation Model for Relations"
        },
        {
            "text": "We use the TransE [3] based objective function as the output model. Our architecture is not limited to TransE. We can also use other KGE models like ConvE [5] , for the output model.",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 21,
                    "text": "[3]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 155,
                    "end": 158,
                    "text": "[5]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Output Model"
        },
        {
            "text": "The score function measures the correctness of a triplet (h,r,t). Smaller scores mean that the triplet is more likely to be true. We use the same score function as in TransE:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Score Function."
        },
        {
            "text": "where v h , v r , v t are the embedding vectors of the head, relation and tail, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Score Function."
        },
        {
            "text": "We utilize the following objective function called the absolute-margin objective function [6] :",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 93,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Absolute-Margin Objective Function."
        },
        {
            "text": "where \u03c4 is a hyperparameter, called the margin. [] + means when the expression is less than zero, we eliminate it. This absolute-margin objective function aims to train the scores of positive triplets towards zero, whereas the scores of negative triplets are at least \u03c4 . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Absolute-Margin Objective Function."
        },
        {
            "text": "We evaluate the effectiveness of our model on two datasets WordNet11 and NELL-995. To shorten the information transfer time, we sample the neighbor entities randomly when an entity has too many neighbors, and we also sample the corresponding entity pairs randomly when a relation is related to too many of entity pairs. The maximum number of corresponding information sources is set to be 64. The parameters are trained by the stochastic gradient descent with backpropagation. Specifically, we use the Adam optimization method. The step size of Adam is \u03b1 1 /(\u03b1 2 \u00b7 k + 1.0), where k indicates the number of epochs performed, \u03b1 1 = 0.01, and \u03b1 2 = 0.0001. The mini-batch size is 5000 and the number of training epochs is 150 in every experiment. Moreover, the dimension of the embedding space is 200. We implement our models using the neural network library Chainer http://chainer.org/.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "In the experiment part, we define three versions of our model. The first version ConvEntity-PoolingMethods means that only entity embeddings are obtained based on the convolutional transition functions and simple pooling methods. The second version FCLEntity-Att means that only entity embeddings are obtained based on the fully-connected transition function and attentionbased aggregation function. We use the pretrained TransE embeddings to initialize the entity and relation embeddings. The third version is ConvEntity-ConvRelation-PoolingMethods, which means the embeddings of entities and relations are all obtained based on the convolutional transition functions and simple pooling methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "Firstly, we compare our models with some baselines in the standard setting. There are no OOKG entities and relations involved at testing time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standard Triplet Classification"
        },
        {
            "text": "Datasets. We use WordNet11, NELL-995 for evaluation. We use corrupted triplets generated from positive triplets as the negative triplets of validation and test sets to evaluate Triplet Classification. The specifications on WordNet11 and NELL-995 are shown in Table 1 . Half of the validation and test sets are negative triplets, and they are included in the numbers of validation and test triplets. During the training time, we also use corrupted triplets as negative samples. From a positive triplet (h, r, t) in knowledge graph G, a corrupted triplet is generated by substituting a random entity sampled from E(G) for h or t with the 'Bernoulli' trick [16] . ",
            "cite_spans": [
                {
                    "start": 654,
                    "end": 658,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 259,
                    "end": 266,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Standard Triplet Classification"
        },
        {
            "text": "The results are shown in Table 2 . As the table shows, in both Word-Net11 and NELL-995 datasets, our proposed model FCLEntity-Att achieves the best accuracy.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 32,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Results."
        },
        {
            "text": "In this part, we compare our model ConvEntity, FCLEntity-Att with Basic-GNN [6] and LAN [15] . Our proposed models ConvEntity and FCLEntity can solve the scenario where there are only OOKG entities.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 79,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 88,
                    "end": 92,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "OOKG Entity Experiment"
        },
        {
            "text": "We process NELL-995 dataset to construct several datasets for our OOKG entity experiment following a similar protocol used in [6] . Also, we directly use the datasets WN11 released by [6] . We take the NELL-995 as an example to explain the process, which consists of two steps: choosing OOKG entities, filtering and splitting triplets. The details of the generated OOKG entity datasets are shown in Table 3. 1. Choosing OOKG entities. We first randomly select 3000 triplets from the NELL-995 test file. For these 3000 triplets, we choose the initial OOKG candidates in three different ways (thereby yielding three datasets in total), called Head, Tail, and Both settings. For the Head setting, we choose the head entities of the 3000 triplets as candidates. The Tail setting is similar, but with tail entities regarded as candidates. In the Both setting, all the head and tail entities are seen as candidates. We consider the candidates set as T c and the final OOKG entity set as T . For every candidate e \u2208 T c , if it does not have any neighbor in the original training set, such an entity is filtered out, yielding the final T . (1), with discarding the triplets that contain removed OOKG candidates.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 129,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 184,
                    "end": 187,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 399,
                    "end": 407,
                    "text": "Table 3.",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Datasets."
        },
        {
            "text": "The results are shown in Table 4 . In three datasets generated from NELL-995, our proposed FCLEntity-Att model outperforms other models. In three datasets generated from WN11, LAN proposed by [15] achieves the best results. But our proposed model performs better than the original Basic-GNN method [6] . The results demonstrate that our proposed convolutional transition function and the attention-based aggregation function are effective for the Outof-KG entity experiments in datasets with many relations. Because of its relationspecific transition function and logic attention mechanism, LAN [15] has better generalization performance in WN11.",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 196,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 298,
                    "end": 301,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 595,
                    "end": 599,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 32,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Results."
        },
        {
            "text": "This part is to verify the effectiveness of our relation information transfer model, ConvEntity-ConvRelation-PoolingMethods, which can generate zero-shot relation embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "OOKG Relation Experiment"
        },
        {
            "text": "We process the NELL-995 dataset to construct three datasets for our OOKG relation experiment with different quantities of OOKG relations. The process consists of two steps: choosing OOKG relations, filtering and splitting triplets. The details of the generated OOKG datasets are shown in set K c , we divide the original training set into two parts. The first part contains the triplets whose relation is not in K c , called the new training set. The second part contains the triplets whose relation is in K c , called auxiliary set, with discarding the triplet which contains entities not shown in the new training set. And for the N test triplets mentioned before, we discard these triplets which contain entities not shown in the new training set. For the original validation set, we only choose triplets which only contain entities and relations shown in the new training set as new validation triplets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets."
        },
        {
            "text": "The results are shown in Table 6 . We use the following method as the baseline in this experiment since this zero-shot scenario is proposed firstly in our work. For an OOKG relation r, based on the auxiliary set, we use the basic assumption h + r = t to compute the temporary relation embeddings set {r i |r i = t i \u2212 h i }. And we apply the simple pooling function to this set to get the representation vector of r. We use the hyperparameter and settings of TransE in the work of [8] . As the table shows, our proposed model ConvEntity-ConvRelation-avg outperforms the baseline methods in all three datasets generated from NELL-995. This experiment shows that the convolutional transition function is also effective in the relation information transfer.",
            "cite_spans": [
                {
                    "start": 481,
                    "end": 484,
                    "text": "[8]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 32,
                    "text": "Table 6",
                    "ref_id": null
                }
            ],
            "section": "Results."
        },
        {
            "text": "In this paper, we propose the convolutional transition and attention-based aggregation graph neural network structure to solve the two zero-shot scenarios where entities and relations are not involved at training time. In particular, the zeroshot scenario for relations is firstly involved in our work. Through the OOKG entity experiment and the OOKG relation experiment, we evaluate the effectiveness of the convolutional transition function and the graph attention-based aggregation function. Our proposed models outperform baseline models significantly in the three experiment settings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "DBpedia: a nucleus for a web of open data",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Auer",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bizer",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kobilarov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cyganiak",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ives",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Bollacker",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Evans",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Paritosh",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Sturge",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Taylor",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data",
            "volume": "",
            "issn": "",
            "pages": "1247--1250",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Translating embeddings for modeling multi-relational data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bordes",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Usunier",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garcia-Duran",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Yakhnenko",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2787--2795",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Meta relational learning for few-shot link prediction in knowledge graphs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EMNLP-IJCNLP",
            "volume": "",
            "issn": "",
            "pages": "4208--4217",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Convolutional 2D knowledge graph embeddings",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dettmers",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Minervini",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Stenetorp",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Riedel",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Knowledge transfer for out-of-knowledge-base entities: a graph neural network approach",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hamaguchi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Oiwa",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shimbo",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Matsumoto",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IJCAI",
            "volume": "",
            "issn": "",
            "pages": "1024--1034",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Knowledge graph embedding via dynamic mapping matrix",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
            "volume": "1",
            "issn": "",
            "pages": "687--696",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Learning entity and relation embeddings for knowledge graph completion",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "WordNet: a lexical database for English",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "A"
                    ],
                    "last": "Miller",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Commun. ACM",
            "volume": "38",
            "issn": "11",
            "pages": "39--41",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Neighborhood mixture model for knowledge base completion",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "Q"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sirts",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CoNLL",
            "volume": "",
            "issn": "",
            "pages": "40--50",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "ProjE: embedding projection for knowledge graph completion",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Weninger",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Open-world knowledge graph completion",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Weninger",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Reasoning with neural tensor networks for knowledge base completion",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "926--934",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Graph attention networks",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Veli\u010dkovi\u0107",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cucurull",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Casanova",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Romero",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lio",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1710.10903"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Logic attention based neighborhood aggregation for inductive knowledge graph embedding",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "33",
            "issn": "",
            "pages": "7152--7159",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Knowledge graph embedding by translating on hyperplanes",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Representation learning of knowledge graphs with entity descriptions",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Luan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Thirtieth AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "OOKG relation problem.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "att (S head (e) \u222a S tail (e)) = ei\u2208E neighbor (e) AttV <e,ei,ri> * v trans (e i ),",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Specifications of the triplet classification datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Standard TC Result. Method with prefix * is our proposed model.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Specifications of the OOKG entity datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Specifications of the OOKG relation datasets.Table 6. Results of the OOKG relation experiment. Filtering and splitting triplets. After getting the OOKG entity set T , we choose the new training set and auxiliary set. For a triplet in the original training set, if it only contains non-OOKG entities, it is added to the new training set. If it only contains one OOKG entity, it is added to the auxiliary set. For the validation triplets, we remove the triplets containing OOKG entities. For the test triplets, we use the 3000 triplets in the NELL-995 test file in step",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Choosing OOKG relations candidates. To decide OOKG relations, we first randomly select N = 1000, 1500, 2000 triplets from the NELL-995 test file. For these N triplets, we choose the relations as the initial OOKG relation candidates set called K c . 2. Filtering and splitting triplets. After getting the OOKG relations candidates",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}