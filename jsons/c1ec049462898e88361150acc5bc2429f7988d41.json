{
    "paper_id": "c1ec049462898e88361150acc5bc2429f7988d41",
    "metadata": {
        "title": "",
        "authors": [
            {
                "first": "Ting-Hao",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Kenneth",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Pennsylvania State University",
                    "location": {
                        "settlement": "University Park",
                        "region": "PA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Chieh-Yang",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Pennsylvania State University",
                    "location": {
                        "settlement": "University Park",
                        "region": "PA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Chien-Kuang",
                "middle": [
                    "Cornelia"
                ],
                "last": "Ding",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of California",
                    "location": {
                        "settlement": "San Francisco",
                        "region": "CA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Yen-Chia",
                "middle": [],
                "last": "Hsu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Carnegie Mellon University",
                    "location": {
                        "settlement": "Pittsburgh",
                        "region": "PA",
                        "country": "USA"
                    }
                },
                "email": "yenchiah@andrew.cmu.edu"
            },
            {
                "first": "C",
                "middle": [
                    "Lee"
                ],
                "last": "Giles",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Pennsylvania State University",
                    "location": {
                        "settlement": "University Park",
                        "region": "PA",
                        "country": "USA"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "This paper introduces CODA-19 1 , a humanannotated dataset that codes the Background, Purpose, Method, Finding/Contribution, and Other sections of 10,966 English abstracts in the COVID-19 Open Research Dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, achieving a label quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were obtained by majority vote. The inter-annotator agreement (Cohen's kappa) between the crowd and the biomedical expert (0.741) is comparable to inter-expert agreement (0.788). CODA-19's labels have an accuracy of 82.2% when compared to the biomedical expert's labels, while the accuracy between experts was 85.0%. Reliable human annotations help scientists to understand the rapidly accelerating coronavirus literature and also serve as the battery of AI/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against COVID-19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Just as COVID-19 is rapidly spreading worldwide, the rapid acceleration in new coronavirus literature makes it hard to keep up with. Researchers have thus teamed up with the White House to release the COVID-19 Open Research Dataset (CORD-19) (Wang et al., 2020) , containing over 59,000 related scholarly articles (as of May 1, 2020). The Open Research Dataset Challenge has also been launched on Kaggle to encourage researchers to use cutting-edge techniques to gain new insights from these papers. However, it often requires largescale human annotations for automated language For successful infection, viruses must recognize their respective host cells. A common mechanism of host recognition by viruses is to utilize a portion of the host cell as a receptor. Bacteriophage Sf6, which infects Shigella flexneri, uses lipopolysaccharide as a primary receptor and then requires interaction with a secondary receptor, a role that can be fulfilled by either outer membrane proteins (Omp) A or C. Our previous work showed that specific residues in the loops of OmpA mediate Sf6 infection. To better understand Sf6 interactions with OmpA loop variants, we determined the kinetics of these interactions through the use of biolayer interferometry, an optical biosensing technique that yields data similar to surface plasmon resonance. Here, we successfully tethered whole Sf6 virions, determined the binding constant of Sf6 to OmpA to be 36 nM. Additionally, we showed that Sf6 bound to five variant OmpAs and the resulting kinetic parameters varied only slightly. Based on these data, we propose a model in which Sf6: Omp receptor recognition is not solely based on kinetics, but likely also on the ability of an Omp to induce a conformational change that results in productive infection. All rights reserved. No reuse allowed without permission. understanding, relation extraction, and question answering to reach good performance levels. Producing such annotations for thousands of papers can be a prolonged process if we only employ expert annotators, whose availability is limited.",
            "cite_spans": [
                {
                    "start": 242,
                    "end": 261,
                    "text": "(Wang et al., 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Data sparsity is one of the challenges for text mining in the biomedical domain because text annotations on scholarly articles were mainly produced by small groups of experts. For example, two researchers manually created the ACL RD-TEC 2.0, a dataset that contains 300 scientific abstracts (QasemiZadeh and Schumann, 2016); a group of annotators \"with rich experience in biomedical content curation\" created MedMentions, a corpus containing 4,000 abstracts (Mohan and Li, 2019) ; and several datasets used in biomed-ical NLP shared tasks were manually created by the organizers and/or their students, such as the Sci-enceIE in SemEval'17 (Augenstein et al., 2017) and Relation Extraction in SemEval'18 (G\u00e1bor et al., 2018) . Obtaining expert annotations can be too slow to respond to COVID-19, so we explore an alternative approach: using non-expert crowds, such as workers on Amazon Mechanical Turk (MTurk), to produce high-quality, useful annotations for thousands of scientific papers.",
            "cite_spans": [
                {
                    "start": 458,
                    "end": 478,
                    "text": "(Mohan and Li, 2019)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 639,
                    "end": 664,
                    "text": "(Augenstein et al., 2017)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 703,
                    "end": 723,
                    "text": "(G\u00e1bor et al., 2018)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This paper introduces CODA-19, the COVID-19 Research Aspect Dataset, presenting the first outcome of our exploration in using non-expert crowds for large-scale scholarly article annotation. CODA-19 contains 10,966 abstracts randomly selected from CORD-19. Each abstract was segmented into sentences, which were further divided into one or more shorter text fragments. All 168,286 text fragments in CODA-19 were labeled with a \"research aspect,\" i.e., Background, Purpose, Method, Finding/Contribution, or Other. This annotation scheme was adapted from SOL-VENT (Chan et al., 2018) , with minor changes.",
            "cite_spans": [
                {
                    "start": 561,
                    "end": 580,
                    "text": "(Chan et al., 2018)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In our project, 248 crowd workers from MTurk were recruited and annotated the whole CODA-19 within ten days. 2 Each abstract was annotated by nine different workers. We aggregated the crowd labels for each text segment using majority voting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The resulting crowd labels had a label accuracy of 82% when compared against the expert labels on 129 abstracts. The inter-annotator agreement (Cohen's kappa) was 0.741 between the crowd labels and the expert labels, while it was 0.788 between two experts. We also established several classification baselines, showing the feasibility of automating such annotation tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "CODA-19 uses a five-class annotation scheme to denote research aspects in scientific articles: Background, Purpose, Method, Finding/Contribution, or Other. Table 1 shows the full annotation guidelines we developed to instruct workers. We updated and expanded this guideline daily during the annotation process to address workers' questions and feedback.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 156,
                    "end": 163,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Annotation Scheme"
        },
        {
            "text": "This scheme was adapted from SOL-VENT (Chan et al., 2018) , with three changes. First, we added an \"Other\" category. Articles in CORD-19 are broad and diverse (Colavizza et al., 2020) , so it is unrealistic to govern all cases with only four categories. We are also aware that CORD-19's data came with occasional formatting or segmenting errors. These cases were also to be put into the \"Other\" category. Second, we replaced the \"Mechanism\" category with \"Method.\" Chan et al. created SOLVENT with the aim of discovering the analogies between research papers at scale. Our goal was to better understand the contribution of each paper, so we decided to use a more general word, \"Method,\" to include the research methods and procedures that cannot be characterized as \"Mechanisms.\" Also, biomedical literature widely used the word \"mechanism,\" which could also be confusing to workers. Third, we modified the name \"Finding\" to \"Finding/Contribution\" to allow broader contributions that are not usually viewed as \"findings.\" Our scheme is also similar to that of DISA (Huang and Chen, 2017), which has an additional \"Conclusion\" category.",
            "cite_spans": [
                {
                    "start": 38,
                    "end": 57,
                    "text": "(Chan et al., 2018)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 159,
                    "end": 183,
                    "text": "(Colavizza et al., 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Annotation Scheme"
        },
        {
            "text": "We selected this scheme because it balances the richness of information and the difficulty level for workers to annotate. We are aware of the long history of research (Kilicoglu, 2018) on composing structured abstracts (Hartley, 2004) , identifying argumentative zones (Teufel et al., 1999; Mizuta et al., 2006; Liakata et al., 2010) , analyzing scientific discourse (de Waard and Maat, 2012; Dasigi et al., 2017; Banerjee et al., 2020) , supporting paper writing (Wang et al., 2019) , and representing papers to reduce information overload (de Waard et al., 2009) . However, most of these schemes assumed expert annotators rather than crowd workers. We eventually narrowed our focus down to two annotation schemes: SOLVENT and the \"Information Type\" (Focus, Polarity, Certainty, Evidence, Trend) proposed by Wilbur et al. (2006) . SOLVENT is easier to annotate and has been tested with workers from MTurk and Upwork, while Wilbur's scheme is informative and specialized for biomedical articles. We implemented annotation interfaces for both schemes and launched a few tasks on MTurk for testing. Workers accomplished the SOLVENT tasks much faster with reasonable label accuracy, while only a few workers accomplished the Information Type annotation task. Therefore, we decided to adapt the SOLVENT scheme.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 184,
                    "text": "(Kilicoglu, 2018)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 219,
                    "end": 234,
                    "text": "(Hartley, 2004)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 269,
                    "end": 290,
                    "text": "(Teufel et al., 1999;",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 291,
                    "end": 311,
                    "text": "Mizuta et al., 2006;",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 312,
                    "end": 333,
                    "text": "Liakata et al., 2010)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 367,
                    "end": 392,
                    "text": "(de Waard and Maat, 2012;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 393,
                    "end": 413,
                    "text": "Dasigi et al., 2017;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 414,
                    "end": 436,
                    "text": "Banerjee et al., 2020)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 464,
                    "end": 483,
                    "text": "(Wang et al., 2019)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 541,
                    "end": 564,
                    "text": "(de Waard et al., 2009)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 809,
                    "end": 829,
                    "text": "Wilbur et al. (2006)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Annotation Scheme"
        },
        {
            "text": "Background \"Background\" text segments answer one or more of these questions: \u2022 Why is this problem important?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "\u2022 What relevant works have been created before?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "\u2022 What is still missing in the previous works?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "\u2022 What are the high-level research questions? \u2022 How might this help other research or researchers?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "Purpose \"Purpose\" text segments answer one or more of these questions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "\u2022 What specific things do the researchers want to do? \u2022 What specific knowledge do the researchers want to gain? \u2022 What specific hypothesis do the researchers want to test?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "Method \"Method\" text segments answer one or more of these questions: \u2022 How did the researchers do the work or find what they sought?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "\u2022 What are the procedures and steps of the research?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Aspect Annotation Guideline"
        },
        {
            "text": "\"Finding/Contribution\" text segments answer one or more of these questions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finding/ Contribution"
        },
        {
            "text": "\u2022 What did the researchers find out? \u2022 Did the proposed methods work?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finding/ Contribution"
        },
        {
            "text": "\u2022 Did the thing behave as the researchers expected?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finding/ Contribution"
        },
        {
            "text": "Other \u2022 Text segments that do not fit into any of the four categories above.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finding/ Contribution"
        },
        {
            "text": "\u2022 Text segments that are not part of the article.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finding/ Contribution"
        },
        {
            "text": "\u2022 Text segments that are not in English.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finding/ Contribution"
        },
        {
            "text": "\u2022 Text segments that contain only reference marks (e.g., \"[1,2,3,4,5\") or dates (e.g., \"April 20, 2008\"). ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Finding/ Contribution"
        },
        {
            "text": "CODA-19 has 10,966 abstracts that contain a total of 2,703,174 tokens and 103,978 sentences, which were divided into 168,286 segments. The data is released as a 80/10/10 train/dev/test split.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CODA-19 Dataset Construction"
        },
        {
            "text": "We used Stanford CoreNLP (Manning et al., 2014) to tokenize and segment sentences for all the abstracts in CORD-19. We further used comma (,), semicolon (;), and period (.) to split each sentence into shorter fragments, where a fragment has no fewer than six tokens (including punctuation marks) and has no orphan parentheses.",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 47,
                    "text": "(Manning et al., 2014)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Data Preparation"
        },
        {
            "text": "As of April 15, 2020, 29,306 articles in CORD-19 had a non-empty abstract. An average abstract had 9.73 sentences (SD = 8.44), which were further divided into 15.75 text segments (SD = 13.26). Each abstract had 252.36 tokens (SD = 192.89) on average. We filtered out the 538 (1.84%) abstracts with only one sentence because many of them had formatting errors. We also removed the 145 (0.49%) abstracts that had more than 1,200 tokens to keep the working time for each task under five minutes (see Section 3.3). We randomly selected 11,000 abstracts from the remaining data for annotation. During the annotation process, work-ers informed us that a few articles were not in English. We identified these automatically using langdetect 3 and excluded them. Figure 2 shows the worker interface, which we designed to guide workers to read and label all the text segments in an abstract. The interface showed the instruction on the top (Figure 2a) and presented the task in three steps: In Step 1, the worker was instructed to spend ten seconds to take a quick glance at the abstract. The goal was to get a high-level sense of the topic rather than to fully understand the abstract. In Step 2, we showed the main annotation interface (Figure 2b) , where the worker can go through each text segment and select the most appropriate category for each segment one by one. In Step 3, the worker can review the labeled text segments (Figure 2c ) and go back to Step 2 to fix any problems.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 754,
                    "end": 762,
                    "text": "Figure 2",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 930,
                    "end": 941,
                    "text": "(Figure 2a)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1228,
                    "end": 1239,
                    "text": "(Figure 2b)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1421,
                    "end": 1431,
                    "text": "(Figure 2c",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Data Preparation"
        },
        {
            "text": "Worker Training and Recruitment We first created a qualification Human Intelligence Task (HIT) to recruit workers on MTurk ($1/HIT). The workers needed to watch a five-minute video to learn the scheme, go through an interactive tutorial to learn the interface, and sign a consent form to obtain the qualification. We granted custom qualifications to 400 workers who accomplished the qualification HIT. Only the workers with this qualification could do our tasks. 4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotation Procedure"
        },
        {
            "text": "Posting Tasks in Smaller Batches We divided 11,000 abstracts into smaller batches, where each batch has no more than 1,000 abstracts. Each abstract forms a single HIT. We recruited nine different workers through nine assignments to label each abstract. Our strategy was to post one batch at a time. When a batch was finished, we assessed its data quality, sent feedback to workers to guide them, or blocked workers who constantly had low accuracy before proceeding with the next batch.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Annotation Procedure"
        },
        {
            "text": "Worker Wage and Total Cost We aimed to pay an hourly wage of $10. The working time of an abstract was estimated by the average reading speed of English native speakers, i.e., 200-300 words per minute (Siegenthaler et al., 2012) . For an abstract, we rounded up (#token/250) to an integer as the estimated working time in minutes and paid ($0.05 + Estimated Working Minutes \u00d7 $0.17) for it. As a result, 59.49% of our HITs were priced at $0.22, 36.41% were at $0.39, 2.74% were at $0.56, 0.81% were at $0.73, and 0.55% were at $0.90. We posted nine assignments per HIT. Adding the 4 Four built-in MTurk qualifications were also used: Locale (US Only), HIT Approval Rate (\u226598%), Number of Approved HITs (\u22653000), and the Adult Content Qualification. 20% MTurk fee, coding each abstract (using nine workers) cost $3.21 on average.",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 227,
                    "text": "(Siegenthaler et al., 2012)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Annotation Procedure"
        },
        {
            "text": "The final labels in CODA-19 were obtained by majority voting over crowd labels, excluding the labels from blocked workers. For each batch of HITs, we manually examined the labels from workers who frequently disagreed with the majority-voted labels (Section 3.3). If a worker had abnormally low accuracy or was apparently spamming, we retracted the worker's qualification to prevent him/her from taking future tasks. We excluded the labels from these removed workers when aggregating the final labels. Note that there can be ties when two or more aspects received the same highest number of votes (e.g., 4/4/1 or 3/3/3). We resolved ties by using the following tiebreakers, in order: Finding, Method, Purpose, Background, Other.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Label Aggregation"
        },
        {
            "text": "We worked with a biomedical expert and a computer scientist to assess label quality; both experts are co-authors of this paper. The biomedical expert (the \"Bio\" Expert in Table 2 ) is an MD and also a PhD in Genetics and Genomics. She is now a resident physician in pathology at the University of California, San Francisco. The other expert (the \"CS\" Expert in Table 2 ) has a PhD in Computer Science and is currently a Project Scientist at Carnegie Mellon University. Table 2 : Crowd performance using both Bio Expert and CS Expert as the gold standard. CODA-19's labels have an accuracy of 0.82 and a kappa of 0.74, when compared against two experts' labels. It is noteworthy that when we compared labels between two experts, the accuracy (0.850) and kappa (0.788) were only slightly higher.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 171,
                    "end": 178,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 361,
                    "end": 368,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 469,
                    "end": 476,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Data Quality Assessment"
        },
        {
            "text": "Both experts annotated the same 129 abstracts randomly selected from CODA-19. The experts used the same interface as that of the workers (Figure 2) . The inter-annotator agreement (Cohen's kappa) between the two experts was 0.788. Table 2 shows the aggregated crowd label's accuracy, along with the precision, recall, and F1-score of each class. CODA-19's labels have an accuracy of 0.82 and a kappa of 0.74 when compared against the two experts' labels. It is noteworthy that when we compared labels between the two experts, the accuracy (0.850) and kappa (0.788) were only slightly higher. The crowd workers performed best in labeling \"Background\" and \"Finding,\" and they had nearly perfect precision for the \"Other\" category. Figure 3 shows the normalized confusion matrix for the aggregated crowd labels versus the biomedical expert's labels. Many \"Purpose\" segments were mislabeled as \"Background,\" which might indicate more ambiguous cases between these two categories. During the annotation period, we received several emails from workers asking about the distinctions between these two aspects. For example, do \"potential applications of the proposed work\" count as \"Background\" or \"Purpose\"?",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 137,
                    "end": 147,
                    "text": "(Figure 2)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 231,
                    "end": 238,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 729,
                    "end": 737,
                    "text": "Figure 3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Data Quality Assessment"
        },
        {
            "text": "We further examined machines' capacity for annotating research aspects automatically. Six baseline models were implemented: Linear SVM, Random Forest, CNN, LSTM, BERT, and SciBERT.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Classification Baselines"
        },
        {
            "text": "The Tf-idf feature was used. We turned all words into lowercase and removed those with frequency lower than 5. The final tf-idf feature contained 16,775 dimensions. For deep-learning approaches, the vocabulary size was 16,135, where tokens with frequency lower than 5 were replaced by <UNK>. Sequences were padded with <PAD> if containing less than 60 tokens and were truncated if containing more than 60 tokens. Models Machine-learning approaches were implemented using Scikit-learn (Pedregosa et al., 2011) and deep-learning approaches were implemented using PyTorch (Paszke et al., 2019) . The following are the training setups.",
            "cite_spans": [
                {
                    "start": 484,
                    "end": 508,
                    "text": "(Pedregosa et al., 2011)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 569,
                    "end": 590,
                    "text": "(Paszke et al., 2019)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "\u2022 Linear SVM: We did a grid search for hyperparameters and found that C = 1, tol = 0.001, and hinge loss yielded the best results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "\u2022 Random Forest: With the grid search, 150 estimators yielded the best result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "\u2022 CNN: The classic CNN (Kim, 2014) was implemented. Three kernel sizes (3,4,5) were used, each with 100 filters. The word embedding size was 256. A dropout rate of 0.3 and L2 regularization with weight 10 \u22126 were used when training. We used the Adam optimizer, with a learning rate of 0.00005. The model was trained for 50 epochs and the one with highest validation score was kept for testing. Table 3 : Baseline performance of automatic labeling using the crowd labels of CODA-19. SciBERT achieves highest accuracy of 0.749 and outperforms other models in every aspects.",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 34,
                    "text": "(Kim, 2014)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 394,
                    "end": 401,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Data Preprocessing"
        },
        {
            "text": "\u2022 LSTM: We used 10 LSTM layers to encode the sequence. The encoded vector was then passed through a dense layer for classification. Word embedding size and LSTM hidden size were both 256. The rest of the hyperparameter and training setting was the same as that of the CNN model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "\u2022 BERT: Hugging Face's implementation (Wolf et al., 2019) of the Pretrained BERT (Devlin et al., 2018) was used for fine-tuning. We fine-tuned the pretrained model with a learning rate of 3 * 10 \u22127 for 50 epochs. Early stopping was used when no improvement occurred in the validation accuracy for five consecutive epochs. The model with the highest validation score was kept for testing.",
            "cite_spans": [
                {
                    "start": 38,
                    "end": 57,
                    "text": "(Wolf et al., 2019)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 81,
                    "end": 102,
                    "text": "(Devlin et al., 2018)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "\u2022 SciBERT: Hugging Face's implementation (Wolf et al., 2019) of the Pretrained SciB-ERT (Beltagy et al., 2019) was used for finetuning. The fine-tuning setting is the same as that of the BERT model.",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 60,
                    "text": "(Wolf et al., 2019)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 88,
                    "end": 110,
                    "text": "(Beltagy et al., 2019)",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "Result Table 3 shows the results for the six baseline models: SciBERT preformed the best in overall accuracy. When looking at each aspect, all the models performed better in classifying \"Background,\" \"Finding,\" and \"Other,\" while identifying \"Purpose\" and \"Method\" was more challenging.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 7,
                    "end": 14,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Data Preprocessing"
        },
        {
            "text": "6 What's Next?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "One obvious future direction is to improve classification performance. We evaluated the automatic labels against the biomedical expert's labels, and the SciBERT model achieved an accuracy of 0.774 and a Cohen's kappa of 0.667, indicating some space for further improvement. Our baseline approaches did not use any contextual information nor domain knowledge. We expect that the classification performance can be further boosted, allowing researchers to label future papers automatically.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "How can these annotations help search and information extraction? Several search engines have been quickly developed and deployed. These engines allow users to navigate CORD-19 more efficiently and could potentially support decisionmaking. One motivation for spotting research aspects automatically is to help search and information extraction (Teufel et al., 1999) . We have teamed up with the group who created COVID-Seer 5 to explore the possible uses of CODA-19 in such systems.",
            "cite_spans": [
                {
                    "start": 344,
                    "end": 365,
                    "text": "(Teufel et al., 1999)",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Data Preprocessing"
        },
        {
            "text": "What other types of biomedical annotations can be crowdsourced? Many prior works that used crowd workers to annotate medical documents (Khare et al., 2016) focused on images (Heim et al., 2018) or named entities (e.g., medical terms (Mohan and Li, 2019), disease (Good et al., 2014) , or medicine (Abaho et al., 2019) .) We will explore what other types of annotations can be created using non-expert workers.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 155,
                    "text": "(Khare et al., 2016)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 174,
                    "end": 193,
                    "text": "(Heim et al., 2018)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 263,
                    "end": 282,
                    "text": "(Good et al., 2014)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 297,
                    "end": 317,
                    "text": "(Abaho et al., 2019)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Data Preprocessing"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Correcting crowdsourced annotations to improve detection of outcome types in evidence based medicine",
            "authors": [
                {
                    "first": "Micheal",
                    "middle": [],
                    "last": "Abaho",
                    "suffix": ""
                },
                {
                    "first": "Danushka",
                    "middle": [],
                    "last": "Bollegala",
                    "suffix": ""
                },
                {
                    "first": "Paula",
                    "middle": [],
                    "last": "Williamson",
                    "suffix": ""
                },
                {
                    "first": "Susanna",
                    "middle": [],
                    "last": "Dodd",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CEUR Workshop Proceedings",
            "volume": "2429",
            "issn": "",
            "pages": "1--5",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Scienceie-extracting keyphrases and relations from scientific publications",
            "authors": [
                {
                    "first": "Isabelle",
                    "middle": [],
                    "last": "Augenstein",
                    "suffix": ""
                },
                {
                    "first": "Mrinal",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Riedel",
                    "suffix": ""
                },
                {
                    "first": "Lakshmi",
                    "middle": [],
                    "last": "Vikraman",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1704.02853"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Segmenting scientific abstracts into discourse categories: A deep learningbased approach for sparse labeled data",
            "authors": [
                {
                    "first": "Soumya",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "Debarshi",
                    "middle": [],
                    "last": "Kumar Sanyal",
                    "suffix": ""
                },
                {
                    "first": "Samiran",
                    "middle": [],
                    "last": "Chattopadhyay",
                    "suffix": ""
                },
                {
                    "first": "Plaban",
                    "middle": [],
                    "last": "Kumar Bhowmick",
                    "suffix": ""
                },
                {
                    "first": "Parthapratim",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.05414"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Scibert: Pretrained language model for scientific text",
            "authors": [
                {
                    "first": "Iz",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "Arman",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EMNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Solvent: A mixed initiative system for finding analogies between research papers",
            "authors": [
                {
                    "first": "Joel",
                    "middle": [],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "Joseph",
                    "middle": [
                        "Chee"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Tom",
                    "middle": [],
                    "last": "Hope",
                    "suffix": ""
                },
                {
                    "first": "Dafna",
                    "middle": [],
                    "last": "Shahaf",
                    "suffix": ""
                },
                {
                    "first": "Aniket",
                    "middle": [],
                    "last": "Kittur",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the ACM on Human-Computer Interaction",
            "volume": "2",
            "issn": "",
            "pages": "1--21",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Nees Jan van Eck",
            "authors": [
                {
                    "first": "Giovanni",
                    "middle": [],
                    "last": "Colavizza",
                    "suffix": ""
                },
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Costas",
                    "suffix": ""
                },
                {
                    "first": "Vincent",
                    "middle": [
                        "A"
                    ],
                    "last": "Traag",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1101/2020.04.20.046144"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Experiment segmentation in scientific discourse as clause-level structured prediction using recurrent neural networks",
            "authors": [
                {
                    "first": "Pradeep",
                    "middle": [],
                    "last": "Dasigi",
                    "suffix": ""
                },
                {
                    "first": "Apc",
                    "middle": [],
                    "last": "Gully",
                    "suffix": ""
                },
                {
                    "first": "Eduard",
                    "middle": [],
                    "last": "Burns",
                    "suffix": ""
                },
                {
                    "first": "Anita",
                    "middle": [],
                    "last": "Hovy",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "De Waard",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1702.05398"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.04805"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Semeval-2018 task 7: Semantic relation extraction and classification in scientific papers",
            "authors": [
                {
                    "first": "Kata",
                    "middle": [],
                    "last": "G\u00e1bor",
                    "suffix": ""
                },
                {
                    "first": "Davide",
                    "middle": [],
                    "last": "Buscaldi",
                    "suffix": ""
                },
                {
                    "first": "Anne-Kathrin",
                    "middle": [],
                    "last": "Schumann",
                    "suffix": ""
                },
                {
                    "first": "Behrang",
                    "middle": [],
                    "last": "Qasemizadeh",
                    "suffix": ""
                },
                {
                    "first": "Haifa",
                    "middle": [],
                    "last": "Zargayouna",
                    "suffix": ""
                },
                {
                    "first": "Thierry",
                    "middle": [],
                    "last": "Charnois",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation",
            "volume": "",
            "issn": "",
            "pages": "679--688",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Microtask crowdsourcing for disease mention annotation in pubmed abstracts",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Benjamin",
                    "suffix": ""
                },
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Good",
                    "suffix": ""
                },
                {
                    "first": "Chunlei",
                    "middle": [],
                    "last": "Nanis",
                    "suffix": ""
                },
                {
                    "first": "Andrew I",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Pacific Symposium on Biocomputing Co-Chairs",
            "volume": "",
            "issn": "",
            "pages": "282--293",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Current findings from research on structured abstracts",
            "authors": [
                {
                    "first": "James",
                    "middle": [],
                    "last": "Hartley",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Journal of the Medical Library Association",
            "volume": "92",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Large-scale medical image annotation with crowd-powered algorithms",
            "authors": [
                {
                    "first": "Eric",
                    "middle": [],
                    "last": "Heim",
                    "suffix": ""
                },
                {
                    "first": "Tobias",
                    "middle": [],
                    "last": "Ro\u00df",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [],
                    "last": "Seitel",
                    "suffix": ""
                },
                {
                    "first": "Keno",
                    "middle": [],
                    "last": "M\u00e4rz",
                    "suffix": ""
                },
                {
                    "first": "Bram",
                    "middle": [],
                    "last": "Stieltjes",
                    "suffix": ""
                },
                {
                    "first": "Matthias",
                    "middle": [],
                    "last": "Eisenmann",
                    "suffix": ""
                },
                {
                    "first": "Johannes",
                    "middle": [],
                    "last": "Lebert",
                    "suffix": ""
                },
                {
                    "first": "Jasmin",
                    "middle": [],
                    "last": "Metzger",
                    "suffix": ""
                },
                {
                    "first": "Gregor",
                    "middle": [],
                    "last": "Sommer",
                    "suffix": ""
                },
                {
                    "first": "Alexander",
                    "middle": [
                        "W"
                    ],
                    "last": "Sauter",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of Medical Imaging",
            "volume": "5",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Disa: A scientific writing advisor with deep information structure analysis",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Hen-Hsen",
                    "suffix": ""
                },
                {
                    "first": "Hsin-Hsi",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IJCAI",
            "volume": "",
            "issn": "",
            "pages": "5229--5231",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Kinetic analysis of bacteriophage sf6 binding to outer membrane protein a using whole virions. bioRxiv",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Natalia",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hubbs",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mareena",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [
                        "L"
                    ],
                    "last": "Whisby-Pitts",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mcmurry",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Crowdsourcing in biomedicine: challenges and opportunities. Briefings in bioinformatics",
            "authors": [
                {
                    "first": "Ritu",
                    "middle": [],
                    "last": "Khare",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Benjamin",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Good",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [
                        "I"
                    ],
                    "last": "Leaman",
                    "suffix": ""
                },
                {
                    "first": "Zhiyong",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "17",
            "issn": "",
            "pages": "23--32",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Biomedical text mining for research rigor and integrity: tasks, challenges, directions",
            "authors": [
                {
                    "first": "Halil",
                    "middle": [],
                    "last": "Kilicoglu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Briefings in bioinformatics",
            "volume": "19",
            "issn": "6",
            "pages": "1400--1414",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Convolutional neural networks for sentence classification",
            "authors": [
                {
                    "first": "Yoon",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1408.5882"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Corpora for the conceptualisation and zoning of scientific papers",
            "authors": [
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Liakata",
                    "suffix": ""
                },
                {
                    "first": "Simone",
                    "middle": [],
                    "last": "Teufel",
                    "suffix": ""
                },
                {
                    "first": "Advaith",
                    "middle": [],
                    "last": "Siddharthan",
                    "suffix": ""
                },
                {
                    "first": "Colin",
                    "middle": [],
                    "last": "Batchelor",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC10)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "The Stanford CoreNLP natural language processing toolkit",
            "authors": [
                {
                    "first": "Christopher",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                },
                {
                    "first": "Mihai",
                    "middle": [],
                    "last": "Surdeanu",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Bauer",
                    "suffix": ""
                },
                {
                    "first": "Jenny",
                    "middle": [],
                    "last": "Finkel",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [
                        "J"
                    ],
                    "last": "Bethard",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Mc-Closky",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Association for Computational Linguistics (ACL) System Demonstrations",
            "volume": "",
            "issn": "",
            "pages": "55--60",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Zone analysis in biology articles as a basis for information extraction. International journal of medical informatics",
            "authors": [
                {
                    "first": "Yoko",
                    "middle": [],
                    "last": "Mizuta",
                    "suffix": ""
                },
                {
                    "first": "Anna",
                    "middle": [],
                    "last": "Korhonen",
                    "suffix": ""
                },
                {
                    "first": "Tony",
                    "middle": [],
                    "last": "Mullen",
                    "suffix": ""
                },
                {
                    "first": "Nigel",
                    "middle": [],
                    "last": "Collier",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "75",
            "issn": "",
            "pages": "468--487",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Medmentions: a large biomedical corpus annotated with umls concepts",
            "authors": [
                {
                    "first": "Sunil",
                    "middle": [],
                    "last": "Mohan",
                    "suffix": ""
                },
                {
                    "first": "Donghui",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1902.09476"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Paszke",
                    "suffix": ""
                },
                {
                    "first": "Sam",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                },
                {
                    "first": "Francisco",
                    "middle": [],
                    "last": "Massa",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Lerer",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Bradbury",
                    "suffix": ""
                },
                {
                    "first": "Gregory",
                    "middle": [],
                    "last": "Chanan",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Killeen",
                    "suffix": ""
                },
                {
                    "first": "Zeming",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Natalia",
                    "middle": [],
                    "last": "Gimelshein",
                    "suffix": ""
                },
                {
                    "first": "Luca",
                    "middle": [],
                    "last": "Antiga",
                    "suffix": ""
                },
                {
                    "first": "Alban",
                    "middle": [],
                    "last": "Desmaison",
                    "suffix": ""
                },
                {
                    "first": "Andreas",
                    "middle": [],
                    "last": "Kopf",
                    "suffix": ""
                },
                {
                    "first": "Edward",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Zachary",
                    "middle": [],
                    "last": "Devito",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Raison",
                    "suffix": ""
                },
                {
                    "first": "Alykhan",
                    "middle": [],
                    "last": "Tejani",
                    "suffix": ""
                },
                {
                    "first": "Sasank",
                    "middle": [],
                    "last": "Chilamkurthy",
                    "suffix": ""
                },
                {
                    "first": "Benoit",
                    "middle": [],
                    "last": "Steiner",
                    "suffix": ""
                },
                {
                    "first": "Lu",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Junjie",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "Soumith",
                    "middle": [],
                    "last": "Chintala",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "32",
            "issn": "",
            "pages": "8024--8035",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Scikit-learn: Machine learning in python",
            "authors": [
                {
                    "first": "Fabian",
                    "middle": [],
                    "last": "Pedregosa",
                    "suffix": ""
                },
                {
                    "first": "Ga\u00ebl",
                    "middle": [],
                    "last": "Varoquaux",
                    "suffix": ""
                },
                {
                    "first": "Alexandre",
                    "middle": [],
                    "last": "Gramfort",
                    "suffix": ""
                },
                {
                    "first": "Vincent",
                    "middle": [],
                    "last": "Michel",
                    "suffix": ""
                },
                {
                    "first": "Bertrand",
                    "middle": [],
                    "last": "Thirion",
                    "suffix": ""
                },
                {
                    "first": "Olivier",
                    "middle": [],
                    "last": "Grisel",
                    "suffix": ""
                },
                {
                    "first": "Mathieu",
                    "middle": [],
                    "last": "Blondel",
                    "suffix": ""
                },
                {
                    "first": "Peter",
                    "middle": [],
                    "last": "Prettenhofer",
                    "suffix": ""
                },
                {
                    "first": "Ron",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                },
                {
                    "first": "Vincent",
                    "middle": [],
                    "last": "Dubourg",
                    "suffix": ""
                },
                {
                    "first": "Jake",
                    "middle": [],
                    "last": "Vanderplas",
                    "suffix": ""
                },
                {
                    "first": "Alexandre",
                    "middle": [],
                    "last": "Passos",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Cournapeau",
                    "suffix": ""
                },
                {
                    "first": "Matthieu",
                    "middle": [],
                    "last": "Brucher",
                    "suffix": ""
                },
                {
                    "first": "Matthieu",
                    "middle": [],
                    "last": "Perrot",
                    "suffix": ""
                },
                {
                    "first": "Duchesnay",
                    "middle": [],
                    "last": "And\u00e9douard",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Mach. Learn. Res",
            "volume": "12",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "The acl rd-tec 2.0: A language resource for evaluating term extraction and entity recognition methods",
            "authors": [
                {
                    "first": "Behrang",
                    "middle": [],
                    "last": "Qasemizadeh",
                    "suffix": ""
                },
                {
                    "first": "Anne-Kathrin",
                    "middle": [],
                    "last": "Schumann",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)",
            "volume": "",
            "issn": "",
            "pages": "1862--1868",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Reading on lcd vs e-ink displays: effects on fatigue and visual strain",
            "authors": [
                {
                    "first": "Eva",
                    "middle": [],
                    "last": "Siegenthaler",
                    "suffix": ""
                },
                {
                    "first": "Yves",
                    "middle": [],
                    "last": "Bochud",
                    "suffix": ""
                },
                {
                    "first": "Per",
                    "middle": [],
                    "last": "Bergamin",
                    "suffix": ""
                },
                {
                    "first": "Pascal",
                    "middle": [],
                    "last": "Wurtz",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Ophthalmic and Physiological Optics",
            "volume": "32",
            "issn": "5",
            "pages": "367--374",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Argumentative zoning: Information extraction from scientific text",
            "authors": [
                {
                    "first": "Simone",
                    "middle": [],
                    "last": "Teufel",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Hypotheses, evidence and relationships: The hyper approach for representing scientific knowledge claims",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "De Waard",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Buckingham",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shum",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Carusi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Samwald",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "S\u00e1ndor",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings 8th International Semantic Web Conference, Workshop on Semantic Web Applications in Scientific Discourse",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Verb form indicates discourse segment type in biological research papers: Experimental evidence",
            "authors": [
                {
                    "first": "Anita",
                    "middle": [],
                    "last": "De Waard",
                    "suffix": ""
                },
                {
                    "first": "Henk Pander",
                    "middle": [],
                    "last": "Maat",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Journal of English for Academic Purposes",
            "volume": "11",
            "issn": "4",
            "pages": "357--366",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Paperrobot: Incremental draft generation of scientific ideas",
            "authors": [
                {
                    "first": "Qingyun",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Lifu",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Zhiying",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Kevin",
                    "middle": [],
                    "last": "Knight",
                    "suffix": ""
                },
                {
                    "first": "Heng",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "Mohit",
                    "middle": [],
                    "last": "Bansal",
                    "suffix": ""
                },
                {
                    "first": "Yi",
                    "middle": [],
                    "last": "Luan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1980--1991",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "New directions in biomedical text annotation: definitions, guidelines and corpus construction",
            "authors": [
                {
                    "first": "W John",
                    "middle": [],
                    "last": "Wilbur",
                    "suffix": ""
                },
                {
                    "first": "Andrey",
                    "middle": [],
                    "last": "Rzhetsky",
                    "suffix": ""
                },
                {
                    "first": "Hagit",
                    "middle": [],
                    "last": "Shatkay",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "BMC bioinformatics",
            "volume": "7",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Huggingface's transformers: State-of-the-art natural language processing",
            "authors": [
                {
                    "first": "Thomas",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                },
                {
                    "first": "Lysandre",
                    "middle": [],
                    "last": "Debut",
                    "suffix": ""
                },
                {
                    "first": "Victor",
                    "middle": [],
                    "last": "Sanh",
                    "suffix": ""
                },
                {
                    "first": "Julien",
                    "middle": [],
                    "last": "Chaumond",
                    "suffix": ""
                },
                {
                    "first": "Clement",
                    "middle": [],
                    "last": "Delangue",
                    "suffix": ""
                },
                {
                    "first": "Anthony",
                    "middle": [],
                    "last": "Moi",
                    "suffix": ""
                },
                {
                    "first": "Pierric",
                    "middle": [],
                    "last": "Cistac",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Rault",
                    "suffix": ""
                },
                {
                    "first": "R&apos;emi",
                    "middle": [],
                    "last": "Louf",
                    "suffix": ""
                },
                {
                    "first": "Morgan",
                    "middle": [],
                    "last": "Funtowicz",
                    "suffix": ""
                },
                {
                    "first": "Jamie",
                    "middle": [],
                    "last": "Brew",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "An example of the final crowd annotation for the abstract of (Hubbs et al., 2019).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "\u2022 Captions for figures and tables (e.g. \"Figure 1: Experimental Result of ...\") \u2022 Formatting errors. \u2022 Text segments the annotator does not know or is not sure about.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The worker interface used to construct CODA-19.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The normalized confusion matrix for the CODA-19 labels versus the biomedical expert's labels.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "CODA-19's annotation guideline for crowd workers.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "RF  .671 .632 .651 .696 .365 .479 .716 .350 .471 .630 .787 .699 .674 .742 .706   .652 CNN .649 .706 .676 .612 .512 .557 .596 .562 .579 .726 .702 .714 .743 .795 .768 .677 LSTM .655 .706 .680 .700 .464 .558 .634 .508 .564 .700 .724 .711 .682 .770 .723 .676 BERT .719 .759 .738 .585 .639 .611 .680 .612 .644 .777 .752 .764 .773 .874 .820 .733 SciBERT .733 .768 .750 .616 .636 .626 .715 .636 .673 .783 .775 .779 .794 .852 .822 .749",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This project is supported by the Huck Institutes of the Life Sciences' Coronavirus Research Seed Fund (CRSF) at Penn State University and the College of IST COVID-19 Seed Fund at Penn State University. We thank the crowd workers for participating in this project and providing useful feedback. We thank VoiceBunny Inc. for granting a 20% discount for the voiceover for the worker tutorial video to support projects relevant to COVID-19. We also thank Tiffany Knearem, Shih-Hong (Alan) Huang, Joseph Chee Chang, and Frank Ritter for the great discussion and useful feedback.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        }
    ]
}