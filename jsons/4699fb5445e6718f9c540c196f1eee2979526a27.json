{
    "paper_id": "4699fb5445e6718f9c540c196f1eee2979526a27",
    "metadata": {
        "title": "SLEDGE: A Simple Yet Effective Baseline for Coronavirus Scientific Knowledge Search",
        "authors": [
            {
                "first": "Sean",
                "middle": [],
                "last": "Macavaney",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Arman",
                "middle": [],
                "last": "Cohan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Allen Institute for AI",
                    "location": {
                        "settlement": "Seattle",
                        "region": "WA"
                    }
                },
                "email": "armanc@allenai.org"
            },
            {
                "first": "Nazli",
                "middle": [],
                "last": "Goharian",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of literature on the virus. Clinicians, researchers, and policy-makers need a way to effectively search these articles. In this work, we present a search system called SLEDGE, which utilizes SciBERT to effectively re-rank articles. We train the model on a general-domain answer ranking dataset, and transfer the relevance signals to SARS-CoV-2 for evaluation. We observe SLEDGE's effectiveness as a strong baseline on the TREC-COVID challenge (topping the learderboard with an nDCG@10 of 0.6844). Insights provided by a detailed analysis provide some potential future directions to explore, including the importance of filtering by date and the potential of neural methods that rely more heavily on count signals. We release the code to facilitate future work on this critical task. 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The emergence of the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) prompted a worldwide research response. In the first 100 days of 2020, over 5,000 research articles were published related to SARS-CoV-2 or COVID-19. Together with articles about similar viruses researched before 2020, the body of research exceeds 50,000 articles. This results in a considerable burden for those seeking information about various facets of the virus, including researchers, clinicians, and policy-makers.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the interest of establishing a strong baseline for retrieving scientific literature related to COVID-19, we introduce SLEDGE: a simple yet effective baseline for coronavirus Scientific knowLEDGE search. Our baseline utilizes a combination of state-of-the-art techniques for neural information retrieval.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recent work in neural information retrieval shows the effectiveness of pretrained language models in document ranking. (MacAvaney et al., 2019; Hofst\u00e4tter et al., 2020; Dai and Callan, 2019b; Nogueira et al., 2020b) . Building upon success of these models, SLEDGE is comprised of a re-ranker based on SciB-ERT (Beltagy et al., 2019) , a pretrained language model optimized for scientific text. Since at the time of writing there is no available training data for COVID-19 related search, we additionally use a domain transfer approach by training SLEDGE on MS- MARCO (Campos et al., 2016) , a generaldomain passage ranking dataset, and apply it to COVID-19 literature search in zero-shot setting.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 143,
                    "text": "(MacAvaney et al., 2019;",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 144,
                    "end": 168,
                    "text": "Hofst\u00e4tter et al., 2020;",
                    "ref_id": null
                },
                {
                    "start": 169,
                    "end": 191,
                    "text": "Dai and Callan, 2019b;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 192,
                    "end": 215,
                    "text": "Nogueira et al., 2020b)",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 310,
                    "end": 332,
                    "text": "(Beltagy et al., 2019)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 561,
                    "end": 588,
                    "text": "MARCO (Campos et al., 2016)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We show that SLEDGE achieves strong results in the task of scientific literature search related to COVID-19. In particular, SLEDGE tops the leaderboard in Round 1 of the TREC-COVID Information Retrieval shared task (Roberts et al., 2020) , 2 a new test bed for evaluating effectiveness of search methods for COVID-19. We also provide an analysis into the hyperparameter tuning conducted, the effect of various query and document fields, and possible shortcomings of the approach. Insights from the analysis highlight the importance of a date filter for improving precision, and the possible benefit of utilizing models that include count-based signals in future work.",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 237,
                    "text": "(Roberts et al., 2020)",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We hope that better natural language processing and search tools can contribute to the fight against the current global crisis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Retrieval of scientific literature has been longstudied (Lawrence et al., 1999; Lalmas and Tombros, 2007; Hersh and Voorhees, 2009; Lin, 2008; Medlar et al., 2016; Sorkhei et al., 2017; Huang et al., 2019) . Most recent work for scientific literature retrieval has focused on tasks such as collaborative filtering (Chen and Lee, 2018), citation recommendation (Nogueira et al., 2020a) , and clinical decision support (Soldaini et al., 2017) , rather than ad-hoc retrieval.",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 79,
                    "text": "(Lawrence et al., 1999;",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 80,
                    "end": 105,
                    "text": "Lalmas and Tombros, 2007;",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 106,
                    "end": 131,
                    "text": "Hersh and Voorhees, 2009;",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 132,
                    "end": 142,
                    "text": "Lin, 2008;",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 143,
                    "end": 163,
                    "text": "Medlar et al., 2016;",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 164,
                    "end": 185,
                    "text": "Sorkhei et al., 2017;",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 186,
                    "end": 205,
                    "text": "Huang et al., 2019)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 360,
                    "end": 384,
                    "text": "(Nogueira et al., 2020a)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 417,
                    "end": 440,
                    "text": "(Soldaini et al., 2017)",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Pre-trained neural language models (such as BERT (Devlin et al., 2019) ) have recently shown to be effective when fine-tuned for ad-hoc ranking. demonstrate that these networks can be fine-tuned for passage ranking tasks. Others later observed effectiveness at document ranking tasks, showing that these models can handle natural-language questions better than prior approaches (Dai and Callan, 2019b) and that they can be incorporated into prior neutral ranking techniques (MacAvaney et al., 2019) . Although computationally expensive, researches have shown that this can be mitigated to an extent by employing more efficient modeling choices (Hofst\u00e4tter et al., 2020; MacAvaney et al., 2020c) , caching intermediate representations (Khattab and Zaharia, 2020; MacAvaney et al., 2020b; Gao et al., 2020) , or by modifying the index with new terms or weights Dai and Callan, 2019a; Nogueira, 2019) . These models also facilitate effective relevance signal transfer; Yilmaz et al. (2019) demonstrate that the relevance signals learned from BERT can easily transfer across collections (reducing the chance of overfitting a particular collection). In this work, we utilize relevance signal transfer from an open-domain question answering dataset to the collection of COVID-19 scientific literature.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 70,
                    "text": "(Devlin et al., 2019)",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 378,
                    "end": 401,
                    "text": "(Dai and Callan, 2019b)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 474,
                    "end": 498,
                    "text": "(MacAvaney et al., 2019)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 644,
                    "end": 669,
                    "text": "(Hofst\u00e4tter et al., 2020;",
                    "ref_id": null
                },
                {
                    "start": 670,
                    "end": 694,
                    "text": "MacAvaney et al., 2020c)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 734,
                    "end": 761,
                    "text": "(Khattab and Zaharia, 2020;",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 762,
                    "end": 786,
                    "text": "MacAvaney et al., 2020b;",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 787,
                    "end": 804,
                    "text": "Gao et al., 2020)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 859,
                    "end": 881,
                    "text": "Dai and Callan, 2019a;",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 882,
                    "end": 897,
                    "text": "Nogueira, 2019)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 966,
                    "end": 986,
                    "text": "Yilmaz et al. (2019)",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In terms of biomedical-related ranking, MacAvaney et al. (2020a) observed the importance of using a domain-tuned language model (SciB-ERT (Beltagy et al., 2019) ) when ranking in the biomedical domain (albeit working with clinical text rather than scientific literature). Some work already investigates document ranking and Question Answering (QA) about COVID-19. chronicled their efforts of building and deploying a search engine for COVID-19 articles, utilizing a variety of available tools ranking techniques. In this work, we find that our approach out-performs this system in terms of ranking effectiveness. Tang et al. (2020) provide a QA dataset consisting of 124 COVID-19 question-answer pairs.",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 64,
                    "text": "MacAvaney et al. (2020a)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 138,
                    "end": 160,
                    "text": "(Beltagy et al., 2019)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 613,
                    "end": 631,
                    "text": "Tang et al. (2020)",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This section describes the details of SLEDGE, our method for searching scientific literature related to COVID-19. We utilize a standard two-stage reranking pipeline for retrieving and ranking COVID-19 articles. The articles are curated from the CORD19 dataset (Wang et al., 2020) and provided by the task organizers. The first stage employs an inexpensive ranking model (namely, BM25) to generate a high-recall collection of candidate documents. The second stage re-ranks the candidate documents using an expensive but high-precision SciBERT-based (Beltagy et al., 2019) neural ranking model.",
            "cite_spans": [
                {
                    "start": 260,
                    "end": 279,
                    "text": "(Wang et al., 2020)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 548,
                    "end": 570,
                    "text": "(Beltagy et al., 2019)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "SLEDGE"
        },
        {
            "text": "We first index the document collection using standard pre-processing methods: English stopword removal and Porter stemming. For the text, we use a concatenation of the title, abstract, and fulltext paragraphs and fulltext headings. The fulltext gives more opportunities for the first-stage ranker to match potentially relevant documents than the title alone would provide. When both the PDF and PubMed XML versions are available, we use the text extracted from the PubMed XML because it is generally cleaner. We then query the index for each topic with BM25. In this system, we used a fixed re-ranking threshold of 500; thus only the top 500 BM25 results are retrieved. In our experiments, we found that there was little recall gained beyond 500.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "First-Stage Retrieval"
        },
        {
            "text": "To best capture the domain-specific language related to scientific text we use the SciBERT (Beltagy et al., 2019) pretrained language model as the basis of a second-stage supervised re-ranker. This model is akin to the Vanilla BERT ranker from (MacAvaney et al., 2019), but utilizing the SciBERT model base (which is trained on scientific literature) instead. The query and document text are encoded sequentially, and relevance prediction is calculated based on the [CLS] token's representation (which was used for next sentence prediction during pre-training). Documents longer than the maximum length imposed by the positional embed-dings are split into arbitrary equal-sized passages. We refer the reader to (MacAvaney et al., 2019) for more details about Vanilla BERT.",
            "cite_spans": [
                {
                    "start": 711,
                    "end": 735,
                    "text": "(MacAvaney et al., 2019)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Neural Re-Ranking"
        },
        {
            "text": "At the time of writing there is no training data available for the COVID-19 related search and collecting such data is expensive. To mitigate this challenge, we utilize a domain transfer approach and apply the learned model to the new domain in a zero-shot setting. This approach also has the advantage of avoiding overfitting on the target dataset. Specifically, we train our model using the standard training sequence of the MS-MARCO passage ranking dataset (Campos et al., 2016) . This dataset consists of over 800,000 query-document pairs in the genral domain with a shallow labeling scheme (typically fewer than two positive relevance labels per query; non-relevance assumed from unlabeled passages). During model training, we employ the following cross-entropy loss function from Nogueira and Cho (2019):",
            "cite_spans": [
                {
                    "start": 460,
                    "end": 481,
                    "text": "(Campos et al., 2016)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Neural Re-Ranking"
        },
        {
            "text": "where q is the query, d + and d \u2212 are the relevant and non-relevant training documents, and R(q, d) is the relevance score.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Re-Ranking"
        },
        {
            "text": "We now explore the ranking effectiveness of our approach. We evaluate the performance of SLEDGE using the TREC-COVID Information Retrieval Challenge dataset (round 1) (Roberts et al., 2020) . TREC-COVID uses the CORD-19 document collection (Wang et al., 2020 ) (2020-04-10 version, 51,045 articles), with a set of 30 topics related to COVID-19. These topics include natural queries such as: Coronavirus response to weather changes and Coronavirus social distancing impact. The top articles of participating systems (56 teams) were judged by expert assessors, who rated each article non-relevant (0), partially-relevant (1), or fullyrelevant (2) to the topic. In total, 8,691 relevance judgments were collected, with 74% non-relevant, 13% partially relevant, and 14% fully-relevant.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 189,
                    "text": "(Roberts et al., 2020)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 240,
                    "end": 258,
                    "text": "(Wang et al., 2020",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "Since the relevance judgments in this dataset are shallow (avg. 290 per query), we measure effectiveness of each system using normalized Discounted Cumulative Gain with a cutoff of 10 (nDCG@10), Precision at 5 of partially and fully-relevant documents (P@5), and Precision at 5 of only fully relevant documents (P@5 (Rel.)). Both nDCG@10 and P@5 are official task metrics; we include the P@5 filtered to only fully-relevance documents because it exposed some interesting trends in our analysis. Since not all submissions contributed to the judgment pool, we also report the percentage of the top 5 documents for each query that have relevance judgments (judged@5). These settings represent a high-precision evaluation; we leave it to future work to evaluate techniques for maximizing system recall, which may require special considerations (Grossman et al., 2015) .",
            "cite_spans": [
                {
                    "start": 840,
                    "end": 863,
                    "text": "(Grossman et al., 2015)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "Our work utilizes a variety of existing opensource tools, including OpenNIR (MacAvaney, 2020), Anserini (Yang et al., 2017) , and the Hug-gingFace Transformers library (Wolf et al., 2019) . Our experiments were conducted with a Quadro RTX 8000 GPU, and a learning rate of 2 \u00d7 10 \u22125 .",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 123,
                    "text": "(Yang et al., 2017)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 168,
                    "end": 187,
                    "text": "(Wolf et al., 2019)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "Note on manual vs automatic runs TREC-COVID makes the distinction between manual and automatic runs. We adhere to the broad definition of manual runs, as specified by the task guidelines: \"Automatic construction is when there is no human involvement of any sort in the query construction process; manual construction is everything else... If you make any change to your retrieval system based on the content of the TREC-COVID topics (say add words to a dictionary or modify a routine after looking at retrieved results), then your runs are manual runs.\" 3 In short, making any change to the system on the basis of observations of the query and/or results qualify as a manual run.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental setup"
        },
        {
            "text": "In this section we discuss our results in two evaluation settings. In the first setting we apply light hyperparmeter tuning on the pipeline which still counts as a manual run as discussed in \u00a74. In the second setting we do not perform any tuning of any sort and thus this setting is an automatic run.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Recall that the first stage of SLEDGE is based on an initial BM25 ranker, topics in the TREC Covid dataset include 3 different fields: query, question and narrative, and the documents have title, abstract and full-text. Choices of the BM25 parameters and which fields to include in the pipeline can affect the final performance. Therefore, in the first setting, we lightly tune these hyperparmeters using minimal human judgments on a subset of the topics. Table 2 : Top results without using any human intervention (automatic runs). No results exhibit a statistically significant difference compared to our system (paired t-test, p < 0.05). \"*\" indicates that some sort of manually specified filtering was used which may contradict the definition of an automatic run by TREC (see note in Section 4).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 456,
                    "end": 463,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Ranking with light hyperparmeter tuning"
        },
        {
            "text": "Specifically, we use shallow relevance judgments from 15 out of 30 topics assessed by non-experts. 4 Unlike manual runs that require human intervention for query reformulation, active learning, or relevance feedback, we expect our system to be able to generalize to unseen queries in the domain because we use manual relevance signals only for hyperparameter tuning. By tuning the hyperparmeters of the initial retrieval method, the fields of the topic (query, question, narrative) and document text (title, abstract, full text), and a date filter, we found the following pipeline to be most effective based on our non-expert annotations (run tag: run1):",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 100,
                    "text": "4",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Ranking with light hyperparmeter tuning"
        },
        {
            "text": "1. Initial retrieval using BM25 tuned for recall using a grid search (k 1 = 3.9, b = 0.55), utilizing the keyword query field over the full text of the article. Articles from before January 1, 2020 are disregarded.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ranking with light hyperparmeter tuning"
        },
        {
            "text": "2. Re-ranking using a Vanilla SciBERT model trained on MS-MARCO. The topic's question 4 Topics 1, 2, 6, 8, 9, 11, 13, 17, 18, 20, 21, 24, 27, 29, 30 . 849 judgments were made in total. We found that our non-expert annotations did not align well with the officially released expert annotations \u00a75.3.",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 103,
                    "text": "6,",
                    "ref_id": null
                },
                {
                    "start": 104,
                    "end": 106,
                    "text": "8,",
                    "ref_id": null
                },
                {
                    "start": 107,
                    "end": 109,
                    "text": "9,",
                    "ref_id": null
                },
                {
                    "start": 110,
                    "end": 113,
                    "text": "11,",
                    "ref_id": null
                },
                {
                    "start": 114,
                    "end": 117,
                    "text": "13,",
                    "ref_id": null
                },
                {
                    "start": 118,
                    "end": 121,
                    "text": "17,",
                    "ref_id": null
                },
                {
                    "start": 122,
                    "end": 125,
                    "text": "18,",
                    "ref_id": null
                },
                {
                    "start": 126,
                    "end": 129,
                    "text": "20,",
                    "ref_id": null
                },
                {
                    "start": 130,
                    "end": 133,
                    "text": "21,",
                    "ref_id": null
                },
                {
                    "start": 134,
                    "end": 137,
                    "text": "24,",
                    "ref_id": null
                },
                {
                    "start": 138,
                    "end": 141,
                    "text": "27,",
                    "ref_id": null
                },
                {
                    "start": 142,
                    "end": 145,
                    "text": "29,",
                    "ref_id": null
                },
                {
                    "start": 146,
                    "end": 148,
                    "text": "30",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Ranking with light hyperparmeter tuning"
        },
        {
            "text": "field is scored over the article's title and abstract.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ranking with light hyperparmeter tuning"
        },
        {
            "text": "We report the performance of the top system from the top 10 teams (among manual runs) for TREC-COVID in Table 1 . Since the utilization of humans-in-the-loop vary considerably, we also indicate for each run the reported human intervention. We find that SLEDGE outperforms all the other manual runs in terms of nDCG@10 and P@5 (relevant only). Of the top 10 systems that report their technique for human intervention, ours is also the only one that relies on human judgments solely for hyperparameter tuning. This is particularly impressive because the next best systems (BBGhe-lani2 and xj4wang run1) involves human-in-theloop active learning to rank documents based on the manual assessor's relevance. In terms of statistical significance (paired t-test, p < 0.05), our approach is on par with these active learning runs, and better than most other submissions in terms of nDCG@10 and P@5 (relevant).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 104,
                    "end": 111,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Ranking with light hyperparmeter tuning"
        },
        {
            "text": "We now evaluate our system in an environment that does not utilize human intervention, hyperparam- Figure 1 : Comparison of grid search heatmaps for BM25 using the topic's query over article full text with (a) our relevance judgments, (b) the full set of official judgments, and (c) the set of official relevance judgments filtered to only the topics we assessed. The x-axis sweeps b \u2208 [0, 1] and the y-axis sweeps k 1 \u2208 [0.1, 6.0], and each cell represents the recall@100. eter tuning, or relevance judgements of any sort. This represents a full domain transfer setting. Our pipeline consists of (run tag: run2):",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 99,
                    "end": 107,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Ranking without hyperparameter tuning"
        },
        {
            "text": "1. Initial retrieval using untuned BM25 (default parameters, k 1 = 0.9, b = 0.4), utilizing the question text over the title and abstract of a article. (No date filtering.)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ranking without hyperparameter tuning"
        },
        {
            "text": "2. Re-ranking using a Vanilla SciBERT model trained on a medical-related subset of MS-MARCO training data. The topic's question field is scored over the article's title and abstract.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ranking without hyperparameter tuning"
        },
        {
            "text": "The purpose of leveraging the medical-related subset of MS-MARCO is to reduce the risk of domain shift. To produce this subset, we use the MedSyn lexicon (Yates and Goharian, 2013) , which includes layperson terminology for various medical conditions. Only queries that contain terms from the lexicon are considered in this dataset, leaving 78,895 of the original 808,531 training queries (9.7%). 5 A list of the query IDs corresponding to this filtered set is available. 6 We observe that our automatic SLEDGE run performs highly competitively among other automatic submissions to the TREC-COVID shared task. Although the highest-scoring system in terms of nDCG@10 utilizes a traditional method, we observe that it falls short of neural (e.g., SLEDGE, IRIT marked base, CSIROmedNIR) in terms of 5 Several common terms were manually excluded to increase the precision of the filter, such as gas, card, bing, died, map, and fall. This does not qualify as manual tuning because these decisions were made only in consideration of the MS-MARCO training queries, not any TREC-COVID topics. 6 https://github.com/Georgetown-IR-Lab/ covid-neural-ir/med-msmarco-train.txt P@5 for fully-relevant articles and the difference between the result are not statistically significant. Furthermore, due to the 88% and 76% judged@5 of SLEDGE and CSIROmedNIR, the actual P@5 scores for these systems may very well be higher. Curiously, however, other neural approaches that are generally high-performing (e.g., those used by ) did not rank in the top 10 runs. We do observe that other traditional approaches, such as those that perform query expansion (e.g., udel fang run3, and uogTrDPH QE) also perform competitively in the automatic setting.",
            "cite_spans": [
                {
                    "start": 154,
                    "end": 180,
                    "text": "(Yates and Goharian, 2013)",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 472,
                    "end": 473,
                    "text": "6",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Ranking without hyperparameter tuning"
        },
        {
            "text": "Initial retrieval parameters We now evaluate the hyperparameter tuning process conducted. We first test the following first-stage ranking functions and tune for recall@100 using our judgments: BM25 (k 1 \u2208 [0.1, 6.0] by 0.1, b \u2208 [0, 1] by 0.05), RM3 query expansion (Jaleel et al., 2004) over default BM25 parameters (feedback terms and feedback docs \u2208 [1, 20] by 1), QL Sequential Dependency Model (SDM (Metzler and Croft, 2005) , term, ordered, and un-ordered weights by 0.05).",
            "cite_spans": [
                {
                    "start": 403,
                    "end": 428,
                    "text": "(Metzler and Croft, 2005)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Analysis"
        },
        {
            "text": "Each of these models is tested using with the query or question topic field, and over the article full text, or just the title and abstract. We find that using BM25 with k 1 = 3.9 and b = 0.55, the topic's query field, and the article's full text to yield the highest recall. We compare the heatmap of this setting using our judgments, the full set of official judgments, and the set of official judgments filtered to only the topics we judged in Figure 1 . Although the precise values for the optimal parameter settings differ, the shapes are similar suggesting that the hyperparameter choices generalize. Table 3 : Performance of our system using various sources for the first-stage query text, re-ranking query text, and date filtering. Our official submission is marked with *.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 447,
                    "end": 455,
                    "text": "Figure 1",
                    "ref_id": null
                },
                {
                    "start": 607,
                    "end": 614,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Analysis"
        },
        {
            "text": "Topic fields and date filtering Important hyperparmeters of our system include which topic field (question, query, or narrative) to use in which stage, and whether to perform date filtering. We present a study of the effects of these parameters in Table 3 . First, we observe that the filtering of articles to only those published after January 1, 2020 always improves the ranking effectiveness (as compared to models that retrieved from all articles). Indeed, we find that only 19% of judged documents from prior to 2020 were considered relevant (with only 7% fully relevant). Meanwhile, 32% of judged documents after 2020 were considered relevant (19% fully relevant). We note that although this filter seems to be effective, it will ultimately limit the recall of the system. This observation underscores the value of including a user-configurable date filter in COVID-19 search engines.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 248,
                    "end": 255,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Analysis"
        },
        {
            "text": "We also observe in Table 3 that both first-stage ranking and re-ranking based on the question field may be more effective than using the query field for first-stage ranking and the question for re-ranking. Considering that the nDCG@10 already outperforms the performance of our official submission, and P@5 (fully relevant only) is not far behind with only 91% of the top documents judged, we can expect that this is likely a better setting going forward. It also simplifies the pipeline and reflects a more realistic search environment in which the user simply enters a natural language question. However, this approach underperforms at identifying partially relevant documents, given by its much lower P@5. In an environment in which recall is important (such as systematic review), the hybrid query-question approach may be preferable. Interestingly, we find that the narrative field usually reduces ranking effectiveness compared to the other settings. This may be due to a large distance between the naturallanguage questions seen during training and the longer-form text seen at evaluation time.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 19,
                    "end": 26,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Analysis"
        },
        {
            "text": "Non-expert judgements We found that our nonexpert relevance labels did not align well with the official labels; there was only a 60% agreement rate among the overlapping labels. In 18% of cases, our labels rated the document as more relevant than the official label; in 23% of cases ours was rated less relevant. A full confusion matrix is shown in Figure 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 349,
                    "end": 357,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Analysis"
        },
        {
            "text": "Despite the low agreement rates, the use of domain transfer, and only leveraging the non-expert labels for hyperparameter tuning suggest that it would be difficult to overfit to the test collection. We further investigate whether the subset of queries we evaluated gained a substantial advantage. To this end, we plot the difference in the evaluation metrics between our system and an untuned BM25 ranker in Figure 3 . As demonstrated by the figure, there was no strong preference of our model towards queries that were annotated (marked with * and in blue). In fact, 9 of the 15 highest-performing queries were not in the annotated set (in terms of \u2206 nDCG@10). This suggests that our approach did not overfit to signals provided by the non-expert assessments, and that our trained ranker is generally applicable.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 408,
                    "end": 416,
                    "text": "Figure 3",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Analysis"
        },
        {
            "text": "Failure cases Although our system generally outperforms BM25 ranking, it substantially underper-23 15 14 7 20* 30* 17* 27* 18* 10 1* 24* 9* 11* 12 28 4 29* 3 13* 6* 2* 19 21* 22 26 forms for Query 23 (coronavirus hypertension). When observing the failure cases, we found that the BM25 model successfully exploited term repetition to identify its top documents as relevant. Meanwhile, our system ranked documents with incidental mentions of hypertension highly. This suggests that more effective utilization of approaches that include a count-based component in the ranking score (such as TK (Hofst\u00e4tter et al., 2020) or CEDR-KNRM (MacAvaney et al., 2019)) could yield improvements.",
            "cite_spans": [
                {
                    "start": 591,
                    "end": 616,
                    "text": "(Hofst\u00e4tter et al., 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Analysis"
        },
        {
            "text": "In this work we present SLEDGE, a baseline for literature search related to COVID-19. SLEDGE is a two stage approach consisting of an initial BM25 ranker followed by SciBERT-based reranker, a domain specific pretrained language model. SLEDGE is trained on the general domain MS-MARCO passage ranking dataset and evaluated on TREC COIVD-search benchmark in a zero-shot transfer setting. SLEDGE tops the leaderboard among the initial round submissions from 55 teams to TREC-COVID Search shared task, demonstrating its effectiveness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Through our analysis we find that the parameters for initial retrieval are fairly robust. We also find that recent articles (i.e., those published in 2020) tend to exhibit higher relevance, suggesting the importance of filtering by date for high-precision retrieval. We also find that our non-expert annotation phase helped converge on good hyperparameters, while not likely contributing to substantial overfitting to the test set. Finally, through failure case analysis, we find that count-based approaches may be a good direction to explore in subsequent rounds of the shared task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "SciB-ERT: A pretrained language model for scientific text",
            "authors": [
                {
                    "first": "Iz",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "Arman",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EMNLP",
            "volume": "",
            "issn": "",
            "pages": "3615--3620",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/D19-1371"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "MS MARCO: A human generated machine reading comprehension dataset",
            "authors": [
                {
                    "first": "Tri",
                    "middle": [],
                    "last": "Daniel Fernando Campos",
                    "suffix": ""
                },
                {
                    "first": "Mir",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "Xia",
                    "middle": [],
                    "last": "Rosenberg",
                    "suffix": ""
                },
                {
                    "first": "Jianfeng",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Saurabh",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Rangan",
                    "middle": [],
                    "last": "Tiwary",
                    "suffix": ""
                },
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Majumder",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Research paper recommender systems on big scholarly data",
            "authors": [
                {
                    "first": "Teng",
                    "middle": [],
                    "last": "Tsung",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [
                        "R"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Contextaware sentence/passage term importance estimation for first stage retrieval",
            "authors": [
                {
                    "first": "Zhuyun",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [
                        "P"
                    ],
                    "last": "Callan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Deeper text understanding for ir with contextual neural language modeling",
            "authors": [
                {
                    "first": "Zhuyun",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "Jamie",
                    "middle": [],
                    "last": "Callan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "EARL: Speedup transformer-based rankers with precomputed representation",
            "authors": [
                {
                    "first": "Luyu",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Zhuyun",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [
                        "P"
                    ],
                    "last": "Callan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "TREC 2016 total recall track overview",
            "authors": [
                {
                    "first": "Maura",
                    "middle": [
                        "R"
                    ],
                    "last": "Grossman",
                    "suffix": ""
                },
                {
                    "first": "Gordon",
                    "middle": [
                        "V"
                    ],
                    "last": "Cormack",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Roegiest",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "TREC",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "TREC genomics special issue overview",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Hersh",
                    "suffix": ""
                },
                {
                    "first": "Ellen",
                    "middle": [],
                    "last": "Voorhees",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Interpretable & time-budgetconstrained contextualization for re-ranking",
            "authors": [],
            "year": null,
            "venue": "ECAI",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Holes in the outline: Subjectdependent abstract quality and its implications for scientific literature search",
            "authors": [
                {
                    "first": "Chien-Yu",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Arlene",
                    "middle": [],
                    "last": "Casey",
                    "suffix": ""
                },
                {
                    "first": "Dorota",
                    "middle": [],
                    "last": "G\u0142owacka",
                    "suffix": ""
                },
                {
                    "first": "Alan",
                    "middle": [],
                    "last": "Medlar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference on Human Information Interaction and Retrieval",
            "volume": "",
            "issn": "",
            "pages": "289--293",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "UMass at TREC 2004: Novelty and HARD",
            "authors": [
                {
                    "first": "Abdul",
                    "middle": [],
                    "last": "Nasreen",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Jaleel",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "Bruce"
                    ],
                    "last": "Allan",
                    "suffix": ""
                },
                {
                    "first": "Fernando",
                    "middle": [],
                    "last": "Croft",
                    "suffix": ""
                },
                {
                    "first": "Leah",
                    "middle": [
                        "S"
                    ],
                    "last": "Diaz",
                    "suffix": ""
                },
                {
                    "first": "Xiaoyan",
                    "middle": [],
                    "last": "Larkey",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [
                        "D"
                    ],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Courtney",
                    "middle": [],
                    "last": "Smucker",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wade",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "TREC",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "ColBERT: Efficient and effective passage search via contextualized late interaction over bert",
            "authors": [
                {
                    "first": "Omar",
                    "middle": [],
                    "last": "Khattab",
                    "suffix": ""
                },
                {
                    "first": "Matei",
                    "middle": [],
                    "last": "Zaharia",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "SIGIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Understanding xml retrieval evaluation",
            "authors": [
                {
                    "first": "Mounia",
                    "middle": [],
                    "last": "Lalmas",
                    "suffix": ""
                },
                {
                    "first": "Anastasios",
                    "middle": [],
                    "last": "Tombros",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "DELOS",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Indexing and retrieval of scientific literature",
            "authors": [
                {
                    "first": "Steve",
                    "middle": [],
                    "last": "Lawrence",
                    "suffix": ""
                },
                {
                    "first": "Kurt",
                    "middle": [
                        "D"
                    ],
                    "last": "Bollacker",
                    "suffix": ""
                },
                {
                    "first": "C. Lee",
                    "middle": [],
                    "last": "Giles",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "CIKM '99",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Is searching full text more effective than searching abstracts?",
            "authors": [
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "BMC Bioinformatics",
            "volume": "10",
            "issn": "",
            "pages": "46--46",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "OpenNIR: A complete neural ad-hoc ranking pipeline",
            "authors": [
                {
                    "first": "Sean",
                    "middle": [],
                    "last": "Macavaney",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the Thirteenth ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "845--848",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Ranking significant discrepancies in clinical reports",
            "authors": [
                {
                    "first": "Sean",
                    "middle": [],
                    "last": "Macavaney",
                    "suffix": ""
                },
                {
                    "first": "Arman",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                },
                {
                    "first": "Nazli",
                    "middle": [],
                    "last": "Goharian",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Filice",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ECIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Efficient document reranking for transformers by precomputing term representations",
            "authors": [
                {
                    "first": "Sean",
                    "middle": [],
                    "last": "Macavaney",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Franco",
                    "suffix": ""
                },
                {
                    "first": "Raffaele",
                    "middle": [],
                    "last": "Nardini",
                    "suffix": ""
                },
                {
                    "first": "Nicola",
                    "middle": [],
                    "last": "Perego",
                    "suffix": ""
                },
                {
                    "first": "Nazli",
                    "middle": [],
                    "last": "Tonellotto",
                    "suffix": ""
                },
                {
                    "first": "Ophir",
                    "middle": [],
                    "last": "Goharian",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Frieder",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "SIGIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Expansion via prediction of importance with contextualization",
            "authors": [
                {
                    "first": "Sean",
                    "middle": [],
                    "last": "Macavaney",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Franco",
                    "suffix": ""
                },
                {
                    "first": "Raffaele",
                    "middle": [],
                    "last": "Nardini",
                    "suffix": ""
                },
                {
                    "first": "Nicola",
                    "middle": [],
                    "last": "Perego",
                    "suffix": ""
                },
                {
                    "first": "Nazli",
                    "middle": [],
                    "last": "Tonellotto",
                    "suffix": ""
                },
                {
                    "first": "Ophir",
                    "middle": [],
                    "last": "Goharian",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Frieder",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "SIGIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "CEDR: Contextualized embeddings for document ranking",
            "authors": [
                {
                    "first": "Sean",
                    "middle": [],
                    "last": "Macavaney",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Yates",
                    "suffix": ""
                },
                {
                    "first": "Arman",
                    "middle": [],
                    "last": "Cohan",
                    "suffix": ""
                },
                {
                    "first": "Nazli",
                    "middle": [],
                    "last": "Goharian",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Pulp: A system for exploratory search of scientific literature",
            "authors": [
                {
                    "first": "Alan",
                    "middle": [],
                    "last": "Medlar",
                    "suffix": ""
                },
                {
                    "first": "Kalle",
                    "middle": [],
                    "last": "Ilves",
                    "suffix": ""
                },
                {
                    "first": "Ping",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Wray",
                    "middle": [],
                    "last": "Buntine",
                    "suffix": ""
                },
                {
                    "first": "Dorota",
                    "middle": [],
                    "last": "Glowacka",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "1133--1136",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "A markov random field model for term dependencies",
            "authors": [
                {
                    "first": "Donald",
                    "middle": [],
                    "last": "Metzler",
                    "suffix": ""
                },
                {
                    "first": "W. Bruce",
                    "middle": [],
                    "last": "Croft",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "SIGIR '05",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "From doc2query to docTTTTTquery",
            "authors": [
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Passage re-ranking with bert. ArXiv",
            "authors": [
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                },
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Evaluating pretrained transformer models for citation recommendation",
            "authors": [
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                },
                {
                    "first": "Zhiying",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "BIR@ECIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Document ranking with a pretrained sequence-to-sequence model",
            "authors": [
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                },
                {
                    "first": "Zhiying",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.06713"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Document expansion by query prediction",
            "authors": [
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "TREC-COVID: Rationale and Structure of an Information Retrieval Shared Task for COVID-19",
            "authors": [
                {
                    "first": "Kirk",
                    "middle": [],
                    "last": "Roberts",
                    "suffix": ""
                },
                {
                    "first": "Tasmeer",
                    "middle": [],
                    "last": "Alam",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Bedrick",
                    "suffix": ""
                },
                {
                    "first": "Dina",
                    "middle": [],
                    "last": "Demner-Fushman",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Soboroff",
                    "suffix": ""
                },
                {
                    "first": "Ellen",
                    "middle": [],
                    "last": "Voorhees",
                    "suffix": ""
                },
                {
                    "first": "Lucy",
                    "middle": [
                        "Lu"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "William R",
                    "middle": [],
                    "last": "Hersh",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of the American Medical Informatics Association. Ocaa091",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Learning to reformulate long queries for clinical decision support",
            "authors": [
                {
                    "first": "Luca",
                    "middle": [],
                    "last": "Soldaini",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Yates",
                    "suffix": ""
                },
                {
                    "first": "Nazli",
                    "middle": [],
                    "last": "Goharian",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J. Assoc. Inf. Sci. Technol",
            "volume": "68",
            "issn": "",
            "pages": "2602--2619",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Exploring scientific literature search through topic models",
            "authors": [
                {
                    "first": "Amin",
                    "middle": [],
                    "last": "Sorkhei",
                    "suffix": ""
                },
                {
                    "first": "Kalle",
                    "middle": [],
                    "last": "Ilves",
                    "suffix": ""
                },
                {
                    "first": "Dorota",
                    "middle": [],
                    "last": "Glowacka",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 ACM Workshop on Exploratory Search and Interactive Data Analytics",
            "volume": "",
            "issn": "",
            "pages": "65--68",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Rapidly bootstrapping a question answering dataset for COVID-19",
            "authors": [
                {
                    "first": "Raphael",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                },
                {
                    "first": "Edwin",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Nikhil",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Phuong",
                    "middle": [],
                    "last": "Cam",
                    "suffix": ""
                },
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Huggingface's transformers: State-of-the-art natural language processing",
            "authors": [
                {
                    "first": "Thomas",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                },
                {
                    "first": "Lysandre",
                    "middle": [],
                    "last": "Debut",
                    "suffix": ""
                },
                {
                    "first": "Victor",
                    "middle": [],
                    "last": "Sanh",
                    "suffix": ""
                },
                {
                    "first": "Julien",
                    "middle": [],
                    "last": "Chaumond",
                    "suffix": ""
                },
                {
                    "first": "Clement",
                    "middle": [],
                    "last": "Delangue",
                    "suffix": ""
                },
                {
                    "first": "Anthony",
                    "middle": [],
                    "last": "Moi",
                    "suffix": ""
                },
                {
                    "first": "Pierric",
                    "middle": [],
                    "last": "Cistac",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Rault",
                    "suffix": ""
                },
                {
                    "first": "R&apos;emi",
                    "middle": [],
                    "last": "Louf",
                    "suffix": ""
                },
                {
                    "first": "Morgan",
                    "middle": [],
                    "last": "Funtowicz",
                    "suffix": ""
                },
                {
                    "first": "Jamie",
                    "middle": [],
                    "last": "Brew",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Anserini: Enabling the use of lucene for information retrieval research",
            "authors": [
                {
                    "first": "Peilin",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Hui",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "SIGIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "ADRTrace: Detecting expected and unexpected adverse drug reactions from user reviews on social media sites",
            "authors": [
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Yates",
                    "suffix": ""
                },
                {
                    "first": "Nazli",
                    "middle": [],
                    "last": "Goharian",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "ECIR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Cross-domain modeling of sentence-level evidence for document retrieval",
            "authors": [
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Zeynep Akkalyoncu Yilmaz",
                    "suffix": ""
                },
                {
                    "first": "Haotian",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "EMNLP/IJCNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Rapidly deploying a neural search engine for the COVID-19 open research dataset: Preliminary thoughts and lessons learned",
            "authors": [
                {
                    "first": "Edwin",
                    "middle": [
                        "M"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Nikhil",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Rodrigo",
                    "middle": [],
                    "last": "Nogueira",
                    "suffix": ""
                },
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Confusion matrix between our non-expert annotations and the official expert TREC labels.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Difference in ranking effectiveness between our system and an untuned BM25 model by query for nDCG@10, P@5, and P@5 (fully relevant only). Queries in blue and marked with * were annotated by nonexperts and used for hyperparameter tuning.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "This work was partially supported by the ARCS Foundation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        }
    ]
}