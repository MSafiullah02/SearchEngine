{
    "paper_id": "7def05579bc74ffeeddbf7ed6d4ba64bcd411b83",
    "metadata": {
        "title": "DELAFO: An Efficient Portfolio Optimization Using Deep Neural Networks",
        "authors": [
            {
                "first": "Hieu",
                "middle": [
                    "K"
                ],
                "last": "Cao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AISIA Research Lab",
                    "institution": "",
                    "location": {
                        "settlement": "Ho Chi Minh City",
                        "country": "Vietnam"
                    }
                },
                "email": ""
            },
            {
                "first": "Han",
                "middle": [
                    "K"
                ],
                "last": "Cao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Inspectorio Research Lab",
                    "institution": "",
                    "location": {
                        "settlement": "Ho Chi Minh City",
                        "country": "Vietnam"
                    }
                },
                "email": ""
            },
            {
                "first": "Binh",
                "middle": [
                    "T"
                ],
                "last": "Nguyen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AISIA Research Lab",
                    "institution": "",
                    "location": {
                        "settlement": "Ho Chi Minh City",
                        "country": "Vietnam"
                    }
                },
                "email": ""
            },
            {
                "first": "Ho",
                "middle": [],
                "last": "Chi",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Minh",
                "middle": [],
                "last": "City",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Vietnam",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Portfolio optimization has been broadly investigated during the last decades and had a lot of applications in finance and economics. In this paper, we study the portfolio optimization problem in the Vietnamese stock market by using deep-learning methodologies and one dataset collected from the Ho Chi Minh City Stock Exchange (VN-HOSE) from the beginning of the year 2013 to the middle of the year 2019. We aim to construct an efficient algorithm that can find the portfolio having the highest Sharpe ratio in the next coming weeks. To overcome this challenge, we propose a novel loss function and transform the original problem into a supervised problem. The input data can be determined as a 3D tensor, while the predicted output is the unnormalized weighted proportion for each ticker in the portfolio to maximize the daily return Y of the stock market after a given number of days. We compare different deep learning models, including Residual Networks (ResNet), Long short-term memory (LSTM), Gated Recurrent Unit (GRU), Self-Attention (SA), Additive Attention (AA), and various combinations: SA + LSTM, SA + GRU, AA + LSTM, and AA + GRU. The experimental results show that the AA + GRU outperforms the rest of the methods on the Sharpe ratio and provides promising results for the portfolio optimization problem not only in Vietnam but also in other countries.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Using historical stock data for portfolio optimization has been one of the most exciting and challenging topics for investors in the financial market during the Electronic supplementary material The online version of this chapter (https:// doi.org/10.1007/978-3-030-47426-3 48) contains supplementary material, which is available to authorized users. last decades [1, 2] . Many factors have different influences on the stock price, and it is essential to extract a list of crucial factors from both historical stock prices and other data sources. As there is no such thing as a free lunch, investors have to find an efficient strategy for a trade-off between getting more profits and reducing the investment risk. Sometimes, they need to invest multiple assets for diversifying the portfolio.",
            "cite_spans": [
                {
                    "start": 364,
                    "end": 367,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 368,
                    "end": 370,
                    "text": "2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Traditionally, one can use statistical methods for predicting a financial time series problem. There are popular techniques, including autoregressive moving average (ARMA) [3] , autoregressive conditional heteroscedastic (ARCH) [4] , and autoregressive integrated moving average (ARIMA) [5] . Importantly, these statistical methods usually consider the stock time series as a linear process and then model the generation process for a latent time series to foresee future stock prices. Practically, a stock time series is generally a nonlinear dynamic process. There are many different approaches, including artificial neural networks (ANN), support vector machines (SVM), and other ensemble methods [6] to capture nonlinear characters from a given dataset without knowing any prior information. Especially, deep neural networks such as e.g. convolutional neural networks (CNN) and recurrent neural networks (RNN) have been proven to work well in many applications and multi-variable time series data.",
            "cite_spans": [
                {
                    "start": 172,
                    "end": 175,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 228,
                    "end": 231,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 287,
                    "end": 290,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 700,
                    "end": 703,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The future price represents the future growth of each company in the stock market. Typically, the stock price of each company listed in a stock market can vary whenever one puts a sell or buy order, and the corresponding transaction completes. Many factors have influenced the stock price of one company, for example, such as the company's net profit, demand stability, competitive strength in the market, new technology used, and production volume. Also, the macro-economic condition can play a unique role in the stock market as well as the currency exchange rate and the change of the government's policies. After boasting increased macro-economic stability and improving the probusiness financial environment, Vietnam has become one of the world's most attractive markets for international investors. With the population of nearly 100 million people and most of whom are young people (under the age of 35), Vietnam can provide a young, motivated, highly skilled, and educated workforce to multiple international startups and enterprises with a competitive cost. At the moment, Vietnam's stock exchange is considered as one of the most promising and prospective market in the Southeast Asia. Especially, the Ho Chi Minh Stock Exchange (HOSE) 1 is becoming one of the largest securities firms in terms of both capital and size. Since launching in 2002, it has been performing strongly and more and more investors continue exhibiting a special interest in both Vietnam stock market. HOSE is currently predicted to be upgraded to an emerging market in 2021.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Up to now, there have existed a large number of useful applications using machine learning techniques in different aspects of daily life. Duy and co-workers combine deep neural networks and Gaussian mixture models for extracting brain tissues from high-resolution magnetic resonance images [7] . Deep neural networks can also be applied to automatic music generation [8] , food recognition [9] , and portfolio optimization problem [10] [11] [12] . In this paper, we aim at investigating a portfolio optimization problem in which by using historical stock data of different tickers, one wants to find the equally weighted portfolio having the highest Sharpe ratio [13] in the future. This is one of the winning solutions in a wellknown data science competition using the HOSE stock data in 2019. In this competition, one can use one training dataset, including all volume and prices of different tickers appearing in the Vietnam stock market from the beginning of the year 2013 to the middle of the year 2019 (July 2019), for learning an appropriate model of the portfolio optimization problem. It is worth noting that the Sharpe ratio is often used as a measure of the health of a portfolio. One usually expects that the higher the Sharpe ratio of one portfolio is in the past, the larger its Sharpe ratio is in the future. In this work, we assume that there are no new tickers joining the stock market during the testing period.",
            "cite_spans": [
                {
                    "start": 290,
                    "end": 293,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 367,
                    "end": 370,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 390,
                    "end": 393,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 431,
                    "end": 435,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 436,
                    "end": 440,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 441,
                    "end": 445,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 663,
                    "end": 667,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We study the portfolio optimization problem by assuming that there are N tickers in the stock market, only using the historical stock data during the last M days for training or updating the proposed model, and then doing prediction for the equally weighted portfolio having the highest Sharpe ratio during the next K days. Different from other approaches using statistical methods or time series algorithms, we transform the input data into a 3D tensor and then consider each input data as an image. As a result, we have a chance to apply different state-ofthe-art methods such as e.g. Residual Networks (ResNet) [14] , Long-short term memory(LSTM) [15] , Gate Recurrent Unit [16] , Self-Attention (SA) [17] , and Additive Attention (AA) [18] for extracting important features as well as learning an appropriate model. Also, we compare them with different combinations of these techniques (SA + LSTM, SA + GRU, AA + LSTM, and AA + GRU) and measure the actual performance in the testing dataset. The experimental results show that the AA + LSTM outperforms other techniques in terms of achieving a much better value of the Sharpe ratio and a comparably smaller value of the corresponding standard deviation.",
            "cite_spans": [
                {
                    "start": 614,
                    "end": 618,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 650,
                    "end": 654,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 677,
                    "end": 681,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 704,
                    "end": 708,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 739,
                    "end": 743,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we present our methods to solve the portfolio optimization using the VN-HOSE dataset and deep neural networks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DELAFO: A New DeEp Learning Approach for portFolio Optimization"
        },
        {
            "text": "We consider a dataset collected from the Vietnamese stock market from the beginning date D 0 and the ending date D 1 and N is the number of tickers appearing during that period of time. We denote T = {T 1 , T 2 , .., T N } as the list of all tickers in the market during the time window. For a given ticker T i , v i,j and p i,j are the corresponding volume and price on the day d j , consecutively. Moreover, we assume that all investors aim to determine the list of potential tickers in their portfolio for the next K days without putting any weight for different tickers (or equally weighted, such as e.g. 1/N ). It is important to note that all the investors usually do not want their portfolios to have a few tickers or \"put all the eggs in one bucket\" or lack of diversity. Having too many tickers may cost a lot of management time and fee as well. As a consequence, the outcome of the problem can be regarded as an N-binary vector (N is the number of tickers), where a one-valued entry means the corresponding ticker is chosen; otherwise, it is not selected. There are two main constraints in this problem: a) Having the same proportion for each tickers in portfolios; (b) The maximum numbers of tickers selected is 50. The daily return, R i,j , of the ticker T i at the day d j can be defined as [13] . The daily return of portfolio can be computed by R =",
            "cite_spans": [
                {
                    "start": 1304,
                    "end": 1308,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "In the equally weighted portfolio optimization problem, one can assume that w i = 1 N , and therefore, the \"Sharpe ratio\" can be determined as [13] :",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 147,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "where n is an annualization factor of period (e.g, n= 252 for trading date in one year) and R f is the risk-free rate, the expected return of any portfolio with no risk. In this work, we choose",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": ", the Sharpe ratio is calculated as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "where\u0175 is an estimated preference vector for the list of all tickers. Typically, the equally weighted portfolio optimization problem can be formulated as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "Here, N 0 is the maximum number of tickers selected in the optimal portfolio (N 0 = 50 in our initial assumption). The main goal in our work is to estimate the optimal solution w opt for the portfolio optimization problem (3) in order to obtain the maximum Sharpe ratio during the next K days.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "To study a deep learning model for the portfolio optimization problem, we aim at only using the historical stock data during the last M days for training and then predict the optimal equally weighted portfolio having the highest Sharpe ratio during the next K days. To solve the optimization problem in (3), we represent each input data as a 3D tensor of size N \u00d7 M \u00d7 2 that includes all stock data (both the volume and the price) of N different tickers during the = 1, . . . , N) is the estimated preference rate of the i-th ticker in the list of all N tickers in the stock market. Finally, the optimal portfolio can be determined by the corresponding estimated solution w opt , where the i-th ticker can be chosen if the corresponding preference\u0175 i \u2265 \u03b8 (or w opt i = 1 ); otherwise, it is not selected.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 465,
                    "end": 480,
                    "text": "= 1, . . . , N)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Problem Formulation"
        },
        {
            "text": "Traditionally, one can estimate the maximum value of the Sharpe ratio by solving the following optimization problem [19] :",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "A New Loss Function for the Sharpe-Ratio Maximization"
        },
        {
            "text": "Although it does not directly optimize the Sharpe ratio as shown in Eq.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A New Loss Function for the Sharpe-Ratio Maximization"
        },
        {
            "text": "(2), one can use the stochastic gradient descent method for approximating the optimal w [20] . In this paper, we propose the following new loss function for the equally weighted portfolio optimization problem:",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 92,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "A New Loss Function for the Sharpe-Ratio Maximization"
        },
        {
            "text": "where \u03bb > 0 and C > 1 are two hyper parameters. One can find more details of how we can derive the loss function L(\u0175) in the section Supplementary Material. After that, by implementing an appropriate deep neural network to estimate the optimal solution\u0175 = (\u0175 1 ,\u0175 2 , . . . ,\u0175 N ) in Eq. (5), we can derive the final optimal vector w opt in Eq. (3) by the following rules:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A New Loss Function for the Sharpe-Ratio Maximization"
        },
        {
            "text": "In our experiments, we choose \u03b8 = 0.5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A New Loss Function for the Sharpe-Ratio Maximization"
        },
        {
            "text": "To estimate the output vector\u0175, we consider different deep learning approaches for solving the portfolio optimization problem based on the proposed loss function in Eq. (5). We select both Long-short term memory(LSTM) [15] and Gate ResNet. ResNet architecture has been proven to become one of the most efficient deep learning models in computer vision, whose the first version was proposed by He et al. [22] . After that, these authors later released the second update for ResNet [14] . By using residual blocks inside its architecture, ResNet can help us to overcome the gradient vanishing problem and then well learn deep features without using too many parameters. In this work, we apply ResNet for estimating the optimal value for the vector\u0175 in the loss function (5) . To the best of our knowledge, this is the first time ResNet is used for the Sharpe ratio maximization and our proposed ResNet architecture can be described in Fig. 4 .",
            "cite_spans": [
                {
                    "start": 218,
                    "end": 222,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 403,
                    "end": 407,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 480,
                    "end": 484,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 768,
                    "end": 771,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 933,
                    "end": 939,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Our Proposed Models for the Portfolio Optimization"
        },
        {
            "text": "SA/AA + LSTM/GRU. The attention mechanism is currently one of the state-of-the-art algorithms, which are ubiquitously used in many NLP problems. There are many types of attention models, including Bahdanau attention [18] , Luong Attention [21] , and Self Attention [17] . Although the attention mechanism has been applying for the stock price prediction [23] , there is few attention scheme used for maximizing the Sharpe ratio in the portfolio optimization problem. In this paper, we exploit two mechanisms, which are Self-Attention and Bahdanau attention (Additive Attention). The corresponding architecture of our four proposed models (SA + LSTM, SA + GRU, AA + LSTM, AA + GRU) can be visualized in Figs. 2 and 3.",
            "cite_spans": [
                {
                    "start": 216,
                    "end": 220,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 265,
                    "end": 269,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 354,
                    "end": 358,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Our Proposed Models for the Portfolio Optimization"
        },
        {
            "text": "In this section, we present our experiments and the corresponding implementation of each proposed model. All tests are performed on a computer with Intel(R) Core(TM) i9-7900X CPU, running at 3.6 GHz with 128 GB of RAM, and two GPUs RTX-2080Ti (2 \u00d7 12 GB of RAM). We collect all stock data from the VN-HOSE stock exchange over six years (from January 1, 2013, to July 31, 2019) for measuring the performance of different models. There are 438 tickers appearing in the Vietnam stock market during this period. However, 57 tickers disappeared in the stock market at the end of 31/07/2019. For this reason, we only consider 381 remaining tickers for training and testing models. In Fig. 1 (a) and 1(b), we visualize the mean values of both volume and price of the top 20 highest volume tickers in HOSE as well as the corresponding average value and the standard deviation of the daily return.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 678,
                    "end": 684,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "In our experiments, all proposed models use the Adam optimizer [24] with the optimal learning rate \u03b1 = 0.0762, \u03b2 1 = 0.9, and \u03b2 2 = 0.999. The learning rate and L2 regularization are tuned by using 141 random samples from the training set. We use the library Hyperas 2 for automatically tuning all hyper-parameters of the proposed models.",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 67,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Model Configuration"
        },
        {
            "text": "For two base line models (LSTM and GRU), we use 32 hidden units in which the L 2 -regularization term is 0.0473. As shown in Fig. 4 , our proposed ResNet model has the input data of the size (381, 64, 2) passing to the first convolution layer where the kernel size is (1 \u00d7 5) and the L 2 -regularization is 0.0932. After that, the data continue going through four different residual blocks, whose corresponding kernel sizes are (1 \u00d7 7),(1 \u00d7 5), (1 \u00d7 7), and (1 \u00d7 3) , respectively, and all kernels have the L 2 -regularization as 10 \u22124 . Using these kernels, we aim at capturing the time dependency from the input data. The last convolution layer in our ResNet model has the kernel size (381, 1) and the L 2 -regularization as 0.0372 for estimating the correlation among all tickers. Its output data continue going through an average pooling layer before passing the final fully connected layer with the Sigmoid activation function to compute the vector\u0175. The last Dense layer has L2 regularization 0.099 and the learning rate of our ResNet model is 0.0256.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 125,
                    "end": 131,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 445,
                    "end": 465,
                    "text": "(1 \u00d7 7), and (1 \u00d7 3)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Model Configuration"
        },
        {
            "text": "For four proposed models (SA/AA + LSTM/GRU), both Self-Attention and Additive Attention have 32 hidden units and the L 2 -regularization term is 0.01. Both GRU and LSTM cells use 32 hidden units, the Sigmoid activation function, and the L 2 -regularization as 0.0473. Two last fully connected layers have 32 hidden unites and the corresponding L 2 -regularization is 0.0727. In our experiments, we choose \u03b8 = 0.5, \u03bb = 0.003, and C = 1.6, where \u03b8, \u03bb, and C are hyper-parameters of our proposed loss function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Configuration"
        },
        {
            "text": "As there are only 381 tickers (N = 381) in the market at the end of the month July, 2019, we use the time windows of M consecutive days for extracting the input data of proposed models. On each day, we collect the information of both \"price\" and \"volume\" of these 381 tickers and y, the daily return on the market in the next K days (K = 19). Consequently, the input data has the shape (381, 64, 2) and we move the time window during the studying period of time (from January 1, 2013, to July 31, 2019) to obtain 1415 samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preparation"
        },
        {
            "text": "To deal with new tickers appeared, we fill all missing values by 0. For these missing data, our model may not learn anything from these data. Meanwhile, for the daily return in the next K days, we fill all missing values by \u2212100. That is, as those tickers have been not disappeared yet, we set its daily return as a negative number so as to ensure chosen portfolios containing these tickers can get a negative Sharpe ratio. During training proposed models, we believe that the optimizer can learn well and avoid selecting these tickers from portfolios as much as possible.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preparation"
        },
        {
            "text": "We evaluate each model by using 10-fold cross-validation or forward chaining validation in time series data. As shown in Fig. 5 , while measuring the performance of each proposed model, we create the training data by moving the selected time window (64 days) during the investigating period (from January 2013 to July 2019) and consider the corresponding sequence of daily returns in the next coming 19 days. It is crucial to make sure all training samples are independent of the testing samples.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 121,
                    "end": 127,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "The experiment results show that the Additive Attention + GRU model outperforms with the others. One of the possible reasons is Additive Attention + GRU can retain the information from the input sequence, which may loose from RNN cells when dealing with a very long sequence. Models using Self Attention can also get good results; however, as the outputs of the Self-Attention module still go to the RNN cell, without keeping any information from the sequence input. For this reason, the mean value of the Sharpe ratio of SA + GRU (0.9047) is a bit lower than AA + GRU (1.1056). Interestingly, the mean value of the Sharpe ratio of SA + LSTM (1.0206) is better than AA + LSTM (0.9235). Although not getting a high Sharpe ration in comparison with SA/AA + LSTM/GRU, the ResNet model has a quite short training time. In our experiments, the total time for its running 10-Fold Cross Validation is only 40 min, while taking over two hours for all SA/AA + LSTM/GRU models. In Fig. 6 , our best-proposed model, AA + GRU, has a better performance than both VN30 3 and VNINDEX 4 in terms of the Sharpe ratio values. These experimental results show that our proposed techniques can achieve promising results and possibly apply not only in the Vietnamese stock market but also in other countries (Table 1) ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 971,
                    "end": 977,
                    "text": "Fig. 6",
                    "ref_id": null
                },
                {
                    "start": 1286,
                    "end": 1295,
                    "text": "(Table 1)",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "We have proposed a novel approach for a portfolio optimization problem with N tickers by using the historical stock data during the last M days to compute an optimal portfolio that maximizes the Sharpe ratio of the daily returns during the next K days. We have also presented a new loss function for the Sharpe ratio maximization problem and transform the input data into a N \u00d7 M \u00d7 2 tensor, and apply seven different deep learning methods (LSTM, GRU, SA + GRU, SA + LSTM, AA + LSTM, AA + GRU, and ResNet) for investigating the problem. To learning a suitable deep learning model for the problem, we collect the stock data in VN-HOSE during the period from January 2013 to July 2019. The experimental results show that the AA + GRU model outperforms with the other techniques and also achieves a better performance in terms of the Sharpe ratio for two popular indexes VN30 and VNINDEX.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Further Work"
        },
        {
            "text": "In future works, we will extend our approaches to similar problems in other countries and continue improving our algorithms. Our project, including datasets and implementation details, will be publicly available 5 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Further Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Portfolio selection using neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "G\u00f3mez",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Comput. Oper. Res",
            "volume": "34",
            "issn": "4",
            "pages": "1177--1191",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Deep learning networks for stock market analysis and prediction: methodology, data representations, and case studies",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Chong",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "C"
                    ],
                    "last": "Park",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Expert Syst. Appl",
            "volume": "83",
            "issn": "",
            "pages": "187--205",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Time Series Analysis, Forecasting and Control",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "E P"
                    ],
                    "last": "Box",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Jenkins",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "F"
                    ],
                    "last": "Engle",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Econometrica",
            "volume": "50",
            "issn": "4",
            "pages": "987--1007",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Time Series Techniques for Economists",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "C"
                    ],
                    "last": "Mills",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "An active learning framework for set inversion",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "T"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "S T"
                    ],
                    "last": "Ho",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dinh",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Knowl. Based Syst",
            "volume": "185",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "3D-brain segmentation using deep neural network and gaussian mixture model",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M H"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "T"
                    ],
                    "last": "Vu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "Q"
                    ],
                    "last": "Ung",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "T"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)",
            "volume": "",
            "issn": "",
            "pages": "815--824",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Automatically generate hymns using variational attention models",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "K"
                    ],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "T"
                    ],
                    "last": "Ly",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "T"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Part II",
            "volume": "11555",
            "issn": "",
            "pages": "317--327",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-22808-8_32"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "A deep learning based food recognition system for lifelog images",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "T"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "D.-T",
                    "middle": [],
                    "last": "Dang-Nguyen",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "X"
                    ],
                    "last": "Dang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Phat",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gurrin",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods",
            "volume": "1",
            "issn": "",
            "pages": "657--664",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A one-layer recurrent neural network for real-time portfolio optimization with probability criterion",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Trans. Cybern",
            "volume": "43",
            "issn": "1",
            "pages": "14--23",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Deep learning for event-driven stock prediction",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Stock prices prediction using deep learning models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chao",
                    "suffix": ""
                },
                {
                    "first": "Y.-C",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "C.-M",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1909.12227"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "The sharpe ratio",
            "authors": [
                {
                    "first": "W",
                    "middle": [
                        "F"
                    ],
                    "last": "Sharpe",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "J. Portfolio Manag",
            "volume": "21",
            "issn": "1",
            "pages": "49--58",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Identity mappings in deep residual networks",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECCV 2016, Part IV",
            "volume": "9908",
            "issn": "",
            "pages": "630--645",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-46493-0_38"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural Comput",
            "volume": "9",
            "issn": "",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1406.1078"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Neural machine translation by jointly learning to align and translate",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Bahdanau",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Portfolio selection",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Markowitz",
                    "suffix": ""
                }
            ],
            "year": 1952,
            "venue": "J. Finan",
            "volume": "7",
            "issn": "1",
            "pages": "77--91",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Maximizing the sharpe ratio",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kopman",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "MSCI Barra Research Paper",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Effective approaches to attention-based neural machine translation",
            "authors": [
                {
                    "first": "M.-T",
                    "middle": [],
                    "last": "Luong",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1508.04025"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Stock price prediction using attention-based multi-input LSTM",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Asian Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "454--469",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Adam: a method for stochastic optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.6980"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Top 20 tickers in VN-HOSE. last M consecutive days and the corresponding output of the deep network is a vector\u0175 = (\u0175 1 ,\u0175 2 , . . . ,\u0175 N ) of size N \u00d7 1, where\u0175 i \u2208 [0, 1] (i",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Our proposed Self Attention + LSTM/GRU. Recurrent Unit[16] architectures as two baseline models. Especially, by converting the input data into an N \u00d7 M \u00d7 2 tensor as an \"image\", we construct a new ResNet architecture for the problem and create four other combinations of deep neural networks. They are SA + LSTM (Self-Attention model and LSTM), SA + GRU (Self-Attention model and GRU), AA + LSTM (Additional Attention and LSTM), and AA + GRU (Additional Attention and GRU). The architecture of RNN, GRU, and LSTM cells can be found more details at[15,16,21].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Our proposed Additive Attention + LSTM/GRU.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "(a) Our proposed Resnet model for the portfolio optimization problem. (b) The first residual block. (c) The second and the third residual block (d) The final residual block. Here, \"BN\" denotes \"Batch Normalization\", N is the number of tickers, and M is the number of days to extract the input data. In our experiments, N = 381 and M = 64.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The 10-Fold cross validation in our experiments. The blue blocks contain the training data and the red blocks contain the testing data. In experiments, we just use from Fold 5 to Fold 10 for evaluating the Sharpe ratio due to the lack of data for training models. At each fold, we train our deep models using 200 epochs. (Colorfigure online)",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": ". VN30 is the bucket of 30 companies having highest market capitalization and highest volume in six months for all the companies listed on the Ho Chi Minh City Stock Exchange. They also have the free float larger than 5%: https://iboard.ssi.com.vn/ bang-gia/vn30.4 VN-Index is a capitalization-weighted index of all the companies listed on the Ho Chi Minh City Stock Exchange: https://www.bloomberg.com/quote/VNINDEX:IND.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The performance of different models by the Sharpe ratioFig. 6. The performance of our AA + GRU model with the VN30 and VNINDEX in terms of the Sharpe ratio in the testing dataset.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We would like to thank The National Foundation for Science and Technology Development (NAFOSTED), University of Science, Inspectorio Research Lab, and AISIA Research Lab for supporting us throughout this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement."
        }
    ]
}