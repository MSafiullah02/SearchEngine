{
    "paper_id": "PMC7206229",
    "metadata": {
        "title": "Canonicalizing Knowledge Bases for Recruitment Domain",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Nausheen",
                "middle": [],
                "last": "Fatma",
                "suffix": "",
                "email": "nausheen.fatma@naukri.com",
                "affiliation": {}
            },
            {
                "first": "Vijay",
                "middle": [],
                "last": "Choudhary",
                "suffix": "",
                "email": "vijay.choudhary@naukri.com",
                "affiliation": {}
            },
            {
                "first": "Niharika",
                "middle": [],
                "last": "Sachdeva",
                "suffix": "",
                "email": "niharika.sachdeva@naukri.com",
                "affiliation": {}
            },
            {
                "first": "Nitendra",
                "middle": [],
                "last": "Rajput",
                "suffix": "",
                "email": "nitendra@acm.org",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "User-generated content from candidate CVs, recruiter job posts, and company descriptions is one of the largest source of data on online recruitment platforms (such as LinkedIn, Indeed, Glassdoor, etc.). This content includes diverse entities from recruitment domain such as company names, institute names, skills, and designations, which are extracted and fed to Knowledge Bases (KBs) maintained by these companies. As this content is user generated, multiple variations of these entities find their way into the KBs. Often a company name or an institute name could have hundreds of variations in the KBs. These name variations lack standard nomenclature or representation, thereby resulting in identical and noisy records for the same entity. For example, KB may have multiple representations named warner bros., warner brothers pictures and warner bros. entertainment inc. for the same real world entity Warner Brothers Studios. Such non-standard representations of named entities when used in applications such as search and recommendations result in sub-optimal information retrieval. Therefore, KB canonicalization i.e, mapping multiple references of same entities into unique clusters is imperative for many applications in the recruitment domain. In this work, we focus on recognizing varied representations of an entity in the KB and group them to form a cluster referring to a unique entity [8, 16, 17]. To canonicalize these varied representations, we address various challenges resulting from previously unseen or out-of-knowledge-base (OOKB) entities, location variations (e.g., Google Singapore vs. Google India or Cal State University Los Angeles vs. Calif. State Univ LA), semantic variations (steam boiler operator vs. steam turbine operator), unconventional abbreviation (Integrated Electronics vs. Intel), misspellings, unrelated or irrelevant entries (Not-applicable or freelancer).",
            "cite_spans": [
                {
                    "start": 1401,
                    "end": 1402,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1404,
                    "end": 1406,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1408,
                    "end": 1410,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Various research works have focused on entity canonicalization [8, 30]. However, these approaches cannot be directly applied to domain-specific KBs due to: (a) low coverage of domain-specific entities in open KB. For example, only 10% of entities in our data were present in Freebase. So research that relies on using Open KBs to help in disambiguation [7, 13] cannot be used for the recruitment domain; (b) Handcrafted features (e.g. lexical, linguistic, and semantic) are limiting as they do not scale to capture the volume of variations. Such approaches have been known to be cost in-efficient and sub-optimal [10, 27]. These limitations warrant for new solutions to enable KB canonicalization in the recruitment domain. We focus on canonicalization with the constraint that there is no external source of information or handcrafted features from open KBs available to us. We build upon [6] where we demonstrated the effectiveness of a deep Siamese BiLSTM network towards learning useful representations from character sequences. In this paper, we use attention mechanism and word embeddings to define a similarity function, thus, capturing similarity both at syntactic and semantic levels in a more focused manner. While in [6], we had used a proprietary KB consisting of 50 million CVs and canonicalized the organization names only, in this paper we also validate the efficacy of our approach for additional entity types as well as open source datasets including ESCO1 and DBpedia [1], thus demonstrating the generalizability of this solution.",
            "cite_spans": [
                {
                    "start": 64,
                    "end": 65,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 67,
                    "end": 69,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 354,
                    "end": 355,
                    "mention": "7",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 357,
                    "end": 359,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 614,
                    "end": 616,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 618,
                    "end": 620,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 891,
                    "end": 892,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1229,
                    "end": 1230,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1487,
                    "end": 1488,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Our approach for finding pairwise similarity achieves state-of-the art performance on all entity types surpassing the baselines widely relied upon in the literature. These results demonstrate the effectiveness and robustness of our approach on real-world data. The main contribution of our work are:We suggest a deep learning (BiLSTM with attention mechanism) module for pair-wise similarity estimation followed by Hierarchical Agglomerative Clustering (HAC). Our proposed architecture scales well for well-known as well as unseen and rare entities that have limited or no deducting information available for identifying canonical forms in the open KBs.We demonstrate the usefulness of capturing sequence patterns at character and word level for calculating pairwise similarity score and compare it to state-of-the-art similarity measures for KB canonicalization. We provide insights on various network architecture choices for finding pairwise similarity.We release our created entity dataset for evaluating the pairwise similarity on both the public datasets (ESCO and DBpedia).\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Our work is related to research on KB Canonicalization, entity linking and clustering. In this section, we discuss the relevance these research works to our paper.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Various research works focus on canonicalizing entities in the open KBs including NELL, TextRunner, and ReVerb45K [8, 27]. These works demonstrate the efficacy of employing string similarity and clustering approaches to identify canonical forms of entities or relations in the Open KB [6, 8, 30]. Such approaches, however, recommend handcrafted features which result in sub-optimal solution as they capture limited similarity (semantic or syntactic) [10, 27]. Research on KB generation from multiple sources such as Freebase and Wikipedia also discusses entity canonicalization solutions [27]. These works utilize side information or external hierarchies for canonicalization. Such information is often missing for entities in a domain-specific setting like ours causing limited scalability for emerging or new OOKB entities [12, 21].",
            "cite_spans": [
                {
                    "start": 115,
                    "end": 116,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 118,
                    "end": 120,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 286,
                    "end": 287,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 289,
                    "end": 290,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 292,
                    "end": 294,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 451,
                    "end": 453,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 455,
                    "end": 457,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 589,
                    "end": 591,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 826,
                    "end": 828,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 830,
                    "end": 832,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Entity Linking research maps entity variations in Open or proprietary KBs to an existing inventory of unique entities [16, 17, 26]. These research works explore candidate entity set generation followed by ranking entities in these candidate set and using this knowledge to predict relatable entities [23]. In [30], authors discuss the creation of mappings from surface forms to possible entities for listing candidate entities. Some systems explore generating a candidate set using string matching/similarity or identifying associated pages through search APIs [5, 8, 30]. Research demonstrates the efficacy of deep neural networks to enhance the sparse features used in Entity Linking. The enhancements improve the generalizability of these features for canonicalization tasks. For instance, feedforward networks have been used on bag-of-words to enhance the entity context or entity class information [11, 24]. Few others propose CNN architecture to capture information about entities at various granularities [7, 13]. However, these approaches do not scale for new or emerging entities and capture limited semantic similarities.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 121,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 123,
                    "end": 125,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 127,
                    "end": 129,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 301,
                    "end": 303,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 310,
                    "end": 312,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 562,
                    "end": 563,
                    "mention": "5",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 565,
                    "end": 566,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 568,
                    "end": 570,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 904,
                    "end": 906,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 908,
                    "end": 910,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1013,
                    "end": 1014,
                    "mention": "7",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1016,
                    "end": 1018,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Clustering Approaches: After evaluating the similarity between entity pairs; these entities need to be clustered into high-quality coherent groups representing unique entities. Researchers have widely explored various methodologies to cluster equivalent entities for canonicalization [2, 8]. These clustering approaches are based on partitions, hierarchy, or graphical structures of the underlying text [15, 20, 29]. These methods perform clustering on the hand-crafted similarity scores generated between different pairs of named-entities. Among these methods, Hierarchical Agglomerative Clustering (HAC) is the most extensively used in the literature [8, 27].\n",
            "cite_spans": [
                {
                    "start": 285,
                    "end": 286,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 288,
                    "end": 289,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 404,
                    "end": 406,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 408,
                    "end": 410,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 412,
                    "end": 414,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 654,
                    "end": 655,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 657,
                    "end": 659,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "We investigate mechanisms for a) similarity using deep Siamese BiLSTM networks with b) clustering on similarity graph for generating high-quality groups of canonicalized entities. We believe, our work will help develop real-time system robust to existing and new or emerging entities with missing information or incomplete entity types in the KB. We use character level embeddings to handle various challenges such as spelling errors and out-of-knowledge-base (OOKB)/emerging entities in the user-generated data.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "We define E as the space consisting of all entity variations in our knowledge base. In this space, each entity cluster \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_x$$\\end{document} has n variations represented as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{e_x^1,e_x^2,...e_x^n\\}$$\\end{document}. First, to calculate the similarity score, we build a pair-wise similarity function f for every entity pair \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(e_x^i,e_y^j)$$\\end{document} in the space E. An entity pair may or may not belong to the same cluster. Hence, we model the function f, such that, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f(e_x^i,e_y^j)$$\\end{document} = 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\forall $$\\end{document} (x,y,i,j) where (x=y) and 0 otherwise. Next, we perform clustering using the pair-wise scores estimated by f into groups representing unique entities. Similar nomenclature was used in our previous solution [6].",
            "cite_spans": [
                {
                    "start": 2079,
                    "end": 2080,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Terminology ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": []
        },
        {
            "text": "We propose a deep Siamese neural network based similarity model (SM) to learn the similarity between entity name variations in the KB (Fig. 1). SM consists of two identical sub-networks, that share the same weights, followed by a merge layer that computes an energy function between them. The twin sub-networks take pre-processed sequence representations for (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_y^j$$\\end{document}) respectively. The output from the merge layer passes through two fully connected layers with non-linearity which finally predicts a similarity score. We will now discuss each stage in detail.",
            "cite_spans": [],
            "section": "Pairwise Similarity Model ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": [
                {
                    "start": 140,
                    "end": 141,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Pre-processing Entity Names: We convert all name variations \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document} to lowercase, and remove infrequent non-informative characters (eg \u2018*\u2019). After pre-processing, the size of character vocabulary for company variations was 64. We use two types of input vectors which were fed together as input to Siamese networks: i) Character input vector of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document} is a concatenated sequence of 64-dimensional one-hot vector for each character in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document}. We limit the number of characters in entity name to first 43 characters with padding, which were sufficient to fully represent 95% of entities mentioned our KB. Given the character vocabulary size of 64, this resulted in each entity variation being represented by a 43 x 64 dimensional character input vector. ii) Word input vector of an entity name \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document} is concatenated sequence of 100 dimensional word embeddings for every word token. Similar to i), we limit to first 7 words with padding resulting in 7 x 100 dimensional word input vector. We generate these fastText word embeddings [4, 14] trained on our proprietary dataset of 50 million resumes. We follow the same pre-processing across all the entity types respectively.\n",
            "cite_spans": [
                {
                    "start": 2165,
                    "end": 2166,
                    "mention": "4",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 2168,
                    "end": 2170,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Pairwise Similarity Model ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": []
        },
        {
            "text": "Network Architecture Choices: We now describe the various architecture choices for the twin sub-networks employed in the SM. We start with describing a base network [6, 19]. This architecture is incrementally improved by two additional networks described subsequently.",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 167,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 169,
                    "end": 171,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Pairwise Similarity Model ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": []
        },
        {
            "text": "\nChar-BiLSTM (Base network): This sub-network consists of a pair of Bi-LSTM layers. The 43 x 64 dimension character vector for an entity name variation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document}, as described in Sect. 3.2, is fed to a pair of LSTM layers which produces a 128-dimensional feature representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_x^i$$\\end{document} for a name variation. This architecture choice [6] forms the basis of the ECS to scale across all the entity types in our recruitment domain specific KB.Char-BiLSTM + Attention layer (Char-BiLSTM + A): We enhance the base network by adding attention layer on top of the base network. User-generated entity variations often consists of less informative subsequences/characters. We employ attention networks [3] to generate a combined weighted representation by selectively attending to the past outputs. The weights learnt by attention network allows the model to differentially attend important vs unimportant content within the sequences. Figure 2 demonstrates the role of attention mechanism while learning pairwise similarity between institute variation names. The visualization depicts how the model learns to selectively focus more certain characters in the sequence like the beginning letters, letters after a space character, and ignore less informative subsequences (eg. college, of) with respect to the entity type- institute.Combining Word and Char BiLSTM representations (Char Bi-LSTM + A + Word + A): From our data, we observe that in the recruitment domain, many entity variations belonging to the same entity consists of synonymous subwords, grammatical variety as well as re-arrangement of subwords (eg. \u2018CAD designer\u2019 vs \u2018computer-aided design draughtsman\u2019). Character-based networks, as described above, have limited capabilities to capture similarity for such cases. Therefore, we also incorporate word-based sequence network by introducing another pair of Bi-LSTM layers, which takes word embeddings input vector as described in Sect. 3.2 separately alongside with the Char-BiLSTM + A sub network. We concatenate the output from the Word-BiLSTM + A with the output from the the Char-BiLSTM + A, which forms the final representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_x^i$$\\end{document} and passed to the merge layer. The final architecture Char-BiLSTM-A + Word-BiLSTM-A  is shown in Fig. 3.\n\n\n",
            "cite_spans": [
                {
                    "start": 898,
                    "end": 899,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1257,
                    "end": 1258,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Pairwise Similarity Model ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": [
                {
                    "start": 1497,
                    "end": 1498,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 3094,
                    "end": 3095,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Lastly, we use a weighted merge layer (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ML_1$$\\end{document}) to combine and estimate the relatedness between the entity names using the representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_x^i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_y^j$$\\end{document} produced by the twin sub-networks:\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} ML_1\\ ( e_x^i,\\ e_y^j)=\\ |h_x^i\\ -\\ h_y^j| \\end{aligned}$$\\end{document}We use output from the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ML_1$$\\end{document} layer as input to a pair of fully connected layers that use sigmoid activation to give probability i.e. similarity score S (Fig. 3).",
            "cite_spans": [],
            "section": "Pairwise Similarity Model ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": [
                {
                    "start": 1839,
                    "end": 1840,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Model Configurations: For every entity type, we learn separate pairwise similarity models using our proposed architecture. The tuned hyper-parameters for each entity type is shown in Table 1. We use Binary Cross Entropy as the final objective function with Adam optimizer.\n",
            "cite_spans": [],
            "section": "Pairwise Similarity Model ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": [
                {
                    "start": 189,
                    "end": 190,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We employ Hierarchical Agglomerative Clustering (HAC), the widely preferred method [25], as our clustering algorithm to map name variations into clusters. Each cluster thus formed, would finally represent a unique real world entity. In our scenario, we use the pairwise similarity scores obtained from SM for every named-entity pair for measuring the distance/closeness between variations in HAC. We use Silhoutte index [28] to determine k, the number of clusters in HAC. This process was repeated for each dataset respectively.",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 86,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 421,
                    "end": 423,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Clustering Using Pairwise Similarity Scores ::: Entity Canonicalization Solution (ECS)",
            "ref_spans": []
        },
        {
            "text": "We use two dataset sources (see Table 2) as described below: 1. Proprietary: From our Naukri database, we used 25602 company clusters, 23690 institute clusters, 606 skill clusters and 3894 designation clusters, which were manually annotated by two domain experts (with kappa agreement of 0.83). 2. Public: We also demonstrate the generalizability of our proposed approaches by evaluating on two open source datasets: i. DBpedia-Company: We query the large scale open KB DBpedia for extracting the type dbo:Company entities and their name variations.ii. ESCO-Skills and ESCO-Designation: ESCO is a recruitment domain specific open dataset. We extract the alternative labels of Skills/Competences and Occupations of ESCO to create the clusters respectively. We find that many name variations in our KB for entity type such as company and Institute were not present in Freebase (10% recall) nor in Wikipedia [9, 18] (30% recall), thus establishing that this dataset warrants a domain-specific solution.",
            "cite_spans": [
                {
                    "start": 906,
                    "end": 907,
                    "mention": "9",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 909,
                    "end": 911,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Dataset ::: Experimental Setup",
            "ref_spans": [
                {
                    "start": 38,
                    "end": 39,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Next, we discuss the methodology for pair generation: a) Similar Pairs: For every entity \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document}, we generate its pair with every other element \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_y^j$$\\end{document} such that x=y; if i\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ne $$\\end{document}j i.e. both the elements belong to the same manually annotated cluster. b) Dissimilar Pairs: We employ two procedures towards this task. In Random selection approach [19], every variation name \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document} is paired with randomly selected variation name \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_y^j$$\\end{document} from the cluster (i.e. x \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ne $$\\end{document} y). In the second approach, we rank all the name variation of entities to create a ranked index I. Next, we query index I for every variation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document} and use the lower ranked variation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_y^j$$\\end{document} from results to create dissimilar name variation (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x^i$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_y^j$$\\end{document}). We release the similar and dissimilar pairs created using our proposed method for future research. We have consolidated the entity clusters for the public dataset here2.",
            "cite_spans": [
                {
                    "start": 1193,
                    "end": 1195,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Dataset ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "We also compare pairwise similarity from our learnt SM with the following baseline approaches: (a) Galarraga-IDF: When \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_x$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_y$$\\end{document} have overlapping words, it may indicate they are referring to the same entity. However, not all the words might be of equal importance. Therefore, we use a weighted word overlap approach Galarraga-IDF proposed by [8], (b) FastText Embeddings: We find pairwise similarity in embeddings space of mentions as another baseline [27]. We use FastText that uses sub-word level information and robust to misspellings. (c) Base network: We compare our improved design choices to the base network mentioned in Sect. 3.2. We use Precision, Recall and F1 score [22] for evaluating the pairwise similarity results. We use Micro\u2013 and Macro\u2013 Precision and Recall, as described in ECS-1 [6] for evaluating the clusters.\n",
            "cite_spans": [
                {
                    "start": 915,
                    "end": 916,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1025,
                    "end": 1027,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1251,
                    "end": 1253,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1373,
                    "end": 1374,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Baselines and Evaluation Metrics ::: Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "Pairwise Similarity: Table 3 summarizes the evaluation of our proposed design choices Char-BiLSTM, Char-BiLSTM + A, Char-BiLSTM + A +Word +A with other baselines approaches for the measuring the pairwise similarity. We now discuss the performance of each model in detail. Table 4 discusses the qualitative analysis of the various approaches through some illustrative examples.",
            "cite_spans": [],
            "section": "Results and Discussion",
            "ref_spans": [
                {
                    "start": 27,
                    "end": 28,
                    "mention": "3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 278,
                    "end": 279,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "Galarraga IDF is a weighted word overlap based similarity measure. The main disadvantage is that it suffers from high similarity bias towards surface forms and therefore do not scale well for entities with dissimilar surface forms (e.g. \u2018Cal State University Los Angeles\u2019 vs \u2018Calif. State Univ LA\u2019). This results in poor similarity score (similarity= 0.16) even for similar variations (See Table 4) thereby resulting in poor recall (See Table 3).",
            "cite_spans": [],
            "section": "Results and Discussion",
            "ref_spans": [
                {
                    "start": 396,
                    "end": 397,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 443,
                    "end": 444,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Table 3 shows employing FastText increases the performance across all entities as FastText captures the semantic similarity from CV. It also helps in OOKB variations and misspellings as it generates word representations as a summation of subword representations learnt from bag-of-character n-grams. However, as shown in Table 4, using such representations for measuring similarity of an entity \u2018Institute of Personnel Management Sri Lanka\u2019 with its own variation such as \u2018IPM Sri Lanka\u2019 results in low similarity (0.54) due to lack of overlapping character n-grams. On the other hand, the summation approach results in high similarity (0.82) between dissimilar entities such as \u2018Syscon Instruments Pvt Ltd\u2019 and \u2018Micron Instruments Pvt Ltd\u2019 which share many overlapping character n-grams. Presumably, it captures limited patterns contained within sequences of characters which distinguishes one entity from the other.",
            "cite_spans": [],
            "section": "Results and Discussion",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 327,
                    "end": 328,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "Our proposed Char-BiLSTM addresses the challenge  of  handling  complex/unseen shorthands and abbreviations which the previous models were unable to do. The sequence-to-sequence modelling learns these character sequence patterns from a large number of variation pairs seen in parallel by the Siamese network during training in supervised learning. The results in Table 4 show that it is infact able to output high similarity for pairs with dissimilar surface forms (\u2018Institute of Personnel Management Sri Lanka\u2019 vs \u2018IPM Sri Lanka\u2019), as well as low similarity for dissimilar pairs with many overlapping characters (\u2018Syscon Instruments Pvt Ltd\u2019 vs \u2018Micron Instruments Pvt Ltd\u2019). Further, these structural patterns learnt by the character model is generalizable even for out-of-KB or emerging entities.\n",
            "cite_spans": [],
            "section": "Results and Discussion",
            "ref_spans": [
                {
                    "start": 369,
                    "end": 370,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "One of the key challenges in finding pairwise similarity is the presence of synonymous substrings within the variations (eg. drainage technician vs sewer and drain inspector) (See Table 4), which leads to false negatives in character based models. For such cases, word-based sequence modelling helps in identifying the similarity between words technician vs inspector, thereby resulting in high overall similarity between the variations. Word sequence information also help in identifying dissimilarity in cases like \u2018London School of Design\u2019 vs \u2019London School of Management Education\u2019 (See Table 4). We observe from the Tables 3 that Char-BiLSTM + A + Word + A significantly outperforms all other models. A one-way repeated-measures ANOVA test showed significant difference (at 95% confidence) in all evaluation metrics (Precision, Recall, and F1-measure) across all entity types (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p < 0.0001$$\\end{document}). Analyzing further using pairwise paired t-tests with Bonferroni correction, we validated the significance for our approach with various other baselines. Therefore, we choose Char + A + Word + A as our preferred network choice for modelling SM in ECS.",
            "cite_spans": [],
            "section": "Results and Discussion",
            "ref_spans": [
                {
                    "start": 186,
                    "end": 187,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 597,
                    "end": 598,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 628,
                    "end": 629,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Clustering: Table 5 presents the results of applying clustering on pairwise similarity to identify unique entity groups used to canonicalize name variations in the test dataset. We find that our approach consistently performs well across all the entity types. In the proprietary dataset, we achieve best performance for company entity with 0.98 F1-scores. Table 6 demonstrates the efficacy of our approach in canonicalizing non-trivial instances involving (dis)similar surface forms and emerging entities. For example, in case of company entity type, ECS identifies various canonical forms for the entity \u2018Warner Brothers Studios\u2019 despite the existence of a variety of dissimilar surface forms like \u2018warnerbros.com\u2019, \u2018warner brothers international television\u2019 and \u2018warner bros. entertainment\u2019. We believe that while Char-BiLSTM could successfully capture the variety of dissimilar surface forms such as shorthands and spelling variations, the attention mechanism could help to focus on the important terms like warner while disregarding noisy but lengthy strings like \u2018first national\u2019. Moreover, the word embeddings could help in boosting the similarity due to the presence of synonymous terms like entertainment, film and studios. We find similar trends in other entity types.\n\n",
            "cite_spans": [],
            "section": "Results and Discussion",
            "ref_spans": [
                {
                    "start": 18,
                    "end": 19,
                    "mention": "5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 362,
                    "end": 363,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "Error Analysis: Table 6 demonstrates some of errors encountered. Sometimes individual word sequences are unable to capture the meaning of the entire phrase. For example, the skill variations \u2018Floor management\u2019 and \u2018Floor planning\u2019 are very different activities. However, due to the overlapping of word floor and presence of similar words like management and planning, they incorrectly get mapped into same cluster.",
            "cite_spans": [],
            "section": "Results and Discussion",
            "ref_spans": [
                {
                    "start": 22,
                    "end": 23,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "Our research focuses on developing a novel solution for entity canonicalization in KBs for the recruitment domain. We canonicalize entities like institutes, companies, skill sets, and designation because: a) of their importance to the recruitment domain for various applications such as recommendation and search; and b) as these entities have the most variations in their representations in KBs for recruitment. The proposed Similarity Model (SM) learns the entity pair similarity using Siamese neural networks comprising of Bi-Directional LSTMs with attention on character embeddings. Using word embeddings, we enhanced the capability of the model to capture the semantic similarity. Further, we applied Hierarchical Agglomerative Clustering (HAC) using these pairwise similarities to create clusters of unique entities. In our work, we efficacy of our architecture on both proprietary and public dataset. Experiments revealed that the Siamese neural network-based approach learns a character level relationships and achieves competitive performance relative to the baselines. We believe our proposed methods are also generalizable to entities from other similar domains that lack sufficient contextual information in open KBs such as Wikipedia.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Tuned hyper-parameters used for training the Siamese Model (SM). For all experiments, Batch-size is 512, BiLSTM layers are 2, Fully Connected layers in are 2, Non-linearity is RelU and Optimizer used is Adam\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Number of entity variation pairs and clusters from proprietary and public datasets.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Test Results for pairwise similarity using SM in comparison with other baseline approaches.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Examples of pairwise similarity scores predicted using SM and other baselines.\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Test Results for clustering using HAC over pairwise distances predicted by SM.\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6.: Sample ECS entity clusters generated from our proposed ECS.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Entity canonicalization solution (ECS) architecture",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Visualization of attention learnt by Char-BiLSTM + A model. Dark color signifies more attention weight on the character.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Siamese Model (SM) Architecture for Char Bi-LSTM + A+ Word + A: Character level representation learned from Bi-LSTM followed by Attention layer and word level representation learned from Bi-LSTM layer. The red vector represents the character input vector and blue vectors represents the word input vector.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "DBpedia: a nucleus for a web of open data",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Auer",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bizer",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kobilarov",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Cyganiak",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ives",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "The Semantic Web",
            "volume": "",
            "issn": "",
            "pages": "722-735",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Knowledge graph identification",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pujara",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Miao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Getoor",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Cohen",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "The Semantic Web \u2013 ISWC 2013",
            "volume": "",
            "issn": "",
            "pages": "542-557",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "A critical investigation of recall and precision as measures of retrieval system performance",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Raghavan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bollmann",
                    "suffix": ""
                },
                {
                    "first": "GS",
                    "middle": [],
                    "last": "Jung",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "ACM Trans. Inf. Syst. (TOIS)",
            "volume": "7",
            "issn": "",
            "pages": "205-229",
            "other_ids": {
                "DOI": [
                    "10.1145/65943.65945"
                ]
            }
        },
        "BIBREF15": {
            "title": "Entity linking with a knowledge base: issues, techniques, and solutions",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Knowl. Data Eng.",
            "volume": "27",
            "issn": "",
            "pages": "443-460",
            "other_ids": {
                "DOI": [
                    "10.1109/TKDE.2014.2327028"
                ]
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [
                {
                    "first": "PN",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Introduction to Data Mining",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Enriching word vectors with subword information",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bojanowski",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Grave",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Joulin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Trans. ACL",
            "volume": "5",
            "issn": "",
            "pages": "135-146",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}