{
    "paper_id": "4290d42677bf8ace6d444d7a31002b6a24705a49",
    "metadata": {
        "title": "Accelerating Hyperparameter Optimization of Deep Neural Network via Progressive Multi-Fidelity Evaluation",
        "authors": [
            {
                "first": "Guanghui",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210023",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": "guanghui.zhu@smail.nju.edu.cn"
            },
            {
                "first": "B",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210023",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Ruancheng",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210023",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Deep neural networks usually require careful tuning of hyperparameters to show their best performance. However, with the size of state-of-the-art neural networks growing larger, the evaluation cost of the traditional Bayesian optimization has become unacceptable in most cases. Moreover, most practical problems usually require good hyperparameter configurations within a limited time budget. To speed up the hyperparameter optimization, the successive halving technique is used to stop poorly-performed configurations as early as possible. In this paper, we propose a novel hyperparameter optimization method FastHO, which combines the progressive multi-fidelity technique with successive halving under a multi-armed bandit framework. Furthermore, we employ Bayesian optimization to guide the selection of initial configurations and an efficient data subsampling based method to warm start the surrogate model of Bayesian optimization. Extensive empirical evaluation on a broad range of neural networks and datasets shows that FastHO is not only effective to speed up hyperparameter optimization but also can achieve better anytime performance and final performance than the state-of-the-art hyperparameter optimization methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In recent years, deep learning has achieved great success on a variety of machine learning problems such as computer vision and natural language processing. However, deep neural networks (DNNs) are with too many hyperparameters and the learning performance depends seriously on the careful tuning of them [8] . The correct setting of hyperparameters for DNNs often needs a tedious endeavor, and typically requires considerable expert knowledge and experience. As a result, both researchers and practitioners desire to set hyperparameters automatically without any human intervention.",
            "cite_spans": [
                {
                    "start": 305,
                    "end": 308,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Unlike traditional machine learning models, hyperparameter optimization for DNNs is more challenging. Since the architecture of DNNs is getting more and more complex, training large DNNs is more computationally expensive. For example, state-of-the-art neural networks often require days or even weeks to train. Thus, the well-known Bayesian optimization [1, 2, 10, 18, 20] methods that view the performance as a black-box function suffer low computational efficiency due to the expensive evaluation of hyperparameters. On the other hand, most practical problems usually require a good hyperparameter configuration within a limited time budget. Besides the strong final performance given a larger budget, practical hyperparameter optimization methods should also achieve strong anytime performance in the case of a small budget.",
            "cite_spans": [
                {
                    "start": 354,
                    "end": 357,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 358,
                    "end": 360,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 361,
                    "end": 364,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 365,
                    "end": 368,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 369,
                    "end": 372,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To speed up the hyperparameter optimization of DNNs, Hyperband [15] uses the successive halving (SH) technique [11] to stop poorly-performed configurations as early as possible and dynamically allocate more resources (i.e., the number of iterations) to well-performed configurations. Another popular method is the multi-fidelity optimization [9, 12, 13, 19] . Generally, the fidelity indicates the sampling ratio of the full dataset. Multi-fidelity optimization uses many cheap low-fidelity evaluations instead of expensive high-fidelity evaluations to extrapolate the performance of hyperparameter configurations on the full dataset.",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 67,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 111,
                    "end": 115,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 342,
                    "end": 345,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 346,
                    "end": 349,
                    "text": "12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 350,
                    "end": 353,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 354,
                    "end": 357,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we propose a novel hyperparameter optimization method FastHO to further accelerate the hyperparameter optimization of DNNs, while achieving good anytime performance and final performance. FastHO combines the progressive multi-fidelity technique with successive halving under a multiarmed bandit framework. Each hyperparameter configuration is viewed as an arm. At first, we aggressively evaluate each arm with fewer resources (i.e., small iteration budget and low fidelity). The poorly-performed arms are discarded and more resources are dynamically allocated to the promising configurations. The process is repeated until the maximum iteration budget and the highest fidelity are reached. Furthermore, we employ Bayesian optimization to guide the selection of initial configurations. Additionally, an efficient warmup method based on data subsampling is proposed to initialize the surrogate model of Bayesian optimization. Extensive empirical evaluation on different neural networks and datasets shows that FastHO outperforms the existing hyperparameter optimization methods. FastHO is not only effective to speed up hyperparameter optimization but also can achieve robust and better final performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Given a machine learning algorithm A having hyperparameters \u03bb 1 ; . . . ; \u03bb n with respective domains \u03b4 1 ; . . . ; \u03b4 n , we define its hyperparameter space as \u03b4 = \u03b4 1 \u00d7 . . . \u00d7 \u03b4 n . For each hyperparameter setting \u03bb, we use A \u03bb to denote the learning algorithm A using this configuration. We further use l(\u03bb) = L(A \u03bb ; D train ; D valid ) to denote the validation loss (e.g., error rate) that A \u03bb achieves on data D valid when trained on D train . The hyperparameter optimization problem is then to find \u03bb minimizing l(\u03bb).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Bayesian optimization (BO) is the most popular hyperparameter optimization method [2, 18, 20] . BO models the conditional probability p(y|\u03bb) of a configuration's performance on an evaluation metric y, given a set of hyperparameters \u03bb.",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 85,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 86,
                    "end": 89,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 90,
                    "end": 93,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The commonly-used probabilistic model in BO is Gaussian process (GP), but GP does not typically scale well to high dimensions and exhibits cubic complexity in the number of data points. Another model-based Bayesian optimization method is SMAC [10] , which uses random forest as the surrogate model. SMAC can perform well in high-dimensional categorical spaces. TPE [1] is a non-standard Bayesian optimization algorithm based on tree-structured Parzen density estimators. Due to the nature of kernel density estimators, TPE easily supports mixed continuous and discrete spaces. The above three BO methods are well-established and successful, but they are inefficient for the hyperparameter optimization of DNNs due to the huge evaluation cost.",
            "cite_spans": [
                {
                    "start": 243,
                    "end": 247,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 365,
                    "end": 368,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Unlike the model-based Bayesian optimization, the bandit-based strategy Hyperband [15] formulates hyperparameter optimization as a pure-exploration problem by addressing how to allocate resources among randomly-sampled hyperparameter configurations. Besides, it uses successive halving to early stop the poorly-performed configurations. Compared to Bayesian optimization, Hyperband shows strong anytime performance, but it may lead to poor final performance because the initial hyperparameter configurations are selected randomly. BOHB [4] takes advantage of both Bayesian optimization and Hyperband and thus achieves the state-of-the-art anytime performance and final performance.",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 86,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 536,
                    "end": 539,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Another efficient method for the tuning of hyperparameters is multi-fidelity optimization, which uses cheap approximations to the function of interest to speed up the overall optimization process. Generally, the fidelity can be represented by the sampling ratio of the full dataset. Multi-fidelity Bayesian optimization that ranges from a finite number of approximations to continuous approximations has been well studied [12, 13, 19] . Furthermore, a general multi-fidelity framework based on the black-box optimization methods was proposed [9] .",
            "cite_spans": [
                {
                    "start": 422,
                    "end": 426,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 427,
                    "end": 430,
                    "text": "13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 431,
                    "end": 434,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 542,
                    "end": 545,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "However, due to the huge training cost of DNNs, the existing hyperparameter optimization methods are inefficient and time-consuming for DNNs. Even for the state-of-the-art method BOHB, it still requires 33 GPU days for optimizing the hyperparameters of a medium-sized residual network [4] . In this paper, we combine the successive halving technique with multi-fidelity optimization to accelerate the hyperparameter optimization of DNNs.",
            "cite_spans": [
                {
                    "start": 285,
                    "end": 288,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "In this section, we propose a novel early-stopping mechanism to speed up the hyperparameter optimization of DNNs by taking the number of iterations and multi-fidelity into account at the same time. We first analyze the low-fidelity evaluation bias of DNNs, which motivates to progressively increase the fidelity. Then, we introduce the IF-SH (Iteration-and-Fidelity Based Successive Halving) method in detail. Moreover, we propose an efficient warmup technique for Bayesian optimization to further improve the performance, especially the anytime performance of hyperparameter optimization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "Multi-fidelity optimization uses many cheap low-fidelity approximations instead of the expensive high-fidelity evaluations to speed up the overall optimization process. The lower the fidelity is, the cheaper the evaluation will be. However, it is intuitive that the evaluation on a part of the dataset is badly biased because it provides less accurate information about the target function. The experimental results of multi-fidelity optimization on LightGBM [14] show that the hyperparameter configurations chosen by low-fidelity evaluations usually perform poorly on the test dataset. We have tried to apply BOHB to find the best hyperparameter configuration of a convolutional neural network LeNet (with two convolutional layers, a full-connection layer, and a softmax layer) on the MNIST and CIFAR-10 datasets. We evaluated the hyperparameter configurations chosen on the data subset (i.e., 10%) and the whole dataset, and then compared the test error rate. The results are shown in Table 1 .",
            "cite_spans": [
                {
                    "start": 459,
                    "end": 463,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 987,
                    "end": 994,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Low-Fidelity Evaluation Bias"
        },
        {
            "text": "It turns out as expected that the configuration chosen by high-fidelity evaluations is superior to those selected by low-fidelity evaluations. Additionally, we note that the main difference between the hyperparameter configurations chosen by the high-fidelity and low-fidelity evaluations is the regularization hyperparameters such as weight decay and dropout rate. It makes sense because the neural networks trained on the data subset usually require more regularization to deal with overfitting. As shown in Table 1 , the evaluation performance on the data subset is biased in different cases. Therefore, it is necessary to develop a new method to balance the low-fidelity and high-fidelity evaluations. To address this issue, we propose a progressive multi-fidelity evaluation technique. We further combine this technique with the existing successive halving optimization. The configurations are first evaluated with a small number of epochs and low fidelity. After filtering the poorly-performed configurations as early as possible, we dynamically increase the number of epochs and fidelity simultaneously for the remaining configurations. The process is repeated until the maximum number of epochs and the maximum fidelity (i.e., the full dataset) are used. We call this procedure IF-SH (Iteration-and-Fidelity Based Successive Halving).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 510,
                    "end": 517,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Low-Fidelity Evaluation Bias"
        },
        {
            "text": "Multi-armed bandit based methods such as Hyperband and BOHB view each hyperparameter configuration as an arm and dynamically allocate resources to different arms. The existing successive halving method can ensure that the poorly-performed configurations are discarded as early as possible and those promising hyperparameter configurations will get more resources overtime automatically. For the hyperparameter optimization of DNNs, the resource means the number of iterations (i.e., epochs). Since using evaluation on fewer iterations as criteria in HyperBand is feasible, it is worth a try that we use evaluation on both fewer iterations and lower fidelity to judge a hyperparameter configuration, which will remarkably reduce the overall time cost.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Progressive Multi-Fidelity Evaluation"
        },
        {
            "text": "Thus, we propose a progressive multi-fidelity evaluation method IF-SH and dynamically allocate multiple resources including iteration budget and fidelity. Specifically, IF-SH usually begins with a small iteration budget on data subsets instead of the whole dataset. Then, IF-SH ranks the configurations by the validation performance and select the top \u03b7 \u22121 to continue running with an iteration budget \u03b7 times larger and a fidelity \u03b8 times larger. This process is repeated until it runs with the largest iteration budget on the whole dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Progressive Multi-Fidelity Evaluation"
        },
        {
            "text": "Another problem of the multi-armed bandit based hyperparameter optimization is how to set the number of initial configurations n. Similar to Hyperband, we consider several possible values of n to balance exploration and exploitation. Associated with each value of n is a minimum iteration budget b min that is allocated to all configurations. A larger value of n corresponds to a smaller b min and hence means more aggressive early stopping. Algorithm 1 shows the process of IF-SH, which requires the following inputs: (1) [b min , b max ] that determines the iteration budget space (2) \u03b7, an input that controls the proportion of configurations discarded and the number of iterations in each round of SH. Also, \u03b7 determines the minimum number of iterations of the next round (3) \u03b8, an input that controls the size of fidelity in each round of SH. \u03b8 also determines the minimum fidelity of the next round.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Progressive Multi-Fidelity Evaluation"
        },
        {
            "text": "This algorithm balances between very aggressive evaluations with many configurations on the minimum resource, and very conservative runs that are directly evaluated on the maximum resource. Table 2 displays the resources allocated within each round of SH in IF-SH. The size of data subsampling is controlled by \u03b8. In practice, the difference caused by various \u03b8 settings is not so notable. We will discuss it in Sect. 4.1. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 190,
                    "end": 197,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Progressive Multi-Fidelity Evaluation"
        },
        {
            "text": "Hyperparameter configurations within each round of SH is selected by Bayesian optimization (Line 4 in Algorithm 1). It is well known that all model-based optimization methods including Bayesian optimization need initial observations to build the surrogate model. The most commonly-used startup is to choose random hyperparameter configurations, which is not efficient and robust. When the randomly-sampled configurations perform poorly, the surrogate model will be slow to work, causing a negative influence on anytime performance. A lot of previous work [5, 16, 21] focuses on meta-learning to handle this issue, but they require historical data or pre-trained models. Obviously, the data subset contains part information of the whole dataset. As discussed in Sect. 3.1, the difference between configurations chosen by the data subset and the whole dataset is not too remarkable. Thus, using the sampling data to warm start the surrogate model is a feasible way. The configurations chosen by low-fidelity evaluations probably exceed the randomly-sampled configurations, although their final performance may not be so satisfying. To improve the efficiency of the warmup phase, we just run one round of SH. We first sample data from the training dataset with the sampling percent r. Then, we run SH on the sampling data D r and select the top-k configurations to warm up the surrogate model. In Sect. 4.1, we will evaluate the effect of r, subsampling percent of the warmup phase. In fact, by selecting more promising hyperparameters rather than random selection, the warmup phase is helpful for improving the final performance of hyperparameter optimization.",
            "cite_spans": [
                {
                    "start": 555,
                    "end": 558,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 559,
                    "end": 562,
                    "text": "16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 563,
                    "end": 566,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Surrogate Model Warmup"
        },
        {
            "text": "In this section, we evaluated the empirical performance of our proposed method FastHO on different neural networks including CNN, Fully-Connected neural network, ResNet18 [7] , and ResNet with Shake-Shake [6] and Cutout [3] regularization. The datasets include MNIST, CIFAR-10, and CIFAR-100. We compared the anytime performance and final performance of FastHO with TPE [1] , Hyperband [15] , and BOHB [4] . BOHB is the state-of-the-art hyperparameter optimization method that combines TPE and Hyperband (HB). We set \u03b7 = 3, \u03b8 = 3, r = 0.1 as default and explore how to set suitable \u03b8 and r. If not stated otherwise, for all methods we report the average error rate on the test dataset.",
            "cite_spans": [
                {
                    "start": 171,
                    "end": 174,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 205,
                    "end": 208,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 220,
                    "end": 223,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 370,
                    "end": 373,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 386,
                    "end": 390,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 402,
                    "end": 405,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We first evaluated FastHO on a CNN with two convolutional layers, a fullconnection layer, and a softmax output layer. We optimized the hyperparameters including learning rate, momentum, weight decay, dropout rate, batch size, the number of full-connection units, kernel size, and weight initialization mode. For this network, we set b min = 2 and b max = 60 for successive halving. The budget indicates the number of epochs. The IF-SH process contains 4 rounds of successive halving, resulting in 240 epochs in total.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "The CIFAR-10 dataset contains 50000 training and 10000 test RGB images with 32 \u00d7 32 pixels. The standard data augmentation techniques (i.e., random crop and horizontal flip) are used. To perform hyperparameter optimization, we split off 10000 training images as a validation set. Figure 1 shows the average test error of FastHO with and without the warmup phase. We also compared FastHO with TPE, Hyperband, and BOHB. The total iteration budget is 16 resource units and each resource unit represents 240 epochs. As a result, the complete FastHO process includes 16 runs of IF-SH.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 280,
                    "end": 288,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "CIFAR-10:"
        },
        {
            "text": "From Fig. 1 , we can see that the traditional Bayesian optimization method TPE does not work well and has the worst anytime performance. HB improves the efficiency of hyperparameter optimization with successive halving and thus achieves better anytime performance than TPE. However, its convergence to the global optimal value is limited by its reliance on randomly-drawn configurations. Thus, the final performance of HB is not very strong. BOHB performs well with limited resources and at the same time can achieve better final performance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 5,
                    "end": 11,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "CIFAR-10:"
        },
        {
            "text": "In contrast, FastHO outperforms BOHB on anytime performance by combining the progressive multi-fidelity optimization with SH. Furthermore, the warmup technique can improve both anytime performance and final performance, while its time cost is negligible compared to the following hyperparameter optimization phase. More importantly, it can help FastHO to reach the best performance with much fewer resources. For instance, FastHO gets the best test error rate within 8 units of resources (8*240 epochs in total), about half of the resources consumed by other methods. Additionally, for the wall clock time, BOHB takes 31 h for hyperparameter optimization within 16 resource units. In contrast, FastHO takes only 19 h which is 63% faster than BOHB.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CIFAR-10:"
        },
        {
            "text": "Evaluation of the \u03b8 and r Setting. We also evaluated two key parameters of FastHO: the proportion of \u03b8 that controls the size of fidelity in each round of successive halving, and the subsampling percent r in the warmup phase.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CIFAR-10:"
        },
        {
            "text": "Intuitively, setting \u03b8 to a smaller value leads to a larger fidelity. If \u03b8 is set to 1, we will evaluate all configurations on the entire dataset. However, this disobeys our purpose to accelerate the evaluation procedure. In contrast, it should not be set to a very large value, because we aim to differentiate between configurations even when they are evaluated on the smallest data subset. Therefore, we set \u03b8 to be 2, 3 (default) and 4, and then compared their performance in Fig. 2(a) .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 479,
                    "end": 488,
                    "text": "Fig. 2(a)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "CIFAR-10:"
        },
        {
            "text": "As shown in Fig. 2(a) , the difference caused by various \u03b8 settings is not so notable. The reason is that the three values are all appropriate ones that guarantee the discrimination of configurations on the smallest data subset. Besides, even if the evaluation bias exists on the low fidelity, the final evaluation is performed on the full dataset, which weakens this bias to some extent. Thus, FastHO is insensitive to the \u03b8 setting.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 21,
                    "text": "Fig. 2(a)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "CIFAR-10:"
        },
        {
            "text": "Next, we discuss the setting of r in the warmup phase. If r is set to a larger value, the data subset contains more information, leading to good hyperparameter configurations to warm start the Bayesian surrogate model. Nevertheless, the time cost of the warmup procedure will become larger. We chose 0.05, 0.1, and 0.2 for r, and then compared their performance in Fig. 2(b) . Note that the warmup technique can improve the performance of FastHO no matter which value to choose. Meanwhile, none of these values is remarkably superior to other ones. The performance difference is acceptable due to the randomness in the data subsampling and hyperparameter optimization phases. Thus, we can infer that r makes little difference. We optimized 6 hyperparameters that control the training procedure (learning rate, batch size, dropout rate, and weight decay). We also optimized the architecture hyperparameters (number of layers, number of units per layer, weight initialization mode, and activation function) of a full-connected neural network. We selected two datasets: Adult and Letter, and set b min = 3, b max = 30. Since BOHB outperforms TPE and HB in most cases, we compared FastHO only with BOHB in the following experiments. In both Fig. 3 (a) and Fig. 3(b) , FastHO outperforms BOHB with not only the better anytime performance but also the better final performance. Moreover, the warmup technique is helpful for converging to optimum faster.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 365,
                    "end": 374,
                    "text": "Fig. 2(b)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1237,
                    "end": 1243,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1252,
                    "end": 1261,
                    "text": "Fig. 3(b)",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "CIFAR-10:"
        },
        {
            "text": "Next, we optimized the hyperparameters of large and widely-used neural networks including ResNet18 [7] and ResNet with Shake-Shake [6] and Cutout [3] regularization on the CIFAR-10 and CIFAR-100 datasets. We used standard data augmentation techniques (i.e., random crop and horizontal flip) and Nesterov momentum SGD optimizer with a cosine learning decay [17] .",
            "cite_spans": [
                {
                    "start": 99,
                    "end": 102,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 131,
                    "end": 134,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 146,
                    "end": 149,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 356,
                    "end": 360,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Large Convolutional Neural Network: ResNet"
        },
        {
            "text": "We tuned 4 hyperparameters including learning rate, momentum, weight decay, and batch size on the CIFAR-10 and CIFAR-100 datasets. CIFAR-100 shares similar input images to CIFAR-10. The only difference is that CIFAR-100 has 100 classes. We split the training dataset into 80% training data and 20% validation data. We set b min = 7 and b max = 200. As shown in Fig. 4 , the performance improvement of FastHO is more significant, which indicates that FastHO is more effective for the hyperparameter optimization of larger neural networks. Moreover, FastHO is 67% faster than BOHB in terms of the total evaluation time cost. ResNet with Shake-Shake and Cutout Regularization: Next, we used the ResNet with Shake-Shake and Cutout regularization. For this network, we set b min = 22 and b max = 600 and optimized learning rate, momentum, weight decay, and batch size. In this case, we just trained and evaluated the network with the best configuration after the complete hyperparameter optimization process (i.e., 16 runs of IF-SH). We ran the complete process 3 times and get a test error of 2.81% \u00b1 0.07%, which is slightly larger than that reported in [4] (2.78% \u00b1 0.09%). However, regardless of the differences in training details or the search space setting, FastHO requires only 19 GPU days, while BOHB needs 33 GPU days. Moreover, as shown in Fig. 4 , FastHO outperforms BOHB with much better anytime performance.",
            "cite_spans": [
                {
                    "start": 1151,
                    "end": 1154,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 361,
                    "end": 367,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1346,
                    "end": 1352,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "ResNet18:"
        },
        {
            "text": "In this paper, we presented a novel method to accelerate the hyperparameter optimization of DNNs by combining the progressive multi-fidelity technique with successive halving under a multi-armed bandit framework. Also, we proposed an efficient warmup method for the surrogate model of Bayesian optimization. Extensive empirical evaluation on a broad range of neural networks and datasets shows that FastHO is not only effective to speed up hyperparameter optimization but also can achieve better anytime performance and final performance than other state-of-the-art methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        },
        {
            "text": "Future work includes taking feature subsampling into account to further accelerate hyperparameter optimization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Algorithms for hyper-parameter optimization",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bergstra",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bardenet",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "K\u00e9gl",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 24th International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2546--2554",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Making a science of model search: hyperparameter optimization in hundreds of dimensions for vision architectures",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bergstra",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yamins",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "D"
                    ],
                    "last": "Cox",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 30th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "115--123",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Improved regularization of convolutional neural networks with cutout",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Devries",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "W"
                    ],
                    "last": "Taylor",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "BOHB: robust and efficient hyperparameter optimization at scale",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Falkner",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 35th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1436--1445",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Initializing bayesian hyperparameter optimization via meta-learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Feurer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Springenberg",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "1128--1135",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Shake-shake regularization",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Gastaldi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Identity mappings in deep residual networks",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECCV 2016",
            "volume": "9908",
            "issn": "",
            "pages": "630--645",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-46493-0_38"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Deep reinforcement learning that matters",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Henderson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Islam",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bachman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pineau",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Precup",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Meger",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 32rd AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "3207--3214",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Multi-fidelity automatic hyper-parameter tuning via transfer series expansion",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 33rd AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "3846--3853",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Sequential model-based optimization for general algorithm configuration",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "H"
                    ],
                    "last": "Hoos",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Leyton-Brown",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "LION 2011",
            "volume": "6683",
            "issn": "",
            "pages": "507--523",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-25566-3_40"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Non-stochastic best arm identification and hyperparameter optimization",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "G"
                    ],
                    "last": "Jamieson",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Talwalkar",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistic",
            "volume": "",
            "issn": "",
            "pages": "240--248",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Gaussian process bandit optimisation with multi-fidelity evaluations",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kandasamy",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dasarathy",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Oliva",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "G"
                    ],
                    "last": "Schneider",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "P\u00f3czos",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1000--1008",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Multi-fidelity bayesian optimisation with continuous approximations",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kandasamy",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dasarathy",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "G"
                    ],
                    "last": "Schneider",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "P\u00f3czos",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 34th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1799--1808",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "LightGBM: a highly efficient gradient boosting decision tree",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Ke",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "3149--3157",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Hyperband: bandit-based configuration evaluation for hyperparameter optimization",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Jamieson",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Desalvo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rostamizadeh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Talwalkar",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J. Mach. Learn. Res",
            "volume": "18",
            "issn": "1",
            "pages": "6765--6816",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Warmstarting of model-based algorithm configuration",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lindauer",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 32rd AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "1355--1362",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "SGDR: stochastic gradient descent with warm restarts",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Loshchilov",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 5th International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Towards automatically-tuned neural networks",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Mendoza",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Feurer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Springenberg",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Workshop on Automatic Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "58--65",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Multi-fidelity black-box optimization with hierarchical partitions",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kandasamy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shakkottai",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 35th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "4545--4554",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Practical bayesian optimization of machine learning algorithms",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Snoek",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Larochelle",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Adams",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 25th International Conference on Neural Information Processing Systems",
            "volume": "2",
            "issn": "",
            "pages": "2951--2959",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Bayesian optimization with robust bayesian neural networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Springenberg",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Falkner",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hutter",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "4141--4149",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Iteration-and-Fidelity Based Successive Halving Input: Iteration budget bmin and bmax, \u03b7, \u03b8 1: smax = log \u03b7 bmax b min 2: for s in {smax; smax \u2212 1; . . . ; 0} do 3: n = smax+1 s+1 * \u03b7 s 4: T = get hyperparameter configurations(n) using Bayesian optimization 5: bmin = bmax * \u03b7 \u2212s 6: fmin = \u03b8 \u2212s //begin the SH inner loop 7: for i in {0; . . . ; s} do 8: ni = n * \u03b7 \u2212i 9: bi = bmin * \u03b7 i 10: fi = fmin * \u03b8 i 11: D sub = sample fi data from the training dataset Dtrain 12: L = { run on D sub then return validation loss(t, bi): t in T } 13: T = top k (T, L, ni/\u03b7) 14: end for 15: end for 16: return Configuration with the smallest intermediate loss seen so far",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "The values of ni, bi and fi in IF-SH corresponding to various values of s, when bmin = 1, bmax = 27, \u03b7 = 3,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Average test error of the best-observed configuration of CNN on CIFAR-10. One resource unit represents 240 epochs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Average test error of the best-observed configuration of CNN on CIFAR-10 with different \u03b8 and r settings. One resource unit represents 240 epochs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Average test error of the best-observed configuration of FC network on Adult and Letter. One resource unit represents 90 epochs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Average test error of the best-observed configuration of ResNet18 on CIFAR-10 and CIFAR-100. One resource unit represents 800 epochs.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Test error rate (%) of LeNet on MNIST and CIFAR-10, using hyperparameter configurations chosen by BOHB with different fidelity evaluations. CIFAR-10+ means CIFAR-10 with standard data augmentation. Results are the average over 5 runs.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We thank Rong Gu, Chunfeng Yuan, and Yihua Huang for helpful advice. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments."
        }
    ]
}