{
    "paper_id": "ee4ab7f8e37fa579ea0927a623a1a2e8bfb917f9",
    "metadata": {
        "title": "A Mixed Semantic Features Model for Chinese NER with Characters and Words",
        "authors": [
            {
                "first": "Ning",
                "middle": [],
                "last": "Chang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chongqing University",
                    "location": {
                        "postCode": "400044",
                        "settlement": "Chongqing",
                        "country": "People's Republic of China"
                    }
                },
                "email": ""
            },
            {
                "first": "Jiang",
                "middle": [],
                "last": "Zhong",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chongqing University",
                    "location": {
                        "postCode": "400044",
                        "settlement": "Chongqing",
                        "country": "People's Republic of China"
                    }
                },
                "email": "zhongjiang@cqu.edu.cn"
            },
            {
                "first": "Qing",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chongqing University",
                    "location": {
                        "postCode": "400044",
                        "settlement": "Chongqing",
                        "country": "People's Republic of China"
                    }
                },
                "email": ""
            },
            {
                "first": "Jiang",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "postCode": "610041",
                        "settlement": "Chengdu",
                        "country": "People's Republic of China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Named Entity Recognition (NER) is an essential part of many natural language processing (NLP) tasks. The existing Chinese NER methods are mostly based on word segmentation, or use the character sequences as input. However, using a single granularity representation would suffer from the problems of out-of-vocabulary and word segmentation errors, and the semantic content is relatively simple. In this paper, we introduce the self-attention mechanism into the BiLSTM-CRF neural network structure for Chinese named entity recognition with two embedding. Different from other models, our method combines character and word features at the sequence level, and the attention mechanism computes similarity on the total sequence consisted of characters and words. The character semantic information and the structure of words work together to improve the accuracy of word boundary segmentation and solve the problem of long-phrase combination. We validate our model on MSRA and Weibo corpora, and experiments demonstrate that our model can significantly improve the performance of the Chinese NER task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In recent years, named entity recognition (NER) has received a lot of attention in the field of natural language processing (NLP), and it is the basis of many downstream NLP tasks. NER refers to the identification of entities with specific meaning in the text, usually including names of people, places, institutions, proper nouns, and so on. For English text, this problem has been studied extensively [13, 20, 23] . However, Chinese NER still faces challenges such as Chinese word segmentation, and it is often difficult to define what constitutes a word in Chinese.",
            "cite_spans": [
                {
                    "start": 403,
                    "end": 407,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 408,
                    "end": 411,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 412,
                    "end": 415,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Most methods of existing state-of-the-art models for Chinese NER are usually based on word segmentation, and train neural network and Conditional Random Field (CRF) to perform sequence labeling on word-level [17] . However the effect of the word segmentation depends heavily on the quality of the dictionaries and segmentation tools, and it's possible to lead to error propagation if the boundaries are partitioned improperly at the very start. Moreover, it can not deal with unseen words. There are also some models which recognize entities in characterlevel, which solve the problem of out-of-vocabulary (OOV) [19] . However, fully character-based models cannot express enough semantic information and word structure, and could lead to wrong word boundaries.",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 212,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 612,
                    "end": 616,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In order to take advantage of both character-level semantic information and word structure content, some models mix word embedding and its corresponding character vectors, and then feed mixed representation into neural network for NER [2, 22, 26] . The generic model mentioned above is shown in Fig. 1(a) , these methods divide each sequence into several characters, and then represent these character vectors as a comprehensive representation through LSTM networks or other models. About word vectors, they concatenate each word vector with the representation of its corresponding characters, and then form a new multigranularity representation of the word. In the process of generating the final word representation, the intermediate dimensional transformation may lead to original information loss. Moreover, for each word, the concatenation of the two granularity representations at the word-level does not express well the relationship between characters and words. These drawbacks affect the accuracy of Chinese word boundary segmentation and entity recognition.",
            "cite_spans": [
                {
                    "start": 235,
                    "end": 238,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 239,
                    "end": 242,
                    "text": "22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 243,
                    "end": 246,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [
                {
                    "start": 295,
                    "end": 304,
                    "text": "Fig. 1(a)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we incorporate the self-attention mechanism into the Chinese named entity recognition model to compute the weighted sum of character and word vectors, and integrate the semantic features of the two representations. Different from the previously mentioned model, our model captures character content features and the information of token structure in word level (as shown in Fig. 1(b) ). Our model uses two sequences of character and word segmentation as input, and outputs the final character-based recognition tags through the attention mechanism. The model preserves the character-level semantic representation and the word tokens structure completely, and uses self-attention to assign the weight of both. Multi-granularity semantic and structural features are combined with word representation to enrich character representation and reduce the loss of original information. Moreover, the character level and the word segmentation structure are complementary to each other, and a single character can correct the word boundary error caused by the word segmentation level.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 389,
                    "end": 398,
                    "text": "Fig. 1(b)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Moreover, phrases that do not appear in the prior dictionary can also be identified. As the example in Fig. 1 shows, given the sequence of \"Beijing/People/Park\" that is segmented using the dictionary, our model could add the segmentation structural information into character-based semantic information. When predicting tags of characters, we can determine the phrase boundary based on the comprehensive context and correctly identify \"Beijing people's park\" as a phrase to be marked.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 103,
                    "end": 109,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "We experiment with our model on MSRA and Weibo data sets, and the results show that using the self-attention mechanism to fuse two granularity semantic and structure representations in sequence context can significantly improve performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The contributions of our paper are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We improve the accuracy of word boundary segmentation by combining two granularity features. Our model retains the primitiveness of character semantics and participle structures completely, and the two embedding information assist each other. Character semantics combined with word tokens structure could modify word boundary segmentation. -We investigate a method to enhance the recognition of Chinese long phrases that do not appear in prior dictionaries. Our model uses a self-attention mechanism to integrate features of word segmentation into a character-level sequence, and predicts it in conjunction with the context of the sentence to merge the short tags into long phrases. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "NER. Early named entity recognition methods are based on rules and statistical machine learning such as Hidden Markov Model (HMM) [1] , Conditional Random Fields (CRF) [12] , and Support Vector Machines (SVM) [11] . In recent years, with the development of machine learning, more and more neural network models are used for the NER task. Collobert et al. [4] propose a unified neural network architecture that can be used in various NLP tasks. Zhou et al. [27] formulate Chinese NER as a joint identification and categorization task. Huang et al. [10] first apply BiLSTM-CRF model to NER, and achieve the advance results at that time. The BiLSTM-CRF model is now also the benchmark model for many pieces of research. Lample et al. [13] use BiLSTM-CRF as the basic model, rely on character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. For Chinese NER, Zhang et al. [26] investigate a lattice-structured LSTM model, utilize information on words and character sequences, and solve the problem of Chinese words boundaries. Dong et al. [6] utilize both character-level and radical-level representations based on bidirectional LSTM-CRF. Besides, incorporating the five-stroke information into the network also achieves outstanding performance [24] . [14] add gazetteer-enhanced sub-tagger on hybrid semi-Markov CRF architecture and observe some promising results. And [5] also propose a neural multi-digraph model with the information of gazetteers.",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 133,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 168,
                    "end": 172,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 209,
                    "end": 213,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 355,
                    "end": 358,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 456,
                    "end": 460,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 547,
                    "end": 551,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 731,
                    "end": 735,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 953,
                    "end": 957,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1120,
                    "end": 1123,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1333,
                    "end": 1337,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1451,
                    "end": 1454,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Vaswani et al. [21] first proposed a self-attention mechanism for machine translation to connect all positions with a constant number of sequentially executed operations, and attract great attention. Subsequently, a large number of studies begin to use the attention mechanism. Zukov et al. [29] use no language-specific features, and the model they proposed is based on RNN structure, coupled with a self-attention mechanism for NER. Yang et al. [25] propose a novel adversarial transfer learning framework and first introduce a self-attention mechanism to the Chinese NER task. And then Zhu et al. [28] propose a convolutional attention network for Chinese named entity recognition. They use a character-based CNN with local-attention and GRU with self-attention to get information from characters of the sentence.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 291,
                    "end": 295,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 447,
                    "end": 451,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 600,
                    "end": 604,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Self-attention."
        },
        {
            "text": "Embedding. Some models join characters with words for sequence tagging. Lample et al. [13] feed the characters of a word into the bidirectional LSTM, and connect the final output of the forward and backward network as character representation. This character-level representation is then concatenated with its corresponding word representation. Rei et al. [18] use the same structure [13] to represent character-level representation. Instead of connecting two-level representations directly, an attention mechanism is used to calculate the weighted sum of character embedding and word embedding. Ma et al. [15] utilize CNN to compute character representation for each word, and concatenate it with word embedding before feeding into the BiLSTM network.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 90,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 356,
                    "end": 360,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 384,
                    "end": 388,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 606,
                    "end": 610,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Joint Character and Word"
        },
        {
            "text": "The main benefit of Chinese characters is they can solve the problem of phrases that is not in the dictionary, and can flexibly determine the phrase boundary. Besides, word-level modeling can provide information about the structure of common words. We propose a model based on self-attention which use the sequence-level joint representation of characters and words to take advantage of two granularity embedding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Joint Character and Word"
        },
        {
            "text": "In this chapter, we will introduce our methodology in detail. Our model utilizes BiLSTM-CRF as our basic structure, and extends a self-attention mechanism to obtain the long distance dependencies of the character encoder and word encoder sequence. As illustrated in Fig. 2 , the architecture of our model mainly consists of character and word embedding, Bi-LSTM network with self-attention and CRF for tagging. We will describe our method in the following sections. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 266,
                    "end": 272,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Methodology"
        },
        {
            "text": "Word embedding, also known as distributed word representation, can capture both the semantic and syntactic information of the words from a large unlabeled corpus. We use open source Chinese word vector corpus of Tencent AI Lab 1 , which includes more than 8 million Chinese words, and each word corresponds to a 200-dimensional vector. For a sentence, we utilize jieba 2 to perform word segmentation. And every word is disintegrated into individual characters. Furthermore, characters in a sentence also contain the rich context of the entities, and Chinese character-based embedding could alleviate problems of long phrases that are not in dictionaries. Our model uses both granularity levels of embedding information to learn the mixture semantic of characters and words.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Characters and Words Representation in Sequence"
        },
        {
            "text": "In this paper, we use Bi-LSTM [9] as our basic structure to use forward and backward information of character and word embedding. We denote the two embedding sequences separately as [E c 1 , E c 2 , ..., E c n ] and [E w 1 , E w 2 , ..., E w m ]. And they are generated by a look-up layer, and are fed into two parallel Bi-LSTM structures respectively, which have the same structure, but with different parameters. The output character and the word level hidden state are represented as h c and h w . Join the two hidden layers to form a total hidden state (represented as h total = [h c , h w ]), where the front part is the characters representation, and the latter part is the words semantic feature. Then a self-attention mechanism operation is performed on the total hidden state sequence.",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 33,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Characters and Words Representation in Sequence"
        },
        {
            "text": "Solve the Problem of Boundary Segmentation. In the process of word segmentation, there may be problems with word boundary errors. As shown in the example in Fig. 1 , the first three characters may be incorrectly split into a person's name in the sentence, which would lead to error propagation, and cause severe bias effects on subsequent predictions. The previous general model can not solve the problem of word segmentation very well, and cause some content loss when combining the embedded information of characters and words. Our model combines two granular hidden states at the sequence level to preserve the original features intact. Also, the self-attention trains the weight information of the total sequence, and preserves the semantic information of the context characters to perform a calculation with the information of the word sequence structure. The character information will correct the error problem of word boundary segmentation, and correctly identify \"Zhang San\" as a person name, while the third character as a preposition.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 157,
                    "end": 163,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Multi-granularity Representation Fusion by Using Self-attention"
        },
        {
            "text": "Solve the Problem of Phrase Combination. In Chinese, long phrases are usually composed of short phrase sequences in order. For example, Beijing People's Park is composed of Beijing/ people/ park. Compared with English, there is usually no need for prepositional connections in phrases, which leads to the poor distinction of the boundaries in long phrases. It is also a difficult point in the recognition of Chinese named entities. Dictionary-based word segmentation usually divides sentences into short words. For long phrases that do not appear in the dictionary, there is currently no good solution. This paper proposes a method to improve the above problem by using two granularity semantic representations to assist each other. The model uses the self-attention mechanism to calculate the similarity on different levels of representation subspace, sequentially calculates each character with all tokens in the total sequence. This method captures the structural information of the word sequence, in order to compute similarity and correlation with character context information to further identify the combined boundaries of the long phrases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-granularity Representation Fusion by Using Self-attention"
        },
        {
            "text": "In addition, the attention mechanism uses the weighted sum calculation to generate the output, which effectively solves the problem of the gradient disappearing. And the self-attention mechanism can be calculated in parallel, which greatly improves efficiency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-granularity Representation Fusion by Using Self-attention"
        },
        {
            "text": "We quote a standard Conditional Random Field (CRF) layer on top of the attention layer. The CRF can use the state feature function and the state transfer function to maximize the characteristics of the text. Besides, it can consider the context information and the annotation information of adjacent words. The feature functions are defined as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CRF for Tag Prediction"
        },
        {
            "text": "Where s indicates the sentence we want to predict. l is the label sequence of the sentence, and l i represents the label of i-th token. i is the current location. The state transition function defines the probability of the (i \u2212 1)-th token label l i\u22121 move to the label l i of the next i-th token in the sentence s. And the state feature function indicates the probability that the current i-th token is marked as l i . Then we normalize the score to get the probability that the label sequence is l given the sentence s. Given all predicted tag sequences l, the probability of label sequence s is calculated as follows: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CRF for Tag Prediction"
        },
        {
            "text": "Where l represents all possible tag sequences. The output of the self-attention mechanism is independent of each other. Although the context information is taken into account when performing the matrix transformation, the outputs do not affect each other. Our model uses CRF for label prediction. By considering the transition characteristics between output labels, we constrain the final label and improve the accuracy of entity label prediction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CRF for Tag Prediction"
        },
        {
            "text": "We use corpora provided by Microsoft Research Asia (MSRA) and Weibo corpus [17] extracted from Sina Weibo to experiment with the model presented in this paper. MSRA contains three entity types: Person (PER), Location (LOC) and Organization (ORG). And Weibo dataset is annotated with four types of entities (in addition to the above three entities, there is also a Geo-Political entity type, GPE). We train on both name mentions and nominal mentions in the Weibo data set. The detailed statistics of the corpora are summarized in Table 1 .",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 529,
                    "end": 536,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "We preprocess the datasets and annotate the entity type using BIO rules, which indicates Begin, Inside and Outside of a named entity. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Our experiments employ character-level precision (P), recall (R), and F1-score (F) as the evaluation criteria. We use Jieba for segmentation, and utilize word embedding dataset published by Tencent AI Lab to perform embedding, and the dimension of word embedding is 200, the same as character embedding. Pytorch library is used to build our model. We train the model using an Adam optimizer with an initial learning rate of 0.001, and the network is finetuned by back-propagating. For the over-fitting and vanishing gradient problems, we employ the dropout method with a probability of 0.5. We control the length of the sentence to be 80, and the number of words after sentence segmentation to be 40. Otherwise, we would pad the shorter sequences, truncate the longer parts. Detailed hyper-parameters are listed in Table 2 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 815,
                    "end": 822,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Settings"
        },
        {
            "text": "Considering that the character-based models are not dependent on the quality of dictionaries and are more flexible, our model would use character-based output in the subsequent experiments. Ablation experiments are designed to verify the necessity of each part in our model and its impact on the experimental results. We gradually add each component to the baseline architecture BiLSTM-CRF. The results are shown in Table 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 416,
                    "end": 423,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Evaluation of Components"
        },
        {
            "text": "To evaluate the effects of two embedding approaches, we perform comparison experiments on character embedding and word embedding respectively. In the third comparative experiment, the embedding for each word and its corresponding characters compose a concatenation to be the input of the Bi-LSTM layer (as [15] did).",
            "cite_spans": [
                {
                    "start": 306,
                    "end": 310,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation of Components"
        },
        {
            "text": "Experimental results show that character-based model performance is better than word-based models on the two data sets. At the same time, the model using two embedding methods for prediction has a slight improvement compared with the original two models, but the effect is not obvious. By contrast, adding a self-attention mechanism can significantly improve the performance of NER.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation of Components"
        },
        {
            "text": "Attention can obtain sentence context information from the long-distance relationship between tokens, overcoming the limitations of recurrent neural networks. In this model, self-attention can capture the dependence of characters and words at the same time over a long distance. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation of Components"
        },
        {
            "text": "In this section, we compare our BiLSTM+Self-Attention+CRF model based on a mixture of characters and words with the previous proposed advanced models on the MSRA and Weibo data sets. The comparison results are listed in the Tables 4 and 5 . Weibo Dataset. We compare our model with the latest models on Weibo corpus. Weibo-NER is in the domain of social media. Results of named mentions, nominal mentions, and the total are demonstrated in Table 5 respectively. As there are many non-standard data in social media data, such as spelling errors, and informal words, the overall result of social media corpus is lower than that of MSRA data set. We can see that the model we proposed has achieved stateof-the-art performance. Peng et al. [16] propose joint training for embedding and achieve 56.05 F1score. Peng et al. [17] utilize word boundary tags as features to provide richer information and improve the F1-score to 58.99%. He et al. [8] propose a unified model for cross domain and improve F1-score to 58.23% from 54.82% [7] . Zhang et al. [26] investigate a lattice network which explicitly leverages word and word sequence information, and achieve F1-score of 58.79%. Our proposed model has a significant improvement in the named entities, which improves 1.96% compared with Ding et al. [5] . And overall performance is significantly better than other models.",
            "cite_spans": [
                {
                    "start": 736,
                    "end": 740,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 817,
                    "end": 821,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 937,
                    "end": 940,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1025,
                    "end": 1028,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1044,
                    "end": 1048,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1293,
                    "end": 1296,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 224,
                    "end": 238,
                    "text": "Tables 4 and 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 440,
                    "end": 447,
                    "text": "Table 5",
                    "ref_id": null
                }
            ],
            "section": "Comparison with Previous Work"
        },
        {
            "text": "From the experimental results, we can see that our model has improved on both datasets compared with previous models. On the MSRA dataset, our model has improved 0.96, and 1.96% on the Weibo dataset. Because MASA data is standard, previous studies have achieved valid results on this data set. While there are many unregistered words in the Weibo dataset, and the recognition model based on two granularity representations with self-attention can effectively improve the recognition results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MSRA"
        },
        {
            "text": "The model improves on existing approaches to reduce out-of-vocabulary and word segmentation issues by using self-attention to fuse the information of the two granularity. The word-level structure make judgment on segmentation of the common words, and character-based semantic information can make more flexible combination of phrase.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MSRA"
        },
        {
            "text": "This paper incorporates self-attention mechanism into BiLSTM-CRF neural network for Chinese named entity recognition. Our model uses self-attention to capture multi-granularity information through the total sequence, which combines the semantic and structural features of characters and words to predict entity tags. We solve the problems of word boundary segmentation and long-phrase combination, and the experimental results show that our method has improved the accuracy of Chinese named entity recognition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Future work will focus on more granular information representations, such as sentence and paragraph levels, and apply this work to specialized entity identification in a variety of areas.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Nymble: a high-performance learning name-finder",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Bikel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schwartz",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Weischedel",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Conference on Applied Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Adversarial transfer learning for Chinese named entity recognition with self-attention mechanism",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "182--192",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Chinese named entity recognition with conditional probabilistic models",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing",
            "volume": "",
            "issn": "",
            "pages": "173--176",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Natural language processing (almost) from scratch",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Collobert",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Karlen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kavukcuoglu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kuksa",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Mach. Learn. Res",
            "volume": "12",
            "issn": "",
            "pages": "2493--2537",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A neural multi-digraph model for Chinese NER with gazetteers",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Si",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1462--1467",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/P19-1141"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Character-based LSTM-CRF with radical-level features for Chinese named entity recognition",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hattori",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Di",
                    "suffix": ""
                },
                {
                    "first": "C.-Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Xue",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ICCPOL/NLPCC 2016",
            "volume": "10102",
            "issn": "",
            "pages": "239--250",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-50496-4_20"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "F-score driven max margin neural network for named entity recognition in Chinese social media",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.04234"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "A unified model for cross-domain and semi-supervised named entity recognition in Chinese social media",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural Comput",
            "volume": "9",
            "issn": "8",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1508.01991"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Efficient support vector classifiers for named entity recognition",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Isozaki",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kazawa",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "International Conference on Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lafferty",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "C"
                    ],
                    "last": "Pereira",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Neural architectures for named entity recognition",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lample",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ballesteros",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Subramanian",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kawakami",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dyer",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Towards improving neural named entity recognition with gazetteers",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Q"
                    ],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "5301--5307",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "End-to-end sequence labeling via bi-directional LSTM",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1603.01354"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Named entity recognition for Chinese social media with jointly trained embeddings",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dredze",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "548--554",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Improving named entity recognition for Chinese social media with word segmentation representation learning",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dredze",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "149--155",
            "other_ids": {
                "arXiv": [
                    "arXiv:1603.00786"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Attending to characters in neural sequence labeling models",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rei",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "K"
                    ],
                    "last": "Crichton",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pyysalo",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.04361"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Character-based joint segmentation and POS tagging for Chinese using bidirectional",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hardmeier",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tiedemann",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Nivre",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1704.01314"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Deep active learning for named entity recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yun",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "C"
                    ],
                    "last": "Lipton",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kronrod",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anandkumar",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1707.05928"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Attention Is All You Need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1706.03762"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Chinese named entity recognition with character-word mixed embedding",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xiang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management",
            "volume": "",
            "issn": "",
            "pages": "2055--2058",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A local detection approach for named entity recognition and mention detection",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Watcharawittayakul",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
            "volume": "1",
            "issn": "",
            "pages": "1237--1247",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Five-stroke based CNN-BiRNN-CRF network for Chinese named entity recognition",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "NLPCC 2018",
            "volume": "11108",
            "issn": "",
            "pages": "184--195",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-99495-6_16"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Adversarial Learning for Chinese NER from Crowd Annotations",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1801.05147"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Chinese NER using lattice LSTM",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1805.02023"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Chinese named entity recognition via joint identification and categorization",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Chin. J. Electron",
            "volume": "22",
            "issn": "2",
            "pages": "225--230",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "F"
                    ],
                    "last": "Karlsson",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1904.02141"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Neural named entity recognition using a self-attention mechanism",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zukov-Gregoric",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bachrach",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Minkovsky",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Coope",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Maksak",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)",
            "volume": "",
            "issn": "",
            "pages": "652--656",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Example of how previous models (a) and our model (b) combine two granularity representations of characters and words.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The general architecture of our proposed model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "p(l|s) = exp[s core(l|s)] l exp [score (l |s)]",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The statistics of datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Hyper-parameter settings.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Experiments of each component on MSRA and Weibo datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "features, and the latter achieve 94.4% F1-score. Zhu et al.[28] investigate a Convolution Attention Network to capture the information from adjacent characters and sentence contexts, which achieves F1-score of 92.97%. Our model utilizes self-attention on character+word hidden state and gets effective performance improvement with 95.36 F1-score.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Results on MSRA dataset.Table 5. Results on Weibo dataset.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}