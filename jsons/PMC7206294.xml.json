{
    "paper_id": "PMC7206294",
    "metadata": {
        "title": "AutoSUM: Automating Feature Extraction and Multi-user Preference Simulation for Entity Summarization",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Dongjun",
                "middle": [],
                "last": "Wei",
                "suffix": "",
                "email": "weidongjun@iie.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Yaxin",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "email": "liuyaxin@iie.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Fuqing",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "email": "zhufuqing@iie.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Liangjun",
                "middle": [],
                "last": "Zang",
                "suffix": "",
                "email": "zangliangjun@iie.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Wei",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "email": "zhouwei@iie.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Yijun",
                "middle": [],
                "last": "Lu",
                "suffix": "",
                "email": "yijun.lyj@alibaba-inc.com",
                "affiliation": {}
            },
            {
                "first": "Songlin",
                "middle": [],
                "last": "Hu",
                "suffix": "",
                "email": "husonglin@iie.ac.cn",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Semantic data enables users or machines to comprehend and manipulate the conveyed information quickly [10]. In major knowledge graphs, semantic data describes entities by Resource Description Framework (RDF) triples, referred as triples [4]. With the growth of knowledge graphs, entity descriptions are becoming extremely lengthy [23]. Since Google first released the knowledge graph, \u201cget the best summary\u201d for entities has been one of the main contributions in Google Search1 [25]. Specifically, Google Search returns a top-k subset of triples which can best describe the entity from a query on the right-hand side of the result pages [15]. Motivated by the success of Google Search, entity summarization task has received an increasing interest recently [7, 25], it aims to generate diverse, comprehensive and representative summaries for entities. In addition, entity summarization has been integrated into various applications such as document browsing, Question Answering (QA), etc. [15].",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 105,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 238,
                    "end": 239,
                    "mention": "4",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 331,
                    "end": 333,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 479,
                    "end": 481,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 638,
                    "end": 640,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 758,
                    "end": 759,
                    "mention": "7",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 761,
                    "end": 763,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 990,
                    "end": 992,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Most previous entity summarization methods are adopted from random surfer [4], clustering [9, 10] and Latent Dirichlet Allocation (LDA) [19] models, depending too much on the hand-crafted templates for feature extraction as well as human expertise for feature selection. Meanwhile, entities are capable to represent diverse information (or multi-aspect information) in knowledge graphs [21], resulting in different user preference (sometimes multi-user preference [27]). Take entity Triathlon_ at_the_2000_Summer_Olympics_Men\u2019s in DBpedia2 for instance, different users may prefer to the medal, event or type of this entity, respectively. In order to generate more diverse summaries, the specific model needs to be selected for providing a more distinguishable multi-user preference simulation [9, 21]. However, due to the countless quantities and unpredictable types of entities in real large-scale knowledge graphs, extracting discriminative features or selecting suitable models based on human expertise could be arduous [15].",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 76,
                    "mention": "4",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 91,
                    "end": 92,
                    "mention": "9",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 94,
                    "end": 96,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 137,
                    "end": 139,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 387,
                    "end": 389,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 465,
                    "end": 467,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 795,
                    "end": 796,
                    "mention": "9",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 798,
                    "end": 800,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1025,
                    "end": 1027,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, a novel integration method called AutoSUM is proposed for automatic feature extraction and multi-user preference simulation to overcome the drawbacks of above previous models. There are two modules in AutoSUM: extractor and simulator. The extractor module operates automatic feature extraction based on a BiLSTM with a combined input representation including word embeddings and graph embeddings. Meanwhile, the simulator module automates multi-user preference simulation based on a well-designed two-phase attention mechanism (i.e., entity-phase attention and user-phase attention). Experimental results demonstrate that AutoSUM produces the state-of-the-art performance on two widely used datasets (i.e., DBpedia and LinkedMDB3) in both F-measure and MAP.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Previous entity summarization methods mainly rely on human expertise. To find the most central triples, RELIN [4] and SUMMARUM [24] compute the relatedness and informativeness based on the features extracted from hand-crafted templates. Meanwhile, FACES [9] and ES-LDA [19] introduce a clustering algorithm and LDA model for capturing multi-aspect information, respectively. In order to generate more diverse summaries, the specific models need to be selected for providing a more distinguishable multi-user preference simulation [9, 19]. However, due to the countless quantities and unpredictable types of entities in the real large-scale knowledge graphs, extracting discriminative features and selecting suitable models based on human expertise could be arduous.",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 112,
                    "mention": "4",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 128,
                    "end": 130,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 255,
                    "end": 256,
                    "mention": "9",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 270,
                    "end": 272,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 531,
                    "end": 532,
                    "mention": "9",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 534,
                    "end": 536,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Recently, deep learning methods relieve the dependency on human expertise in Natural Language Processing (NLP) [17] community. To generate the summaries without human expertise, an entity summarization method with a single-layer attention (ESA) [29] is proposed to calculate the attention score for each triple. Then top-k triples which have the highest attention scores are selected as the final results. However, ESA cannot extract features and capture multi-aspect information with the single-layer attention mechanism. Following ESA work, our proposed AutoSUM automates feature extraction and multi-user preference based on a novel extractor-simulator structure. In extractor, a BiLSTM with a combined input representation is utilized for feature extraction. The word embeddings and graph embeddings are included. Meanwhile, in simulator, a two-phase attention mechanism is designed for multi-user preference simulation.",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 114,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 246,
                    "end": 248,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "An RDF triple is composed of a subject, a predicate, and an object. In major knowledge graphs, an entity of which is then defined as a subject with all predicates and corresponding objects to those predicates. When a user queries an entity in a knowledge graph, a set of triples \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\left\\{ t_{1},t_{2},\\cdots ,t_{n} \\right\\} $$\\end{document} related with the entity will be returned, referred as an entity description document d, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document} is the i-th triple in d. Following Google Search [7, 15], given a positive integer k, the summary of an entity is a top-k subset of d which can best describe the entity.",
            "cite_spans": [
                {
                    "start": 1041,
                    "end": 1042,
                    "mention": "7",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1044,
                    "end": 1046,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Problem Description ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "As shown in Fig. 1, AutoSUM has a novel extractor-simulator structure. The extractor extracts the features of triples in d as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ h=\\left\\{ h_{1}, h_{2}, \\cdots , h_{n} \\right\\} $$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_i$$\\end{document} is the feature vector of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document}. Given h, the simulator calculates the attention scores \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a = \\{a_1, a_2, \\cdots , a_n\\}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_i$$\\end{document} is the attention score of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document}. Then top-k triples with the highest attention scores will be selected as the summary of an entity.\n",
            "cite_spans": [],
            "section": "Overview ::: Proposed Model",
            "ref_spans": [
                {
                    "start": 17,
                    "end": 18,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The extractor module in AutoSUM aims at extracting features of triples automatically. In this section, we introduce the input representation and the automatic feature extraction in details.",
            "cite_spans": [],
            "section": "Extractor ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "Input Representation: As discussed above, the triples related with an entity share the same subject with different predicates and corresponding objects to those predicates. In order to map predicates and objects into a continuous vector space for feature extraction, we apply a combined input representation method including word embeddings and graph embeddings. Then we concatenate the embeddings of the predicates and corresponding objects as the representation for each triple.",
            "cite_spans": [],
            "section": "Extractor ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "Word Embedding: Learning word embeddings has been an effective method to enhance the performance of entity summarizers. In \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {ES-LDA}_{ext}$$\\end{document} [20], Pouriyeh et al. stated the key point of learning word embeddings was the definition for \u201cwords\u201d. Following Pouriyeh\u2019s work, we extract predicates and objects of triples as our words. Take \u201chttp://dbpedia.org/ontology/goldMedalist\u201d for instance, we extract \u201cgoldMedalist\u201d as the word for the above predicate. Given the embeddings of words, we then initialize a word embedding (lookup) table for future training.",
            "cite_spans": [
                {
                    "start": 430,
                    "end": 432,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Extractor ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "Graph Embedding: Obviously, simple word embeddings cannot represent triples with a graph structure. To fully encode the graph information, we utilize a graph embedding technique called TransE [3] to pretrain the whole knowledge graph in the dataset. Given the embeddings of tirples, we then initialize a graph embedding table for future training.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 194,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Extractor ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "Automatic Feature Extraction. In Named Entity Recognition (NER) task, the bidirectional LSTM (BiLSTM) has been widely used for automatic feature extraction [14]. For instance, in order to automatically extract features from a small and supervised training corpus, an LSTM-CRF model was proposed by Lample et al. [14], utilizing a BiLSTM for feature extraction and conditional random fields [13] for entity recognition. The BiLSTM extracted representative and contextual features of a word, aligning with other words in the same sentence [8]. As for summarizing entities, we also apply a BiLSTM to extract features of a triple, aligning with other triples related with the same entity. Specifically, due to the uncertain timing sequence of triples, we first map (serialize) the triples into a sequence comes randomly. Then we feed the input representation of triples in the sequence to the BiLSTM, and take the outputs as the extracted features for those triples.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 159,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 313,
                    "end": 315,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 391,
                    "end": 393,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 538,
                    "end": 539,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Extractor ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "The simulator in AutoSUM aims at simulating multi-user preference based on a well-designed two-phase attention mechanism (i.e., entity-phase attention and user-phase attention). Entity-phase attention captures multi-aspect information from an entity, user-phase attention then simulates multi-user preference based on the captured information. In this section, we present the details of entity-phase attention and user-phase attention.",
            "cite_spans": [],
            "section": "Simulator ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "Entity-Phase Attention. The intuition of entity-phase attention is straightforward. Since the single-layer attention mechanism in ESA [29] cannot capture multi-aspect information, we then design a multi-aspect attention mechanism with multiple (stacked) attention layers to overcome the drawback of ESA. One seminal work using stacked attention layers is neural machine translation (NMT) [17], where the stacked attention layers (Transformer) [26] are utilized to capture the multi-aspect information from a sentence. To our knowledge, we are the first to utilize the stacked attention layers to capture the multi-aspect information from an entity. Specifically, different attention layers capture information from an entity in different aspects. In each attention layer, a general attention function [17] is utilized to calculate the relevance between each triple and the information captured from the attention layer, termed attention scores. Here, instead of combining all attention layers to generate overall attention scores of Transformer [26], we directly output the attention scores from each attention layer for multi-user preference simulation in user-phase attention. Notice that the number of the attention layers is a hyper-parameter which can be tuned during training.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 137,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 389,
                    "end": 391,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 444,
                    "end": 446,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 802,
                    "end": 804,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1046,
                    "end": 1048,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Simulator ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "User-Phase Attention. When users browse triples, they will allocate high preference values (more attention) to triples which are more related with the information they are interested in [9]. Meanwhile, as described above, entity-phase attention consists of different attention layers for capturing information in different aspects. In each attention layer, a general attention function is utilized to allocate higher attention scores to the triples which are more relevant to the information captured from the attention layer. To simulate the preference of users who are interested in the information captured by the current attention layer, user-phase attention assigns the user preference values of each triple with the same attention scores from the attention layer. Then different distributions of attention scores in different attention layers simulate the different preference of different users (multi-user preference).",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 188,
                    "mention": "9",
                    "ref_id": "BIBREF29"
                }
            ],
            "section": "Simulator ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "After simulating the multi-user preference, we have to allocate different attention scores for different user preference rather than treating them equally. The main reason is that some user preference may represent the preference of most users for an entity, while others may represent the preference of few users for the same entity. Allocating proper attention scores for each user preference is critical to generate a more comprehensive entity summarization result. Therefore, we combine a BiLSTM with a general attention score function for allocation. In NER, a BiLSTM can maintain the independence and capture the intrinsic relationships among words [8]. Similarly, a BiLSTM is adopted in user-phase attention to preserve independence as well as capture the intrinsic relationships between different user preference. Then the outputs of the BiLSTM are taken as the inputs to a general attention score function, in order to allocate attention scores for each user preference. At last, we integrate all the user preference based on the allocated attention scores. In addition, due to the uncertain order in user preference like triples, we also randomly map the user preference into a sequence as our input of the BiLSTM.",
            "cite_spans": [
                {
                    "start": 656,
                    "end": 657,
                    "mention": "8",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Simulator ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "In this section, we demonstrate the complete pipeline of AutoSUM. As described in Sect. 3.1, the input of AutoSUM is an entity description document \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ d = \\{ t_{1}, t_{2}, \\cdots , t_{n} \\} $$\\end{document}. Here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document} is the i-th triple in d, which is composed of a same subject s, a predicate \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_i$$\\end{document} and an object \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$o_i$$\\end{document}. Given d, we first split d into a predicate set \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p= \\{ p_{1}, p_{2}, \\cdots , p_{n} \\} $$\\end{document} and an object set \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$o= \\{ o_{1}, o_{2}, \\cdots , o_{n} \\}$$\\end{document}, respectively. Given p and o, we combine word embeddings and graph embeddings to map \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_{i}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$o_{i}$$\\end{document} into a continuous vector space and concatenate them as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_{i}$$\\end{document}, recursively. Given \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e=\\{ e_{1}, e_{2}, \\cdots , e_{n} \\}$$\\end{document}, we randomly map e into a sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q = ( q_{1}, q_{2}, \\cdots , q_{n} )$$\\end{document}. Then we apply a BiLSTM to extract the features vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_{i}$$\\end{document} of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_{i}$$\\end{document} as follows,1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned}&\\overrightarrow{h_{i}} = LSTM_{L}(q_{i}, \\overrightarrow{h_{i-1}}), i \\in [ 1, n ],&\\nonumber \\\\&\\overleftarrow{h_{i}} = LSTM_{R}(q_{i}, \\overleftarrow{h_{i-1}}), i \\in [ 1, n ],&\\nonumber \\\\&\\quad h_{i}=[\\overrightarrow{h_{i}}, \\overleftarrow{h_{i}}], c = [ \\overrightarrow{c}, \\overleftarrow{c} ], \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{c}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overleftarrow{c}$$\\end{document} are the final hidden states in forward and backward LSTM networks. Given \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h=\\{ h_{1},h_{2},\\cdots ,h_{n} \\}$$\\end{document} and c, we utilize the multi-aspect attention mechanism to capture multi-aspect information. Specifically, for the j-th attention layer in multi-aspect attention mechanism, we calculate the attention score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_{j}^i$$\\end{document} for triple \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_{i}$$\\end{document} with a general score attention function as follows,2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned}&s_{j}^i = score_{j}(h_{j}, c)=h_{i}^T W_{j} c, i \\in [1,n], j \\in [1,m],&\\nonumber \\\\&\\quad \\;\\, s_{j}=[s_{j}^1, s_{j}^2, \\cdots , s_{j}^n], \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_{j}$$\\end{document} is a parameter matrix of the general attention score function in the j-th attention layer, and m is the number of attention layers in the multi-aspect attention mechanism. Given \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s=\\{s_{1}, s_{2}, \\cdots , s_{m}\\}$$\\end{document}, we then simulate the preference of the j-th user \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_j$$\\end{document} who is interested in the information of triple \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_{i}$$\\end{document} captured by the j-th attention layer as follows,3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned}&u_{j}^i = s_{j}^i, i \\in [1,n], j \\in [1,m],&\\nonumber \\\\&\\;\\; u_{j}=[u_{j}^1, u_{j}^2, \\cdots , u_{j}^n], \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{j}^i$$\\end{document} is the preference value allocated to triple \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document} by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_j$$\\end{document}. Given\n\n, we randomly map u into a sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^* = (q^*_{1}, q^*_{2}, \\cdots , q^*_{m})$$\\end{document} and utilize a BiLSTM to encode \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{j}$$\\end{document} into \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u^{*}_{j}$$\\end{document} as follows,4where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{c^{*}}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overleftarrow{c^{*}}$$\\end{document} are the final hidden states from forward and backward LSTM networks. Then we calculate the attention score for user preference as follows,5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} a^{*} = [u^{*}_{1}, u^{*}_{2}, \\cdots , u^{*}_{m}]W^{*}c^{{*}^T}, \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W^{*}$$\\end{document} is a parameter matrix of the general attention score function. Having obtained \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a^{*}$$\\end{document}, we integrate different user preference to generate the final attention score for each triple \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document} in d as follows,6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} a = Softmax([u_{1}, u_{2}, \\cdots , u_{m}]a^{*^{T}}) = [a_1,a_2,\\cdots ,a_n ]. \\end{aligned}$$\\end{document}Finally, we employ cross-entropy loss and define the loss function L for AutoSUM,7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L(a, \\overline{a}) = CrossEntropy(a, \\overline{a}). \\end{aligned}$$\\end{document}Here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{a}=\\{ \\overline{a}_{1}, \\overline{a}_{2}, \\cdots , \\overline{a}_{n} \\}$$\\end{document} is a gold(real) attention score vector associated with above entity from ESBM dataset. Specifically, we count the frequency of the i-th triple \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document} selected by users in ESBM dataset following ESA work, denoted as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ c_{i} $$\\end{document}. Then the gold attention score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{\\alpha }_{i}$$\\end{document} of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t_i$$\\end{document} is formulated as follows,8\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\overline{\\alpha }_{i}=\\frac{c_{i}}{\\sum _{i=1}^nc_{i}}. \\end{aligned}$$\\end{document}\n",
            "cite_spans": [],
            "section": "The Complete Pipeline ::: Proposed Model",
            "ref_spans": []
        },
        {
            "text": "Dataset. In this paper, we utilize ESBM dataset v1.1, which consists of 6.8k triples related with 125 entities from DBpedia [2] and 2.6k triples related with 50 entities from LinkedMDB [5]. Given an entity, ESBM asks 5 different users to select top-5 and top-10 triples which can best describe the entity. In addition, ESBM provides an evaluator for the comparison of different entity summarization methods. Both datasets and evaluator can be accessed from the ESBM website4.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 126,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 186,
                    "end": 187,
                    "mention": "5",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Experimental Setup ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Baselines. Our baselines consist of some existing state-of-the-art entity summarization methods, including RELIN [4], DIVERSUM [21], CD [30], FACES [9], LinkSUM [23], MPSUM [28] and ESA [29]. MPSUM5 is an open source implementation of ES-LDA. To provide ablation studies, we also modify the original AutoSUM into 5 different versions, denoted as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^{1\\sim 5}$$\\end{document}, which will be futher illustrated in Sect. 4.3.",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 115,
                    "mention": "4",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 128,
                    "end": 130,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 137,
                    "end": 139,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 149,
                    "end": 150,
                    "mention": "9",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 162,
                    "end": 164,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 174,
                    "end": 176,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 187,
                    "end": 189,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Experimental Setup ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Evaluation Methodology. Summarization tasks can be mainly divided into extractive and non-extractive tasks [1, 16], which orient to unstructured and structured data, respectively. Sydow et al. [22] stated that entity summarization task could be treated as an extractive task of information retrieval (IR). IR returns the most relevant documents for a query, while entity summarization selects the top-k triples related with an entity. Following previous work, we utilize F-measure and mean average precision (MAP) metrics for evaluation, which are two standard evaluation metrics in IR [12, 15]. F-measure is the harmonic mean of recall and precision, and MAP is the mean average of precision. Meanwhile, given the limited number of entities in ESBM, we conduct 5-fold cross-validation to reduce the risk of overfitting without losing the number of learning instances [11]. Specifically, the entities in ESBM are divided into 5 folds randomly. The parameters for each model are tuned on 4-of-5 folds. The final fold in each case is utilized to evaluate the optimal parameters. Since ESA has significantly better than all other state-of-the-art methods in our baselines, we then compare the statistical significance among ESA and AutoSUMs (i.e., the original AutoSUM and the modified \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^{1\\sim 5}$$\\end{document}, respectively) utilizing Student\u2019s paired t-test (p-value\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\le 0.05$$\\end{document}) [12].",
            "cite_spans": [
                {
                    "start": 108,
                    "end": 109,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 111,
                    "end": 113,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 194,
                    "end": 196,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 587,
                    "end": 589,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 591,
                    "end": 593,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 869,
                    "end": 871,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1950,
                    "end": 1952,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Experimental Setup ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Experimental Details. For experimental details, we tune the parameters on a validation set (i.e., a part of the training set). Specifically, to learn graph embeddings, we utilize TransE to pretrain the whole ESBM dataset. Here, the dimension of each triple is set to 100. As for word embeddings, we initialize the lookup table randomly, where the dimension of each word is set to 100. Then we apply a BiLSTM with a single layer in each LSTM cell for feature extraction, where the number of the layers in multi-aspect mechanism is set to 6. In addition, the graph embedding of each triple is fixed after pretraining, while all other parameters in AutoSUM are initialized randomly and tuned without weight sharing. We train the AutoSUM model for 200 epochs, and report the results of the best epoch under early stopping.\n",
            "cite_spans": [],
            "section": "Experimental Setup ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "As shown in Table 1 and 2, AutoSUM is significantly better than some existing state-of-art methods in our baselines.",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 18,
                    "end": 19,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 24,
                    "end": 25,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Comparison with Traditional Methods: Compared with traditional methods depending on manual feature extraction and multi-user preference simulation, AutoSUM automates the above processes without any human expertise effectively. The average improvement of AutoSUM over the best outperforming traditional methods is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$38\\%$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$36\\%$$\\end{document}, in terms of F-measure and MAP, respectively.\n",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Comparison with Deep Learning Methods: Compared with ESA, which calculates attention scores without feature extraction and multi-user preference, AutoSUM achieves the state-of-the-art performance. The average improvement of AutoSUM over ESA is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$26\\%$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$23\\%$$\\end{document}, in terms of F-measure and MAP, respectively.",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In addition, we track the attention scores of entity Triathlon (Triathlon_at_ the_ 2000_Summer_Olympics_Men\u2019s) in user-phase attention, as shown in Fig. 2. We can observe that the user-phase attention simulates 3 groups of user preference of the entity, and the entity-phase attention allocates high attention scores to users who prefer medal as well as event than property, which is in accordance with the preference of most users in real world.\n",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 153,
                    "end": 154,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "In this section, we provide ablation studies to demonstrate the effectiveness of the primary modules in AutoSUM.",
            "cite_spans": [],
            "section": "Ablation Studies ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {AutoSUM}^{1}{} \\mathbf{:}$$\\end{document} To evaluate the features extracted by AutoSUM, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^{1}$$\\end{document} removes the BiLSTM in extractor and feeds the input representation of triples into simulator directly. Experimental results show the original AutoSUM is significantly better than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^{1}$$\\end{document}, proving that the BiLSTM extracts high-quality features for user-preference simulation.",
            "cite_spans": [],
            "section": "Ablation Studies ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {AutoSUM}^{2}$$\\end{document}\nand\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {AutoSUM}^{4}\\mathbf{:}$$\\end{document} To explore whether the attention scores of different user preference are appropriate, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^{2}$$\\end{document} removes the BiLSTM in simulator and allocates equal attention scores for each user preference. Meanwhile, we also attempt to replace the BiLSTM with an FCN, referred as Auto-SUM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{4}$$\\end{document}. As shown in Table 1 and 2, the original AutoSUM gains a significant improvement over \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^2$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^4$$\\end{document}, indicating the BiLSTM with a general attention function allocates appropriate attention scores for each user preference. In addition, we can observe that the performance of FCN (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^{2}$$\\end{document}) is even worse than allocating equal attention scores (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^{4}$$\\end{document}) in our experiments.",
            "cite_spans": [],
            "section": "Ablation Studies ::: Experiments",
            "ref_spans": [
                {
                    "start": 1507,
                    "end": 1508,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1513,
                    "end": 1514,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {AutoSUM}^3\\mathbf{:}$$\\end{document} For comparison, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^4$$\\end{document} removes the BiLSTM in both extractor and simulator. Experimental results show that the performance of Auto-SUM\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^3$$\\end{document} is worse than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^2$$\\end{document}, which remove the BiLSTM in extractor and simulator respectively, further proving the irreplaceable role of BiLSTM in AutoSUM.",
            "cite_spans": [],
            "section": "Ablation Studies ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {AutoSUM}^5$$\\end{document} To explore whether the multi-aspect mechanism captures the multi-aspect information from an entity, we replace the multi-aspect mechanism with a single-aspect mechanism, i.e., setting the number of attention layers to 1. As shown in Table 1 and 2, we can observe that the original AutoSUM outperforms \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^5$$\\end{document} in both F-measure and MAP. Experimental results indicate that the multi-aspect attention mechanism successfully captures the multi-aspect information. We also notice that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathrm {AutoSUM}^5$$\\end{document} with a single-layer attention mechanism still outperforms all other methods in our baselines including ESA.",
            "cite_spans": [],
            "section": "Ablation Studies ::: Experiments",
            "ref_spans": [
                {
                    "start": 544,
                    "end": 545,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 550,
                    "end": 551,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "In this paper, we propose a novel integration model called AutoSUM to automate feature extraction and multi-user preference simulation for entity summarization. The performance of our proposed AutoSUM is significantly better than other state-of-the-art methods in both F-measure and MAP. Meanwhile, sufficient ablation studies are provided to demonstrate the effectiveness of each module in AutoSUM. In the future, we expect to expand the ESBM dataset and introduce the notion of AutoSUM into other applications such as recommender systems [6, 18].",
            "cite_spans": [
                {
                    "start": 541,
                    "end": 542,
                    "mention": "6",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 544,
                    "end": 546,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: F-measure comparison for top-5 and top-10 entity summarization. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow \\%$$\\end{document} is the relative improvement of AutoSUM, and (+/\u2212) is the indicator of significant improvement or degradation with respect to ESA (p-value\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\le 0.05$$\\end{document}).\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: MAP comparison for top-5 and top-10 entity summarization. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow \\%$$\\end{document} is the relative improvement of AutoSUM, and (+/\u2212) is the indicator of significant improvement or degradation with respect to ESA (p-value\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\le 0.05$$\\end{document}).\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: The architecture of AutoSUM.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: The attention scores of Triathlon_at_the_2000_Summer_Olympics_Men\u2019s.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Data summarization: a survey",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Knowl. Inf. Syst.",
            "volume": "58",
            "issn": "2",
            "pages": "249-273",
            "other_ids": {
                "DOI": [
                    "10.1007/s10115-018-1183-0"
                ]
            }
        },
        "BIBREF1": {
            "title": "Gleaning types for literals in RDF triples with application to entity summarization",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Gunaratna",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Thirunarayan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sheth",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The Semantic Web. Latest Advances and New Domains",
            "volume": "",
            "issn": "",
            "pages": "85-100",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "A novel top-N recommendation approach based on conditional variational auto-encoder",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "357-368",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Dbpedia - a crystallization point for the web of data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bizer",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "J. Web Semant.",
            "volume": "7",
            "issn": "",
            "pages": "154-165",
            "other_ids": {
                "DOI": [
                    "10.1016/j.websem.2009.07.002"
                ]
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "The notion of diversity in graphical entity summarisation on semantic knowledge graphs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sydow",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pikula",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schenkel",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J. Intell. Inf. Syst.",
            "volume": "41",
            "issn": "",
            "pages": "109-149",
            "other_ids": {
                "DOI": [
                    "10.1007/s10844-013-0239-6"
                ]
            }
        },
        "BIBREF15": {
            "title": "LinkSUM: using link analysis to summarize entity data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Thalhammer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Lasierra",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rettinger",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Web Engineering",
            "volume": "",
            "issn": "",
            "pages": "244-261",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Browsing DBpedia entities with summaries",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Thalhammer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rettinger",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "The Semantic Web: ESWC 2014 Satellite Events",
            "volume": "",
            "issn": "",
            "pages": "511-515",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "User preference-aware review generation",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H-T",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "225-236",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "RELIN: relatedness and informativeness-based centrality for entity summarization",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "The Semantic Web \u2013 ISWC 2011",
            "volume": "",
            "issn": "",
            "pages": "114-129",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "An attentive spatio-temporal neural model for successive point of interest recommendation",
            "authors": [
                {
                    "first": "KD",
                    "middle": [],
                    "last": "Doan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "CK",
                    "middle": [],
                    "last": "Reddy",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Advances in Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "346-358",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}