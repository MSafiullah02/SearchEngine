{
    "paper_id": "886442daa986fd766c25d37d43ee857db3420238",
    "metadata": {
        "title": "Identifying Notable News Stories",
        "authors": [
            {
                "first": "Antonia",
                "middle": [],
                "last": "Saravanou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "National and Kapodistrian University of Athens",
                    "location": {
                        "addrLine": "2 Bloomberg",
                        "settlement": "Athens, London",
                        "country": "Greece, UK"
                    }
                },
                "email": "antoniasar@di.uoa.gr"
            },
            {
                "first": "Giorgio",
                "middle": [],
                "last": "Stefanoni",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "National and Kapodistrian University of Athens",
                    "location": {
                        "addrLine": "2 Bloomberg",
                        "settlement": "Athens, London",
                        "country": "Greece, UK"
                    }
                },
                "email": "giorgio.stefanoni@gmail.com"
            },
            {
                "first": "Edgar",
                "middle": [],
                "last": "Meij",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "National and Kapodistrian University of Athens",
                    "location": {
                        "addrLine": "2 Bloomberg",
                        "settlement": "Athens, London",
                        "country": "Greece, UK"
                    }
                },
                "email": "emeij@bloomberg.net"
            }
        ]
    },
    "abstract": [
        {
            "text": "The volume of news content has increased significantly in recent years and systems to process and deliver this information in an automated fashion at scale are becoming increasingly prevalent. One critical component that is required in such systems is a method to automatically determine how notable a certain news story is, in order to prioritize these stories during delivery. One way to do so is to compare each story in a stream of news stories to a notable event. In other words, the problem of detecting notable news can be defined as a ranking task; given a trusted source of notable events and a stream of candidate news stories, we aim to answer the question: \"Which of the candidate news stories is most similar to the notable one?\". We employ different combinations of features and learning to rank (LTR) models and gather relevance labels using crowdsourcing. In our approach, we use structured representations of candidate news stories (triples) and we link them to corresponding entities. Our evaluation shows that the features in our proposed method outperform standard ranking methods, and that the trained model generalizes well to unseen news stories.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "A. Saravanou-This work was done whilst interning at Bloomberg.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "With the rise in popularity of social media and the increase in citizen journalism, news is increasing in volume and coverage all around the world. As a result, news consumers run the risk of either being overwhelmed due to the sheer amount of news being produced, or missing out on news stories due to heavy filtering. To deal with the information overload, it is crucial to develop systems that can filter the noise in an intelligent fashion. Due to the highly condensed language used in news, automated systems have been developed to process them and generate well-defined structured representations from their content [9] . Each structure is a so-called triple that represents an event in the form of who did what to whom, with additional metadata information about when and where this happened. Such representations (triples) form a knowledge graph (KG). There are multiple computational benefits when searching, labeling, and processing KGs due to their clean and simple structure [11, 14] . A common approach to measure notability of a news event is to track it through a proxy metric. For example, Naseri et al. [7] decide whether an article describes a notable event by counting the user interactions, while Setty et al. [10] cluster together similar news articles and then use the cluster size to decide if the common theme is notable. Wang et al. [12] propose a recommendation framework that takes as input a stream of news and predicts the user's clickthrough rate.",
            "cite_spans": [
                {
                    "start": 622,
                    "end": 625,
                    "text": "[9]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 987,
                    "end": 991,
                    "text": "[11,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 992,
                    "end": 995,
                    "text": "14]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1120,
                    "end": 1123,
                    "text": "[7]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "text": "[10]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1358,
                    "end": 1362,
                    "text": "[12]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we approach the problem of identifying notable news stories as a ranking task, i.e., we rank structured news stories represented as triples against notable events. We use Wikipedia's Current Events Portal (WCEP) [2] as curated notable events and, using a combination of textual and semantic features, we build a learning to rank (LTR) model to solve the ranking problem.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 230,
                    "text": "[2]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Let Q = [q 0 , . . . , q k ] denote a stream of events, where each query event q i \u2208 Q is a notable event composed of a textual description and of a publication date. Let C = [c 0 , . . . , c l ] denote a stream of candidate events. Each c j \u2208 C is a structured representation of a news story that consists of a triple of the form (s, p, o), where s is the subject, p is the predicate, and o is the object, together with information about the location (city, country) and the date of the news story.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statement"
        },
        {
            "text": "Given a query q i \u2208 Q and a stream of candidates C, we aim to rank each candidate c j \u2208 C by its relevance to the query q i . A pair (q i , c j ) is considered as very relevant when the information from q i and c j matches completely; it is considered as relevant when some of the information matches; otherwise, it is considered as not relevant. Table 1 shows a query q 0 and two candidates c 0 and c 1 . The pair (q 0 , c 0 ) is very relevant because c 0 matches q 0 completely; in contrast, the pair (q 0 , c 1 ) is relevant because q 0 and c 1 disagree only on the location of the event.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 347,
                    "end": 354,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Problem Statement"
        },
        {
            "text": "In this section we present our method to identify notable news stories which consists of three steps: (1) creating (query, candidate) pairs, (2) extracting textual and semantic features, and (3) training a learning to rank (LTR) model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "(1) Creating Pairs. We create the set of all possible (query, candidate) pairs where (i) the query and the candidate have the same publication date, and (ii) the query and the candidate have at least one word in common as a pair is unlikely to be relevant if they share no words.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "(2) Extracting Features. We extract a set of features for each constructed pair. Our features can be classified into three groups as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "(i) Features related to a component. We compute the size of the query or the candidate (i.e., the number of terms in the query/candidate). (ii) Features related to the pair. We calculate the Okapi BM25 score, the term frequency (TF ) and the term frequency-inverse document frequency (TF-IDF ) for the query/candidate in the pair. We calculate these scores using the stemmed versions of the query/candidate (using the Porter Stemmer [8] ). We further define a similarity score, element match, EM (q i , ele cj ) = |q i \u2229 ele cj |/|ele cj | where an element ele cj is one of the: subject, predicate, object, description of the predicate, location, and the date in the candidate c j . For each of those, we calculate the fraction of the number of common terms between the element ele cj and the query q i to the total number of terms in ele cj . In addition, we compute all EM scores using the stemmed versions of the pair components. We also extract similarity scores for combinations of elements, as for example EM (q i , subject \u2229 predicate \u2229 object) and EM (q i , city \u2229 country). (iii) Semantic features. An entity is a well-defined, meaningful and unique way to characterize a word/phrase. We therefore apply entity linking using the TagMe API [5] to identify entities (an example is shown in Table 1 ). Given the tagged query and the tagged candidate, we calculate the number of common entities using the Jaccard similarity.",
            "cite_spans": [
                {
                    "start": 433,
                    "end": 436,
                    "text": "[8]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "text": "[5]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 1297,
                    "end": 1304,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Method"
        },
        {
            "text": "(3) Ranking Pairs. We then use our features to train a learning to rank model in order to obtain a ranking of pairs. More details on the training and the tuning can be found in Sect. 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Method"
        },
        {
            "text": "For the candidate news stories, we use the Integrated Crisis Early Warning System (ICEWS) [1] dataset which contains events that are automatically extracted from news articles using TABARI [3, 9] . This system uses grammatical parsing to identify events (who did what to whom, when and where) using human-generated rules. The events are triples consisting of coded actions between socio-political actors. The actors refer to individuals, groups, sectors and nation states. The actions are coded into 312 categories. Geographical and temporal metadata are also associated with each triple (examples are shown in Table 1 ).",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 192,
                    "text": "[3,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 193,
                    "end": 195,
                    "text": "9]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 611,
                    "end": 618,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experimental Setup"
        },
        {
            "text": "In our experiments, we use the same two weeks of data from ICEWS and WCEP. We remove triples with the generic action type \"Make statement\" as they do not convey any meaningful information. We then create pairs as described in Sect. 3. We build a crowdsourcing task (see below) to get golden truth labels. From the resulting annotated dataset, we only keep queries with at least one relevant ICEWS triple as there are, e.g., sports events in the WCEP dataset but not in the ICEWS dataset. In total, the resulting dataset contains 9.1K pairs; 74 queries and 123 candidates per query on average. To evaluate our method in a real-world setting we split the dataset by date and use the first ten days for training, the next two days for validation, and the last two for testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "Golden Truth. We employ crowdsourcing on the Figure-eight platform and ask annotators to judge the relevance of each pair on a 3-point scale (very relevant, relevant, not relevant). 1 Each pair (q i , c j ) is annotated by at least three annotators and we use majority voting to obtain the gold labels. Our task obtains a inter-annotator agreement of 96.57%. Table 2 shows the distribution of relevance labels among pairs. The resulting dataset is highly skewed; with 3% annotated as very relevant, 1% as relevant, and 96% as not relevant.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 359,
                    "end": 366,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Setup"
        },
        {
            "text": "We explore various LTR algorithms and include results from Rank-Boost (RB) [6] , lambdaMART (LM) [13] , and Random Forest (RF) [4] . We experiment using different sets of features: all features (All), all except entityrelated features (All \u2212 ), selected features (Sel) and baseline features (B). Sel features include BM25 and TF-IDF scores calculated from the original/stemmed versions, EM scores for subject, predicate, object and location, and the number of entities in common and Jaccard similarity between the query and the candidate. For B features, we only consider BM25 and TF-IDF scores calculated from the original/stemmed versions of the query and the candidate. We evaluate using MAP, Precision@k, NDCG@k and MRR.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 78,
                    "text": "[6]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 97,
                    "end": 101,
                    "text": "[13]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 127,
                    "end": 130,
                    "text": "[4]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Models."
        },
        {
            "text": "In this section we discuss our experimental results and answer the following research questions. How does our method compare against the baselines? Does the performance vary with different parameter settings? Does the number of relevant pairs affect performance? Do we benefit from tagging entities?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Discussion"
        },
        {
            "text": "We compare the three LTR models on the All and B feature sets and show the results in Table 3 . Our method (using All features) achieves better results than using just the baseline B features. For each model and feature set, we only show the best tuned model on the validation set. Our method consistently outperforms all baselines, achieving 5-12% improvements on NDCG@10. These improvements are statistically significant with p \u2264 0.01 using paired t-test. We tune the parameters for each model on the validation set using NDCG@10. Figure 1 (left) shows the performance quartile plot using different parameter settings. RB and RF models show less sensitivity in the parameters tuning compared to LM. We evaluate the models when ranking pairs using all annotations (VR, R, and NR). We perform the same experiment using only the VR and NR labeled pairs. Figure 1 (right) shows that the model achieves better results when excluding the R labeled pairs. This is expected as the relevant label is very rare (only 1%, see Table 2 ) and the models tend to consider it as noise.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 86,
                    "end": 93,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 533,
                    "end": 548,
                    "text": "Figure 1 (left)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 853,
                    "end": 869,
                    "text": "Figure 1 (right)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1017,
                    "end": 1024,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Overall Performance"
        },
        {
            "text": "Our next step is to evaluate different combinations of features (All, All \u2212 , Sel, B). We show our findings in Table 4 . First, we compare our method using All \u2212 and B feature sets. We show that using the proposed features (Sect. 3) we achieve better performance for all LTR models. Second, we evaluate the performance of the models when we add the entity features by comparing All and All \u2212 . In Table 4 , we show that there is a statistically significant improvement (p \u2264 0.01) on MRR (+7%) when we add the entity-related features.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 111,
                    "end": 118,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 397,
                    "end": 404,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Overall Performance"
        },
        {
            "text": "In this section, we show examples of the output from our best performing setting, i.e., RF with All features using the VR and NR labeled pairs. We show our best and worst per-query NDCG@10 performance. The best one achieves a score of 1, which indicates that our method was able to rank all pairs in the right order. The top-1 ranked pair is the query \"At least 15 children are killed and 45 more are injured after a school bus collides with a truck in Etah, India. In summary, we provide quantitative and qualitative performance analyses of our proposed method and we conclude that learning to rank is a viable method to determine notability of news stories. Among the key steps of our method are: (i) the extraction of textual and semantic features, and (ii) the exclusion of the pairs that do not convey strong signal, i.e., the ones labeled as 'relevant'. The RF model outperforms all baselines and it is also more robust with respect to hyperparameter settings. These findings show that our approach to detect notable news through ranking is a promising one. Although our method obtains high performance (MRR = 81%), we believe we can attain further improvements by leveraging relations of the identified entities to discover implicitly relevant ones, such as <Narendra Modi, isPrimeMinisterOf, India>.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Analysis"
        },
        {
            "text": "In this paper, we present a method to rank notable news representations which leverages textual and semantic features. Our evaluation on labeled pairs from WCEP and the ICEWS shows that our method is effective. In the future, we intend to include features based on the relations of the tagged entities from external KGs, such as DBPedia and Freebase.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Wikipedia's Current Events Portal (WCEP)",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "17",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "ICEWS automated daily event data",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Boschee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lautenschlager",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "O&apos;brien",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shellman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Starz",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Harvard Dataverse",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Random forests",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Breiman",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Mach. Learn",
            "volume": "45",
            "issn": "1",
            "pages": "5--32",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1010933404324"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "TAGME: on-the-fly annotation of short text fragments (by Wikipedia entities)",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ferragina",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Scaiella",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM 2010",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "An efficient boosting algorithm for combining preferences",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Iyer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Schapire",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res",
            "volume": "4",
            "issn": "",
            "pages": "933--969",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Analyzing and predicting news popularity in an instant messaging service",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Naseri",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zamani",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "An algorithm for suffix stripping",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "F"
                    ],
                    "last": "Porter",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Readings in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Automated coding of international event data using sparse parsing techniques",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Schrodt",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Annual Meeting of the International Studies Association",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Modeling event importance for ranking daily news events",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Setty",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anand",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mishra",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anand",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Learning to explain entity relationships in knowledge graphs",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Voskarides",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Meij",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tsagkias",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "De Rijke",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Weerkamp",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ACL-IJCNLP",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "DKN: deep knowledge-aware network for news recommendation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "World Wide Web Conference",
            "volume": "2018",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Adapting boosting for information retrieval measures",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Burges",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Svore",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Inf. Retrieval",
            "volume": "13",
            "issn": "",
            "pages": "254--270",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-009-9112-1"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Fast top-k search in knowledge graphs",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE 32nd International Conference on Data Engineering (ICDE)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "(Left) Results for each model on the validation set. Each box shows the median and upper/lower quartiles. (Right) Performance using RB with selected features on two datasets.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Example of a query q0 and two candidate events c0 and c1.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Distribution of the relevance labels in the dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Main results of the LTR models on our dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Results using binary relevance labels. Date: 20 Jan. 2017 \" and the candidate <Attacker (from India), Kill by physical assault, Children (from India)> with metadata <Etah, India, 20 Jan. 2017 >. The item with the worst per-query NDCG@10 performance is \"Mexican drug lord Joaquin Guzman is extradited to the USA, where he will face charges for his role as leader of the Sinaloa Cartel. Date: 20 Jan. 2017 \" paired with the candidate <USA, Host a visit, Narendra Modi > with metadata <-, USA, 20 Jan. 2017 >.This query is about the extradition of a drug lord, while the candidate is about a visit of the Prime Minister of India. However, among the top-10 ranked candidates, the most relevant one is the triple <USA, Arrest, detain, or charge with legal action, Men (from Mexico)> with metadata <Kansas City, USA, 20 Jan. 2017 >, ranked 9th. This shows that even in the worst ranking per-query, our method ranks a relevant candidate in the top-10.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}