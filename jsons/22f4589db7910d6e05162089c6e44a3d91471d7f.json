{
    "paper_id": "22f4589db7910d6e05162089c6e44a3d91471d7f",
    "metadata": {
        "title": "A Microscopic Epidemic Model and Pandemic Pre- diction Using Multi-Agent Reinforcement Learning",
        "authors": [
            {
                "first": "Changliu",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "C. Liu is with Carnegie Mellon Uni-versity",
                    "location": {
                        "postCode": "15213",
                        "settlement": "Pittsburgh",
                        "region": "PA"
                    }
                },
                "email": "cliu6@andrew.cmu.edu"
            }
        ]
    },
    "abstract": [
        {
            "text": "This paper introduces a microscopic approach to model epidemics, which can explicitly consider the consequences of individual's decisions on the spread of the disease. We first formulate a microscopic multi-agent epidemic model where every agent can choose its activity level that affects the spread of the disease. Then by minimizing agents' cost functions, we solve for the optimal decisions for individual agents in the framework of game theory and multi-agent reinforcement learning. Given the optimal decisions of all agents, we can make predictions about the spread of the disease. We show that there are negative externalities in the sense that infected agents do not have enough incentives to protect others, which then necessitates external interventions to regulate agents' behaviors. In the discussion section, future directions are pointed out to make the model more realistic. 2 2 This paper is adapted from the lecture notes for Lectures 20 of 16-899 Adaptive Control and Reinforcement Learning (Spring 2020) at CMU. The course slides are available at https: //piazza.com/class_profile/ get_resource/k54pll5057h79m/ k8qaky3o8x65bm. The code is available at https://github.com/ intelligent-control-lab/ Microscopic_Epidemic_Model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "is R0, the regeneration number, which tells how fast the disease can spread. R0 can be regressed from data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Limitations of Existing Models Although these models are useful in predicting the spread of epidemics, they lack the granularity needed for analyzing individual behaviors during an epidemic and understanding the relationship between individual decisions and the spread of the disease. 5 For example, many countries now announced However, their effects are very different across different countries, or even across different counties in the same country. One factor that can possibly explain these differences is the cultural difference. In different cultures, individuals make different choices. For instance, in the west, people exhibit greater inertia to give up their working/life routines so that they do not follow the orders seriously. While in the east, people tend to obey the rules better. These different individual choices can result in significantly different outcomes in disease propagation that cannot be captured by a macroscopic model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In this paper, we develop a microscopic epidemic model by explicitly considering individual decisions and the interaction among different individuals in the population, in the framework of multi-agent systems. The aforementioned cultural difference can be understood as a difference in agents' cost functions, which then affect their behaviors when they are trying to minimize their cost functions. The details of the microscopic epidemic model will be explained in the next section, followed by the analysis of the dynamics of the multi-agent system, and the prediction of system trajectories using multi-agent reinforcement learning. The model is still in its preliminary form. In the discussion section, future directions are pointed out to make the model more realistic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "With the COVID-19 pandemic souring across the world, a reliable model is needed to describe the observed spread of the disease, make predictions about future, and guide public policy design to control the spread.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Existing Epidemic Models There are many existing macroscopic epidemic models. 3 For example, the SI model describes the growth of 3 Daryl J Daley and Joe Gani. Epidemic modelling: an introduction, volume 15. Cambridge University Press, 2001 infection rate as the product of the current infection rate and the current susceptible rate. The SIR model further incorporates the effect of recovery into the model, i.e., when the infected population turns into immune population after a certain period of time. The SIRS model considers the case that immunity is not for lifetime and that the immune population can become susceptible population again. In addition to these models, the SEIR model incorporates the incubation period into analysis. Incubation period refers to the duration before symptoms show up. 4 The most important factor in all those models",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 131,
                    "text": "3",
                    "ref_id": null
                },
                {
                    "start": 229,
                    "end": 240,
                    "text": "Press, 2001",
                    "ref_id": null
                },
                {
                    "start": 805,
                    "end": 806,
                    "text": "4",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Suppose there are M agents in the environment. Initially, m 0 agents are infected. Agents are indexed from 1 to M. Every agent has its own state and control input. The model is in discrete time. The time interval is set to be one day. The evolution of the infection rate for consecutive days depends on agents' actions. The questions of interest are: How many agents will eventually be infected? How fast they will be infected? How can we slow down the growth of the infection rate?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Microscopic Epidemic Model"
        },
        {
            "text": "We consider two state values for an agent, e.g., for agent i, x i = 0 means healthy (susceptible), x i = 1 means infected. Everyday, every agent i decides its level of activities u i \u2208 [0, 1]. The level of activities for agent i can be understood as the expected percentage of other agents in the system that agent i wants to meet. For example, u i = 1/M means agent i expects to meet one other agent. The actual number of agents that agent i meets depends not only on agent i's activity level, but also on other agents' activity level. For example, if all microscopic epidemic model and marl 3 other agents choose an activity level 0, then agent i will not be able to meet any other agent no matter what u i it chooses. Mathematically, the chance for agent i and agent j to meet each other depends on the minimum of the activity levels of these two agents, i.e., min{u i , u j }. In the extreme cases, if agent i decides to meet everyone in the system by choosing u i = 1, then the chance for agent j to meet with agent i is u j . If agent i decides to not meet anyone in the system by choosing u i = 0, then the chance for agent j to meet with agent i is 0.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Agent Model"
        },
        {
            "text": "Before we derive the system dynamic model, the assumptions are listed below: 6 6 These assumptions can all be relaxed in future work. They are introduced mainly for the simplicity of the discussion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Agent Model"
        },
        {
            "text": "1. In the agent model, we only consider two states: healthy (susceptible) and infected. All healthy agents are susceptible to the disease. There is no recovery and no death for infected agents. There is no incubation period for infected agents, i.e., once infected, the agent can start to infect other healthy agents. To relax this assumption, we may introduce more states for every agent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Agent Model"
        },
        {
            "text": "2. The interactions among agents are assumed to be uniform, although it is not true in the real world. In the real world, given a fixed activity level, agents are more likely to meet with close families, friends, colleagues than strangers on the street. To incorporate this non-uniformity into the model, we need to redefine the chance for agent i and agent j to meet each other to be \u03b2 i,j min{u i , u j }, where \u03b2 i,j \u2208 [0, 1] is a coefficient that encodes the proximity between agent i and agent j and will affect the chance for them to meet with each other. For simplicity, we assume that the interaction patterns are uniform in this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Agent Model"
        },
        {
            "text": "3. Meeting with infected agents will result in immediate infection. To relax this assumption, we may introduce an infection probability to describe how likely it is for a healthy agent to be infected if it meets with an infected agent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Agent Model"
        },
        {
            "text": "On day k, denote agent i's state and control as x i,k \u2208 X and u i,k \u2208 U . By definition, the agent state space is X = {0, 1} and the agent control space is U = [0, 1]. The system state space is denoted X M := X \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X . The system control space is denoted U M := U \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 U . Define m k = \u2211 i x i,k as the number of infected agents at time k. The set of infected agents is denoted:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Dynamic Model"
        },
        {
            "text": "The state transition probability for the multi-agent system is a microscopic epidemic model and marl 4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Dynamic Model"
        },
        {
            "text": "According to the assumptions, an infected agent will always remain infected. Hence the state transition probability for an infected agent i does not depend on other agents' states or any control. However, the state transition probability for a healthy agent i depends on others. The chance for a healthy agent i to not meet an infected agent j \u2208 I k is 1 \u2212 min{u i , u j }. A healthy agent can stay healthy if and only if it does not meet any infected agent, the probability of which is \u03a0 j\u2208I k (1 \u2212 min{u i , u j }). Then the probability for a healthy agent to be infected is 1 \u2212 \u03a0 j\u2208I k (1 \u2212 min{u i , u j }). From the expression \u03a0 j\u2208I k (1 \u2212 min{u i , u j }), we can infer that: the chance for a healthy agent i to stay health is higher if \u2022 the agent i limits its own activity by choosing a smaller u i ;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Dynamic Model"
        },
        {
            "text": "\u2022 the number of infected agents is smaller;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Dynamic Model"
        },
        {
            "text": "\u2022 the infected agents in I k limit their activities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Dynamic Model"
        },
        {
            "text": "The state transition probability for an agent i is summarized in table 1. Example Consider a four-agent system shown in Fig. 1 . Only agent 1 is infected. And the agents choose the following activity levels: u 1 = 0.1, u 2 = 0.2, u 3 = 0.3, u 4 = 0.4. Then the chance p i,j for agents i and j to meet with each other is p 1,2 = p 1,3 = p 1,4 = 0.1, p 2,3 = p 2,4 = 0.2, and p 3,4 = 0.3. Note that p i,j = p j,i . The chance for agents 2, 3, and 4 to stay healthy is 0.9, although they have different activity levels. Other agents are healthy. The numbers on the links denote the probability for agents to meet with each other, which depend on the chosen activity levels of different agents.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 120,
                    "end": 126,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "System Dynamic Model"
        },
        {
            "text": "Before we start to derive the optimal strategies for individual agents and analyze the closed-loop multi-agent system, we first characterize the (open-loop) multi-agent system dynamics by Monte Carlo simulation according to the state transition probability in table 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case Study"
        },
        {
            "text": "Suppose we have M = 1000 agents. At the beginning, only agent 1 is infected. We consider two levels of activities: normal activity level u and reduced activity level u * . The two activity levels are assigned to different agents following different strategies as described below. In particular, we consider \"no intervention\" case where all agents microscopic epidemic model and marl 5 continue to follow the normal activity level, \"immediate isolation\" case where the activity levels of infected agents immediately drop to the reduced level, \"delayed isolation\" case where the activity levels of infected agents drop to the reduced level after several days, and \"lockdown\" case where the activity levels of all agents drop to the reduced level immediately. The vertical axis corresponds to agent ID i. The color in the graph represents the value of x i,k , blue for 0 (healthy) and yellow for 1 (infected).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case Study"
        },
        {
            "text": "For each case, we simulate 200 system trajectories and compute the average, maximum, and minimum m k (number of infected agents) versus k from all trajectories. A system trajectory in the \"no intervention\" case is illustrated in Fig. 2 , where u = 1/M for all agents. The m k trajectories under different cases are shown in Fig. 3 , where the solid curves illustrate the average m k and the shaded area corresponds to the range from min m k to max m k . The results are explained below.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 229,
                    "end": 235,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 324,
                    "end": 330,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Case Study"
        },
        {
            "text": "\u2022 Case 0: no intervention.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case Study"
        },
        {
            "text": "All agents keep the normal activity level u. The scenarios for u = 1/M and u = 2/M are illustrated in Fig. 3 . As expected, a higher activity level for all agents will lead to faster infection. The trajectory of m k has a S shape, whose growth rate is relatively slow when either the infected population is small or the healthy population is small, and is maximized when 50% agents are infected. It will be shown in the following discussion that (empirical) macroscopic models also generate S-curves.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 102,
                    "end": 108,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Case Study"
        },
        {
            "text": "\u2022 Case 1: immediate isolation of infected agents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case Study"
        },
        {
            "text": "The activity levels of infected agents immediately drop to u * , while others remain u. The scenario for u = 1/M and u * = 0.1/M is illustrated in Fig. 3 . Immediate isolation significantly slows down the growth of the infections rate. As expected, it has the best performance in terms of flattening the curve, same as the lockdown case. The trajectory also has a S shape.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 147,
                    "end": 153,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Case Study"
        },
        {
            "text": "\u2022 Case 2: delayed isolation of infected agents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case Study"
        },
        {
            "text": "The activity levels of infected agents drop to u * after T days, while others remain u. In the simulation, u = 1/M and u * = 0.1/M. The scenarios for T = 1 and T = 2 are illustrated in Fig. 3 . As expected, the longer the delay, the faster the infection rate grows, though the growth of the infection rate is still slower than the \"no intervention\" case. Moreover, the peak growth rate (when 50% agents are infected) is higher when the delay is longer.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 185,
                    "end": 191,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Case Study"
        },
        {
            "text": "\u2022 Case 3: lockdown.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case Study"
        },
        {
            "text": "The activity levels of all agents drop to u * . The scenario for u * = 0.1/M is illustrated in Fig. 3 . As expected, it has the best perfor-microscopic epidemic model and marl 6 mance in terms of flattening the curve, same as the immediate isolation case. 7 7 In the case that infected population can be asymptomatic or have a long incubation period before they show any symptom, like what we observe for COVID-19, immediate identification of infected person and then immediate isolation is not achievable. Then lockdown is the only best way to control the spread of the disease in our model. Since the epidemic model is monotone, every agent will eventually be infected as long as the probability to meet infected agents does not drop to zero. Moreover, we have not discussed decision making by individual agents yet. The activity levels are just predefined in the simulation. We simulate the system trajectory under different infection coefficients as shown in Fig. 4 . The trajectories also have S shapes, similar to the ones in the microscopic model. However, since this macroscopic SI model is deterministic, there is no \"uncertainty\" range as shown in the microscopic model. The infection coefficient \u03b2 depends on the agents' choices of activity levels. However, there is not an explicit relationship yet. It is better to directly use the microscopic model to analyze the consequences of individual agents' choices.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 95,
                    "end": 101,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 963,
                    "end": 969,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Case Study"
        },
        {
            "text": "This section tries to answer the following question: in the microscopic multi-agent epidemic model, what is the best control strategy for individual agents? To answer that, we need to first specify the knowledge and observation models as well as the cost (reward) functions for individual agents. Then we will derive the optimal choices of agents in a distributed manner. The resulting system dynamics correspond to a Nash Equilibrium of the system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Distributed Optimal Control"
        },
        {
            "text": "A knowledge and observation model for agent i includes two aspects: what does agent i know about itself, and what does agent i know about others? The knowledge about any agent j includes the dynamic function of agent j and the cost function of agent j. The observation corresponds to run-time measurements, i.e., the observation of any agent j includes the run-time state x j,k and the run-time control u j,k . In the following discussion, regarding the knowledge and observation model, we make the following assumptions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Knowledge and Observation Model"
        },
        {
            "text": "\u2022 An agent knows its own dynamics and cost function;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Knowledge and Observation Model"
        },
        {
            "text": "\u2022 All agents are homogeneous in the sense that they share the same dynamics and cost functions. And agents know that all agents are homogeneous, hence they know others' dynamics and cost functions; 8",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Knowledge and Observation Model"
        },
        {
            "text": "\u2022 At time k, agents can measure x j,k for all j. But they cannot measure u j,k until time k + 1. Hence, the agents are playing a simultaneous game. They need to infer others' decisions when making their own decisions at any time k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Knowledge and Observation Model"
        },
        {
            "text": "We consider two conflicting interests for every agent: 9 9 The identification of these two conflicting interests is purely empirical. To build realistic cost functions, we need to either study the real world data or conduct human subject experiments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "\u2022 Limit the activity level to minimize the chance to get infected;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "\u2022 Maintain a certain activity level for living.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "We define the run-time cost for agent i at time k as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "where x i,k+1 corresponds to the first interest, p(u i,k ) corresponds to the second interest, and \u03b1 i > 0 adjusts the preference between the two interests. The function p(u) is assumed to be smooth. 10 Due to 10 The function p(u) can be a decreasing function on [0, 1], meaning that the higher the activity level, the better. The function p(u) can also be a convex parabolic function on [0, 1] with the minimum attained at some u * , meaning that the activity level should be maintained around u * .",
            "cite_spans": [
                {
                    "start": 210,
                    "end": 212,
                    "text": "10",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "our homogeneity assumption on agents, they should have identical preferences, i.e., \u03b1 i = \u03b1 for all i. Agent i chooses its action at time k by minimizing the expected cumulative cost in the future:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "where \u03b3 \u2208 [0, 1] is a discount factor. The objective function depends on all agents' current and future actions. It is difficult to directly obtain an analytical solution of (5). Later we will use multi-agent reinforcement learning to obtain a numerical solution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "In this section, to simplify the problem, we consider a single stage game 11 where the agents have zero discount of the future, i.e., \u03b3 = 0. 11 The formulation (5) corresponds to a repeated game as opposed to the single stage game. Repeated games capture the idea that an agent will have to take into account the impact of its current action on the future actions of others. This impact is called the agent's reputation. The interaction is more complex in a repeated game than that in a single stage game.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "Hence the objective function is reduced to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "which only depends on the current actions of agents. According to the state transition probability in table 1, the expected cost is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "Nash Equilibrium According to (7), the expect cost for an infected agent only depends on its own action. Hence the optimal choice for an infected agent is u i,k =\u016b := arg min u p(u). Then the optimal choice for a healthy agent satisfies:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "Note that the term 1 \u2212 (1 \u2212 min{u,\u016b}) m k is positive and is increasing for u \u2208 [0,\u016b] and then constant for u \u2208 [\u016b, 1]. Hence, the optimal solution for (9) should be smaller than\u016b = arg min u p(u). 12 Then 12 If u \u2265\u016b, then (9) becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "Since \u2202 \u2202u |\u016b J(u) > 0, the optimal solution satisfies that u <\u016b with cost J(u) < J(\u016b). Note that J(\u016b) equals to the smallest cost for the case u \u2264\u016b. Hence the optimal solution for (9) satisfies that u <\u016b.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "the objective in (9) can be simplified as 1 \u2212 (1 \u2212 u) m k + \u03b1 i p(u). In summary, the optimal actions for both the infected and the healthy agents in the Nash Equilibrium can be compactly written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "Example Consider the previous example with four agents shown in Fig. 1 . Define",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 64,
                    "end": 70,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Cost Function"
        },
        {
            "text": "microscopic epidemic model and marl 9",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cost Function"
        },
        {
            "text": "which is a monotonically decreasing function as illustrated in Fig. 5 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 63,
                    "end": 69,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Cost Function"
        },
        {
            "text": "Then the optimal actions in the Nash Equilibrium for this specific problem satisfy: Solving for (12), for infected agents, u i,k = 1. For healthy agents, the choice also depends on \u03b1 i as illustrated in Fig. 6 . We have assumed that \u03b1 i = \u03b1 which is identical for all agents. We further assume that \u03b1 < 2 such that the optimal solution for healthy agents should be u i,k = 0. The optimal actions and the corresponding costs for all agents are listed in table 2. In the Nash Equilibrium, no agent will meet each other, since all agents except agent 1 reduce their activity levels to zero. The actual cost (received at the next time step) equals to the expected cost (computed at the current time step). However, let us consider another situation where the infected agent chooses 0 activity level and all other healthy agents choose 1 activity level. The resulting costs are summarized in table 3. Obviously, the overall cost is reduced in the new situation. However, this better situation cannot be attained spontaneously by the agents, due to externality of the system which will be explained below.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 203,
                    "end": 209,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Cost Function"
        },
        {
            "text": "Actual l i,k 1 1 0 1+\u03b1 exp(\u22121) 1+\u03b1 exp(\u22121) 2,3,4 0 1 0 0 Total 1 + \u03b1 exp(\u22121) Table 3 : List of the agent decisions and associated costs in a situation better than the Nash Equilibrium in the four-agent example.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 77,
                    "end": 84,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Cost Function"
        },
        {
            "text": "For a multi-agent system, define the system cost as a summation of the individual costs:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "The system cost in the Nash Equilibrium is denoted L * k , which corresponds to the evaluation of L k under agent actions specified in (10). On the other hand, the optimal system cost is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "microscopic epidemic model and marl 10",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "The optimization problem (14) is solved in a centralized manner, which is different from how the Nash Equilibrium is obtained. To obtain the Nash Equilibrium, all agents are solving their own optimization problems independently. Although their objective functions depend on other agents' actions, they are not jointly make the decisions, but only \"infer\" what others will do. By definition, L o k \u2264 L * k . In the example above, L * k = 1 + 3\u03b1 exp(\u22121) and L o k = 1 + \u03b1 exp(\u22121). The difference L * k \u2212 L o k is called the loss of social welfare. In the epidemic model, the loss of social welfare is due to the fact that bad consequences (i.e., infecting others) are not penalized in the cost functions of the infected agents. Those unpenalized consequences are called externality. There can be both positive externality and negative externality. Under positive externality, agents are lacking motivations to do things that are good for the society. Under negative externality, agents are lacking motivations to prevent things that are bad for the society. In the epidemic model, there are negative externality with infected agents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "To improve social welfare, we need to \"internalize\" externality, i.e., add penalty for \"spreading\" the disease. Now let us redefine agent i's run-time cost asl",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "where q(\u00b7) is a monotonically increasing function. The last term x i,k q(u i,k ) does not affect healthy agents since x i,k = 0, but adds a penalty for infected agents if they choose large activity level. One candidate function for q(u) is 1 \u2212 (1 \u2212 u) m k . In the real world, such \"cost shaping\" using q can be achieved through social norms or government regulation. The expected cost becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "Suppose the function q is well tuned such that the arg min u [\u03b1 i p(u) + q(u)] = 0. Then although the expected costs for infected agents are still independent from others, their decision is considerate to healthy agents. When the infected agents choose u = 0, then for healthy agents, the expected cost becomes \u03b1 i p(u i,k ), meaning that they do not need to worry about getting infected. Let us now compute the resulting Nash Equilibrium under the shaped costs using the previous example.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "Example In the four-agent example, set q(u) = u. Then arg min u [\u03b1p(u) + u] = 0. Hence agent 1 will choose u 1,k = 0. For agents i = 2, 3, 4, they will choose u i,k = 1 since they are only minimizing p(u). The resulting costs are summarized in table 4. With the shaped costs, the microscopic epidemic model and marl 11 system enters into a better Nash Equilibrium which indeed aligns with the system optimum in (14). A few remarks:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "\u2022 Cost shaping did not increase the overall cost for the multi-agent system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "\u2022 The system optimum remains the same before and after cost shaping.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "\u2022 Cost shaping helped agents to arrive at the system optimum without centralized optimization. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dealing with Externality"
        },
        {
            "text": "We have shown how to compute the Nash Equilibrium of the multiagent epidemic model in a single stage. However, it is analytically intractable to compute the Nash Equilibrium when we consider repeated games (5). The complexity will further grow when the number of agents increases and when there are information asymmetry. Nonetheless, we can apply multi-agent reinforcement learning 13 to 13 Lucian Bu\u015foniu, Robert Babu\u0161ka, and Bart De Schutter. Multi-agent reinforcement learning: An overview.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Agent Reinforcement Learning"
        },
        {
            "text": "In Innovations in multi-agent systems and applications-1, pages 183-221. Springer, 2010",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Agent Reinforcement Learning"
        },
        {
            "text": "numerically compute the Nash Equilibrium. Then the evolution of the pandemic can be predicted by simulating the system under the Nash Equilibrium.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-Agent Reinforcement Learning"
        },
        {
            "text": "As evident from (10), the optimal action for agent i at time k is a function of x i,k and m k . Hence we can define a Q function (action value function) for agent i as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "According to the assumptions made in the observation model, all agents can observe m k at time k. For a single stage game, we have derived in (10) ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "For repeated games (5), we can learn the Q function using temporal different learning. At every time k, agent i chooses its action as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "microscopic epidemic model and marl 12",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "After taking the action u i,k , agent i observes x i,k+1 and m k+1 and receives the cost l i,k at time k + 1. Then agent i updates its Q function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "where \u03b7 is the learning gain and \u03b4 i,k is the temporal difference error. All agents can run the above algorithm to learn their Q functions during the interaction with others. However, the algorithm introduced above has several problems:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "\u2022 Exploration and limited rationality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "There is no exploration in (18). Indeed, Q-learning is usually applied together with -greedy where with probability 1 \u2212 , the action u i,k is chosen to be the optimal action in (18), and with probability , the action is randomly chosen with a uniform distribution over the action space. The -greedy approach is introduced mainly from an algorithmic perspective to improve convergence of the learning process. When applied to the epidemic model, it has a unique societal implication. When agents are randomly choosing their behaviors, it represents the fact that agents have only limited rationality. Hence in the learning process, we apply -greedy as a way to incorporate exploration for faster convergence as well as to take into account limited rationality of agents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "\u2022 Data efficiency and parameter sharing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "Keeping separated Q functions for individual agents is not data efficient. An agent may not be able to collect enough samples to properly learn the desired Q function. Due to the homogeneity assumptions we made earlier about agents' cost functions, it is more data efficient to share the Q function for all agents. Its societal implication is that agents are sharing information and knowledge with each other. Hence, we apply parameter sharing 14 as a way to 14 Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems, pages 66-83. Springer, 2017 improve data efficiency as well as to consider information sharing among agents during the learning process. 15 15 In a more complex situation where agents are not homogeneous, it is desired to have parameter sharing with a smaller group of agents, instead of parameter sharing will all agents.",
            "cite_spans": [
                {
                    "start": 459,
                    "end": 461,
                    "text": "14",
                    "ref_id": null
                },
                {
                    "start": 796,
                    "end": 798,
                    "text": "15",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "With the above modifications, the multi-agent Q learning algorithm 16 is summarized below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "16 Junling Hu and Michael P Wellman. Nash Q-learning for general-sum stochastic games. Journal of machine learning research, 4(Nov): 2003 \u2022 For every time step k, agents choose their actions as:",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 137,
                    "text": "2003",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "\u2022 At the next time step k + 1, agents observe the new states x i,k+1 and receive rewards l i,k for all i. Then the Q function is updated:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "Example In this example, we consider M = 50 agents in the system. Only one agent is infected in the beginning. The run-time cost is the same as in the example in the distributed optimal control section, i.e., l i,k = x i,k+1 + \u03b1 exp( 1 u i,k \u22121 ) where \u03b1 is chosen to be 1. For simplicity, the action space is discretized to be {0, 1/M, 10/M}, called as low, medium, and high. Hence the Q function can be stored as a 2 \u00d7 M \u00d7 3 matrix. In the learning algorithm, the learning rate is set to \u03b7 = 1. The exploration rate is set to decay in different episodes, i.e., = 0.5(1 \u2212 E/ max E) where E denotes the current episode and the maximum episode is max E = 200. The Q function is initialized to be 10 for all entries. Three different cases are considered. For each case, we illustrate the Q function learned after 200 episodes as well as the system trajectories for episodes 10, 20, . . . , 200, blue for earlier episodes and red for later episodes. The results are shown in Fig. 7 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 972,
                    "end": 978,
                    "text": "Fig. 7",
                    "ref_id": null
                }
            ],
            "section": "Q Learning"
        },
        {
            "text": "\u2022 Case 1: discount \u03b3 = 0 with runtime cost l i,k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "With \u03b3 = 0, this case reduces to a single stage game as discussed in the distributed optimal control section. The result should align with the analytical Nash Equilibrium in (10). As shown in the left plot in Fig. 7(a) , the optimal action for a healthy agent is always low (solid green), while the optimal action for an infected agent is always high (dashed magenta). The Q values for infected agents do not depend on m k . The Q values for healthy agents increase when m k increases if the activity level is not zero, due to the fact that: for a fixed activity level, the chance to get infected is higher when there are more infected agents in the system. All these results align with our previous theoretical analysis. Moreover, as shown in the right plot in Fig. 7(a) , the agents are learning to flatten the curve across different episodes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 209,
                    "end": 218,
                    "text": "Fig. 7(a)",
                    "ref_id": null
                },
                {
                    "start": 762,
                    "end": 771,
                    "text": "Fig. 7(a)",
                    "ref_id": null
                }
            ],
            "section": "Q Learning"
        },
        {
            "text": "\u2022 Case 2: discount \u03b3 = 0.5 with runtime cost l i,k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "Since the agents are now computing cumulative costs as in (5), the corresponding Q values are higher than those in case 1. However, the optimal actions remain the same, low (solid green) for healthy agents, high (dashed magenta) for infected agents, as shown in the left plot in Fig. 7(b) . The trends of the Q curves also remain the same: the Q values do not depend on m k for infected agents and for healthy agents whose activity levels are zero. However, as shown in the right plot in Fig. 7(b) , the agents learned to flatten the curve faster than in case 1, mainly because healthy agents are more cautious (converge faster to low activity levels) when they start to consider cumulative costs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 279,
                    "end": 288,
                    "text": "Fig. 7(b)",
                    "ref_id": null
                },
                {
                    "start": 488,
                    "end": 497,
                    "text": "Fig. 7(b)",
                    "ref_id": null
                }
            ],
            "section": "Q Learning"
        },
        {
            "text": "\u2022 Case 3: discount \u03b3 = 0.5 with shaped runtime costl i,k in (15).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Q Learning"
        },
        {
            "text": "The shaped cost changes the optimal actions for all agents as well as the resulting Q values. As shown in the left plot in Fig. 7(c) , the optimal action for an infected agent is low (dashed green), while that for a healthy agent is high (solid magenta) when m k is small and low (solid green) when m k is big. Note that when m k is high, the healthy agents still prefer low activity level, though the optimal actions for infected agents are low. That is because: due to the randomization introduced in -greedy, there is still chance for infected agents to have medium or high activity levels. When m k is high, the healthy agents would rather limit their own activity levels to avoid the risk to meet with infected agents that are taking random actions. This result captures the fact that agents understand others may have limited rationality and prefer more conservative behaviors. We observe the same trends for the Q curves as the previous two cases: the Q values do not depend on m k for infected agents and for healthy agents whose activity levels are not zero. In terms of absolute values, the Q values for infected agents are higher than those in case 2 due to the additional cost q(u) inl i,k . The Q values for healthy agents are smaller than those in case 2 for medium and high activity levels, since the chance to get infected is smaller as infected agents now prefer low activity levels. The Q values remain the same for healthy agents with zero activity levels. With shaped costs, the agents learned to flatten the curve even faster than in case 2, as shown in the right plot in Fig. 7(c) , since the shaped cost encourages infected agents to lower their activity levels.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 123,
                    "end": 132,
                    "text": "Fig. 7(c)",
                    "ref_id": null
                },
                {
                    "start": 1593,
                    "end": 1602,
                    "text": "Fig. 7(c)",
                    "ref_id": null
                }
            ],
            "section": "microscopic epidemic model and marl 14"
        },
        {
            "text": "Agents vs humans The epidemic model can be used to analyze realworld societal problems. Nonetheless, it is important to understand the differences between agents and humans. We can directly design and shape the cost function for agents, but not for humans. For agents, their behavior is predictable once we fully specify the problem (i.e., cost, dynamics, measurement, etc). Hence we can optimize the design (i.e., the cost function) to get desired system trajectory. For humans, their behavior is not fully predictable due to limited rationality. We need to constantly modify the knowledge and observation model as well as the cost function to match the true human behavior.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "Future work The proposed model is in its preliminary form. Many future directions can be pursued.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "\u2022 Relaxation of assumptions. We may add more agent states to consider recovery, incubation period, and death. We may consider the fact that the interaction patterns among agents are not uniform. We may consider a wide variety of agents who are not homogeneous. For example, health providers and equipment suppliers are key parts in fighting the disease. They should receive lower cost (higher reward) for maintaining or even expanding their activity levels than ordinary people. Their services can then lead to higher recovery rate. In addition, we may relax the assumptions on agents' knowledge and observation models, to consider information asymmetry as well as partial observation. For example, agents cannot get immediate measurement whether they are infected or not, or how many agents are infected in the system.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "\u2022 Realistic cost functions for agents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "The cost functions for agents are currently hand-tuned. We may learn those cost functions from data through inverse reinforcement learning. Those cost functions can vary for agents from different countries, different age groups, and different occupations. Moreover, the cost functions carry important cultural, demographical, economical, and political information. A realistic cost function can help us understand why we observe significantly different outcomes of the pandemic around the world, as well as enable more realistic predictions into the future by fully considering those cultural, demographical, economical, and political factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "\u2022 Incorporation of public policies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "For now, the only external intervention we introduced is cost shaping. We may consider a wider range of public policies that can change the closed-loop system dynamics. For example, shut-down of transportation, isolation of infected agents, contact tracing, antibody testing, etc.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "\u2022 Transient vs steady state system behaviors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "We have focused on the steady state system behaviors in the Nash Equilibrium. However, as agents live in a highly dynamic world, it is not guaranteed that a Nash Equilibrium can always be attained. While agents are learning to deal with unforeseen situations, there are many interesting transient dynamics, some of which is captured in Fig. 7, i.e., agents may learn to flatten the curve at different rates. Methods to understand and predict transient dynamics may be developed in the future.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 336,
                    "end": 343,
                    "text": "Fig. 7,",
                    "ref_id": null
                }
            ],
            "section": "Discussion and Future Work"
        },
        {
            "text": "\u2022 Validation against real world historical data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Future Work"
        },
        {
            "text": "To use the proposed model for prediction in the real world, we need to validate its fidelity again the historical data. The validation can be performed on the m k trajectories, i.e., for the same initial condition, the predicted m k trajectories should align with the ground truth m k trajectories.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "microscopic epidemic model and marl 17"
        },
        {
            "text": "This paper introduced a microscopic multi-agent epidemic model, which explicitly considered the consequences of individual's decisions on the spread of the disease. In the model, every agent can choose its activity level to minimize its cost function consisting of two conflicting components: staying healthy by limiting activities and maintaining high activity levels for living. We solved for the optimal decisions for individual agents in the framework of game theory and multi-agent reinforcement learning. Given the optimal decisions of all agents, we can make predictions about the spread of the disease. The system had negative externality in the sense that infected agents did not have enough incentives to protect others, which then required external interventions such as cost shaping. We identified future directions were pointed out to make the model more realistic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Estimating the impact of public and private strategies for controlling an epidemic: A multi-agent approach",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Christopher",
                    "suffix": ""
                },
                {
                    "first": "Keith",
                    "middle": [],
                    "last": "Barrett",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [],
                    "last": "Bisset",
                    "suffix": ""
                },
                {
                    "first": "Achla",
                    "middle": [],
                    "last": "Leidig",
                    "suffix": ""
                },
                {
                    "first": "Madhav",
                    "middle": [],
                    "last": "Marathe",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Marathe",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Twenty-First IAAI Conference",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Multi-agent reinforcement learning: An overview",
            "authors": [
                {
                    "first": "Lucian",
                    "middle": [],
                    "last": "Bu\u015foniu",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Babu\u0161ka",
                    "suffix": ""
                },
                {
                    "first": "Bart",
                    "middle": [
                        "De"
                    ],
                    "last": "Schutter",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Innovations in multi-agent systems and applications-1",
            "volume": "",
            "issn": "",
            "pages": "183--221",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Epidemic modelling: an introduction",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Daryl",
                    "suffix": ""
                },
                {
                    "first": "Joe",
                    "middle": [],
                    "last": "Daley",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gani",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "15",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Cooperative multi-agent control using deep reinforcement learning",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Jayesh",
                    "suffix": ""
                },
                {
                    "first": "Maxim",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "Mykel",
                    "middle": [],
                    "last": "Egorov",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kochenderfer",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Autonomous Agents and Multiagent Systems",
            "volume": "",
            "issn": "",
            "pages": "66--83",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Nash Q-learning for generalsum stochastic games",
            "authors": [
                {
                    "first": "Junling",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Michael P Wellman",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Journal of machine learning research",
            "volume": "4",
            "issn": "",
            "pages": "1039--1069",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "SEIR epidemic model with delay",
            "authors": [
                {
                    "first": "Ping",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "Shengqiang",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "The ANZIAM Journal",
            "volume": "48",
            "issn": "1",
            "pages": "119--134",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Example of a four-agent system. Agent 1 is infected (shaded).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Illustration of the result of one Monte Carlo simulation when all agents have the activity level u = 1/M. The horizontal axis corresponds to day k.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The growth of the infection rate under different activity levels of agents. For every scenario, the result is extracted from 200 Monte Carlo simulations. The horizontal axis corresponds to day k. The vertical axis corresponds number of infected agents m k . The solid curves are the average m k and the shaded area corresponds to the range from min m k to max m k . Remark The model we introduced is microscopic, in the sense that interactions among individual agents are considered. The simulated open-loop trajectories are indeed similar to those from a macroscopic model. Since only susceptible and infected populations are considered in the proposed microscopic model, we then compare it with the macroscopic Susceptible-Infected (SI) model. Define the state s \u2208 [0, 1] as the fraction of infected population. The growth of the infected population is proportional to the susceptible population and the infected population. Suppose the infection coefficient is \u03b2, the system dynamics in the SI model follow:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The system trajectories in the macroscopic SI model. The horizontal axis corresponds to days. The vertical axis corresponds to the infection rate s.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Illustration of the curve exp( 1 u\u22121 ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Illustration of the objective function in (12) under different conditions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Case 1 with discount \u03b3 = 0 and the original runtime cost. Case 2 with discount \u03b3 = 0.5 and the original runtime cost. Case 3 with discount \u03b3 = 0.5 and the shaped runtime cost.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "The state transition probability from x i,k to x i,k+1 for an agent.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "List of the agent decisions and associated costs in the Nash Equilibrium in the four-agent example.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "List",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Results in the multi-agent Q learning under the microscopic epidemic model. The left plots show the learned Q values after 200 episodes.The horizontal axis corresponds to m k . The vertical axis corresponds to the Q values. Solid curves are for healthy agents x i,k = 0. Dashed curves are for infected agents x i,k = 1. Green curves are for low activity levels u i,k = 0. Purple curves are for medium activity levels u i,k = 1/M. Magenta curves are for high activity levels u i,k = 10/M. The right plots illustrate the system trajectories for episodes 10, 20, . . . , 200, blue for earlier episodes and red for later episodes. In the last episode in all cases where there is no exploration = 0, the system trajectories are horizontal with m k \u2261 1.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}