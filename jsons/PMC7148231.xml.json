{
    "paper_id": "PMC7148231",
    "metadata": {
        "title": "Utilising Information Foraging Theory for User Interaction with Image Query Auto-Completion",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Amit",
                "middle": [
                    "Kumar"
                ],
                "last": "Jaiswal",
                "suffix": "",
                "email": "amitkumar.jaiswal@beds.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Haiming",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "email": "haiming.liu@beds.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Ingo",
                "middle": [],
                "last": "Frommholz",
                "suffix": "",
                "email": "ingo.frommholz@beds.ac.uk",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Query auto-completion (QAC) is an action of signalling full queries once the user starts typing a prefix of a few characters that eases user query compositions [4]. It is also termed as (dynamic) query suggestion [17], query completion [35] and real-time query expansion [37]. Popular features such as QAC make people more dependent on search engines to find any relevant information. However, such kind of factor lets users express their queries only ambiguously, which are then overly vague to be completely interpreted by search engines. This makes query auto-completion a bottleneck construct in the usability of search engines [5]. Also, users often apply several rounds of search to reformulate their queries further to adhere to their information needs given they find some relevant results. Past work [6, 20] demonstrated the use of information scent to model users\u2019 information need during web search, and it has been used to understand the factors affecting search and what takes a user to stop the search. Despite the good observation, the exploitation of information scent (from Information Foraging Theory [27]) is under-explored in case of ambiguous queries and have not been extended to take into account an image in query expansion (or suggestion) tasks. For the users\u2019 convenience, current search engines generally endue query suggestions for them in order to describe their queries more explicitly. They have been explored extensively in query auto-completion tasks, especially the traditional approach known as Most Popular Completion (MPC) [3] which at the extreme is incapable of anticipating a query it has never seen before. Solutions further improved by recent semantically-driven models [23, 24] and neural model [26] approaches which are the current state-of-the art in QAC. However, most of the language embedding models [13] have obtained strong results on multiple benchmarks for understanding the polarity of word compositions. Unsupervised pre-trained natural language embeddings [7, 21] successfully model long term dependencies with the purpose of predicting masked terms and assessing if sentences ensue one another, which showed strong results on several natural language processing and information retrieval tasks. Empirically, recent advances in sequence models have been adapted to span a prefix to full text and index [12] but despite the attainment, it has not been generalised to take an image into account. Also, deep neural networks are mature enough and capable of segmenting regions within an image [9, 10].",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 162,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 214,
                    "end": 216,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 237,
                    "end": 239,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 272,
                    "end": 274,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 633,
                    "end": 634,
                    "mention": "5",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 810,
                    "end": 811,
                    "mention": "6",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 813,
                    "end": 815,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1120,
                    "end": 1122,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1561,
                    "end": 1562,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1713,
                    "end": 1715,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1717,
                    "end": 1719,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1739,
                    "end": 1741,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1849,
                    "end": 1851,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 2012,
                    "end": 2013,
                    "mention": "7",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 2015,
                    "end": 2017,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 2358,
                    "end": 2360,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 2545,
                    "end": 2546,
                    "mention": "9",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 2548,
                    "end": 2550,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To address the above mentioned gaps, we move one step forward to present a method that extends and modifies the state-of-the-art approaches in query completion and text embedding. We apply our ideas to an image search scenario where we assume patches are regions of images that are relevant to the user\u2019s information need. Our work is concerned with providing users of image search engines with a useful query suggestion (via a visually-oriented patch form) during interaction, to further amplify their exploratory search experience. Hence, finding useful patches for query expansion in an image based on textual queries (or descriptions) is the primary focus of our work. Past work [11, 30] used both the query and image for typical retrieval and segmentation tasks. In our task formulation, we rely only upon a given arbitrary text prefix rather than having the entire text query which is used to perform search based on the image and supported by a modified deep language model [12] to find the most relevant patch in the image. We break down the task into three sub-tasks: (a) completing the query from user query prefix and an image; (b) finding patch probabilities based on the complete user query, and (c) aligning and segmenting all patches in the image. We summarise our contributions of this paper as follows: To the best of our knowledge, we are the first to present a method for image query auto-completion where a user query prefix is adapted upon an image.We elaborate the analogy of query auto-completion based on Information Foraging Theory and propose an explainable strategy for the observed challenges of query formulation and the varying users\u2019 information need.We propose iBERT inspired by [7] to compute probabilities of patches and rank them efficiently in the image.\n\n",
            "cite_spans": [
                {
                    "start": 684,
                    "end": 686,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 688,
                    "end": 690,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 982,
                    "end": 984,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1712,
                    "end": 1713,
                    "mention": "7",
                    "ref_id": "BIBREF36"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "This section details a brief overview of query auto-completion, image search suggestion, Information Foraging Theory and BERT pre-trained language embedding model. We will investigate the latter approach experimentally in the following section.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Query Auto-Completion: Query auto-completion is an important aspect for information retrieval systems which allow it to predict what could be the next character (or query item) right after the first key was pressed by a user. The predictions in IR systems are generally driven by the query logs (or query history) which are the factual queries that users have previously entered as they were trying to satisfy their information need [14, 37]. [3] introduced a method called NearestCompletion that addresses the situation of \u201ccontext\u201d which depicts the users\u2019 preceding queries in suggestion-based IR systems. The authors\u2019 proposed MPC mechanism relies on the entire popularity of the queries conforming to the provided prefix. Recent work reported in [15] studies user reformulation behaviour by leveraging textual features, whereas [31] introduced personalised query auto-completion and found that utilising a user\u2019s long-term search logs and locations as well as both context-based textual features and demographic features is more effective. More recent advances in QAC using neural language models are proposed in [26] using recurrent neural networks that effectuate the performance on immediately unseen queries. A generalised and adaptable language model for personalised QAC is introduced in [12]. We extend this adaptable language model to query completion in an image search scenario in the following section.",
            "cite_spans": [
                {
                    "start": 434,
                    "end": 436,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 438,
                    "end": 440,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 444,
                    "end": 445,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 752,
                    "end": 754,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 834,
                    "end": 836,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1119,
                    "end": 1121,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1300,
                    "end": 1302,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Query Suggestion in Image Search: Query suggestion and query completion differs in their end goal in which the former search aspect outputs a list of ranked queries against an input query, whereas the latter search aspect outputs queries with the first few characters (or text) similar to the user\u2019s input. Recent work [39] introduced a learning-based personalised suggestion framework for query suggestion which uses both visual and textual queries. Their work uses users\u2019 click-through data. A new paradigm of attention-based mechanisms for referring expressions in image segmentation [30] is proposed which contains a keyword-aware network and query attention model that demonstrates the relationships with various image regions for a given query. Inspired by the idea of attention models, we modify this mechanism for patch alignments within images via information scent in the following section.",
            "cite_spans": [
                {
                    "start": 320,
                    "end": 322,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 588,
                    "end": 590,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Information Foraging Theory: Information Foraging Theory (IFT) [27] is a theoretical framework for understanding information access behaviour, derived from the ecological science concept of optimal foraging theory which applies to how humans access information. IFT stands on three different models, namely information scent model, information patch model and information diet model, which can illustrate users\u2019 search preferences and behaviours [19]: (1) The information within a certain environment scattered in form of patches (images, text snippets, documents) consisting of information features (colors, words) refers to the information patch model; (2) A user can go from one patch to another via a cue (e.g., typing a query by following perceptual or heuristic cues [32]), which meets the user\u2019s information need. The goal of such cues is to characterise the contents that will be envisaged by trailing the links, which refers to the information scent model; (3) Different types of information sources will vary in their information access costs. Users will assess the information sources based on information gain per unit cost or varied profitability, and then the users will narrow or expand diversities of information sources based on their profitability. This user behaviour refers to the information diet model.",
            "cite_spans": [
                {
                    "start": 64,
                    "end": 66,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 447,
                    "end": 449,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 774,
                    "end": 776,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "One of the main IFT concepts are information patches. For instance, sections and their associated features in search engine results can be considered patches. From a foraging perspective in image search, the searcher is the predator (or forager [38]), the information patch is any segment or a region within an image (or image itself) in a given information environment. The piece of information a user is looking for is the prey, and the consumed (or gained) information is the information diet. Something on the user interface that informs users about a specific place they should look next is referred to as a cue of the information scent.",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 248,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Language Embeddings: Nowadays, many information retrieval or natural language processing tasks rely on language embeddings, such as word2vec [22], Glove1, and fastText2. They use vector word embeddings for word representation to transform a distinct space of human language into a continuous space, which will be further processed usually through a neural network. In query auto-completion, embeddings have been employed for distributed representation of queries based on a convolutional latent semantic model [23]. Word embeddings have been used to compute query similarity for query auto-completion [29], incorporating the features with the Most Popular Completion model. Very recent work [7] introduced a pre-trained deep language model known as BERT which has shown promising results on several IR and natural language processing tasks. However, it is still not well-explored how to leverage such pre-trained language models for QAC, which poses certain challenges both regarding the task and training. Based on this work, we describe our proposed BERT-based model for computing patch probabilities in the following section.",
            "cite_spans": [
                {
                    "start": 142,
                    "end": 144,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 511,
                    "end": 513,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 602,
                    "end": 604,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 692,
                    "end": 693,
                    "mention": "7",
                    "ref_id": "BIBREF36"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The challenge of query auto-completion is to predict and generate queries from prefixes that have never been seen in the training set. An initial attempt using neural language models has been introduced in [33]. The benefit of using character-level neural language models is providing more fine-grained predictions but they suffer from the semantic understanding that word-level models provide. For a prefix that has not been seen before (such as an incomplete word), their model enriches the shared information among comparable prefixes to create prediction nonetheless. In our scenario, we are given a prefix to complete a query conditioned on an image. To solve this new QAC problem, we exploit and extend the Long Short-Term Memory (LSTM) language model [12] with combined input and forget gates to auto complete queries. The language model is made up of a single-layer character-level LSTM with layer normalisation [2]. Our extension and modification to this language model is that we replace user embeddings with a low-dimensional representation of images. We adapt this LSTM language model alongside a context-dependent weight matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {W}$$\\end{document} replaced by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {W_C} = \\mathbf {W} + \\mathbf {M_A}$$\\end{document}. We are providing a character embedding \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_{c}\\in \\mathbb {R}^e$$\\end{document}, a preceding hidden state \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_{c-1}\\in \\mathbb {R}^h$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {M_A}$$\\end{document} is the adaptation matrix constructed by the product (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times _i$$\\end{document} denotes the i-th-order tensor product) of the context c with two basis tensors, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {T_L}\\in \\mathbb {R}^{u\\times (e+h)\\times v}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {T_R}\\in \\mathbb {R}^{v\\times h\\times u}$$\\end{document}. Alternatively, the two basis tensors i.e., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {T_L}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {T_R}$$\\end{document} are re-shaped to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbb {R}^{u\\times (v(e+h))}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbb {R}^{vh\\times u}$$\\end{document}. So the next predicted hidden state and the adaptation matrix can be equated as follows:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} h_{c} = \\sigma ([w_c, h_{c-1}]\\mathbf {W_{C}} + b) \\\\ \\mathbf {M_A} = (c\\times _1 \\mathbf {T_L} )(\\mathbf {T_R}\\times _3 c) \\end{aligned} \\end{aligned}$$\\end{document}We combine the context-driven weight matrix and the immediate preceding hidden state followed by the generated adaptation matrix which able to alter each query completion to be personalised to a particular image representation. We perform feature extraction on an input image using a Convolutional Neural Network (CNN) trained on ImageNet (pre-trained CNN), where we retrain only the last two fully connected layers shown in Fig. 2. The generated image feature vector is then fed into the LSTM language model via the adaptation matrix. We apply beam search decoding [34] in the generated array of predicted characters to select the optimal completion for the user query prefix.",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 209,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 759,
                    "end": 761,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 921,
                    "end": 922,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 6290,
                    "end": 6292,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "Image Query Auto Completion ::: Our Model",
            "ref_spans": [
                {
                    "start": 6153,
                    "end": 6154,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "We describe our approach to compute the probability of image patches which addresses an important aspect of query auto-completion systems. We assume that during the search process, users are typically interested in some part of the image as well as the image itself if it matches the mental picture of their belief [36]. Our work focuses on a new perspective of query auto-completion on images and the proposed model finds image patches which match the user context based on the query prefix using Eq. (1). BERT (Bidirectional Encoder Representations from Transformers) [7] shows promising results in multiple tasks of natural language processing and information retrieval [25] and is presently the state-of-the-art embedding model. We propose to fine-tune the BERT model as a transfer learning task for patch selection, using images composed of several patches (regions of an image), hence the name iBERT3. To the best of our knowledge, BERT has not yet been retraced for the QAC task. We use the BERT embedding model, which has a twelve layer implementation, extending it by adding a dense layer with 10% dropout which then is mapped to the final pooled layer connected the object class, and which outputs patch probabilities as shown in Fig. 2.",
            "cite_spans": [
                {
                    "start": 316,
                    "end": 318,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 571,
                    "end": 572,
                    "mention": "7",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 674,
                    "end": 676,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "iBERT - BERT for Patch Probability ::: Our Model",
            "ref_spans": [
                {
                    "start": 1245,
                    "end": 1246,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Our goal of using Information Foraging Theory [27] from a cognitive viewpoint is to find explanations for the observed behaviour in query auto-completion and to model the information need within query sessions. IFT postulates that the human information seekers follow an information scent to navigate from one information region to another in an information environment that is instinctively patchy in nature, and from one information patch to another within a region. IFT implies that foragers adapt their behaviour to the structure of the information environment in which they prevail such that the entire system (encompassing the information seeker, the information environment, and the interactions among these two) tries to maximise the ratio of the expected value of the information gained to the total cost of the interaction. Following the IFT analogy, when users start typing a prefix to auto-complete, their perceptual cues (such as mental beliefs [36]) either allow them to type the next character or to access the provided suggestion (under the query field) which acts as a distal cue and visually inspires the user to acquire them instantly to forage or seek. Query auto-completion, from an IFT perspective as query-level user interaction, is initiated by the user typing as little as a single-character query prefix. The user then may follow suggestions in case a completion is generated (which again follows the earlier mentioned strategy). In case the query prefix is unknown to the system (e.g. by being entered for the first time) the information scent associated with a result might be too poor [6] to immediately infer information needs. In this case we are applying beam search to generate the query based on image features. Suggestions are based on information scent values as described in the following subsection. These query suggestions represent the diversity of information scent patterns which elicits a varied distribution of relevant queries in the search field.",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 49,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 959,
                    "end": 961,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1615,
                    "end": 1616,
                    "mention": "6",
                    "ref_id": "BIBREF35"
                }
            ],
            "section": "Information Foraging Explanation ::: Our Model",
            "ref_spans": []
        },
        {
            "text": "Patch Selection. This section describes the foraging-based strategy for patch selection. The technicalities of ranking patches (after patch selection) in the image (from image search results) are illustrated in Sect. 3.2. We utilise IFT to infer the user\u2019s information need utilising the Inferring User Need by Information Scent (IUNIS) algorithm [6] which was proposed to weigh each page vector along with the two factors i.e., TF-IDF weight and time, that were used to quantify the associated information scent with the page. In our image search scenario, we have images as search results where an image is considered as a set of patches containing features such as color, shape, texture, etc. In our proposed iBERT model, we use information scent to inspect patches based on image features and select patches which have higher probability estimated by the iBERT model. Probabilistic Patch Selection Model (PPSM) is a first attempt to reflect users\u2019 information need coherently by means of information scent. PPSM is used for a task that extends finding patches and makes the quantification of semantic uncertainties an important choice in selection. The important requirement for PPSM is a model (iBERT) that identifies patches in an image which are relevant to the user\u2019s information need (query). Inspired by the concept of TF-IDF in IR, we represent the categorical distribution of frequency (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_{{p}_{i}}$$\\end{document}) of each patch in an image (from the search results) in a given query session \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_s$$\\end{document} and the ratio of total number of query session (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_T$$\\end{document}) during the entire search process to the number of query sessions (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_q$$\\end{document}) that contain the given patches (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_i$$\\end{document}) found in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_s$$\\end{document}. We also consider the time spent (T) on the resulting images in a given query session to estimate the information scent (IS) within a query session as:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} IS(Q_s) = \\sum _{i=1}^n f_{{p}_{i}}\\log (\\frac{Q_T}{N_q})T(p_i). \\end{aligned}$$\\end{document}The user effort in terms of time is a function of patches which can be diverse and of different image class category. To generalise this for finding the information scent of a patch which then is assessed to select patches with higher information scent and then compared against the patch probability obtained via iBERT to distinguish the result. If we assume that the generated auto-completions induce several suggested queries (representing different information needs) simultaneously, every suggestion is in a competition to be discriminated as evident to the user. In the same way, an image contains multiple related or unrelated patches within it, and users find it difficult to judge which patches are relevant among images, which is due to the high uncertainty of correlated features within an image spread via patches. This motivates us to estimate the information scent of an image patch. There are two ways to compute the information scent of an image; one is to hire individual judges to rate scent on a scale [27] and the second approach is an algorithmic approach [28]. To estimate the information scent of a patch, we consider that PPSM constitutes patches that are probability distributions over images as observations. We assume image features as activators to perceptual cues because the user interpretation to image features when matched gives rise to a selection of an item (i.e., patch). The distributions are independent Bernoulli distributions of the features. Each observation is allocated to a patch, but the number of patches is not necessarily fixed i.e., the model is a non-parametric mixture with a product of independent Bernoullis as observation model. Therefore, the log-probability of selecting an image I for patch \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_i$$\\end{document}5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} p(I \\mid p_i) = \\prod _{q_p} {r_{pf}}^{i_f} \\, (1 - r_{pf})^{1 - i_f} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_pf = f((\\pi _i,s_i), (1 - \\pi _i)s_i)$$\\end{document} is the Bernoulli rate for patch p to emit feature f, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i_f$$\\end{document} is the image containing feature f, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_pf$$\\end{document} is a function of prior parameters representing activators (perceptual cues) for the selected patch. There can be a situation when most patches have only one observation (image) and features are very sparse i.e., the possibility of multiple perceptual cues per patch (i.e., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\pi _s \\ll 1$$\\end{document}) is low. To interpret Bernoulli\u2019s prior parameters such as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s_i$$\\end{document}, we find the probability to observe a feature (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f\\in i$$\\end{document} meaning \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i = 1$$\\end{document}) provided that it has been observed for a patch p (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=1$$\\end{document}) is:6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} p(i = 1|k=1,n=1) = \\frac{s_1\\pi _s+n}{s_1+n} = \\frac{s_1\\pi _s+1}{s_1+1}\\approx \\frac{1}{s_1+1} \\end{aligned}$$\\end{document}if \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\pi _s\\ll 1$$\\end{document}. The probability of observing a feature in a new image, given that it has been observed before, is a measure of its reliability. We use this probabilistic model to compare the results based on the probabilities of patches obtained from iBERT.",
            "cite_spans": [
                {
                    "start": 348,
                    "end": 349,
                    "mention": "6",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 4930,
                    "end": 4932,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 4986,
                    "end": 4988,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Information Foraging Explanation ::: Our Model",
            "ref_spans": []
        },
        {
            "text": "We use two well-known and diverse datasets: a visual dataset with large-scale knowledge bases that provide a rich collection of language annotations for visual concepts known as Visual Genome [18] with over 100k images where most image categories fall within a long tail, and the ReferIt dataset [16] which contains \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim $$\\end{document}42k image regions with descriptions. These two datasets fit well for our tasks. The Visual Genome dataset includes images, region descriptions, question-answers, objects, relationships, and attributes. The region descriptions confer a substitution for queries as they refer to several objects in various regions of every image. Few region descriptions are referring phrases and few of them are quite alike to descriptions. For example, referring descriptions are \u201cguy sitting on the couch\u201d, \u201cwhite keyboard on the desk\u201d and non-referring descriptions are \u201ccouch is brown\u201d and \u201cmouse is in the charger\u201d. The huge number of instances from the Visual Genome dataset makes it quite convenient for our task. The ReferIt dataset is a collection of referring expressions engaged to images which quite intently resemble probable user queries of images. We separately train models for query auto-completion and patch selection using both datasets.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 195,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 297,
                    "end": 299,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Dataset ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We combine query and image as pairs by utilising the region descriptions from the Visual Genome dataset and referring to expressions from the ReferIt dataset. During training, we taken 85% of the Visual Genome data as the training set consists of 16,000 images and 740,000 corresponding region descriptions in which there are approximately 40\u201345 text descriptions per image. The training data from the ReferIt dataset consists of 9,000 images and 54,000 referring expression with approximately 4\u20136 referring expression per image.",
            "cite_spans": [],
            "section": "Training ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "For the query auto-completion task, we train our extended LSTM language model where the dimension of image representation is 128, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r = 64$$\\end{document} is the rank of the matching personalised matrix (component from Fig. 2). We use character embeddings with dimension 24, the dimension of the LSTM hidden units is 512, and a maximum length of 50 characters per query with Adam optimizer at a learning rate of 5e-4 for 50,000 iterations as well as a batch size of 32. For the patch selection task, we train our proposed iBERT model using pairs of (region description, patch set) from the Visual Genome dataset, giving rise to a training set of approximately 1.73 million samples. The extra 0.3 million samples are split into test and validation set. We conduct training for the patch selection model that fine-tunes BERT having twelve layers with batch size of 32 for 250,000 iteration using Adam as optimizer at a learning rate of 5e-5 in which the performance increases steeply for the initial 10% of iterations. We use a NVIDIA Tesla T4 GPU which takes a day and half for the complete training activity.",
            "cite_spans": [],
            "section": "Training ::: Experiments",
            "ref_spans": [
                {
                    "start": 491,
                    "end": 492,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "We evaluate the quality of our predictions and estimations using the following performance metrics:",
            "cite_spans": [],
            "section": "Performance Measure ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Mean Reciprocal Rank: The most standard metrics for QAC tasks is the mean reciprocal rank (MRR), which is the average of the reciprocal ranks of the final queries in the QAC outcomes. The MRR for the query auto-completion system \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_A$$\\end{document} provided the test dataset \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_T$$\\end{document} is as follows:\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} MRR(Q_A) = \\frac{1}{\\mid D_T \\mid }\\sum _{q\\in D_T} RR(q, Q_A(q_p)) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_p$$\\end{document} is a prefix of query q and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_A(q_p)$$\\end{document} is the list ranked for candidate completions of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_p$$\\end{document} from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_A$$\\end{document}. RR denotes the reciprocal rank of q if q is present in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_A(q_p)$$\\end{document}, in other cases reciprocal is 0.",
            "cite_spans": [],
            "section": "Performance Measure ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Language Perplexity: Perplexity is a measure to encapsulate uncertainty of the model for a given query prefix. This metric has been explored earlier for an information retrieval task [8] and its correlation with the standard precision-recall measures has been investigated [1]. The average inverse probability is perplexity. A better model has lower perplexity.\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Perplexity(q_p) = \\root N \\of {\\prod _{i=1}^{N}\\frac{1}{P(q_i|q_{i-1})}} \\end{aligned}$$\\end{document}where N is the normalised length of the query and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P(q_i|q_{i-1})$$\\end{document} is the probability of the complete query given the immediate preceding query prefix.",
            "cite_spans": [
                {
                    "start": 184,
                    "end": 185,
                    "mention": "8",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 274,
                    "end": 275,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Performance Measure ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We evaluate the patch selection by F1 score.",
            "cite_spans": [],
            "section": "Performance Measure ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We report the evaluation result in Table 1. We perform our evaluation in two parts. Firstly, we evaluate the quality of our query completion (query prefix of length one or more character) by mean reciprocal rank and perplexity. Secondly, we evaluate the patch selection task by F1 score. We evaluate the query completion task on Visual Genome and ReferIt datasets which have character vocabulary sizes of 89 and 77. We match index \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_q$$\\end{document} of the true query prefix in the top 10 predicted completions where we estimate the MRR score as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sum _n \\frac{1}{T_q}$$\\end{document} and reinstate the reciprocal rank with 0 in case if query does not appear in the top 10 completions. The perplexity comparison on both collection of test queries utilising corresponding contexts i.e., images and indiscriminate noise. The perplexity on the Visual Genome and ReferIt test queries with both contexts is shown in Table 2. During the evaluation on the Visual Genome and ReferIt test sets (or queries), we analyse the query prefix with different length for the corresponding context (noise and image). We found that mean reciprocal rank is altered by the query prefix length, as long-tailed queries are comparatively more difficult than queries of average length to match. Hence, we examine quite better performance for all prefix lengths on the ReferIt dataset (from Table 2).\n\n",
            "cite_spans": [],
            "section": "Results and Discussion ::: Experiments",
            "ref_spans": [
                {
                    "start": 41,
                    "end": 42,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1455,
                    "end": 1456,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1908,
                    "end": 1909,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "We evaluated our proposed iBERT model for finding patch probabilities which is used to select and rank patches in the image. We achieve an F1 score4 of 0.7638 over 3,000 patch classes.",
            "cite_spans": [],
            "section": "Results and Discussion ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In this work, we propose an extended LSTM language model for a new task of query auto-completion adapted upon an image. The language model enriches both image features and text information in which the surplus of beam search over our model is efficiently able to predict future queries at least on a single character prefix. The significant increase in MRR is due to the inclusion of visual information within textual queries as explained by IFT model. Also, we present iBERT for patch selection to efficiently rank them in the image and eventually predicts the most suitable image for the auto-completed query, and compare against the result from probabilistic patch selection model. This work is among the first attempt to apply foraging-based strategy to QAC. The self-explanatory power of IFT to understand user interaction at query level leads the foundation of probabilistic patch selection model to devise users\u2019 information need. Our future work is to generalise the referring expression with contextual model to distinguish referring and non-referring region descriptions. We intend to aggregate information from textual queries and visual descriptions to scale it for multimodal query auto-completion in a single model.",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Evaluation results of the query completion task. Our MRR score is in bold face.\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^\\mathrm{a}$$\\end{document}E-LSTM LM: Extended LSTM Language Model",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Perplexity of image query auto-completion on both datasets utilising an image and indiscriminate noise. Inclusion of image results in a better (lower) perplexity\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Query auto-completion using our extended LSTM language model",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: The end-to-end architecture of Image Query Auto-Completion: User query prefix with the image features generated from a pre-trained CNN are input to an extended LSTM model (by incorporating a context-dependent weight matrix) which predicts a complete query. The resulting query is fed into a fine-tuned BERT pre-trained embedding model which outputs patch probabilities for patch selection.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "Visual genome: connecting language and vision using crowdsourced dense image annotations",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Krishna",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int. J. Comput. Vis.",
            "volume": "123",
            "issn": "1",
            "pages": "32-73",
            "other_ids": {
                "DOI": [
                    "10.1007/s11263-016-0981-7"
                ]
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Information scent, searching and stopping",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Maxwell",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Azzopardi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "210-222",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "Information foraging",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Pirolli",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Card",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Psychol. Rev.",
            "volume": "106",
            "issn": "4",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1037/0033-295X.106.4.643"
                ]
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "Key-Word-aware network for referring expression image segmentation",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Computer Vision \u2013 ECCV 2018",
            "volume": "",
            "issn": "",
            "pages": "38-54",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "News cues: information scent and cognitive heuristics",
            "authors": [
                {
                    "first": "SS",
                    "middle": [],
                    "last": "Sundar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Knobloch-Westerwick",
                    "suffix": ""
                },
                {
                    "first": "MR",
                    "middle": [],
                    "last": "Hastall",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J. Am. Soc. Inform. Sci. Technol.",
            "volume": "58",
            "issn": "3",
            "pages": "366-378",
            "other_ids": {
                "DOI": [
                    "10.1002/asi.20511"
                ]
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "Examining the effectiveness of real-time query expansion",
            "authors": [
                {
                    "first": "RW",
                    "middle": [],
                    "last": "White",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Marchionini",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Inf. Process. Manag.",
            "volume": "43",
            "issn": "3",
            "pages": "685-704",
            "other_ids": {
                "DOI": [
                    "10.1016/j.ipm.2006.06.005"
                ]
            }
        },
        "BIBREF31": {
            "title": "Risk and ambiguity in information seeking: eye gaze patterns reveal contextual behavior in dealing with uncertainty",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Wittek",
                    "suffix": ""
                },
                {
                    "first": "YH",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dar\u00e1nyi",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Gedeon",
                    "suffix": ""
                },
                {
                    "first": "IS",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Front. Psychol.",
            "volume": "7",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.3389/fpsyg.2016.01790"
                ]
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "A survey of query auto completion in information retrieval",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "De Rijke",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Found. Trends\u00ae Inf. Retrieval",
            "volume": "10",
            "issn": "4",
            "pages": "273-363",
            "other_ids": {
                "DOI": [
                    "10.1561/1500000055"
                ]
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}