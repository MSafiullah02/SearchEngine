{
    "paper_id": "9b8eaad6e6ebf2b480067ce3d923a0344f64e7c6",
    "metadata": {
        "title": "COVID-CAPS: A Capsule Network-based Framework for Identification of COVID-19 cases from X-ray Images",
        "authors": [
            {
                "first": "Parnian",
                "middle": [],
                "last": "Afshar",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Concordia University",
                    "location": {
                        "settlement": "Montreal",
                        "region": "QC",
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Shahin",
                "middle": [],
                "last": "Heidarian",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Concordia University",
                    "location": {
                        "settlement": "Montreal",
                        "region": "QC",
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Farnoosh",
                "middle": [],
                "last": "Naderkhani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Concordia University",
                    "location": {
                        "settlement": "Montreal",
                        "region": "QC",
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Anastasia",
                "middle": [],
                "last": "Oikonomou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Toronto",
                    "location": {
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Konstantinos",
                "middle": [
                    "N"
                ],
                "last": "Plataniotis",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Toronto",
                    "location": {
                        "settlement": "Toronto",
                        "region": "ON",
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Arash",
                "middle": [],
                "last": "Mohammadi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Concordia University",
                    "location": {
                        "settlement": "Montreal",
                        "region": "QC",
                        "country": "Canada"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Novel Coronavirus disease has abruptly and undoubtedly changed the world as we know it at the end of the 2nd decade of the 21st century. COVID-19 is extremely contagious and quickly spreading globally making its early diagnosis of paramount importance. Early diagnosis of COVID-19 enables health care professionals and government authorities to break the chain of transition and flatten the epidemic curve. The common type of COVID-19 diagnosis test, however, requires specific equipment and has relatively low sensitivity. Computed tomography (CT) scans and X-ray images, on the other hand, reveal specific manifestations associated with this disease. Overlap with other lung infections makes human-centered diagnosis of COVID-19 challenging. Consequently, there has been an urgent surge of interest to develop Deep Neural Network (DNN)-based diagnosis solutions, mainly based on Convolutional Neural Networks (CNNs), to facilitate identification of positive COVID-19 cases. CNNs, however, are prone to lose spatial information between image instances and require large datasets. The paper presents an alternative modeling framework based on Capsule Networks, referred to as the COVID-CAPS, being capable of handling small datasets, which is of significant importance due to sudden and rapid emergence of COVID-19. Our results based on a dataset of X-ray images show that COVID-CAPS has advantage over previous CNN-based models. COVID-CAPS achieved an Accuracy of 95.7%, Sensitivity of 90%, Specificity of 95.8%, and Area Under the Curve (AUC) of 0.97, while having far less number of trainable parameters in comparison to its counterparts. To potentially and further improve diagnosis capabilities of the COVID-CAPS, pre-training and transfer learning are utilized based on a new dataset constructed from an external dataset of X-ray images. This is in contrary to existing works where pre-training is performed based on natural images. Pretraining with a dataset of similar nature further improved accuracy to 98.3% and specificity to 98.6%.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Novel Coronavirus disease (COVID- 19) , first emerged in Wuhan, China [1] , has abruptly and significantly changed the world as we know it at the end of the 2nd decade of the 21st century. COVID-19 seems to be extremely contagious and quickly spreading globally with common symptoms such as fever, cough, myalgia, or fatigue resulting in ever increasing number of human fatalities. Besides having a rapid human-tohuman transition rate, COVID-19 is associated with high Intensive Care Unit (ICU) admissions resulting in an urgent quest for development of fast and accurate diagnosis solutions [1] . Identifying positive COVID-19 cases in early stages helps with isolating the patients as quickly as possible [2] , hence breaking the chain of transition and flattening the epidemic curve.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 37,
                    "text": "19)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 70,
                    "end": 73,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 592,
                    "end": 595,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 707,
                    "end": 710,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Reverse Transcription Polymerase Chain Reaction (RT-PCR), which is currently the gold standard in COVID-19 diagnosis [1] , involves detecting the viral RNA from sputum or nasopharyngeal swab. The RT-PCR test is, however, associated with relatively low sensitivity (true positive rate) and requires specific material and equipment, which are not easily accessible [1] . Moreover, this test is relatively time-consuming, which is not desirable as the positive COVID-19 cases should be identified and tracked as fast as possible [2] . Images [3] in COVID-19 patients, on the other hand, have shown specific findings, such as ground-glass opacities with rounded morphology and a peripheral lung distribution. Although imaging studies and theirs results can be obtained in a timely fashion, the previously described imaging finding may be seen in other viral or fungal infections or other entities such as organizing pneumonia, which limits the specificity of images and reduces the accuracy of a human-centered diagnosis.",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 120,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 363,
                    "end": 366,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 526,
                    "end": 529,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 539,
                    "end": 542,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Literature Review: Since revealing the potentials of computed tomography (CT) scans and X-ray images in detecting COVID-19 and weakness of the human-centered diagnosis, there have been several studies [5] - [7] trying to develop automatic COVID-19 classification systems, mainly using Convolutional Neural Networks (CNNs) [4] . Xu et al. [1] have first adopted a pre-trained 3D CNN to extract potential infected regions from the CT scans. These candidates are subsequently fed to a second CNN to classify them into three groups of COVID-19, Influenza-A-viral-pneumonia, and irrelevant-to-infection, with an overall accuracy of 86.7%. Wang et al. [2] have first extracted candidates using a threshold-based strategy. Consequently, for each case two or three regions are randomly selected to form the dataset. A pre-trained CNN is fine-tuned using the developed dataset. Finally, features are extracted from the CNN and fed to an ensemble of classifiers for the COVID-19 prediction, reaching an accuracy of 88%. CT scans are also utilized in Reference [8] to identify positive COVID-19 cases, where all slices are separately fed to the model and outputs are aggregated using a Max-pooling operation, reaching a sensitivity of 90%. In a study by Wang and Wong [9] , a CNN model is first pre-trained on the ImageNet dataset [10] , followed by finetuning using a dataset of X-ray images to classify subjects as normal, bacterial, non-COVID-19 viral, and COVID-19 viral infection, achieving an overall accuracy of 83.5%. In a similar study by Sethy and Behera [11] , different CNN models are trained on X-ray images, followed by a Support Vector Machine (SVM) classifier to identify positive COVID-19 cases, reaching an accuracy of 95.38%.",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 204,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 207,
                    "end": 210,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 322,
                    "end": 325,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 338,
                    "end": 341,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 646,
                    "end": 649,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "text": "[8]",
                    "ref_id": null
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1320,
                    "end": 1324,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1554,
                    "end": 1558,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Contributions: All the studies on deep learning-based COVID-19 classification have so far utilized CNNs, which although being powerful image processing techniques, are prone to an important drawback. They are unable to capture spacial relations between image instances. As a result of this inability, CNNs cannot recognize the same object when it is rotated or subject to another type of transformation. Adopting a big dataset, including all the possible transformations, is the solution to this problem. However, in medical imaging problems, including the COVID-19 classification, huge datasets are not easily accessible. In particular, COVID-19 has been identified only recently, and large enough datasets are not yet developed. Capsule Networks (CapsNets) [12] are alternative models that are capable of capturing spatial information using routing by agreement, through which Capsules try to reach a mutual agree-ment on the existence of the objects. This agreement leverages the information coming from instances and object parts, and is therefore able to recognize their relations, without a huge dataset. Through several studies [13] - [18] , we have shown the superiority of the CapsNets for different medical problems such as brain tumor [13] - [17] and lung tumor classification [18] . In this study, we propose a Capsule Network-based framework, referred to as the COVID-CAPS, for COVID-19 identification using X-ray images. The proposed COVID-CAPS achieved an accuracy of 95.7%, a sensitivity of 90%, specificity of 95.8%, and Area Under the Curve (AUC) of 0.97.",
            "cite_spans": [
                {
                    "start": 759,
                    "end": 763,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To potentially and further improve diagnosis capabilities of the COVID-CAPS, we considered pre-training and transfer learning using an external dataset of X-ray images, consisting of 94, 323 frontal view chest X-ray images for common thorax diseases. This dataset is extracted from the NIH Chest Xray dataset [21] including 112, 120 X-ray images for 14 thorax abnormalities. From existing 15 diseases in this dataset, 5 classes were constructed with the help of a thoracic radiologist, with 18 years of experience in thoracic imaging (A. O.). It is worth mentioning that our pre-training strategy is in contrary to that of Reference [9] where pre-training is performed based on natural images (ImageNet dataset). Intuitively speaking, pretraining based on an X-ray dataset of similar nature is expected to result in better transfer learning in comparison to the case where natural images were used for this purpose. In summary, pre-training with an external dataset of X-ray images further improved accuracy of COVID-CAPS to 98.3%, specificity to 98.6%, and AUC to 0.97, however, with a lower sensitivity of 80%. Trained COVID-CAPS model is available publicly for open access at https://github.com/ShahinSHH/COVID-CAPS. To the best of our knowledge, this is the first study investigating applicability of the CapsNet for the problem at hand.",
            "cite_spans": [
                {
                    "start": 309,
                    "end": 313,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 633,
                    "end": 636,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest of the manuscript is organized as follows: Section 2 briefly introduces the Capsule networks. The COVID-CAPS is presented in Section 3. Utilized dataset for evaluation of the proposed COVID-CAPS, and our results are presented in Section 4. Finally, Section 5 concludes the work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Each layer of a Capsule Network (CapsNet) consists of several Capsules, each of which represents a specific image instance at a specific location, through several neurons. The length of a Capsule determines the existence probability of the associated instance. Similar to a regular CNN, each Capsule i, having the instantiation parameter u i , tries to predict the outputs of the next layer's Capsules, using a trainable weight matrix W i j , as follows\u00fb",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Capsule Networks"
        },
        {
            "text": "where\u00fb j|i denotes the prediction of Capsule i for Capsule j.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Capsule Networks"
        },
        {
            "text": "The predictions, however, are taken into account based on a coefficient, through the \"Routing by Agreement\" process, to determine the actual output of the Capsule j, denoted by s j , as follows ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Capsule Networks"
        },
        {
            "text": "and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Capsule Networks"
        },
        {
            "text": "where a i j denotes the agreement between predictions and outputs, and c i j is the score given to the predictions. In other words, this score determines the contribution of the prediction to the output. Routing by agreement is what makes the CapsNet different from a CNN and helps it identify the spatial relations. The CapsNet loss function, l k , associated with Capsule k, is calculated as follows",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Capsule Networks"
        },
        {
            "text": "where T k is one whenever the class k is present and zero otherwise. Terms m + , m \u2212 , and \u03bb are the hyper parameters of the model. The final loss is the summation over all the l k s. This completes a brief introduction to CapsuleNets, next we present the COVID-CAPS framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Capsule Networks"
        },
        {
            "text": "The architecture of the proposed COVID-CAPS is shown in Fig. 1 , which consists of 4 convolutional layers and 3 Capsule layers. The inputs to the network are 3D X-ray images. The first layer is a convolutional one, followed by batch-normalization. The second layer is also a convolutional one, followed by average pooling. Similarly, the third and forth layers are convolutional ones, where the forth layer is reshaped to form the first Capsule layer. Consequently, three Capsule layers are embedded in the COVID-CAPS to perform the routing by agreement process. The last Capsule layer contains the instantiation parameters of the two classes of positive and negative COVID-19.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 56,
                    "end": 62,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "The Proposed COVID-CAPS"
        },
        {
            "text": "The length of these two Capsules represents the probability of each class being present.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Proposed COVID-CAPS"
        },
        {
            "text": "Since we have developed a Capsule Network-based architecture, which does not need a large dataset, we did not perform any data augmentation. However, since the number of positive cases, N + , are less than the negative ones, N \u2212 , we modified the loss function to handle the class imbalance problem. In other words, more weight is given to positive samples in the loss function, where weights are determined based on the proportion of the positive and negative cases, as follows",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Proposed COVID-CAPS"
        },
        {
            "text": "where loss + denotes the loss associated with positive samples, and loss \u2212 denotes the loss associated with negative samples. As stated previously, to potentially and further improve diagnosis capabilities of the COVID-CAPS, we considered pretraining the model in an initial step. In contrary to Reference [9] where ImageNet dataset [10] is used for pre-training, however, we constructed and utilized an X-ray dataset. The reason for not using ImageNet for pre-training is that the nature of images (natural images) in that dataset is totally different from COVID-19 X-ray dataset. It is expected that using a model pretrained on X-ray images of similar nature would result in better boosting of the COVID-CAPS. For pre-training with an external dataset, the whole COVID-CAPS model is first trained on the external data, where the number of final Capsules is set to the number of output classes in the external set. From existing 15 disease in the external dataset, 5 classes were constructed with the help of a thoracic radiologist, with 18 years of experience in thoracic imaging (A. O.). To fine-tune the model using the COVID-19 dataset, the last Capsule layer is replaced with two Capsules to represent positive and negative COVID-19 cases. All the other Capsule layers are fine-tuned, whereas the conventional layers are fixed to the weights obtained in pretraining.",
            "cite_spans": [
                {
                    "start": 306,
                    "end": 309,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 333,
                    "end": 337,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "The Proposed COVID-CAPS"
        },
        {
            "text": "We used Adam optimizer with an initial learning rate of 10 \u22123 , 100 epochs, and a batch size of 16. We have split the training dataset, described in Section 4, into two sets of training (90%) and validation (10%), where training set is used to train the model and the validation set is used to select a model that has the best performance. Selected model is then tested on the testing set, for the final evaluation. The following four metrics are utilized to represent the performance: Accuracy; Sensitivity; Specificity, and Area Under the Curve (AUC). Next, we present the obtained results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Proposed COVID-CAPS"
        },
        {
            "text": "To conduct our experiments, we used the same dataset as Reference [9] . This dataset is generated from two publicly available chest X-ray datasets [19, 20] . As shown in Fig. 2 , the generated dataset contains four different labels, i.e., Normal; Bacterial; Non-COVID Viral, and; COVID-19. As the main goal of this study is to identify positive COVID-19 cases, we binarized the labels as either positive or negative. In other words, the three labels of normal, bacterial, and non-COVID viral together form the negative class.",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 69,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 147,
                    "end": 151,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 152,
                    "end": 155,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 170,
                    "end": 176,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Using the aforementioned dataset, the proposed COVID-CAPS achieved an accuracy of 95.7%, a sensitivity of 90%, specificity of 95.8%, and AUC of 0.97. The obtained receiver operating characteristic (ROC) curve is shown in Fig. 3 . In particular, false positive cases have been further investigated to have an insight on what types are more subject to being misclassified by COVID-19. It is observed that 54% of the false positives are normal cases, whereas bacterial and non-COVID cases form only 27% and 19% of the false positives, respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 221,
                    "end": 227,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "As shown in Table 1 , we compare our results with Reference [11] that has used the binarized version of the same dataset. COVID-CAPS outperforms its counterpart in terms of accuracy and specificity. Sensitivity is higher in the model proposed in Reference [11] , that contains 23 million trainable parameters. Reference [6] is another study on the binarized version of the same X-ray images. However, as the negative label contains only normal cases (in contrast to including all normal, bacterial, and non-covid viral cases as negative), we did not compare the performance of the COVID-CAPS with this study. It is worth mentioning that the proposed COVID-CAPS has only 295, 488 trainable parameters. Compared to 23 million trainable parameters of the model proposed in Reference [11] , therefore, COVID-CAPS can be trained and used in a more timely fashion, and eliminates the need for availability of powerful computational resources.",
            "cite_spans": [
                {
                    "start": 60,
                    "end": 64,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 256,
                    "end": 260,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 320,
                    "end": 323,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 780,
                    "end": 784,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 19,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "In another experiment, we pre-trained the proposed COVID-CAPS using an external dataset of X-ray images, consisting of 94, 323 frontal view chest X-ray images for common thorax diseases. This dataset is extracted from the NIH Chest X-ray dataset [21] including 112, 120 X-ray images for 14 thorax abnormalities. This dataset also contains normal cases without specific findings in their corresponding images. In order to reduce the number of categories, we classified these 15 groups into 5 categories based on the relations between the abnormalities in each disease. The first four groups are dedicated to No findings, Tumors, Pleural diseases, and Lung infections categories. The fifth group encompasses other images without specific relations with the first four groups. We then removed 17, 797 cases with multiple labels (appeared in more than one category) to reduce the complexity. The adopted dataset is then used to pre-train our model. Table 2 demonstrates our classification scheme and distribution of the data. Results obtained from fine-tuning the pre-trained COVID-CAPS is also shown in Table 1 , according to which, pre-training improves accuracy and specificity. The ROC curve is shown in Fig. 3 , according to which, the obtained AUC of 0.99 outperforms that of COVID-CAPS without pre-training.",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 250,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [
                {
                    "start": 945,
                    "end": 952,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 1100,
                    "end": 1107,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1204,
                    "end": 1210,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "In this study, we proposed a Capsule Network-based framework, referred to as the COVID-CAPS, for diagnosis of COVID-19 from X-ray images. The proposed framework consists of several Capsule and convolutional layers, and the lost function is modified to account for the classimbalance problem. The obtained results show that the COVID-CAPS has a satisfying performance with a low number of trainable parameters. Pre-training was able to further improve the accuracy, specificity, and AUC. Trained COVID-CAPS model is available publicly for open access at https://github.com/ShahinSHH/COVID-CAPS. As more and more COVID-19 cases are being identified all around the world, larger datasets are being generated. We will continue to further modify the architecture of the COVID-CAPS and incorporate new available datasets. New versions of the COVID-CAPS will be released upon development through the aforementioned link.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Deep Learning System to Screen Coronavirus Disease 2019 Pneumonia",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2002.09334"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "A Deep Learning Algorithm using CT Images to Screen for Corona Virus Disease (COVID-19)",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Sh",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "From Handcrafted to Deep-Learning-Based Cancer Radiomics: Challenges and Opportunities",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "N"
                    ],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oikonomou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Benali",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Signal Processing Magazine",
            "volume": "36",
            "issn": "4",
            "pages": "132--160",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Convolutional Neural Networks: An Overview and Application in Radiology",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Yamashita",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Nishio",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Insights into Imaging",
            "volume": "9",
            "issn": "4",
            "pages": "611--629",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Rapid AI Development Cycle for the Coronavirus (COVID-19) Pandemic: Initial Results for Automated Detection & Patient Monitoring using Deep Learning CT Image Analysis",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Gozes",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Frid-Adar",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.05037"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images and Deep Convolutional Neural Networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Narin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kaya",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Pamuk",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.10849"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "COVID-ResNet: A Deep Learning Framework for Screening of COVID19 from Radiograph",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Farooq",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hafeez",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.14395"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest Radiography Images",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wong",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.09871"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Neural Information Processing Systems (NIPS)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Detection of Coronavirus Disease (COVID-19) Based on Deep Features",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "K"
                    ],
                    "last": "Sethy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Behera",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Matrix Capsules With EM Routing",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sabour",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Frosst",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Brain Tumor Type Classification via Capsule Networks",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "N"
                    ],
                    "last": "Plataniotis",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE International Conference on Image Processing (ICIP)",
            "volume": "",
            "issn": "",
            "pages": "3129--3133",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Capsule Networks for Brain Tumor Classification Based on Mri Images and Coarse Tumor Boundaries",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "volume": "",
            "issn": "",
            "pages": "1368--1372",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Capsule Networks' Interpretability for Brain Tumor Classification Via Radiomics Analyses",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE International Conference on Image Processing (ICIP)",
            "volume": "",
            "issn": "",
            "pages": "13816--3820",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "BoostCaps: A Boosted Capsule Network for Brain Tumor Classification",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Accepted in IEEE Engineering in Medicine and Biology Society",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A Bayesian Approach to Brain Tumor Classification Using Capsule Networks",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Submitted to IEEE International Conference on Image Processing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "3D-MCN: A 3D Multi-Scale Capsule Network for Lung Nodule Malignancy Classification",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Afshar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oikonomou",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "N"
                    ],
                    "last": "Tyrrell",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Farahani",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Plataniotis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mohammadi",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Covid Chest x-ray Dataset",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Kaggle Chest x-ray Images (Pneumonia) Dataset",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Mooney",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "3462--3471",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The proposed COVID-CAPS architecture.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Labels available in the dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "ROC curve from the proposed COVID-CAPS.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Results obtained from the proposed COVID-CAPS, along with the results from Reference[11].Table 2. Description of the External X-ray images dataset used for pre-training COVID-CAPS.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This work was partially supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada through the NSERC Discovery Grant RGPIN-2016-04988.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        }
    ]
}