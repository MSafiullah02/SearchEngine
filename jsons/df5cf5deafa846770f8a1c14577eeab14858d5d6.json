{
    "paper_id": "df5cf5deafa846770f8a1c14577eeab14858d5d6",
    "metadata": {
        "title": "DeepRC: Immune repertoire classification with attention-based deep massive multiple instance learning",
        "authors": [
            {
                "first": "Michael",
                "middle": [],
                "last": "Widrich",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Johannes Kepler University Linz",
                    "location": {
                        "country": "Austria"
                    }
                },
                "email": ""
            },
            {
                "first": "Bernhard",
                "middle": [],
                "last": "Sch\u00e4fl",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Johannes Kepler University Linz",
                    "location": {
                        "country": "Austria"
                    }
                },
                "email": ""
            },
            {
                "first": "Milena",
                "middle": [],
                "last": "Pavlovi\u0107",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Oslo",
                    "location": {
                        "settlement": "Oslo",
                        "region": "Nor-way"
                    }
                },
                "email": ""
            },
            {
                "first": "Geir",
                "middle": [
                    "Kjetil"
                ],
                "last": "Sandve",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Oslo",
                    "location": {
                        "settlement": "Oslo",
                        "country": "Norway"
                    }
                },
                "email": ""
            },
            {
                "first": "Sepp",
                "middle": [],
                "last": "Hochreiter",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Johannes Kepler University Linz",
                    "location": {
                        "country": "Austria"
                    }
                },
                "email": ""
            },
            {
                "first": "Victor",
                "middle": [],
                "last": "Greiff",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Oslo",
                    "location": {
                        "settlement": "Oslo",
                        "region": "Nor-way"
                    }
                },
                "email": ""
            },
            {
                "first": "G\u00fcnter",
                "middle": [],
                "last": "Klambauer",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Johannes Kepler University Linz",
                    "location": {
                        "country": "Austria"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "High-throughput immunosequencing allows reconstructing the immune repertoire of an individual, which is an exceptional opportunity for new immunotherapies, immunodiagnostics, and vaccine design. Such immune repertoires are shaped by past and current immune events, for example infection and disease, and thus record an individual's state of health. Consequently, immune repertoire sequencing data may enable the prediction of health and disease using machine learning. However, finding the connections between an individual's repertoire and the individual's disease class, with potentially hundreds of thousands to millions of short sequences per individual, poses a difficult and unique challenge for machine learning methods. In this work, we present our method DeepRC that combines a Deep Learning architecture with attention-based multiple instance learning. To validate that DeepRC accurately predicts an individual's disease class based on its immune repertoire and determines the associated classspecific sequence motifs, we applied DeepRC in four large-scale experiments encompassing ground-truth simulated as well as real-world virus infection data. We demonstrate that DeepRC outperforms all tested methods with respect to predictive performance and enables the extraction of those sequence motifs that are connected to a given disease class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Immune receptors enable the immune system to specifically recognize disease agents or pathogens. Each immune encounter is recorded as an immune event into immune memory by preserving and amplifying immune receptors in the repertoire used to fight a given disease. This is, for example, the principle of vaccination. Each human has about 10 7 -10 8 unique immune receptors with low overlap across individuals and sampled from a potential diversity of > 10 14 receptors (Mora & Walczak, 2019) . The ability to sequence and analyze human immune receptors at large scale has led to fundamental and mechanistic insight into the adaptive immune system and has also opened the opportunity for the development of novel diagnostics and therapy approaches (Georgiou et al., 2014; Brown et al., 2019) .",
            "cite_spans": [
                {
                    "start": 468,
                    "end": 490,
                    "text": "(Mora & Walczak, 2019)",
                    "ref_id": null
                },
                {
                    "start": 746,
                    "end": 769,
                    "text": "(Georgiou et al., 2014;",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 770,
                    "end": 789,
                    "text": "Brown et al., 2019)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Each individual is uniquely characterized by their immune repertoire, which is acquired and changed during life. This repertoire may be influenced by all diseases that an individual is exposed to during their lives and hence contains highly valuable information about those diseases. As a prominent example, Emerson et al. (2017) were able to discriminate between cytomegalovirus (CMV) positive and negative individuals based solely on the presence of CMV-associated immune receptor T-cell receptor (TCR) sequences and could identify receptor sequences associated with the immune status. In a similar approach, Ostmeyer et al. (2019) have identified motifs in TCR sequence repertoires that distinguish between tumor-infiltrating lymphocyte and adjacent healthy tissues. Those studies established that the sequenced immune repertoire is a highly complex data type, which can be tackled by a large variety of computational methods. It is envisioned that the analysis of such immune repertoires at large scale could bring insights into diseases that are currently difficult to treat, such as autoimmune diseases (Bashford-Rogers et al., 2019; Liu et al., 2019) .",
            "cite_spans": [
                {
                    "start": 308,
                    "end": 329,
                    "text": "Emerson et al. (2017)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1109,
                    "end": 1139,
                    "text": "(Bashford-Rogers et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 1140,
                    "end": 1157,
                    "text": "Liu et al., 2019)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Immune repertoire classification, i.e. classifying the immune status based on the immune repertoire sequences, is essentially a text-book example for a multiple instance learning problem (Dietterich et al., 1997; Maron & Lozano-P\u00e9rez, 1998; Wang et al., 2018) . Briefly, the immune repertoire of an individual consists of an immensely large bag of immune Figure 1 . Schematic representation of the DeepRC approach. a) An immune repertoire X is represented by large bags of immune receptor sequences (colored). A neural network (NN) h serves to recognize patterns in each of the sequences s k and maps them to sequence-representations z k . A pooling function f is used to obtain a repertoire-representation z for the input object. Finally, an output network o predicts the class label\u0177. b) DeepRC uses stacked 1D convolutions for a parameterized function h due to their computational efficiency. Potentially, millions of sequences have to be processed for each input object. In principle, also RNNs, such as LSTMs (Hochreiter et al., 2007) , or Transformer networks (Vaswani et al., 2017c) may be used but are currently computationally too costly. c) Attention-pooling is used to obtain a repertoire-representation z for each input object, where DeepRC uses weighted averages of sequence-representations. The weight values are predicted by a Transformer-like attention mechanism. receptors, represented as biological sequences. Usually the presence of only a small fraction of particular receptor determines the immune status with respect to a particular disease (Christophersen et al., 2014; Emerson et al., 2017) . This situation is exactly described by multiple instance learning (MIL), in which the presence of a single instance in a bag can determine the class of the whole bag. Attention-based MIL has been successfully used for image data, for example to identify tiny objects in large images (Ilse et al., 2018; Pawlowski et al., 2019; Tomita et al., 2019; Kimeswenger et al., 2019) . MIL problems are typically the more difficult, the larger the bag of instances and the fewer the instances that cause the label are. Therefore, classification of immune repertoires bears a high difficulty since each immune repertoire can contain millions of sequences as instances and only few indicate the class. Further properties of the data that complicate the problem are (a) the overlap of immune repertoires of different individuals is low (in most cases, maximally low single-digit percentage values) (Greiff et al., 2017; Elhanati et al., 2018) , (b) multiple different sequences can bind to the same pathogen (Wucherpfennig et al., 2007) , and (c) only subsequences within the sequences determines whether binding to an pathogen is possible (Dash et al., 2017; Glanville et al., 2017; Akbar et al., 2019; Springer et al., 2020; Fischer et al., 2019) . In summary, immune repertoire classification can be formulated as multiple instance learning with sparse signals and large numbers of instances, which represents a challenge for currently available machine learning methods. Additionally, ideally, methods should be interpretable, since the extraction of class-associated sequence motifs is desired to gain crucial biological insight.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 212,
                    "text": "(Dietterich et al., 1997;",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 213,
                    "end": 240,
                    "text": "Maron & Lozano-P\u00e9rez, 1998;",
                    "ref_id": null
                },
                {
                    "start": 241,
                    "end": 259,
                    "text": "Wang et al., 2018)",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 1014,
                    "end": 1039,
                    "text": "(Hochreiter et al., 2007)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1066,
                    "end": 1089,
                    "text": "(Vaswani et al., 2017c)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1563,
                    "end": 1592,
                    "text": "(Christophersen et al., 2014;",
                    "ref_id": null
                },
                {
                    "start": 1593,
                    "end": 1614,
                    "text": "Emerson et al., 2017)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1900,
                    "end": 1919,
                    "text": "(Ilse et al., 2018;",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1920,
                    "end": 1943,
                    "text": "Pawlowski et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 1944,
                    "end": 1964,
                    "text": "Tomita et al., 2019;",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1965,
                    "end": 1990,
                    "text": "Kimeswenger et al., 2019)",
                    "ref_id": null
                },
                {
                    "start": 2502,
                    "end": 2523,
                    "text": "(Greiff et al., 2017;",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 2524,
                    "end": 2546,
                    "text": "Elhanati et al., 2018)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 2612,
                    "end": 2640,
                    "text": "(Wucherpfennig et al., 2007)",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 2744,
                    "end": 2763,
                    "text": "(Dash et al., 2017;",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 2764,
                    "end": 2787,
                    "text": "Glanville et al., 2017;",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 2788,
                    "end": 2807,
                    "text": "Akbar et al., 2019;",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 2808,
                    "end": 2830,
                    "text": "Springer et al., 2020;",
                    "ref_id": null
                },
                {
                    "start": 2831,
                    "end": 2852,
                    "text": "Fischer et al., 2019)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 355,
                    "end": 363,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "In this work, we contribute the following:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We propose a deep attention-based multiple-instance learning method for large bags of complex sequences, as they occur in immune-repertoire classification.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We conduct a large comparative study of machine learning approaches for the classification of immune repertoires.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 We suggest an interpretation method for our approach to identify those sequence motifs that are associated with a given class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Immunosequencing data has been analyzed with computational methods for a variety of different tasks (Greiff et al., 2015; Shugay et al., 2015; Miho et al., 2018; Yaari & Kleinstein, 2015; Wardemann & Busse, 2017) . A large part of the currently available machine learning methods for immmune receptor data have focused on the individual immune receptors in a repertoire, with the aim to, for example, predict the antigen or antigen portion (epitope) to which these sequences bind or to predict sharing of receptors across individuals (Gielis et al., 2019; Springer et al., 2020; Jurtz et al., 2018; Moris et al., 2019; Fischer et al., 2019; Greiff et al., 2017; Sidhom et al., 2019; Elhanati et al., 2018) . Recently, Jurtz et al. (2018) used 1D convolutional neural networks (CNNs) to predict antigen binding of TCR sequences (specifically, binding of TCR sequences to peptide-MHC complexes) and demonstrated that motifs can be extracted from these models. This result has motivated our approach to use 1D CNNs at the sequence level. Similarly, Konishi et al. (2019) use CNNs, Gradient Boosting, and other ML techniques on B-cell receptor (BCR) sequences to classify tumor tissue from normal tissue. However, the methods presented so far are working on individual sequences, which is a supervised machine learning task, in which a particular class, the epitope, has to be predicted based on a single input sequence.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 121,
                    "text": "(Greiff et al., 2015;",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 122,
                    "end": 142,
                    "text": "Shugay et al., 2015;",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 143,
                    "end": 161,
                    "text": "Miho et al., 2018;",
                    "ref_id": null
                },
                {
                    "start": 162,
                    "end": 187,
                    "text": "Yaari & Kleinstein, 2015;",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 188,
                    "end": 212,
                    "text": "Wardemann & Busse, 2017)",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 534,
                    "end": 555,
                    "text": "(Gielis et al., 2019;",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 556,
                    "end": 578,
                    "text": "Springer et al., 2020;",
                    "ref_id": null
                },
                {
                    "start": 579,
                    "end": 598,
                    "text": "Jurtz et al., 2018;",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 599,
                    "end": 618,
                    "text": "Moris et al., 2019;",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 619,
                    "end": 640,
                    "text": "Fischer et al., 2019;",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 641,
                    "end": 661,
                    "text": "Greiff et al., 2017;",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 662,
                    "end": 682,
                    "text": "Sidhom et al., 2019;",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 683,
                    "end": 705,
                    "text": "Elhanati et al., 2018)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 718,
                    "end": 737,
                    "text": "Jurtz et al. (2018)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1046,
                    "end": 1067,
                    "text": "Konishi et al. (2019)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "Immune repertoire classification has been considered as multiple instance learning (MIL) problem in the following works. A Deep Learning framework called DeepTCR (Sidhom et al., 2019) implements several Deep Learning approaches for immunosequencing data. The computational framework, inter alia, allows for attention-based MIL repertoire classifiers and implements a basic form of attentionbased averaging. Ostmeyer et al. (2019) already suggested a MIL method for immune repertoire classification. This method considers 4-mers, fixed sub-sequences of length 4, as instances of an input object and trained a logistic regression model with these 4-mers as input. The predictions of the logistic regression model for each 4-mer were max-pooled to obtain one prediction per input object. This approach is characterized by (a) the rigidity of the k-mer features as compared to convolutional kernels (Alipanahi et al., 2015; Zhou & Troyanskaya, 2015; Zeng et al., 2016) , (b) the max-pooling operation, which constrains the network to learn from a single, top-ranked k-mer for each iteration over the input object, and (c) the pooling of predictions scores rather than representations (Wang et al., 2018) . Our experiments also support that these choices in the design of the method can lead to constraints on the predictive performance (see Section 5).",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 183,
                    "text": "(Sidhom et al., 2019)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 895,
                    "end": 919,
                    "text": "(Alipanahi et al., 2015;",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 920,
                    "end": 945,
                    "text": "Zhou & Troyanskaya, 2015;",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 946,
                    "end": 964,
                    "text": "Zeng et al., 2016)",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1180,
                    "end": 1199,
                    "text": "(Wang et al., 2018)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "Our DeepRC method also uses a multiple-instance learning approach but considers sequences rather than k-mers as instances within an input object and uses a transformer-like attention mechanism. DeepRC sets out to avoid the the abovementioned constraints of current methods by (a) using 1D convolutions as feature extractors, (b) applying transformerlike attention-based pooling instead of max-pooling and learning a classifier on the repertoire-representation rather than a classifier on the sequence-representation, and (c) pooling learned representations rather than predictions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related work"
        },
        {
            "text": "We consider a multiple-instance learning (MIL) problem, in which an input object X is a bag of K instances X = {s 1 , . . . , s d k }, which do not have dependencies nor orderings between them and K can be different for every object. We assume that each instance s k is associated with a label y k \u2208 {0, 1}, assuming a binary classification task, to which we do not have access. We only have access to a label Y = max k y k for an input object or bag. Note that this poses a credit assignment problem, since the sequences that are responsible for the label Y have to be identified.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "A model\u0177 = g(X) should be (a) invariant to permutations of the instances and (b) able to cope with the fact that K varies across input objects (Ilse et al., 2018) , which is a problem also posed by point sets (Qi et al., 2017) . Two principled approaches exist. The first approach is to learn an instance-level scoring function h : S \u2192 [0, 1], which is then pooled across instances with a pooling function f , for example by average-pooling or max-pooling (see below). The second approach is to construct an instance representation z k of each instance by h : S \u2192 R dv and then code the bag, or the input object, by pooling these instance representations (Wang et al., 2018) via a function f . An output function o : R dv \u2192 [0, 1] subsequently classifies the bag. The second approach, in which representations rather than scoring functions are pooled, is currently best performing (Wang et al., 2018) . This approach is enhanced by an attention-mechanisms for pooling.",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 162,
                    "text": "(Ilse et al., 2018)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 209,
                    "end": 226,
                    "text": "(Qi et al., 2017)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 655,
                    "end": 674,
                    "text": "(Wang et al., 2018)",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 881,
                    "end": 900,
                    "text": "(Wang et al., 2018)",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "In the problem at hand, the input object X is the immune repertoire of an individual that consists of a large set of immune receptor sequences (T-cell receptors or antibodies). Immune receptors are primarily represented as sequences s k from a space s k \u2208 S. These sequences act as the instances in the MIL problem. An immune repertoire X is a bag of a large number of sequences X = {s 1 , . . . , s d k }, which corresponds to an input object.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "Although immune repertoire classification can readily be formulated as multiple-instance learning problem, it is yet unclear how well machine learning methods solve the abovedescribed problem with a large number of instances d k 10, 000 and with instances s k being complex sequences. We next describe currently used pooling functions for multipleinstance learning problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "Pooling functions for MIL problems. Different pooling functions equip a model g with the property to be invariant to permutations of instances and with the ability to process different numbers of instances. Typically, a neural network h \u03b8 with parameters \u03b8 is trained to obtain a function that maps each instance onto a representation: z k = h \u03b8 (s k ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "A representation of the input object X = {s 1 , . . . , s d k } is constructed via one of the following pooling functions:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "\u2022 Average-pooling:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "where e m = (0, . . . , 1 m-th position , . . .).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "\u2022 Attention-pooling:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "where a k are non-negative (a k \u2265 0), sum to one ( d k k=1 a k = 1), and are determined by an attention mechanism.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "These pooling-functions are invariant to permutations of {1, . . . , d k } and are differentiable. Therefore, they are suited as building blocks for Deep Learning architectures. Other types of pooling functions, that operate on predictions rather than representations, have also been suggested, for example the noisy-AND function (Kraus et al., 2016) . We employ attention pooling in our DeepRC model as detailed in the following.",
            "cite_spans": [
                {
                    "start": 330,
                    "end": 350,
                    "text": "(Kraus et al., 2016)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "Transformer-like attention mechanism. The key-valueattention mechanism has been very successful and became popular through the Transformer (Vaswani et al., 2017a; and BERT (Devlin et al., 2018; models in natural language processing. Recently it was found that the keyvalue-attention mechanism corresponds to a modern Hopfield network with a storage capacity that is exponential in the dimension of the vector space and which converges after just one update (Ramsauer et al., 2020) . Therefore using the key-value-attention mechanism is the natural choice for our task and is theoretically justified for the large number of vectors (sequence patterns) that appear in the immune repertoire classification task.",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 162,
                    "text": "(Vaswani et al., 2017a;",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 172,
                    "end": 193,
                    "text": "(Devlin et al., 2018;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 457,
                    "end": 480,
                    "text": "(Ramsauer et al., 2020)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "The attention mechanism assumes a similarity space of dimension d e for keys and queries to compare them. A set of d k key vectors are combined to the matrix K \u2208 R d k \u00d7de . A set of d q query vectors are combined to the matrix Q \u2208 R dq\u00d7de . Similarities between queries and keys are computed by inner products, therefore queries can search for similar keys that are stored. Another set of d k value vectors are combined to the matrix V \u2208 R d k \u00d7dv . The output of the attention mechanism is a weighted average of the value vectors for each query q. The i-th vector v i is weighted by the similarity between the i-th key k i and the query q. The similarity is given by the softmax of the inner products of the query q with the keys k i . All queries are calculated in parallel via matrix operations. Consequently, the attention function Att(Q, K, V ) maps queries Q, keys K, and values V to d v -dimensional outputs:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "where typically \u03b2 = 1 \u221a de and the softmax-function is applied row-wise. While this attention mechanism has originally been developed for sequence tasks (Vaswani et al., 2017c) , it can readily be transferred to sets (Lee et al., 2019; Ye et al., 2018) . We will employ this attention mechanism in DeepRC.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 176,
                    "text": "(Vaswani et al., 2017c)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 217,
                    "end": 235,
                    "text": "(Lee et al., 2019;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 236,
                    "end": 252,
                    "text": "Ye et al., 2018)",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Problem setting and notation"
        },
        {
            "text": "We propose a novel method Deep Repertoire Classification (DeepRC) for immune repertoire classification with attention-based deep massive multiple instance learning and compare it against other machine learning approaches. For DeepRC, we consider immune repertoires as input objects, which are represented as bags of instances. In a bag, each instance is an immune receptor sequence and each bag can contain a large number of sequences. Note that we will use z k to denote the sequence-representation of the k-th sequence and z to denote the repertoire-representation. At the core, DeepRC consists of a Transformer-like attentionmechanism that extracts the most important information from each repertoire. We first give an overview of the attention mechanism and then provide details on each of the sub-networks h 1 , h 2 , and o of DeepRC. (Overview: Fig. 1 ; Architecture: Fig.2 ; Implementation details: App. A2.)",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 851,
                    "end": 857,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 874,
                    "end": 879,
                    "text": "Fig.2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Deep Repertoire Classification"
        },
        {
            "text": "Attention mechanism in DeepRC. This mechanism is based on the three matrices K (the keys), Q (the queries), and V (the values) together with a parameter \u03b2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep Repertoire Classification"
        },
        {
            "text": "Values. DeepRC uses a 1D-convolutional network h 1 (Le-Cun et al., 1998; Hu et al., 2014; Kelley et al., 2016) that supplies a sequence-representation z k = h 1 (s k ), which acts as the values V = Z = (z 1 , . . . , z d k ) in the attention mechanism (see Figure 2 ).",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 72,
                    "text": "(Le-Cun et al., 1998;",
                    "ref_id": null
                },
                {
                    "start": 73,
                    "end": 89,
                    "text": "Hu et al., 2014;",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 90,
                    "end": 110,
                    "text": "Kelley et al., 2016)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [
                {
                    "start": 257,
                    "end": 265,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Deep Repertoire Classification"
        },
        {
            "text": "A second neural network h 2 , which shares its first layers with h 1 , is used to obtain keys K \u2208 R d k \u00d7de for each sequence in the repertoire. This network uses 2 selfnormalizing layers (Klambauer et al., 2017) with 32 units per layer (see Figure 2 ).",
            "cite_spans": [
                {
                    "start": 188,
                    "end": 212,
                    "text": "(Klambauer et al., 2017)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 242,
                    "end": 250,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Keys."
        },
        {
            "text": "Query. We use a fixed d e -dimensional query (row-)vector \u03be which is learned via backpropagation. For more attention heads, each head has a fixed query vector.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "With the quantities introduced above, the Transformer attention mechanism (4) of DeepRC is implemented as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "where Z \u2208 R d k \u00d7dv are the sequence-representations stacked row-wise, K are the keys, and z is the repertoirerepresentation and at the same time a weighted mean of sequence-representations z k . The attention mechanism can readily be extended to multiple queries and heads, however, computational efficiency currently constrains this (see paragraph \"1D-ConvNet for pattern recognition\").",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "Attention-pooling and interpretability. Each input object, i.e. repertoire, consists of a large number d k of sequences, which are reduced to a single fixed-size feature vector of length d v representing the whole input object by an attention-pooling function. To this end, a Transformer-like attention mechanism adapted to sets is realized in DeepRC which supplies a k -the importance of the sequence s k . This importance value for each sequence, is an interpretable quantity, which is highly desired for the immunological problem at hand.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "Classification layer and network parameters. The repertoire-representation z is then used as input for a fullyconnected output network\u0177 = o(z) that predicts the immune status, where we found it sufficient to train singlelayer networks. In the simplest case, DeepRC is predicts a scalar target y. That is, it predicts the class label, e.g. the immune status of an immune repertoire, using one output value. For this, the output values would be activated using a sigmoid function. However, since DeepRC is an end-toend deep learning model, multiple targets may be predicted simultaneously in classification or regression settings or a mix of both. This allows for the introduction of additional information into the system via auxiliary targets such as age, sex, or other metadata.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "Network parameters, training, and inference. DeepRC is trained using standard gradient descent methods to minimize a regularized cross-entropy loss. The network parameters are \u03b8 1 , \u03b8 2 , \u03b8 o for the sub-networks h 1 , h 2 , and o, respectively, and additionally \u03be. In more detail, we train DeepRC using Adam (Kingma & Ba, 2014) with a batch size 4 and learning rate 10 \u22124 . To increase numerical stability for 16 bit float computations, the parameter was set to an increased value of = 10 \u22124 .",
            "cite_spans": [
                {
                    "start": 309,
                    "end": 328,
                    "text": "(Kingma & Ba, 2014)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "1D-ConvNet for pattern recognition. In the following, we describe how DeepRC identifies patterns in the individual sequences and reduces each sequence in the input object to a fixed-size feature vector. DeepRC employs 1D convolution layers to extract patterns, where trainable weight kernels are convolved over the sequence positions. In principle, also recurrent neural networks or Transformer networks could be used instead of 1D CNNs, however, (a) the computational complexity of the network must be low to be able to process millions of sequences for a single update. Additionally, (b) the learned network should be able to provide insights in the recognized patterns in form of motifs. Both properties (a) and (b) are fulfilled by 1D convolution operations that are used by DeepRC.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "For the input layer of the CNN, the characters in the input sequence, i.e. the amino acids (AAs), are encoded in a onehot vector of length 20. To also provide information about the position of an AA in the sequence, we add 3 additional input features with values in range [0, 1] to encode the position of an AA relative to the sequence. These 3 positional features encode whether the AA is located at the beginning, the center, or the end of the sequence, respectively, as shown in Figure A1 . We concatenate these 3 positional features to the one-hot vector of AAs, which results in a feature vector of size 23 per sequence position. Each repertoire, now represented as bag of feature vectors, is then normalized to unit variance. Since the CMV dataset provides sequences with an associated abundance or absolute frequency value per sequence, we incorporate this information into the input of DeepRC. The one-hot AA features of a sequence are multiplied by a scaling factor max (1, log c a ) before normalization, where c a is the count of a sequence. We feed the sequences with 23 features per position into the CNN, as shown in Figure 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 482,
                    "end": 491,
                    "text": "Figure A1",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1131,
                    "end": 1139,
                    "text": "Figure 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Keys."
        },
        {
            "text": "We use one 1D CNN layer (Hu et al., 2014) with SELU activation functions (Klambauer et al., 2017) to identify the relevant patterns in the input sequences with a computationally light-weight operation. The larger the kernel size, the more surrounding sequence positions are taken into account, which influences the length of the motifs that can be extracted. We therefore adjust the kernel size during hyperparameter search. In prior works (Ostmeyer et al., 2019), a k-mer size of 4 yielded good predictive performance, which could indicate that a kernel size in the range of 4 may be a proficient choice. For d v trainable kernels, this produces a feature vector of length d v at each sequence position. Subsequently, global max-pooling over all sequence positions reduces the sequence-representations z k to the fixed length d v . Given the challenging size of the input data per repertoire, the computation of the CNN activations and weight updates is performed using 16-bit floating point values. A list of hyperparameters evaluated for DeepRC is given in Table 1 .",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 41,
                    "text": "(Hu et al., 2014)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 73,
                    "end": 97,
                    "text": "(Klambauer et al., 2017)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 1060,
                    "end": 1067,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Keys."
        },
        {
            "text": "Regularization. We apply random and attention-based subsampling of repertoire sequences to reduce over-fitting and increase computational efficiency. During training, each repertoire is subsampled to 10, 000 input sequences, which are randomly drawn from the respective repertoire. This can also be interpreted as random drop-out (Hinton et al., 2012) on the input sequences or attention weights. During training and evaluation, the attention weights computed by the attention network are furthermore used to rank the input sequences. Based on this ranking, the repertoire is reduced to the 10% of sequences with the highest attention weights. These top 10% of sequences are then used to compute the weight updates and the prediction for the repertoire. Additionally, one might employ further regularization techniques, which we did not investigate further due to the high demands to computation time. Such regularization techniques include l1 and l2 weight decay, noise in the form of random AA permutations in the input sequences, noise on the attention weights, or random shuffling of sequences between repertoires that belong to the negative, i.e. healthy, class. The last regularization technique assumes that the sequences in positive, i.e. diseased, class repertoires carry a signal, such as an AA motif corresponding to an immune response, whereas the sequences in negative repertoires do not. Hence, the sequences can be shuffled randomly between negative class repertoires without obscuring the signal in the positive class repertoires.",
            "cite_spans": [
                {
                    "start": 330,
                    "end": 351,
                    "text": "(Hinton et al., 2012)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "Interpretability. DeepRC allows for two forms of interpretability methods. (a) Due to its attention-based design, a trained model can be used to compute the attention weights of a sequence, which directly indicates its importance. . We apply IG to identify the input patterns that are relevant for the classification. To identify AA patterns with high contributions in the input sequences, we apply IG to the AAs in the input sequences. Additionally, we apply IG to the kernels of the 1D CNN, which allows us to identify AA motifs with high contributions. In detail, we compute the IG contributions for the AAs and positional features in the kernels for every repertoire in the validation and test set, so as to exclude potential artifacts caused by over-fitting. Averaging the IG values over these repertoires then results in concise AA motifs. We include qualitative visual analyses of the IG method on different datasets in Appendix A5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Keys."
        },
        {
            "text": "Hyperparameters. Table 1 provides an overview of the hyperparameter search, which was conducted as a gridsearch for each of the datasets in a nested 5-fold crossvalidation (CV) procedure, as described in section 5.2.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 17,
                    "end": 24,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Keys."
        },
        {
            "text": "We aimed at constructing immune repertoire classification scenarios with varying degree of realism and difficulties in order to compare and analyze the suggested machine updates, the current model was evaluated against the validation fold. The early stopping hyperparameter was determined by selecting the model with the best loss on the validation fold after 10 5 updates. * : Experiments for {64; 128; 256} kernels were omitted for datasets with motif implantation probabilities below 1% in category \"simulated immunosequencing data\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "learning methods. To this end, we either use simulated or experimentally-observed immune receptor sequences and we implant signals, specifically, sequence motifs (Akbar et al., 2019; Weber et al., 2020) , at different frequencies into sequences of repertoires of the positive class. It has been shown previously that interaction of immune receptors with antigen occurs via short sequence stretches (Akbar et al., 2019) . Thus, implantation of short motif sequences simulating an immune signal is biologically meaningful. Our benchmarking study comprises four different categories of datasets: (a) Simulated immunosequencing data with implanted signals (where the signal is defined as sets of motifs), (b) LSTM-generated immunosequencing data with implanted signals, (c) real-world immunosequencing data with implanted signals, and (d) real-world immunosequencing data. Each of the first three categories consists of multiple datasets with varying difficulty depending on the type and frequency of the implanted signal. We consider binary classification tasks to simulate the immune status of healthy and diseased individuals. We randomly generate immune repertoires with varying numbers of sequences, where we implant sequence motifs in the repertoires of the diseased individuals, i.e. the positive class. The sequences of a repertoire are also randomly generated by different procedures (detailed below). Each sequence is composed of 20 different characters, corresponding to amino acids, and has an average length of 14.5 AAs.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 182,
                    "text": "(Akbar et al., 2019;",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 183,
                    "end": 202,
                    "text": "Weber et al., 2020)",
                    "ref_id": null
                },
                {
                    "start": 398,
                    "end": 418,
                    "text": "(Akbar et al., 2019)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Simulated immunosequencing data. In the first category, we aim at investigating the impact of the signal frequency and signal complexity on the performance of the different methods. To this end, we created 18 datasets, which each contain a large number of repertoires with a large number of random AA sequences in each repertoire. We then implanted signals in the AA sequences of the positive class repertoires, where the 18 datasets differ in frequency and complexity of the implanted signals. In detail, the AAs in the sequences were sampled randomly independent of their position in the sequence, while the frequencies of AAs, distribution of sequence lengths, and distribution of numbers of sequences per repertoires are following the respective distributions observed in the real-world CMV dataset (Emerson et al., 2017) . For this, we first sampled the number of sequences for a repertoire from a Gaussian N (\u00b5 = 316k, \u03c3 = 132k) distribution and rounded to a positive integer. We re-sampled if the size was below 5k. We then generated random sequences of AAs with a length of N (\u00b5 = 14.5, \u03c3 = 1.8), again rounded to positive integers. Each simulated repertoire was then randomly assigned to either the positive or negative class, with 2, 500 repertoires per class. In the repertoires assigned to the positive class, we implanted motifs with an average length of 4 AA, following the results of the experimental analysis of antigen-binding motifs in antibodies and T-cell receptor sequences (Akbar et al., 2019) . We varied the characteristics of the implanted motifs for each of the 18 datasets with respect to the following parameters: (a) \u03c1, the probability of a motif being implanted in a sequence of a positive repertoire, i.e. the average ratio of sequences containing the motif. In this way, we generated 18 different datasets of variable difficulty containing in total roughly 28.7 billion sequences. See Table 2 for an overview of the properties of the implanted motifs in the 18 datasets.",
            "cite_spans": [
                {
                    "start": 803,
                    "end": 825,
                    "text": "(Emerson et al., 2017)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1495,
                    "end": 1515,
                    "text": "(Akbar et al., 2019)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 1917,
                    "end": 1924,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "LSTM-generated data. In the second dataset category, we investigate the impact of the signal frequency and complexity in combination with more plausible immune receptor sequences by taking into account the positional AA distributions and other sequence properties. To this end, we trained an LSTM (Hochreiter & Schmidhuber, 1997 ) in a standard next character prediction (Graves, 2013) setting to create AA sequences with properties similar to experimentally observed immune receptor sequences.",
            "cite_spans": [
                {
                    "start": 297,
                    "end": 328,
                    "text": "(Hochreiter & Schmidhuber, 1997",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 371,
                    "end": 385,
                    "text": "(Graves, 2013)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "In the first step, the LSTM model was trained on all immunosequences in the CMV dataset (Emerson et al., 2017) that contain valid information about sequence abundance and have a known CMV label. Such an LSTM model is able to capture various properties of the sequence, including position-dependent probability distributions and combinations, relationships, and order of AAs. We then used the trained LSTM model to generate 1, 000 repertoires in an autoregressive fashion, starting with a start sequence that was randomly sampled from the trained-on dataset. Based on a visual inspection of the frequencies of 4-mers (see section A4), the similarity of LSTM generated sequences and real sequences was deemed sufficient for the purpose of generating the AA sequences for the datasets in this category. Details on LSTM training and repertoire generation are given in Appendix A4.",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 110,
                    "text": "(Emerson et al., 2017)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "After generation, each repertoire was assigned to either the positive or negative class, with 500 repertoires per class. We implanted motifs of length 4 and with varying properties in the center of the sequences of the positive class to obtain 5 different datasets. Each sequence in the positive repertoires has a probability \u03c1 to carry the motif, which was varied throughout 5 datasets (see Table 2 ). Each position in the motif has a probability of 0.9 to be implanted, and consequently a probability of 0.1 that the original AA in the sequence remains, which can be seen as noise on the motif. Table 2 . Properties of simulated repertoires, variations of motifs, and motif frequencies for the datasets in categories \"simulated immunosequencing data\", \"LSTM-generated data\", and \"real-world data with implanted signals\". Noise types for * are explained in paragraph \"real-world data with implanted signals\".",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 392,
                    "end": 399,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 597,
                    "end": 604,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Datasets"
        },
        {
            "text": "Real-world data with implanted signals. In the third category, we implanted signals into experimentally obtained immunosequences, where we considered 4 dataset variations. Each dataset consists of 750 repertoires for each of the two classes, where each repertoire consists of 10k sequences. In this way, we aim to simulate datasets with a low sequencing coverage, which means that only relatively few sequences per repertoire are available. The sequences were randomly sampled from healthy (CMV negative) individuals from the CMV dataset (see below paragraph for explanation). Two signal types were considered: (a) One signal with one motif. The AA motif LDR was implanted in a certain fraction of sequences. The pattern is randomly altered at one of the three positions with probabilities 0.2, 0.6, and 0.2, respectively. (b) One signal with multiple motifs. One of the three possible motifs LDR, CAS, and GL-N was implanted with equal probability. Again, the motifs were randomly altered before implantation. The AA motif LDR changed as described above. The AA motif CAS was altered at the second position with probability 0.6 and with probability 0.3 at the first position. The pattern GL-N,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "wheredenotes a gap location, is randomly altered at the first position with probability 0.6 and the gap has a length of 0, 1, or 2 AAs with equal probability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Additionally, the datasets differ in the values for \u03c1, the average ratio of sequences carrying a signal, which were chosen as 1% or 0.1%. The motifs were implanted at positions 3, 5, or 10 in the sequence with probabilities 0.3, 0.35, and 0.2, respectively. With the remaining 0.15 chance, the motif is implanted at any other sequence position.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "Real-world data: CMV dataset. We used a real-world dataset of 785 repertoires, each of which containing between 4, 371 to 973, 081 (avg. 299, 319) TCR sequences with a length of 1 to 27 (avg. 14.5) AAs, originally collected and provided by Emerson et al. (2017) . 340 out of 785 repertoires were labelled as positive for cytomegalovirus (CMV) serostatus, which we consider as the positive class, 420 repertoires with negative CMV serostatus, considered as negative class, and 25 repertoires with unknown status. We changed the number of sequence counts per repertoire from \u22121 to 1 for 3 sequences. Furthermore, we exclude a total of 99 repertoires with unknown CMV status or unknown information about the sequence abundance within a repertoire, reducing the dataset for our analysis to 686 repertoires, 312 of which with positive and 374 with negative CMV status.",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 261,
                    "text": "Emerson et al. (2017)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "We evaluate and compare the performance of DeepRC against a set of machine learning methods that serve as baseline, were suggested, or can readily be adapted to immune repertoire classification. In this section, we describe these compared methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods compared"
        },
        {
            "text": "Known motif. This method serves as an estimate for the achievable classification performance using prior knowledge about which motif was implanted. Note that this does not necessarily lead to perfect predictive performance since motifs are implanted with a certain amount of noise and could also be present in the negative class by chance. The known motif method counts how often the known implanted motif occurs per sequence for each repertoire and uses this count to rank the repertoires. From this ranking, the Area Under the receiver operator Curve (AUC) is computed as performance measure. Probabilistic AA changes in the known motif are not considered for this count, with the exception of gap positions. We consider two versions of this method: (a) Known motif binary: counts the occurrence of the known motif in a sequence and (b) Known motif continuous: counts the maximum number of overlapping AAs between the known motif and all sequence positions, which corresponds to a convolution operation with a binary kernel followed by max-pooling. Since the implanted signal is not known in the experimentally obtained CMV dataset, this method cannot be applied to this dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods compared"
        },
        {
            "text": "SVM. The Support Vector Machine (SVM) approach uses a fixed mapping from a bag of sequences to the corresponding k-mer counts. The function h kmer maps each sequence s k to a vector representing the occurrence of k-mers in the sequence. To avoid confusion with the sequencerepresentation obtained from the CNN layers of DeepRC, we denote u k = h kmer (s k ), which is analogous to z k . Specifically, u km = (h kmer (s k )) m = #{p m \u2208 s k }, where #{p m \u2208 s k } denotes how often the k-mer pattern p m occurs in sequence s k . Afterwards, average-pooling is applied to obtain u = 1/d k d k k=1 u k , the k-mer representation of the input object X. For two input objects X (n) and X (l) with representations u (n) and u (l) , respectively, we implement the MinMax kernel (Ralaivola et al., 2005) as follows:",
            "cite_spans": [
                {
                    "start": 771,
                    "end": 795,
                    "text": "(Ralaivola et al., 2005)",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Methods compared"
        },
        {
            "text": "where u (n) m is the m-th element of the vector u (n) . The Jaccard kernel (Levandowsky & Winter, 1971 ) is identical to the MinMax kernel except that it operates on binary u (n) . We used a standard C-SVM, as introduced by Cortes & Vapnik (1995) . The corresponding hyperparameter C is optimized by random search. The settings of the full hyperparameter search as well as the respective value ranges are given in Appendix Table A1 .",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 102,
                    "text": "(Levandowsky & Winter, 1971",
                    "ref_id": null
                },
                {
                    "start": 224,
                    "end": 246,
                    "text": "Cortes & Vapnik (1995)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 414,
                    "end": 431,
                    "text": "Appendix Table A1",
                    "ref_id": null
                }
            ],
            "section": "Methods compared"
        },
        {
            "text": "KNN. The same k-mer representation of a repertoire, as introduced above for the SVM baseline, is used for the knearest neighbor (KNN) approach. As this method clusters samples according to distances between them, the previous kernel definitions cannot be applied directly. It is therefore necessary to transform the MinMax as well as the Jaccard kernel from similarities to distances by constructing the following (Levandowsky & Winter, 1971) :",
            "cite_spans": [
                {
                    "start": 414,
                    "end": 442,
                    "text": "(Levandowsky & Winter, 1971)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Methods compared"
        },
        {
            "text": "The amount of neighbors is treated as the hyperparameter and optimized by an exhaustive grid search. The settings of the full hyperparameter search as well as the respective value ranges are given in Appendix Table A2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 200,
                    "end": 217,
                    "text": "Appendix Table A2",
                    "ref_id": null
                }
            ],
            "section": "Methods compared"
        },
        {
            "text": "Logistic regression. We implemented logistic regression on the k-mer representation u of an immune repertoire. The model is trained by gradient descent using the Adam optimizer (Kingma & Ba, 2014) . The learning rate is treated as the hyperparameter and optimized by grid search. Furthermore, we explored 2 regularization settings using combinations of l1 and l2 weight decay. The settings of the full hyperparameter search as well as the respective value ranges are given in Appendix Table A3 .",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 196,
                    "text": "(Kingma & Ba, 2014)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 476,
                    "end": 493,
                    "text": "Appendix Table A3",
                    "ref_id": null
                }
            ],
            "section": "Methods compared"
        },
        {
            "text": "Logistic MIL (Ostmeyer et al). The logistic multiple instance learning (MIL) approach for immune repertoire classification (Ostmeyer et al., 2019) applies a logistic regression model to each k-mer representation in a bag. The resulting scores are then summarized by max-pooling to obtain a prediction for the bag. Each amino acid of each k-mer is represented by 5 features, the so-called Atchley factors (Atchley et al., 2005) . As k-mers of length 4 are used, this gives a total of 4 \u00d7 5 = 20 features. One additional feature per 4-mer is added, which represents the relative frequency of this 4-mer with respect to its containing bag, resulting in 21 features per 4-mer. Two options for the relative frequency feature exist, which are (a) whether the frequency of the 4-mer (\"4MER\") or (b) the frequency of the sequence in which the 4-mer appeared (\"TCR\u03b2\") is used. We optimized the learning rate, batch size, and early stopping parameter on the validation set. The settings of the full hyperparameter search as well as the respective value ranges are given in Appendix Table A4 .",
            "cite_spans": [
                {
                    "start": 404,
                    "end": 426,
                    "text": "(Atchley et al., 2005)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 1072,
                    "end": 1080,
                    "text": "Table A4",
                    "ref_id": null
                }
            ],
            "section": "Methods compared"
        },
        {
            "text": "In this section, we report and analyze the predictive power of DeepRC and the compared methods on the datasets described in Section 5.1. The AUC is used as main metric for the predictive power to focus on the ranking of the repertoires instead of the classification boundary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "Hyperparameter selection. We used a nested 5-fold cross validation (CV) procedure to estimate the performance of each of the methods. For all competing methods a hyperparameter search was performed, for which we split each of the 5 training sets into an inner training set and inner validation set. The models were trained on the inner training set and evaluated on the inner validation set. The model with the highest AUC score on the inner validation set is then used to calculate the score on the respective test set. For the hyperparameter search of DeepRC for the category \"simulated immunosequencing data\", we only conducted a large-scale hyperparameter search on the datasets with motif implantation probabilities below 1% due to computational constraints, as described in Table 1 . This process is repeated for all 5 folds of the 5-fold CV and the average score on the 5 test sets constitutes the final score of a method. The results are reported in Table 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 780,
                    "end": 787,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 958,
                    "end": 965,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Results. In each of the four categories, \"real-world data\", \"real-world data with implanted signals\", \"LSTM-generated data\", and \"simulated immunosequencing data\", DeepRC outperforms all competing methods with respect to average AUC (see Table 3 Table 3 . Results in terms of AUC of the method comparisons on all datasets. The reported errors are standard deviations across 5 cross-validation folds (except for the column \"Simulated\"). Real-world CMV: Average performance over 5 cross-validation folds in terms of AUC. Real-world data with implanted signals: Average performance over 5 cross-validation folds in terms of AUC for each of the four datasets. In each dataset, a signal was implanted with different frequency of 1% or 0.1%, and either a single motif (\"OM\") or multiple motifs (\"MM\") were implanted. LSTM-generated data: Average performance over 5 cross-validation folds in terms of AUC for each of the 5 datasets. In each dataset, a signal was implanted with different frequency of 10%, 1%, 0.5%, 0.1%, and 0.05%, respectively. Simulated: Here we report the mean over 18 simulated datasets with implanted signals (see Table A5 for details). The error reported is the standard deviation of the AUC values across datasets that have varying difficulties. More detailed results are provided in Appendix A3.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 238,
                    "end": 245,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 246,
                    "end": 253,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 1130,
                    "end": 1138,
                    "text": "Table A5",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "categories, the runner-up method is the SVM with MinMax kernel.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "Results on simulated immunosequencing data. As mentioned in section 5.1, in this setting the complexity of the implanted signal is in focus and varies throughout 18 simulated datasets. Some datasets are difficult to classify because the implanted motif is hidden by noise and others are challenging to classify because only a small fraction of sequences carry the motif. These difficulties become evident by the method called \"known motif binary\", which assumes the implanted motif is known. The performance of this method ranges from a perfect AUC of 1.00 in several datasets to an AUC of 0.532 in dataset '17' (see Appendix  Table A5 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 617,
                    "end": 635,
                    "text": "Appendix  Table A5",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "DeepRC outperforms all other methods with an average AUC of 0.864 \u00b1 0.223, followed by the SVM with Min-Max kernel with an average AUC of 0.827 \u00b1 0.210 (see Appendix Table A5 ). The predictive performance of all methods suffers if the signal occurs only in an extremely small fraction of sequences, which becomes evident from datasets '8', '11', '14', and '17'. In these datasets, only 0.01% of the sequences carry the motif and all AUC values are below 0.55.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 174,
                    "text": "Table A5",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Results on LSTM-generated data. On the LSTMgenerated data, in which we implanted noisy motifs with frequencies of 10%, 1%, 0.5%, 0.1%, and 0.05%, DeepRC yields almost perfect predictive performance with an average AUC of 1.000 \u00b1 0.001 (see Table A6 ). The second best method, SVM with MinMax kernel, has a similar predictive performance to DeepRC on all 5 datasets but the other competing methods have a lower predictive performance at the datasets with low frequency of the signal (0.05%).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 240,
                    "end": 248,
                    "text": "Table A6",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Results on Real-world data with implanted motifs. In this dataset category, we used real immuno-sequences and implanted single or multiple noisy motifs. Again, DeepRC outperforms all other methods with an average AUC of 0.980 \u00b1 0.029, with the second best method being the SVM with MinMax kernel an average AUC of 0.777 \u00b1 0.258. Notably, all methods except for DeepRC have difficulties with noisy motifs at a frequency of 0.1% (see columns \"One 0.1%\" and \"Multi 0.1%\" in Table A7 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 471,
                    "end": 479,
                    "text": "Table A7",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Results on Real-world data. On the real-world dataset, in which the immune status of persons affected by the cytomegalovirus has to be detected, the competing methods yield predictive AUCs between 0.515 and 0.831. We note that this dataset is not the exact dataset that was used in Emerson et al. (2017) . It differs in several ways in preprocessing steps and also comprises a different set of repertoires, which leads to a more challenging dataset. We are currently working on an immune receptor machine learning platform (Pavlovi\u0107 et al., 2020 ) that aims to enable direct replication and comparability with previous studies. The best performing method is DeepRC with an AUC of 0.831 \u00b1 0.002, followed by the SVM with MinMax kernel (AUC 0.825 \u00b1 0.022) and KNN with the same kernel with an AUC of 0.678 \u00b1 0.076. The top-ranked sequences by DeepRC significantly correspond to those detected by Emerson et al. (2017) , which we tested by a U-test with the null hypothesis that the attention values of the sequences detected by Emerson et al. (2017) would be equal to the attention values of the remaining sequences (p-value 1.3 \u00b7 10 \u221293 ). The sequences and their attention values are displayed in Appendix Table A10 .",
            "cite_spans": [
                {
                    "start": 282,
                    "end": 303,
                    "text": "Emerson et al. (2017)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 523,
                    "end": 545,
                    "text": "(Pavlovi\u0107 et al., 2020",
                    "ref_id": null
                },
                {
                    "start": 894,
                    "end": 915,
                    "text": "Emerson et al. (2017)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1026,
                    "end": 1047,
                    "text": "Emerson et al. (2017)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 1206,
                    "end": 1215,
                    "text": "Table A10",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "We have demonstrated how attention-based deep multiple instance learning can be adapted to the task of classifying the immune status of immune repertoires. For immune state classification, methods have to identify the discriminating sequences among a large set of sequences in an immune repertoire, specifically, even motifs within those sequences have to be identified. We have shown that a 1Dconvolutional network combined with a Transformer-like attention mechanism with a fixed query can solve this difficult task across a range of different experimental conditions. In large-scale experiments, we have compared a set of machine learning methods and previously suggested methods with DeepRC and found that DeepRC yields the best predictive quality with respect to AUC. Furthermore, DeepRC can be interpreted with suitable methods, by which we extract motifs from the sequences (Appendix A5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The current methods are mostly limited by computational complexity, since both hyperparameter and model selection is computationally demanding. For hyperparameter selection, a substantial set of architectures have to be searched to identify a proficient one. For model selection, a single repertoire requires the propagatation of many thousands of sequences through a neural network and keep those quantities in GPU memory for the attention mechanism. Thus, increased GPU memory would significantly boost our approach. Increased computational power would also allow for more advanced architectures and attention mechanisms, which may further improve predictive performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We envision that with the increasing availability of large immunosequencing datasets (Kovaltsuk et al., 2018; Corrie et al., 2018; Christley et al., 2018; Zhang et al., 2020) , further fine-tuning of ground-truth benchmarking immune receptor datasets (Weber et al., 2020; Olson et al., 2019; Marcou et al., 2018) , increased GPU memory, and increased computing power, it will be possible to identify discriminating immune receptor motifs for many diseases, potentially even for the current SARS-CoV-2 (CoViD-19) pandemic. Such results would pave the way towards antibody and TCR-driven immunotherapies and immunodiagnostics as well as rational vaccine design.",
            "cite_spans": [
                {
                    "start": 85,
                    "end": 109,
                    "text": "(Kovaltsuk et al., 2018;",
                    "ref_id": null
                },
                {
                    "start": 110,
                    "end": 130,
                    "text": "Corrie et al., 2018;",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 131,
                    "end": 154,
                    "text": "Christley et al., 2018;",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 155,
                    "end": 174,
                    "text": "Zhang et al., 2020)",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 251,
                    "end": 271,
                    "text": "(Weber et al., 2020;",
                    "ref_id": null
                },
                {
                    "start": 272,
                    "end": 291,
                    "text": "Olson et al., 2019;",
                    "ref_id": null
                },
                {
                    "start": 292,
                    "end": 312,
                    "text": "Marcou et al., 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "All datasets and code will be fully released at https://github.com/ml-jku/DeepRC. The CMV dataset is publicly available at https: //clients.adaptivebiotech.com/pub/ Emerson-2017-NatGen. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Availability"
        },
        {
            "text": "Known motif. This method has no hyperparameters and has been applied to all datasets except for the CMV dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A1. Details on hyperparameter selection"
        },
        {
            "text": "The corresponding hyperparameter C of the SVM is optimized by randomly drawing 10 3 values in the range of [\u22126; 6] according to a uniform distribution. These values act as the exponents of a power of 10 and are applied for each of the two kernel types (see Appendix A1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SVM."
        },
        {
            "text": "C 10 [\u22126;6] ] type of kernel {MinMax; Jaccard} number of trials 10 3 Table A1 . Settings used in the hyperparameter search of the SVM baseline approach. The number of trials defines the quantity of random values of the C penalty term (per type of kernel).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 69,
                    "end": 77,
                    "text": "Table A1",
                    "ref_id": null
                }
            ],
            "section": "SVM."
        },
        {
            "text": "KNN. The amount of neighbors is treated as the hyperparameter and optimized by grid search operating in the discrete range of [1; max{N, 10 3 }] with a step size of 1. The corresponding tight upper bound is automatically defined by the total amount of samples N \u2208 N >0 in the training set, capped at 10 3 (see Appendix A2). number of neighbors 1; max{N, 10 3 } type of kernel {MinMax; Jaccard} Table A2 . Settings used in the hyperparameter search of the KNN baseline approach. The number of trials (per type of kernel) is automatically defined by the total amount of samples N \u2208 N>0 in the training set, capped at 10 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 394,
                    "end": 402,
                    "text": "Table A2",
                    "ref_id": null
                }
            ],
            "section": "SVM."
        },
        {
            "text": "Logistic regression. Hyperparameter optimization strategy: grid search.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SVM."
        },
        {
            "text": "learning rate 10 \u2212{2;3;4} batch size 4 max. updates 10 5 coefficient \u03b2 1 (Adam) 0.9 coefficient \u03b2 2 (Adam) 0.999 weight decay weightings {(l1 = 10 \u22127 , l2 = 10 \u22123 ); (l1 = 10 \u22127 , l2 = 10 \u22125 )} Table A3 . Settings used in the hyperparameter search of the Logistic Regression baseline approach.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 194,
                    "end": 202,
                    "text": "Table A3",
                    "ref_id": null
                }
            ],
            "section": "SVM."
        },
        {
            "text": "Logistic MIL. For this method, we adjusted the learning rate as well as the batch size as hyperparameters by randomly drawing 25 different hyperparameter combinations from a uniform distribution. The corresponding range of the learning rate is [\u22124.5; \u22121.5], which acts as the exponent of a power of 10. The batch size lies within the range of [1; 32]. For each hyperparameter combination, a model is optimized by gradient descent using Adam, whereas the early stopping parameter is adjusted according to the corresponding validation set (see Table A4 ). learning rate 10 [\u22124.5;\u22121.5] batch size [1; 32] relative abundance term {4MER; TCR\u03b2} number of trials 25 max. epochs 10 2 coefficient \u03b2 1 0.9 coefficient \u03b2 2 0.999 Table A4 . Settings used in the hyperparameter search of the Logistic MIL baseline approach. The number of trials (per type of relative abundance) defines the quantity of combinations of random values of the learning rate as well as the batch size.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 542,
                    "end": 550,
                    "text": "Table A4",
                    "ref_id": null
                },
                {
                    "start": 718,
                    "end": 726,
                    "text": "Table A4",
                    "ref_id": null
                }
            ],
            "section": "SVM."
        },
        {
            "text": "In this section we provide more detailed information on the implementation of DeepRC.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A2. DeepRC implementation details"
        },
        {
            "text": "Positional features. We use 3 features to encode the relative position of each AA, as illustrated in Figure A1 . Incorporation of additional inputs and metadata. Additional metadata in the form of sequence-level or repertoire-level features could be input via concatenation to the feature vectors that result from the maximum over the sequence positions of the 1D-CNN output. This has the benefit that the attention mechanism and output network can utilize the sequence-level or repertoire-level features for their predictions. Sparse metadata or metadata that is only available during training could be used as auxiliary targets to incorporate the information via gradients into the DeepRC model. \u00b1 0.000 \u00b1 0.000 \u00b1 0.271 \u00b1 0.000 \u00b1 0.000 \u00b1 0.218 \u00b1 0.000 \u00b1 0.000 \u00b1 0.029 \u00b1 0.000 \u00b1 0.001 \u00b1 0.017 \u00b1 0.001 \u00b1 0.002 \u00b1 0.023 \u00b1 0.001 \u00b1 0.048 \u00b1 0.013 \u00b1 0.223 SVM (MinMax) 1.000 1.000 0.764 1.000 1.000 0.603 1.000 0.998 0.539 1.000 0.994 0.529 1.000 0.741 0.513 1.000 0.706 0.503 0.827 \u00b1 0.000 \u00b1 0.000 \u00b1 0.016 \u00b1 0.000 \u00b1 0.000 \u00b1 0.021 \u00b1 0.000 \u00b1 0.002 \u00b1 0.024 \u00b1 0.000 \u00b1 0.004 \u00b1 0.016 \u00b1 0.000 \u00b1 0.024 \u00b1 0.006 \u00b1 0.000 \u00b1 0.013 \u00b1 0.013 \u00b1 0.035 \u00b1 0.020 \u00b1 0.013 \u00b1 0.015 \u00b1 0.019 \u00b1 0.014 \u00b1 0.017 \u00b1 0.011 \u00b1 0.018 \u00b1 0.013 \u00b1 0.004 \u00b1 0.017 \u00b1 0.011 \u00b1 0.017 \u00b1 0.022 \u00b1 0.015 \u00b1 0.020 \u00b1 0.012 \u00b1 0.007 Logistic Regression 1.000 1.000 0.786 1.000 1.000 0.607 1.000 0.997 0.527 1.000 0.992 0.526 1.000 0.719 0.505 1.000 0.694 0.510 0.826 \u00b1 0.000 \u00b1 0.000 \u00b1 0.009 \u00b1 0.000 \u00b1 0.000 \u00b1 0.025 \u00b1 0.000 \u00b1 0.002 \u00b1 0.018 \u00b1 0.000 \u00b1 0.004 \u00b1 0.019 \u00b1 0.000 \u00b1 0.019 \u00b1 0.015 \u00b1 0.001 \u00b1 0.021 \u00b1 0.017 \u00b1 0.211",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 101,
                    "end": 110,
                    "text": "Figure A1",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "A2. DeepRC implementation details"
        },
        {
            "text": "Logistic MIL (KMER) \u00b1 0.000 \u00b1 0.000 \u00b1 0.004 \u00b1 0.000 \u00b1 0.000 \u00b1 0.004 \u00b1 0.000 \u00b1 0.000 \u00b1 0.020 \u00b1 0.000 \u00b1 0.002 \u00b1 0.017 \u00b1 0.000 \u00b1 0.010 \u00b1 0.024 \u00b1 0.000 \u00b1 0.016 \u00b1 0.020 \u00b1 0.001 \u00b1 0.014 \u00b1 0.020 \u00b1 0.001 \u00b1 0.013 \u00b1 0.017 \u00b1 0.001 \u00b1 0.012 \u00b1 0.012 \u00b1 0.001 \u00b1 0.018 \u00b1 0.018 \u00b1 0.002 \u00b1 0.010 \u00b1 0.009 \u00b1 0.002 \u00b1 0.012 \u00b1 0.013 \u00b1 0.202 Table A5 . AUC estimates based on 5-fold CV for all 18 datasets in category \"simulated immunosequencing data\". The reported errors are standard deviations across the 5 cross-validation folds except for the last column \"avg.\", in which they show standard deviations across datasets. Wild-card characters in motifs are indicated by Z, characters with 50% probability of being removed by d . Known motif c.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 316,
                    "end": 324,
                    "text": "Table A5",
                    "ref_id": null
                }
            ],
            "section": "ID"
        },
        {
            "text": "1.000 \u00b1 0.000 1.000 \u00b1 0.000 0.989 \u00b1 0.011 0.722 \u00b1 0.085 0.626 \u00b1 0.094 0.867 \u00b1 0.180 Table A6 . AUC estimates based on 5-fold CV for all 5 datasets in category \"LSTM-generated data\". The reported errors are standard deviations across the 5 cross-validation folds except for the last column \"avg.\", in which they show standard deviations across datasets. Characters affected by noise, as described in 5.1, paragraph \"\"LSTM-generated data\", are indicated by r . Table A7 . AUC estimates based on 5-fold CV for all 4 datasets in category \"real-world data with implanted signals\". The reported errors are standard deviations across the 5 cross-validation folds except for the last column \"avg.\", in which they show standard deviations across datasets. One Motif 1%: In this dataset, a single motif with a frequency of 1% was implanted. One 0.1%: In this dataset, a single motif with a frequency of 0.1% was implanted. Multi 1%: In this dataset, multiple motifs with a frequency of 1% were implanted. Multi 0.1%: In this dataset, multiple motifs with a frequency of 0.1% were implanted. A detailed description of the motifs is provided in section 5. Logistic MIL (TCRB) 0.515 \u00b1 0.073 0.000 \u00b1 0.000 0.496 \u00b1 0.008 0.541 \u00b1 0.039 Table A8 . Results on the CMV dataset (real world data) in terms of AUC, F1 score, balanced accuracy, and accuracy. For F1 score, balanced accuracy, and accuracy, all methods use their default thresholds. Each entry shows mean and standard deviation across 5 cross-validation folds.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 92,
                    "text": "Table A6",
                    "ref_id": null
                },
                {
                    "start": 459,
                    "end": 467,
                    "text": "Table A7",
                    "ref_id": null
                },
                {
                    "start": 1220,
                    "end": 1228,
                    "text": "Table A8",
                    "ref_id": null
                }
            ],
            "section": "ID"
        },
        {
            "text": "We trained a classic next-character LSTM model (Graves, 2013) based on the implementation https://github.com/ spro/practical-pytorch using PyTorch 1.3.1 (Paszke et al., 2019). For this, we appled an LSTM model with 100 LSTM blocks in 2 layers, which was trained for 5, 000 epochs using the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.01, an input batchsize of 100 character chunks, and a character chunk lenght of 200. As input we used the immuno-sequences in the CDR3 column of the CMV dataset, where we repeated sequences according to the counts of the sequences in the repertoires, as specified in the templates column of the CMV dataset. We excluded repertoires with unknown CMV status and unknown sequence abundance from training.",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 61,
                    "text": "(Graves, 2013)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "A4. Next-character LSTM"
        },
        {
            "text": "After training, we generated 1, 000 repertoires using a temperature value of 0.8. The number of sequences per repertoire was sampled from a Gaussian N (\u00b5 = 285k, \u03c3 = 156k) distribution, where the whole repertoire was generated by the LSTM at once. That is, the LSTM can base the generation of the individual AA sequences in a repertoire, including the AAs and the lengths of the sequences, on the generated repertoire. A random immuno-sequence from the trained-on repertoires was used as initialization for the generation process. This immuno-sequence was not included in the generated repertoire.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A4. Next-character LSTM"
        },
        {
            "text": "Finally, we randomly assigned 500 of the generated repertoires to the positive (diseased) and 500 to the negative (healthy) class. We then implanted motifs in the positive class repertoires as described in section 5, paragraph \"LSTM-generated data.\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A4. Next-character LSTM"
        },
        {
            "text": "As illustrated in the comparison of histograms given in Table A4 , the generated immuno-sequences exhibit a very similar distribution of 4-mers and AAs compared to the original CMV dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 56,
                    "end": 64,
                    "text": "Table A4",
                    "ref_id": null
                }
            ],
            "section": "A4. Next-character LSTM"
        },
        {
            "text": "Real-World data LSTM generated data ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A4. Next-character LSTM"
        },
        {
            "text": "In this section, we provide examples for the interpretation of trained DeepRC models using Integrated Gradients (IG) (Sundararajan et al., 2017) as contribution analysis method. Application of IG was performed as described in section 4, paragraph \"Interpretability\". The following illustrations were created using 50 IG steps, which we found sufficient to achieve stable IG results.",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 144,
                    "text": "(Sundararajan et al., 2017)",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "A5. Interpreting DeepRC"
        },
        {
            "text": "A visual analysis of DeepRC models on the simulated datasets, as illustrated in Tab. A9 and Fig. A3 , shows that the implanted motifs can be successfully extracted from the trained model and are straight-forward to interpret. In the real-world CMV dataset, DeepRC finds complex patterns with high variability in the center regions of the immuno-sequences, as illustrated in figure A4 . implanted motif(s) G r S r A r F r L r D r R r {L r D r R r ; C r A r S; G r L-N} motif freq. \u03c1 0.05% 0.1% 0.1% Table A9 . Visualization of motifs extracted from trained DeepRC models for datasets from categories \"simulated immunosequencing data\", \"LSTM-generated data\", and \"real-world data with implanted signals\". Motif extraction was performed using Integrated Gradients on the 1D CNN kernels over the validation set and test set repertoires of one CV fold. Wild-card characters are indicated by Z, random noise on characters by r , characters with 50% probability of being removed by d , and gap locations of random lengths of {0; 1; 2} by -. Larger characters in the extracted motifs indicate higher contribution, with blue indicating positive contribution and red indicating negative contribution towards the prediction of the diseased class. Contributions to positional encoding are indicated by < (beginning of sequence), \u2227 (center of sequence), and > (end of sequence). Only kernels with relatively high contributions are shown, i.e. with contributions roughly greater than the average contribution of all kernels. a) b) c) Figure A3 . Integrated Gradients applied to input sequences of positive class repertoires. Three sequences with the highest contributions to the prediction of their respective repertoires are shown. a) Input sequence taken from \"simulated immunosequencing data\" with implanted motif SZ d Z d N and motif implantation probability 0.1%. The DeepRC model reacts to the S and N at the 5 th and 8 th sequence position, thereby identifying the implanted motif in this sequence. b) and c) Input sequence taken from \"real-world data with implanted signals\" with implanted motifs {L r D r R r ; C r A r S; G r L-N} and motif implantation probability 0.1%. The DeepRC model reacts to the fully implanted motif CAS (b) and to the partly implanted motif AAs C and A at the 5 th and 7 th sequence position (c), thereby identifying the implanted motif in the sequences. Wild-card characters in implanted motifs are indicated by Z, characters with 50% probability of being removed by d , and gap locations of random lengths of {0; 1; 2} by -. Larger characters in the sequences indicate higher contribution, with blue indicating positive contribution and red indicating negative contribution towards the prediction of the diseased class. Figure A4 . Visualization of the contributions of characters within a sequence via IG. Each sequence was selected from a different repertoire and showed the highest contribution in its repertoire. Model was trained on CMV dataset, using a kernel size of 9, 32 kernels and 137 repertoires for early stopping. Larger characters in the extracted motifs indicate higher contribution, with blue indicating positive contribution and red indicating negative contribution towards the prediction of the disease class. Table A10 . TCR\u03b2 sequences that had been discovered by Emerson et al. (2017) with their associated attention-values by DeepRC. These sequences have significantly (p-value 1.3e-93) higher attention values than other sequences. The column \"quantile\" provides the quantile values of the empiricial distribution of attention values across all sequences in the dataset.",
            "cite_spans": [
                {
                    "start": 3307,
                    "end": 3328,
                    "text": "Emerson et al. (2017)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 92,
                    "end": 99,
                    "text": "Fig. A3",
                    "ref_id": null
                },
                {
                    "start": 374,
                    "end": 383,
                    "text": "figure A4",
                    "ref_id": null
                },
                {
                    "start": 498,
                    "end": 506,
                    "text": "Table A9",
                    "ref_id": null
                },
                {
                    "start": 1520,
                    "end": 1529,
                    "text": "Figure A3",
                    "ref_id": null
                },
                {
                    "start": 2743,
                    "end": 2752,
                    "text": "Figure A4",
                    "ref_id": null
                },
                {
                    "start": 3252,
                    "end": 3261,
                    "text": "Table A10",
                    "ref_id": null
                }
            ],
            "section": "A5. Interpreting DeepRC"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A compact vocabulary of paratope-epitope interactions enables predictability of antibody-antigen binding. bioRxiv",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Akbar",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "A"
                    ],
                    "last": "Robert",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pavlovi\u0107",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Jeliazkov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Snapkov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Slabodkin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Weber",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Scheffer",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Miho",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "H"
                    ],
                    "last": "Haff",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Predicting the sequence specificities of dna-and rna-binding proteins by deep learning",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Alipanahi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Delong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "T"
                    ],
                    "last": "Weirauch",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "J"
                    ],
                    "last": "Frey",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Nature biotechnology",
            "volume": "33",
            "issn": "8",
            "pages": "831--838",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "visualized gluten-specific cd4+ t cells in blood as a potential diagnostic marker for coeliac disease without oral gluten challenge",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Arras",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Arjona-Medina",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Widrich",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Montavon",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gillhofer",
                    "suffix": ""
                },
                {
                    "first": "K.-R",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Samek",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Explainable ai: Interpreting, explaining and visualizing deep learning",
            "volume": "2",
            "issn": "",
            "pages": "268--278",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "ireceptor: A platform for querying and analyzing antibody/b-cell and t-cell receptor repertoire data across federated repositories",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "D"
                    ],
                    "last": "Corrie",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Marthandan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zimonja",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jaglale",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Barr",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Knoetze",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "M"
                    ],
                    "last": "Breden",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Christley",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Scott",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Immunological reviews",
            "volume": "284",
            "issn": "1",
            "pages": "24--41",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Support-vector networks. Machine learning",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cortes",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vapnik",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "",
            "volume": "20",
            "issn": "",
            "pages": "273--297",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Quantifiable predictive features define epitope-specific t cell receptor repertoires",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dash",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Fiore-Gartland",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hertz",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "C"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Souquette",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Crawford",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "B"
                    ],
                    "last": "Clemens",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "H"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kedzierska",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nature",
            "volume": "547",
            "issn": "7661",
            "pages": "89--93",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "pre-training of deep bidirectional transformers for language understanding. ArXiv",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "M.-W",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bert",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "M.-W",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {
                "DOI": [
                    "10.18653/v1/N19-1423"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Solving the multiple instance problem with axis-parallel rectangles",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "G"
                    ],
                    "last": "Dietterich",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "H"
                    ],
                    "last": "Lathrop",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Lozano-P\u00e9rez",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Artificial intelligence",
            "volume": "89",
            "issn": "1-2",
            "pages": "31--71",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Predicting the spectrum of tcr repertoire sharing with a data-driven model of recombination",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Elhanati",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sethna",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "G"
                    ],
                    "last": "Callan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mora",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Walczak",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Immunological reviews",
            "volume": "284",
            "issn": "1",
            "pages": "167--179",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Immunosequencing identifies signatures of cytomegalovirus exposure history and hla-mediated effects on the t cell repertoire",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "O"
                    ],
                    "last": "Emerson",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "S"
                    ],
                    "last": "Dewitt",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Vignali",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gravley",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "J"
                    ],
                    "last": "Osborne",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Desmarais",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Klinger",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "S"
                    ],
                    "last": "Carlson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Hansen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nature genetics",
            "volume": "49",
            "issn": "5",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Predicting antigen-specificity of single t-cells based on tcr cdr3 regions",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schubert",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "J"
                    ],
                    "last": "Theis",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "BioRxiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "The promise and challenge of high-throughput sequencing of the antibody repertoire",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Georgiou",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "C"
                    ],
                    "last": "Ippolito",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Beausang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "E"
                    ],
                    "last": "Busse",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wardemann",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Quake",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Nature biotechnology",
            "volume": "32",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Tcrex: detection of enriched t cell epitope specificity in full t cell receptor sequence repertoires. bioRxiv",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gielis",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Moris",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Bittremieux",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "De Neuter",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ogunjimi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Laukens",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Meysman",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Identifying specificity groups in the t cell receptor repertoire",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Glanville",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nau",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Hatton",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "E"
                    ],
                    "last": "Wagar",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Rubelt",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Krams",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pettus",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nature",
            "volume": "547",
            "issn": "7661",
            "pages": "94--98",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Generating sequences with recurrent neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Graves",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1308.0850"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A bioinformatic framework for immune repertoire diversity profiling enables detection of immunological status",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Greiff",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bhat",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Cook",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Menzel",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Reddy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Genome medicine",
            "volume": "7",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Learning the highdimensional immunogenomic features that predict public and private antibody repertoires",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Greiff",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Weber",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Palme",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Bodenhofer",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Miho",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Menzel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Reddy",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "The Journal of Immunology",
            "volume": "199",
            "issn": "8",
            "pages": "2985--2997",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Improving neural networks by preventing co-adaptation of feature detectors",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "R"
                    ],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1207.0580"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural computation",
            "volume": "9",
            "issn": "8",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Fast modelbased protein homology detection without alignment",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Heusel",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Obermayer",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Bioinformatics",
            "volume": "23",
            "issn": "14",
            "pages": "1728--1736",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Convolutional neural network architectures for matching natural language sentences",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Chen",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "2042--2050",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Attention-based deep multiple instance learning",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ilse",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Tomczak",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Machine Learning (ICML)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Nettcr: sequence-based prediction of tcr binding to peptide-mhc complexes using convolutional neural networks. bioRxiv",
            "authors": [
                {
                    "first": "V",
                    "middle": [
                        "I"
                    ],
                    "last": "Jurtz",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "E"
                    ],
                    "last": "Jessen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Bentzen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "Jespersen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mahajan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vita",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "Jensen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Marcatili",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Hadrup",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Peters",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "R"
                    ],
                    "last": "Kelley",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Snoek",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "L"
                    ],
                    "last": "Rinn",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Genome research",
            "volume": "26",
            "issn": "7",
            "pages": "990--999",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Deep learning on point sets for 3d classification and segmentation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kimeswenger",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Rumetshofer",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hofmarcher",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tschandl",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kittler",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "H\u00f6tzenecker",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mo",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Guibas",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pointnet",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "652--660",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Graph kernels for chemical informatics",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ralaivola",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Swamidass",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Saigo",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Baldi",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Neural networks",
            "volume": "18",
            "issn": "8",
            "pages": "1093--1110",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Vdjtools: unifying post-analysis of t cell receptor repertoires",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Shugay",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "V"
                    ],
                    "last": "Bagaev",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Turchaninova",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "A"
                    ],
                    "last": "Bolotin",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [
                        "V"
                    ],
                    "last": "Britanova",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "V"
                    ],
                    "last": "Putintseva",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "V"
                    ],
                    "last": "Pogorelyy",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "I"
                    ],
                    "last": "Nazarov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "V"
                    ],
                    "last": "Zvyagin",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "I"
                    ],
                    "last": "Kirgizova",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "PLoS computational biology",
            "volume": "11",
            "issn": "11",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Deeptcr: a deep learning framework for understanding t-cell receptor sequence signatures within complex t-cell repertoires. bioRxiv",
            "authors": [
                {
                    "first": "J.-W",
                    "middle": [],
                    "last": "Sidhom",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "B"
                    ],
                    "last": "Larman",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ross-Macdonald",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wind-Rotolo",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Pardoll",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Baras",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Prediction of specific tcr-peptide binding from large dictionaries of tcr-peptide pairs. bioRxiv",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Springer",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Besser",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tickotsky-Moskovitz",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dvorkin",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Louzoun",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Axiomatic attribution for deep networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sundararajan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Taly",
                    "suffix": ""
                },
                {
                    "first": "Yan",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 34th International Conference on Machine Learning",
            "volume": "70",
            "issn": "",
            "pages": "3319--3328",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Attention-based deep neural networks for detection of cancerous and precancerous esophagus tissue on histopathological slides",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tomita",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Abdollahi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Suriawinata",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hassanpour",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "JAMA network open",
            "volume": "2",
            "issn": "11",
            "pages": "1914645--1914645",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        ";"
                    ],
                    "last": "Polosukhin",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [
                        "V"
                    ],
                    "last": "Luxburg",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wallach",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vishwanathan",
                    "suffix": ""
                },
                {
                    "first": "Garnett",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "30",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "\u0141",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Revisiting multiple instance neural networks",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Pattern Recognition",
            "volume": "74",
            "issn": "",
            "pages": "15--24",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Novel approaches to analyze immunoglobulin repertoires",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wardemann",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "E"
                    ],
                    "last": "Busse",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Trends in immunology",
            "volume": "38",
            "issn": "7",
            "pages": "471--482",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "immuneSIM: tunable multi-feature simulation of B-and T-cell receptor repertoires for immunoinformatics benchmarking",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Weber",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Akbar",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Yermanos",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pavlovi\u0107",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Snapkov",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "K"
                    ],
                    "last": "Sandve",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Reddy",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Greiff",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Bioinformatics",
            "volume": "03",
            "issn": "2020",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btaa158"
                ]
            }
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Polyspecificity of t cell and b cell receptor recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "W"
                    ],
                    "last": "Wucherpfennig",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "M"
                    ],
                    "last": "Allen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Celada",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "R"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "De Boer",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "C"
                    ],
                    "last": "Garcia",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Goldstein",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Greenspan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Hafler",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hodgkin",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Seminars in immunology",
            "volume": "19",
            "issn": "",
            "pages": "216--224",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Practical guidelines for b-cell receptor repertoire sequencing analysis",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yaari",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Kleinstein",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Genome medicine",
            "volume": "7",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Learning embedding adaptation for few-shot learning",
            "authors": [
                {
                    "first": "H.-J",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "D.-C",
                    "middle": [],
                    "last": "Zhan",
                    "suffix": ""
                },
                {
                    "first": "Sha",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1812.03664"
                ]
            }
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Convolutional neural network architectures for predicting dna-protein binding",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "D"
                    ],
                    "last": "Edwards",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "K"
                    ],
                    "last": "Gifford",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Bioinformatics",
            "volume": "32",
            "issn": "12",
            "pages": "121--127",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Pan immune repertoire database",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Du",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "36",
            "issn": "",
            "pages": "897--903",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Predicting effects of noncoding variants with deep learning-based sequence model",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [
                        "G"
                    ],
                    "last": "Troyanskaya",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Nature methods",
            "volume": "12",
            "issn": "10",
            "pages": "931--934",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Schematic representation of the DeepRC architecture. d l indicates the sequence length.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "(b) DeepRC furthermore allows for the usage of contribution analysis methods, such as Integrated Gradients (IG) (Sundararajan et al., 2017) or Layer-Wise Relevance Propagation (Montavon et al., 2018; Arras et al., 2019; Montavon et al., 2019; Preuer et al., 2019)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "(b) The number of wild-card positions in the motif. A wild-card position contains a random AA character which is randomly sampled for each sequence. Wild-card positions are located in the center of the implanted motif. (c) The number of deletion positions in the implanted motif. A deletion position has probability of 0.5 of being removed from the motif. Deletion positions are located in the center of the implanted motifs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Atchley, W. R., Zhao, J., Fernandes, A. D., and Dr\u00fcke, T. Solving the protein sequence metric problem. Proceedings of the National Academy of Sciences, 102(18): 6395-6400, 2005. Bashford-Rogers, R., Bergamaschi, L., McKinney, E., Pombal, D., Mescia, F., Lee, J., Thomas, D., Flint, S., Kellam, P., Jayne, D., et al. Analysis of the b cell receptor repertoire in six immune-mediated diseases. Nature, 574 (7776):122-126, 2019. Brown, A. J., Snapkov, I., Akbar, R., Pavlovi\u0107, M., Miho, E., Sandve, G. K., and Greiff, V. Augmenting adaptive immunity: progress and challenges in the quantitative engineering and analysis of adaptive immune receptor repertoires. Molecular Systems Design & Engineering, 4 (4):701-736, 2019. Christley, S., Scarborough, W., Salinas, E., Rounds, W. H., Toby, I. T., Fonner, J. M., Levin, M. K., Kim, M., Mock, S. A., Jordan, C., et al. Vdjserver: a cloud-based analysis portal and data commons for immune repertoire sequences and rearrangements. Frontiers in immunology, 9: 976, 2018. Christophersen, A., R\u00e1ki, M., Bergseng, E., Lundin, K. E., Jahnsen, J., Sollid, L. M., and Qiao, S.-W. Tetramer-W., and Klambauer, G. Detecting cutaneous basal cell carcinomas in ultra-high resolution and weakly labelled histopathological images. arXiv preprint arXiv:1911.06616, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. Self-normalizing neural networks. In Advances in neural information processing systems, pp. 971-980, 2017. Konishi, H., Komura, D., Katoh, H., Atsumi, S., Koda, H., Yamamoto, A., Seto, Y., Fukayama, M., Yamaguchi, R., Imoto, S., et al. Capturing the differences between humoral immunity in the normal and tumor environments from repertoire-seq of b-cell receptors using supervised machine learning. BMC bioinformatics, 20(1): 1-11, 2019. Kovaltsuk, A., Leem, J., Kelm, S., Snowden, J., Deane, C. M., and Krawczyk, K. Observed antibody space: A resource for data mining next-generation sequencing of antibody repertoires. The Journal of Immunology, 201(8): 2502-2509, 2018. Kraus, O. Z., Ba, J. L., and Frey, B. J. Classifying and segmenting microscopy images with deep multiple instance learning. Bioinformatics, 32(12):i52-i59, 2016. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pp. 3744-3753, 2019. Levandowsky, M. and Winter, D. Distance between sets. Nature, 234(5323):34-35, 1971. Liu, X., Zhang, W., Zhao, M., Fu, L., Liu, L., Wu, J., Luo, S., Wang, L., Wang, Z., Lin, L., et al. T cell receptor \u03b2 repertoires as novel diagnostic markers for systemic lupus erythematosus and rheumatoid arthritis. Annals of the rheumatic diseases, 78(8):1070-1078, 2019. Marcou, Q., Mora, T., and Walczak, A. M. High-throughput immune repertoire analysis with igor. Nature communications, 9(1):1-10, 2018. Maron, O. and Lozano-P\u00e9rez, T. A framework for multipleinstance learning. In Advances in neural information processing systems, pp. 570-576, 1998. Miho, E., Yermanos, A., Weber, C. R., Berger, C. T., Reddy, S. T., and Greiff, V. Computational strategies for dissecting the high-dimensional complexity of adaptive immune repertoires. Frontiers in immunology, 9:224, 2018.Montavon, G., Samek, W., and M\u00fcller, K.-R. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 73:1-15, 2018. Montavon, G., Binder, A., Lapuschkin, S., Samek, W., and M\u00fcller, K.-R. Layer-wise relevance propagation: an overview. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 193-209. Springer, 2019. Mora, T. and Walczak, A. M. How many different clonotypes do immune repertoires contain? Current Opinion in Systems Biology, 18:104 -110, 2019. ISSN 2452-3100. doi: https://doi.org/10.1016/j.coisb.2019.10. 001. URL http://www.sciencedirect.com/ science/article/pii/S2452310019300289.Moris, P., De Pauw, J., Postovskaya, A., Ogunjimi, B., Laukens, K., and Meysman, P. Treating biomolecular interaction as an image classification problem-a case study on t-cell receptor-epitope recognition prediction. bioRxiv, 2019. Olson, B. J., Moghimi, P., Schramm, C., Obraztsova, A., Ralph, D. K., Vander Heiden, J. A., Shugay, M., Shepherd, A. J., Lees, W. D., Matsen, I., et al. sumrep: a summary statistic framework for immune receptor repertoire comparison and model validation. Frontiers in immunology, 10:2533, 2019. Ostmeyer, J., Christley, S., Toby, I. T., and Cowell, L. G. Biophysicochemical motifs in t-cell receptor sequences distinguish repertoires from tumor-infiltrating lymphocyte and adjacent healthy tissue. Cancer research, 79(7): 1671-1680, 2019. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024-8035, 2019. Pavlovi\u0107, M., Scheffer, L., Chernigovskaya, M., Widrich, M., Klambauer, G., Greiff, V., and Sandve, G.K. An open-source platform for novice to expert-operated interpretable machine learning analysis of immune receptor data. In preparation, 2020.Pawlowski, N., Bhooshan, S., Ballas, N., Ciompi, F., Glocker, B., and Drozdzal, M. Needles in haystacks: On classifying tiny objects in large images. arXiv preprint arXiv:1908.06037, 2019.Preuer, K., Klambauer, G., Rippmann, F., Hochreiter, S., and Unterthiner, T. Interpretable deep learning in drug discovery. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 331-345. Springer, 2019.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The inputs are provided to the DeepRC network as 16 bit floating point values. Sequences of different lengths were zero-padded to the maximum sequence length per batch at the sequence ends. We use 3 input features with values in range [0, 1] to encode the relative position of each AA in a sequence with respect to the sequence. \"feature 1\" encodes if an AA is close to the sequence start, \"feature 2\" to the sequence center, and \"feature 3\" to the sequence end. For every position in the sequence, the values of all three features sum up to 1.Computation time and optimization. As mentioned in section 4, we took measures to meet the high demands of computation time and GPU memory consumption in our implementation, in order to make the large number of experiments feasible. We train the DeepRC model with a small batch size of 4 samples and perform computation of inference and updates of the 1D CNN using 16 bit floating point values. The rest of the network is trained using 32 bit floating point values. The Adam parameter for numerical stability was therefore set to an increased value of = 10 \u22124 . Training was performed on various GPU types, mainly RTX 2080 Ti. Computation times were highly dependent on the number of sequences in the repertoires and the number and sizes of CNN kernels. A single update on a RTX 2080 Ti GPU took approximately 0.0129 to 0.0135 seconds, while requiring approximately 8 to 11 GB GPU memory. Taking these optimizations and GPUs with larger memory (\u2265 16 GB) into account, it would already be feasible to train DeepRC, possibly with multi-head attention and a larger network architecture, on larger datasets. Our network implementation is based on PyTorch 1.3.1 (Paszke et al., 2019).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "0.010 \u00b1 0.009 \u00b1 0.010 \u00b1 0.009 \u00b1 0.018 \u00b1 0.018 \u00b1 0.011 \u00b1 0.010 \u00b1 0.009 \u00b1 0.007 \u00b1 0.017 \u00b1 0.013 \u00b1 0.007 \u00b1 0.006 \u00b1 0.019 \u00b1 0.013 \u00b1 0.012 \u00b1 0.017 \u00b1 0.204 \u00b1 0.265 \u00b1 0.038 \u00b1 0.214 \u00b1 0.255 \u00b1 0.017 \u00b1 0.241 \u00b1 0.165 \u00b1 0.014 \u00b1 0.237 \u00b1 0.139 \u00b1 0.015 \u00b1 0.271 \u00b1 0.023 \u00b1 0.014 \u00b1 0.270 \u00b1 0.037 \u00b1 0.006 \u00b1",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Distribution of AAs and k-mers in real-world CMV dataset and LSTM-generated data. Left: Histograms of real-world data. Right: Histograms of LSTM-generated data. a) Frequency of AAs in sequences of the CMV dataset. b) Frequency of AAs in sequences of the LSTM-generated datasets. c) Frequency of top 200 4-mers in sequences of the CMV dataset. d) Frequency of top 200 4-mers in sequences of the LSTM-generated datasets. e) Frequency of top 20 4-mers in sequences of the CMV dataset. f) Frequency of top 20 4-mers in sequences of the LSTM-generated datasets. Overall the distributions of AAs and 4-mers are similar in both datasets.",
            "latex": null,
            "type": "figure"
        },
        "TABREF4": {
            "text": "SFEN SFEN SFEN SF d EN SF d EN SF d EN SFZN SFZN SFZN SF d ZN SF d ZN SF d ZN SZZN SZZN SZZN SZ d ZN SZ d ZN SZ d ZN -",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "1, paragraph \"Real-world data with implanted signals.\".",
            "latex": null,
            "type": "table"
        },
        "TABREF10": {
            "text": "A6. Attention values for previously associated CMV sequences",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}