{
    "paper_id": "PMC6683650",
    "metadata": {
        "title": "Projection Word Embedding Model With Hybrid Sampling Training for Classifying ICD-10-CM Codes: Longitudinal Observational Study",
        "authors": [
            {
                "first": "Gunther",
                "middle": [],
                "last": "Eysenbach",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Yanshan",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "David",
                "middle": [],
                "last": "Mendes",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Klerisson",
                "middle": [],
                "last": "Paixao",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Sujay",
                "middle": [],
                "last": "Kakarmath",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Chin",
                "middle": [],
                "last": "Lin",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Yu-Sheng",
                "middle": [],
                "last": "Lou",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Dung-Jang",
                "middle": [],
                "last": "Tsai",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Chia-Cheng",
                "middle": [],
                "last": "Lee",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Chia-Jung",
                "middle": [],
                "last": "Hsu",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Ding-Chung",
                "middle": [],
                "last": "Wu",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Mei-Chuen",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Wen-Hui",
                "middle": [],
                "last": "Fang",
                "suffix": "",
                "email": "rumaf.fang@gmail.com",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Most medical information is recorded as unstructured data [1]. For example, approximately 96% of cancer diagnoses are reported in pathology reports, but are recorded as free-text narrative or images [2]. Disease coding is a common practical data structuralization method that is critical in many fields such as disease surveillance [3], health services management [4], and clinical research [5]. The coding quality can still be improved, and computer-aided coding systems have been considered to increase the accuracy [6,7]. Numerous models have been implemented in recent years [8-11], but they were considered inapplicable [2]. These methods are based on traditional natural language processing (NLP), and their performance is limited by an incomplete medical dictionary. However, compiling a complete medical dictionary may be impossible because of the variability of clinical vocabularies; this is a major challenge for the effective use of electronic health records (EHRs) [12].",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 60,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 200,
                    "end": 201,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 333,
                    "end": 334,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 365,
                    "end": 366,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 392,
                    "end": 393,
                    "mention": "5",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 519,
                    "end": 520,
                    "mention": "6",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 521,
                    "end": 522,
                    "mention": "7",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 580,
                    "end": 581,
                    "mention": "8",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 582,
                    "end": 584,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 626,
                    "end": 627,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 979,
                    "end": 981,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "With the third artificial intelligence revolution started by the AlexNet win in 2012 [13], further complex deep-learning models such as VGGNet [14], Inception Net [15], ResNet [16], and DenseNet [17] have been developed to achieve performance improvement. The deep-learning model can automatically extract a large amount of useful features to use for prediction [16,18,19]. More than 300 contributions have successfully applied deep-learning technology in medical image analysis [20]. Apart from image analysis, excellent results have been achieved in NLP tasks such as semantic parsing [21], search query retrieval [22], and sentence classification [23]. This has prompted us to develop an artificial intelligence\u2013based model to assist in disease coding in order to achieve faster and more accurate coding.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 88,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 144,
                    "end": 146,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 164,
                    "end": 166,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 177,
                    "end": 179,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 196,
                    "end": 198,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 363,
                    "end": 365,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 366,
                    "end": 368,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 369,
                    "end": 371,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 480,
                    "end": 482,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 588,
                    "end": 590,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 617,
                    "end": 619,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 651,
                    "end": 653,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Word embedding has been prevalently used in current NLP applications. An effective word embedding model is a major breakthrough feature-learning technique where vocabularies are mapped to vectors of real numbers [24-26]. The most popular word embedding models, such as word2vec [26], currently need large free-text resources. Most studies have used two main resources to train the word embedding model for biomedical NLP applications: internal task corpora (eg, EHR) and external internet data resources (eg, Wikipedia). Two studies have evaluated the training of word embedding models using different textual resources for biomedical NLP applications and revealed that the word embedding trained using EHR may capture semantic properties better than that trained using Wikipedia [27,28]. However, Wikipedia has an advantage, which is often overlooked: Its vocabulary diversity of external internet data resources is significantly greater than that of internal task corpora. This advantage has a major effect in real-world disease coding tasks. For example, severe acute respiratory syndrome (SARS) only broke out in 2003 and could not have been recorded in other years. Hence, the word embedding model trained using only internal corpora could not capture the semantic properties of SARS, whereas the internet resources have preserved SARS-related records. The disease coding model applied in the real world should be able to handle emerging diseases; for this purpose, most disease coding tasks are still carried out by human experts who can learn from external resources. Thus, there is a need to develop a word embedding training process that maintains the vocabulary diversity of internet resources and incorporates the medical terminology understanding of internal task corpora.",
            "cite_spans": [
                {
                    "start": 213,
                    "end": 215,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 216,
                    "end": 218,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 279,
                    "end": 281,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 781,
                    "end": 783,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 784,
                    "end": 786,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In addition to the influence of word embedding, the subsequent machine learning model also plays a key role in classification accuracy. Word embedding combined with a convolutional neural network (CNN) exhibited outstanding performance compared with traditional methods [29]. However, its performance is still deficient compared with human experts. Studies have designed rule-based approaches for conducting disease coding, which have demonstrated superior performance [8,30]. Upon carefully observing the keyword list presented in these papers, we found that the number of positive terms is more than the number of negative terms. This is an important characteristic to be considered in the design of a model for imitating human experts. However, rule-based approaches in the development of the disease coding model are expensive. To the best of our knowledge, no methods have been proposed to prevent the machine-learning model from identifying negative terms.",
            "cite_spans": [
                {
                    "start": 271,
                    "end": 273,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 470,
                    "end": 471,
                    "mention": "8",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 472,
                    "end": 474,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We propose a projection word2vec model to solve the limitation of vocabulary size in EHRs by incorporating internet sources and a hybrid sampling training method that avoids negative term identification. An experiment involving 193,647 discharge notes was conducted to verify the effectiveness. The primary aim of this experiment was to identify three\u2013character-level International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) diagnostic codes in the discharge notes.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Word embedding technology is useful for integrating synonyms; word2vec [26] is the most popular word embedding model. In this study, we used two internet corpora\u2014English Wikipedia and PubMed journal abstracts\u2014and an internal task corpus\u2014the EHRs of discharge notes. Wikipedia is an encyclopedia that is a written compendium of knowledge. PubMed is a free biomedical and life science resource developed and maintained by the National Center for Biotechnology Information, and more than 27 million journal articles have been published as of January 1, 2017. The EHRs used in this study were obtained from Tri-Service General Hospital, Taipei, Taiwan, and the details of these databases are described in the subsequent section. The three corpora were used to train the traditional word2vec model.",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 74,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Word Embedding ::: Methods",
            "ref_spans": []
        },
        {
            "text": "A recent word embedding comparison study demonstrated that word embedding trained using EHRs can usually better capture medical semantics [27]. However, the total number of words in our EHRs was only approximately 30,000, which is considerably less than those in the English Wikipedia (~365,000) and PubMed journal abstracts (~375,000). This difference was also present in previous studies, despite a larger data volume in their EHRs [27,28]. This is due to the absence of some rare diseases and periodic diseases in the database, for example, SARS outbreak in 2003 and H1N1 influenza outbreak in 2009. Thus, the word embedding model trained using EHRs cannot include sufficient vocabularies, and the subsequent machine learning model cannot handle diseases not present in the internal database. Thus, we sought to develop a word embedding training process that can maintain the vocabulary diversity of Wikipedia/PubMed and the medical semantic understanding of EHRs.",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 141,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 435,
                    "end": 437,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 438,
                    "end": 440,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Word Embedding ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The basic concept is presented in Figure 1A. The linear algebra projection is based on matrix multiplication, and all coordinates can be transformed into a new coordinate system. This conversion changes the relevance of some points but maintains all existing coordinates simultaneously. The example presented in Figure 1A indicates that the distance between the original green point and blue point is equal to the distance between the original green point and orange point, but their relationships have changed after projection. Using this method, we revised the traditional word2vec model, as presented in Figure 1B. The traditional word2vec model has two trainable layers, and the embedding weights can be used to express the terminology meanings. Here, we added a convolutional operator after the embedding layer to realize the projection word2vec model. The training process of this projection word2vec model was as follows: (1) the traditional word2vec model was trained by larger internet corpora (ie, Wikipedia and PubMed) and (2) the embedding layer was fixed and a projection word2vec model was trained by the smaller internal corpus (ie, EHRs). The detailed projection word2vec model architecture started from an embedding layer, followed by a fully connected layer for linear projection. Subsequently, another fully connected layer was followed by the linear projection output. The output layer was a logistic output with a noise contrastive estimation loss function.",
            "cite_spans": [],
            "section": "Word Embedding ::: Methods",
            "ref_spans": [
                {
                    "start": 34,
                    "end": 42,
                    "mention": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 312,
                    "end": 320,
                    "mention": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 607,
                    "end": 615,
                    "mention": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "We used the MXNet version 1.3.0 open-source package to implement these word2vec models. The training parameters of traditional and projection word2vec models employed default settings [26] as follows: skip-gram architecture, a window size of 12, a dimension of 50, a minimum word frequency of 20, a negative sampling parameter of 5, a learning rate of 0.1, and a momentum of 0.9. The well-trained projection Wikipedia/PubMed embeddings can be downloaded from Multimedia Appendix 1.",
            "cite_spans": [
                {
                    "start": 185,
                    "end": 187,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Word Embedding ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Because the projection Wikipedia/PubMed embeddings were actually trained by one of the open internet databases and EHRs, we additionally used two combinations of embeddings\u2014original EHR+Wikipedia embeddings and original EHR+PubMed embeddings\u2014as the baseline comparison. The method of combination is a simple concatenation of two vectors, so the length of the vector will be changed to 100. However, the simple concatenation cannot increase the vocabulary size; therefore, we will only compare the performance of the simple combination and our projection word2vec model in medical semantic understanding.",
            "cite_spans": [],
            "section": "Word Embedding ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We used the following seven published datasets to measure semantic similarity between medical terms: Hliaoutakis [31], MayoSRS [32], MiniMayoSRS [33,34], UMNSRS-Relatedness [35], UMNSRS-Relatedness-MOD [28], UMNSRS-Similarity [35], and UMNSRS-Similarity-MOD [28]. These databases provided the relevance of each medical term assessed by experts. For example, a relation score of 391 for the terms \u201ccataracts\u201d and \u201cinsulin\u201d and a score of 1142 for the terms \u201cobesity\u201d and \u201cdiabetes\u201d indicated that the similarity of the second pair was higher. We used different word embedding models for these term pairs and compared the correlation of the word embedding model and original data. The relation scores of each word embedding model were defined as the cosine similarity. If the number of words in a term was more than one, the average vector value from a previous study was used [27]. When the word that needed to be compared did not have any embedding, we chose the most similar word based on a character-level comparison to replace it in order to obtain its embeddings.",
            "cite_spans": [
                {
                    "start": 114,
                    "end": 116,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 128,
                    "end": 130,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 146,
                    "end": 148,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 149,
                    "end": 151,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 174,
                    "end": 176,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 203,
                    "end": 205,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 227,
                    "end": 229,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 259,
                    "end": 261,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 876,
                    "end": 878,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Medical Semantic Understanding Evaluation ::: Methods",
            "ref_spans": []
        },
        {
            "text": "In addition to qualitative data, we also selected the following five words, which are the most common diseases in our EHRs, to determine corresponding similar words in different word embeddings: neoplasm, hypertension, diabetes, pneumonia, and sepsis. The cosine similarity was again used to calculate the semantic similarity of these words. The top five most similar words were shown to provide qualitative evidence for measuring the performance of each word2vec model.",
            "cite_spans": [],
            "section": "Medical Semantic Understanding Evaluation ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The Tri-Service General Hospital supplied de-identified free-text discharge notes from June 1, 2015, to December 31, 2017. Research ethics approval was issued by the Institutional Ethical Committee and medical records office of the Tri-Service General Hospital to collect data without individual consent for sites where data are directly collected (institutional review board no. 1-107-05-097). The details of this hospital have been described previously [29]. We collected 119,315 discharge notes from the hospital and corrected misspellings using the R hunspell version 2.3 package developed by Jeroen Ooms. Discharge notes are often labeled with multiple ICD-10-CM codes, and in this study, all ICD-10-CM codes were truncated at the three-character level. Table 1 presents the frequency distribution of one\u2013character-level codes. Because of the policy change that entailed the 20th level-1 category, V00-Y99, which was not needed after 2017, we excluded the three\u2013character-level codes in the 20th level-1 category. We divided the sample by date and ensured their proportion to be 0.7, 0.1, and 0.2 in the training, validation, and testing sets, respectively. A classifier can only be trained using retrospective data in the real world, and it is then used to classify future data. Moreover, this study included data from seven hospitals (namely, Taichung Armed Forces General Hospital, Taoyuan Armed Forces General Hospital, Taichung Armed Forces General Hospital Zhongqing Branch, Hualien Armed Forces General Hospital, Tri-Service General Hospital Penghu Branch, Tri-Service General Hospital SongShan Branch, and Zuoying Branch of Kaohsiung Armed Forces General Hospital). The second testing set used 74,324 labeled discharge notes collected from these seven hospitals.",
            "cite_spans": [
                {
                    "start": 456,
                    "end": 458,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Discharge Note Database ::: Methods",
            "ref_spans": [
                {
                    "start": 759,
                    "end": 766,
                    "mention": "Table 1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "One study proposed a model combining a word embedding model and a CNN, which exhibited outstanding performance compared with traditional methods [29]. Here, we used the aforementioned model architecture and revised part of the embedding layer on the basis of our projection word2vec model. Figure 2 shows the details of the model architecture. The input data is an n\u00d71 word sequence, which is converted to a 50\u00d7n\u00d71 matrix through a designated embedding table. Subsequently, this matrix is analyzed by our analysis unit, and the output is a vector. The analysis unit is a five-channel coevolution with a filter region size of 1-5 for the disease coding task developed in a previous paper [29]. Here, we slightly revised the architecture for adapting the three\u00ad\u2013character-level ICD-10-CM classification task. The convolution channels with 1-5 filter regions have K1, K2, K3, K4, and K5 filters, respectively, and Ktotal represents the sum of the number of these filters. Figure 2 shows that Ktotal is different in each experiment, to ensure that the total number of parameters is the same in all models. For example, in the double-channel model with Ktotal/2 filters in its analysis unit, the filters are concatenated for the subsequent prediction. In our experiment, we designed K1, K2, K3, K4, and K5 to be 2400, 1800, 900, 600, and 300, respectively, in the one-channel model.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 148,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 688,
                    "end": 690,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Artificial Intelligence Model ::: Methods",
            "ref_spans": [
                {
                    "start": 290,
                    "end": 298,
                    "mention": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 969,
                    "end": 977,
                    "mention": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Another revision of the previous model is the ICD classification unit. In this study, to extend our model to identify three\u2013character-level ICD-10-CM codes, the number of outputs of the first logistic output layer was revised to the number of the three\u2013character-level ICD-10-CM codes in different one\u2013character-level ICD-10-CM codes. For example, the \u201cNeoplasms\u201d classifier includes 141 outputs, each representing its three\u2013character-level ICD-10-CM code. Subsequently, these output probabilities pass the maximum pooling-layer grouping by their specific two\u2013character-level ICD-10-CM codes, followed by a maximum pooling layer for the one\u2013character-level ICD-10-CM code identification.",
            "cite_spans": [],
            "section": "Artificial Intelligence Model ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Seven different embedding situations can be used to test each performance. Situation a is the baseline setting in which we used EHR embeddings to train the coding model. In situations b and c, embeddings trained from the internet resources Wikipedia and PubMed were used. These models are presented in the first architecture in Figure 2. Situation d is an integrated model that includes the two abovementioned models, as shown in the second architecture in Figure 2. This design was used because of the finding that the vocabularies are highly inconsistent in Wikipedia and PubMed. Because only approximately 100,000 words are included in both Wikipedia and PubMed, this design may help the model recognize more words. Situations e and f are similar to situations b and c, but with the projection Wikipedia and PubMed embeddings used to replace the embedding parameters. Finally, situation g is also an integrated model combining situations e and f.",
            "cite_spans": [],
            "section": "Artificial Intelligence Model ::: Methods",
            "ref_spans": [
                {
                    "start": 328,
                    "end": 336,
                    "mention": "Figure 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 457,
                    "end": 465,
                    "mention": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "We used the R MXNet version 1.3.0 package developed by Distributed (Deep) Machine Learning Community to implement the aforementioned architecture. The settings used for the training model are based on our previous paper [29] as follows: the stochastic gradient descent optimizer with 0.05 initial learning rate and 32 bench size for optimization, a weight decay of 10\u22124 [36], a Nesterov momentum [37] of 0.9 without dampening, and the learning rate lowered by 10 three times when validation loss plateaus after an epoch. The cross-entropy was used as the loss function in this study. Because oversampling was adopted for rare categories to improve the model performance [38], we weighed the benefits of cross-entropy on the basis of the frequency of each code. The F-measure was the major evaluation index in our study and is calculated as follows:",
            "cite_spans": [
                {
                    "start": 221,
                    "end": 223,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 371,
                    "end": 373,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 397,
                    "end": 399,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 671,
                    "end": 673,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Artificial Intelligence Model ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Moreover, the precision and recall values are provided.",
            "cite_spans": [],
            "section": "Artificial Intelligence Model ::: Methods",
            "ref_spans": []
        },
        {
            "text": "A novel ICD-10-CM-specific augmentation method called \u201chybrid sampling\u201d is proposed for improving model performance. Figure 3 shows the practical details. Data augmentation is a key method for avoiding overfitting and is widely used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [13]. With regard to the disease coding task of discharge notes, the negative terms are useless because the discharge notes include only positive disease descriptions. Thus, a successful training process needs to prevent the model from learning negative terms. The hybrid sampling is based on the hybridization of positive and negative samples. We paste the positive discharge note and a random negative discharge note as a new positive sample for model training, which will disrupt the correlation between keywords. For example, pregnancy-related terms rarely appear in cancer-related discharge notes; hence, the machine-learning model training by the traditional process will discover that the pregnancy-related terms are negative terms for the cancer identification task. However, this is logically incorrect. If human experts consider a discharge note not involving cancer, they will verify that there are no cancer-related terms after carefully reading all descriptions. Hybrid sampling may solve this problem by letting our model only identify positive terms.",
            "cite_spans": [
                {
                    "start": 300,
                    "end": 302,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Hybrid Sampling Training Method ::: Methods",
            "ref_spans": [
                {
                    "start": 117,
                    "end": 125,
                    "mention": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "We tested word embeddings on seven published biomedical measurement datasets commonly used to measure the semantic similarity between medical terms. Table 2 lists the Pearson correlation coefficient results for the seven datasets. For Hliaoutakis\u2019 dataset [31], consisting of 34 medical term pairs with similarity scores obtained by human judgments, the previous study resulted in correlation coefficients of 0.482, 0.311, and 0.247 in EHRs, PubMed, and Wikipedia, respectively [27]. Our results are similar, with correlation coefficients of 0.4815, 0.4968, and 0.2820 in original EHRs, PubMed, and Wikipedia embeddings, respectively. The correlation coefficients of the combination of EHR and Wikipedia are between coefficients of the two of them (0.3488), and the combination of EHR and PubMed also shows a similar trend (0.4914). After the projection word2vec training, the correlation coefficients of PubMed and Wikipedia embeddings increased to 0.5255 and 0.3202, respectively. The performances of the simple concatenation and projection model are similar, but the projection model can maintain vocabulary diversity while simple concatenation cannot. The MayoSRS dataset [32] consists of 101 clinical term pairs whose relatedness was determined by nine medical coders and three physicians from the Mayo Clinic, whereas MiniMayoSRS, which is a subset of MayoSRS, includes 29 of 101 term pairs. The previous study demonstrated that the highest correlations of 0.412 and 0.632, respectively, were found in EHR embeddings [27]. Our EHR embeddings also yielded the highest correlation of 0.6082 in MayoSRS, and after the projection word2vec model, the correlations of PubMed and Wikipedia embeddings increased from 0.5087 to 0.5148 and from 0.0082 to 0.0930, respectively.",
            "cite_spans": [
                {
                    "start": 257,
                    "end": 259,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 479,
                    "end": 481,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1177,
                    "end": 1179,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1524,
                    "end": 1526,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 149,
                    "end": 156,
                    "mention": "Table 2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "However, the original PubMed embeddings yielded the highest correlation of 0.7200 in MiniMayoSRS; hence, the projection word2vec model successfully improved the performance of only Wikipedia embeddings (PubMed: 0.7200\u21920.5903; Wikipedia: 0.3363\u21920.4709). The simple concatenation embeddings look slightly better than projection embeddings in these two datasets but are still limited by the vocabulary size of EHRs. This situation was the same for the following four similar datasets: UMNSRS-Relatedness [35], UMNSRS-Relatedness-MOD [28], UMNSRS-Similarity [35], and UMNSRS-Similarity-MOD [28]. The projection word2vec model improved the performance of Wikipedia embeddings but not that of PubMed embeddings because the performance of original PubMed embeddings was higher than that of the original EHR embeddings. The simple concatenation embeddings are still slightly better than projection embeddings. In summary, the proposed projection word2vec model has the potential to improve the performance of capturing semantic properties when the embeddings trained from the original corpus are worse than those from the target corpus. The details of all term pair comparisons are provided in Multimedia Appendix 1.",
            "cite_spans": [
                {
                    "start": 502,
                    "end": 504,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 531,
                    "end": 533,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 555,
                    "end": 557,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 587,
                    "end": 589,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Results",
            "ref_spans": []
        },
        {
            "text": "In the qualitative evaluation, we selected five medical words because they are most common disorders in our discharge notes: neoplasm, hypertension, diabetes, pneumonia, and sepsis. Word embeddings trained from one internal corpus and two internet corpora were utilized to compute the five most similar words to each selected medical word according to the cosine similarity; the results are listed in Table 3. Similar to the quantitative results, an obvious superiority of PubMed/EHR embeddings compared with Wikipedia embeddings was observed when using the traditional word2vec model. For example, the word most similar to \u201chypertension,\u201d given by PubMed embeddings, was \u201chypertensive,\u201d which is the adjective of the original word; this was also present in the result of EHR embeddings. In contrast, the first five words most similar to \u201chypertension\u201d as per the Wikipedia embeddings were all less relevant. However, the performance of the projection Wikipedia embedding model exhibited no obvious improvement compared with the original Wikipedia embedding model. The only notable improvement in the case of the word \u201chypertension\u201d was the removal of the word \u201casthma\u201d in the most similar list, which is an obvious unrelated term. This phenomenon was also present in other selected words. Moreover, the results of simple concatenation embeddings resemble those of combining the first five words of two embeddings and reordering them. Because the performance of the original PubMed and EHR embeddings was similar, there was no apparent improvement in the projection technology results compared with the original PubMed embeddings. In summary, we considered the qualitative and quantitative analyses results to be similar.",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 401,
                    "end": 408,
                    "mention": "Table 3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Furthermore, we applied the abovementioned embedding models on the three\u2013character-level ICD-10-CM coding task; Table 4 shows the global means of F-measures of the tests. In the task, the first testing samples were divided according to the date, and the second samples were from the seven other hospitals. Because some three\u2013character-level codes were never or less frequently used, we only present the results of the 90% most used three\u2013character-level ICD-10-CM codes. The usage rates of all included codes were more than 0.2%; this situation was somewhat reversed. The performance of the model trained by PubMed embeddings was worse than that of Wikipedia and EHR embeddings. The model trained by EHR embeddings (0.7250/0.6574) yielded a higher mean of F-measures than Wikipedia embeddings (0.7213/0.6479), followed by the PubMed embeddings (0.6974/0.6260), both in the first and second test sets. It is worth mentioning that the integrated model that used both Wikipedia and PubMed embeddings (0.7208) achieved similar performance to the model that used only Wikipedia embeddings in the first test set but the former showed better performance (0.6540) in the second test set. Therefore, the projection technique showed an improvement on the model performance in all embeddings consistently in all situations (Wiki: 0.7213/0.6479 to 0.7316/0.6617; PubMed: 0.6974/0.6260 to 0.7187/0.6561; Wiki+PubMed: 0.7208/0.6540 to 0.7362/0.6693). The model that used both projection Wikipedia and PubMed embeddings exhibited the best performance compared with all models. However, the model that used projection Wikipedia embeddings was only slightly behind it. The best model, determined on the basis of the comparison of embeddings and, namely, the hybrid sampling method, was used for improving the model performance. Although the improvement was not large, the hybrid sampling training further improved the model performance (0.7371/0.6698). The details of all precisions, recalls, and F-measures are presented in Multimedia Appendix 2.",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 112,
                    "end": 119,
                    "mention": "Table 4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "To further understand the effect of hybrid sampling training, we compared the predictions of each word in the model with (situation h in Table 4) and without (situation g in Table 4) hybrid sampling training. We included all words in our EHRs, and Figure 4 presents the density plot of predictive results in 20 one\u2013character-level codes. The prediction values are defined as the last fully connected output before logistic transformation; therefore, a value greater than 0 implies that the model results in a probability greater than 50% for only single\u2013character-level words. The percentage presented in Figure 4 represents the proportion of words with a value more than 0; therefore, a higher value implies that the model often uses positive terms for predictions. It is noteworthy that the model with hybrid sampling training exhibited the highest proportion of positive terms used in all one\u2013character-level codes. We further present the ICD-10-CM identification results of two simulated discharge notes generated by the models with and without hybrid sampling training to further understand the hybrid model\u2019s effect; the results are listed in Table 5. In our discharge notes, we identified a strong negative correlation between cancer and pregnancy; hence, in this experiment, we tried to simulate the discharge notes with cancer and pregnancy. The first case was a primipara with duodenal adenocarcinoma. The model without hybrid sampling training ignored two three\u2013character-level codes: O60 and C17; omission of C17 is unacceptable because it is the main code in this case. The model with hybrid sampling training successfully recognized these codes but also identified an error code, K91. This example clearly indicates that the second model performed better, but the average accuracies of the two models were similar. The second case was another description style by strip format; the model with hybrid sampling training successfully recognized the code C53 again, whereas the model without hybrid sampling training could not. We understand the defects of average F-measures through these two examples. Thus, the hybrid sampling training, in fact, improved the model, although there was only a slight improvement in the average F-measures.",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 248,
                    "end": 256,
                    "mention": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 605,
                    "end": 613,
                    "mention": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 137,
                    "end": 144,
                    "mention": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 174,
                    "end": 181,
                    "mention": "Table 4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1149,
                    "end": 1156,
                    "mention": "Table 5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "The EHR embeddings and PubMed embeddings trained by the traditional word2vec model have a similar ability to capture medical semantic properties, and they are better than the Wikipedia embedding model. After the projection word2vec training, the projection Wikipedia embedding exhibited an obvious improvement compared with the original version. In the three\u2013character-level ICD-10-CM coding task, the projection word2vec model performed better, and the model that used both projection Wikipedia and PubMed embeddings was the best of them. Although the proposed \u201chybrid sampling\u201d method only slightly improved the model performance, it successfully avoided the interference of negative terms. In summary, the proposed projection word embedding model and hybrid sampling training method provide a new opportunity to improve the performance of medical NLP.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "The most significant advantage of the proposed projection word2vec model is that it can maintain vocabulary diversity from external internet resources and provide a more accurate understanding of medical semantics from internal resources. Because of the limitations imposed by relevant regulations, such as the Health Insurance Portability and Accountability Act and General Data Protection Regulation, the EHR resources may not be publicly available. This limits the vocabulary size of models trained by EHRs that are owned by research teams. However, previous studies have found that word embeddings trained using EHRs may capture semantic properties better than those trained using Wikipedia [27,28]. A common alternative has been to replace the Wikipedia resource with the PubMed resource, which demonstrates the advantage of PubMed embeddings in medical semantic understanding [27,28]. However, a machine learning model using PubMed embeddings exhibited the worst performance in multiple tasks compared with that using EHR embeddings, because PubMed is a biomedical and life science journal article resource [27]. In our ICD-10-CM coding task, the model using PubMed embeddings performed even worse than that using Wikipedia embeddings. In short, although EHR embeddings are necessary in medical NLP tasks, vocabulary diversity is inevitably restricted because the vocabulary size is less than 100,000 words, even in a large EHR [27,28]. We overcome this problem through the use of the proposed projection word2vec model, and the experimental results demonstrated the superiority of projection Wikipedia and PubMed embeddings. The proposed projection word2vec model can not only deal with the vocabulary size problem in the medical NLP task but also be used in other fields that require confidentiality of data. Thus, the proposed projection word2vec model simultaneously maintains the advantages of both internal and external corpora but does not focus on improving the model performance.",
            "cite_spans": [
                {
                    "start": 696,
                    "end": 698,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 699,
                    "end": 701,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 883,
                    "end": 885,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 886,
                    "end": 888,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1114,
                    "end": 1116,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1435,
                    "end": 1437,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1438,
                    "end": 1440,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "The basic idea of our projection word2vec model is very similar to transfer learning [39], but it is not a direct application because of the particularity of our task. Most transfer learning was initially trained by a large dataset and kept the same architecture to continuously train on a specific domain. However, the vocabulary lists of open internet databases and EHRs are inevitably different, and the embeddings of some vocabulary not included in EHRs will not be changed when we train them by EHRs. This will destroy the semantic relationship in original open internet databases. Our projection design keeps the original embeddings and changes all weights together, and the embeddings of vocabulary not included in EHRs will also be changed by their similar terms included in EHRs. This idea can also be used in other NLP tasks to add to the vocabulary diversity and terminology understanding of their word embeddings.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 88,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "An unexpected finding in the medical semantic understanding evaluation was that original PubMed embeddings were better than original EHR embeddings; this was because our EHR was smaller than those in previous studies [27,28]. However, only the MayoSRS dataset showed an opposite result. The reason is the different word compositions in these seven datasets. The MayoSRS included more symptom and sign words than the other datasets. Because EHRs describe the medical records with more symptoms and signs than journal articles, the embeddings trained by EHRs are superior in capturing symptom or sign semantics. Moreover, due to the attenuation, performance of the projection PubMed embeddings was worse than both the original EHR embeddings and original PubMed embeddings in MiniMayoSRS and all of the UMNSRS datasets. In our experiment, there was only one additional projection matrix with 2500 parameters for modifying the medical terminology understanding by EHRs, and this is relatively small compared to the number of parameters in original EHR embeddings. Thus, the projection may only be able to enforce a part of the medical terminology understanding. The EHRs used more nondiagnostic and drug words, so the projection model may not correct the understanding of diagnosis and drug words, which is the major issue in UMNSRS databases and MiniMayoSRS. However, the most significant advantage of the projection model is to maintain the vocabulary diversity. Further, the ICD-10-CM coding task shows that projection embeddings are better than original embeddings. Therefore, we believe that this unexpected attenuation may not negatively affect the advantage of the purposed projection model.",
            "cite_spans": [
                {
                    "start": 218,
                    "end": 220,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 221,
                    "end": 223,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Medical semantics learning using PubMed is expected to be better than that using Wikipedia. In the similarity scores test, the PubMed embeddings exhibited a superior ability to capture medical semantic properties compared with Wikipedia embeddings, which is consistent with previous studies [27,28]. However, further machine learning using PubMed embeddings performed worse in the ICD-10-CM coding task compared with Wikipedia embeddings. From a theoretical view, the frequency with which medical terms appear in journal abstracts is higher than that in general articles; hence, their characteristics can be learned better in the PubMed database. The reason for this experimental result is likely that the medical records are still different from journal resources. The model trained using EHRs exhibited the best performance probably because the key points of the three\u2013character-level task were organ names. Only a few medical studies have explored more than one organ; hence, semantic learning from Wikipedia and PubMed has advantages in different situations. We propose a double-channel model that includes both Wikipedia and PubMed embeddings to solve this problem. This model not only improved the vocabulary size because the vocabularies are highly inconsistent in Wikipedia and PubMed but also achieved the best performance in our ICD-10-CM coding experiments. The projection word2vec model can still improve the performance of the double-channel model. Further investigation can follow this design to perform disease coding tasks.",
            "cite_spans": [
                {
                    "start": 292,
                    "end": 294,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 295,
                    "end": 297,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "The discharge notes almost only describe the positive statements, and this is very different from other NLP tasks. Most previous rule-based systems list only the positive terms and demonstrate superior performance [8,30]; therefore, designing a method for the model to avoid negative weighting words was crucial. A naive idea was to limit model parameters to positive numbers in the training process. However, current artificial intelligence technology is based on backpropagation, which utilizes gradient transfer and the chain rule, so all mathematical functions used in artificial intelligence models need to be differentiable. Thus, we could not directly limit model parameters to positive numbers. The hybrid sampling method was a breakthrough concept. We designed a soft limit for model parameters through the modification of input data. In further analysis, the model with hybrid sampling used positive words more often. However, the model performance improved only slightly through implementation of the hybrid sampling method in our experiments; this may be due to the similarity of discharge notes between the training set and test set in our experiments. In the subsequent virtual medical records analysis, we tried to simulate medical records that did not appear in our hospital EHRs by using the model with hybrid sampling training, and superior performance was achieved. Although we could not provide qualitative evidence for this improvement, it must be focused upon in further analysis. A fully automatic model applied in practical use should be able to handle this challenge. We expect this technique to be widely used in subsequent disease coding research, and only positive descriptions will be presented for some free-text document classification tasks.",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 216,
                    "mention": "8",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 217,
                    "end": 219,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Although the accuracy of disease coding was improved only slightly by our proposed methods, we achieved the best accuracy reported in the literature. Only a few studies have reported the ability to automatically identify three\u2013character-level ICD-10-CM codes from the free-text medical records because of its difficulty. Koopman et al [40] claimed that their model could effectively determine common types of cancers (mean F-measure=0.7) [40], and our model archive discerned a huge lead in the same 20 cancer types (0.7579 in the testing set from the same source). In fact, these 20 cancers are not the first 20 common cancer types in our sample. The mean F-measure in our first 20 common cancer types was 0.8617. This suggests the advantages of our model as well as the success of the modern artificial intelligence model. Existing deep learning models have been proven to achieve human-level performance and to be effective in medical applications where large annotated datasets are available [16,18-20]. Our study integrated state-of-the-art artificial intelligence into the model to easily perform the disease coding task.",
            "cite_spans": [
                {
                    "start": 336,
                    "end": 338,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 439,
                    "end": 441,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 997,
                    "end": 999,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1000,
                    "end": 1002,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1003,
                    "end": 1005,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "This study has several potential limitations. First, we used only a 50-dimension embedding model to process our data. This related small number may also cause additional attenuation in medical terminology understanding, because the number of parameters in the projection matrix is the square of the small number. However, one study presented data processing for the ICD-10-CM coding task [29], and another proposed that a 60-dimension embedding model is better than a 100-dimension embedding model [27]. We consider that the optimal dimension number of embeddings may need more study. Second, the data volume of our EHRs was smaller than that of previous studies,[27,28] which may have affected the performance of EHR embeddings and projection embeddings based on EHR. However, the correlations of our EHR embeddings in the database consisting of seven medical term pairs were not lower than the correlations in these studies [27,28]. Third, this study used only a set of hyperparameters for all model trainings due to limitations of computing resources; hence, the performance can still be improved. However, the model performance was better than that of previously proposed methods. Moreover, this study collected multicenter data sources to validate the model performance. The similarity trends confirmed the robustness of the set of hyperparameters. Therefore, our experimental setting is convincing from the perspective of model research.",
            "cite_spans": [
                {
                    "start": 389,
                    "end": 391,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 499,
                    "end": 501,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 664,
                    "end": 666,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 667,
                    "end": 669,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 927,
                    "end": 929,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 930,
                    "end": 932,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "In conclusion, in this paper, we proposed a projection word2vec model to use for expressing the meaning of medical terminology with more accuracy, and we confirmed the effectiveness of the architecture in disease classification using free-text discharge notes from hospitals. Moreover, a novel augmentation method\u2014the hybrid sampling method\u2014was proposed to prevent models from identifying negative terms. With the third generation of artificial intelligence revolution initiated in the ILSVRC 2012, the artificial intelligence model is expected to change the health care system. We believe that the projection word2vec model can be applied in discharge note classification as well as other situations. When there is a small high-quality corpus and a large external corpus, the projection word2vec model can help maintain both vocabulary diversity and medical semantic understanding. Future NLP can become more powerful and robust due to the improved performance of the proposed models.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1: Prevalence of different one\u2013character-level International Classification of Diseases, Tenth Revision, Clinical Modification codes used in discharge notes in this study.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2: Pearson correlation coefficients between similarity scores of disease coding performed by human judgment and those calculated using four-word embeddings.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3: Selected words and the corresponding five most similar words obtained from different word embedding models.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4: Results of the three\u2013character-level ICD-10-CM coding task using different word embeddings (italicized font indicates the best precision, recall, and F-measure).\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5: ICD-10-CM coding results of selected models in several simulated discharge notes (italicized font indicates inconsistent predictions among the models with and without hybrid sampling training).a\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Figure 1: Concept of the projection word embedding model.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Figure 2: Model architectures in our experiments. ICD: International Classification of Diseases.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Figure 3: Hybrid sampling method.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Figure 4: Density plots of predictions of each single word provided by the model with and without hybrid sampling training.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "The inevitable application of big data to health care",
            "authors": [
                {
                    "first": "TB",
                    "middle": [],
                    "last": "Murdoch",
                    "suffix": ""
                },
                {
                    "first": "AS",
                    "middle": [],
                    "last": "Detsky",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "JAMA",
            "volume": "309",
            "issn": "13",
            "pages": "1351-2",
            "other_ids": {
                "DOI": [
                    "10.1001/jama.2013.393"
                ]
            }
        },
        "BIBREF1": {
            "title": "Natural Language Processing Based Instrument for Classification of Free Text Medical Records",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Khachidze",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tsintsadze",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Archuadze",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Biomed Res Int",
            "volume": "2016",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1155/2016/8313454",
                    "10.1155/2016/8313454"
                ]
            }
        },
        "BIBREF2": {
            "title": "Automatic ICD-10 multi-class classification of cause of death from plaintext autopsy reports through expert-driven feature selection",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Mujtaba",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shuib",
                    "suffix": ""
                },
                {
                    "first": "RG",
                    "middle": [],
                    "last": "Raj",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Rajandram",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shaikh",
                    "suffix": ""
                },
                {
                    "first": "MA",
                    "middle": [],
                    "last": "Al-Garadi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "PLoS One",
            "volume": "12",
            "issn": "2",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0170242"
                ]
            }
        },
        "BIBREF3": {
            "title": "Scalable and accurate deep learning with electronic health records",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rajkomar",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Oren",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hajaj",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hardt",
                    "suffix": ""
                },
                {
                    "first": "Pj",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Marcus",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sundberg",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Flores",
                    "suffix": ""
                },
                {
                    "first": "Ge",
                    "middle": [],
                    "last": "Duggan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Irvine",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Litsch",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mossin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tansuwan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wexler",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wilson",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ludwig",
                    "suffix": ""
                },
                {
                    "first": "Sl",
                    "middle": [],
                    "last": "Volchenboum",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pearson",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Madabushi",
                    "suffix": ""
                },
                {
                    "first": "Nh",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "Aj",
                    "middle": [],
                    "last": "Butte",
                    "suffix": ""
                },
                {
                    "first": "Md",
                    "middle": [],
                    "last": "Howell",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cui",
                    "suffix": ""
                },
                {
                    "first": "Gs",
                    "middle": [],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "npj Digital Med",
            "volume": "1",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/s41746-018-0029-1"
                ]
            }
        },
        "BIBREF4": {
            "title": "ImageNet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Cornell University",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sermanet",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Reed",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Anguelov",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Cornell University",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Deep Residual Learning for Image Recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Densely Connected Convolutional Networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Weinberger",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "van der Maaten",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Amodei",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ananthanarayanan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Anubhai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Battenberg",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Case",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Cornell University",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Droppo",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Seide",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Seltzer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Stolcke",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Cornell University",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Text mining of cancer-related information: review of current status and future directions",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Spasi\u0107",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Livsey",
                    "suffix": ""
                },
                {
                    "first": "JA",
                    "middle": [],
                    "last": "Keane",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Nenadi\u0107",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Int J Med Inform",
            "volume": "83",
            "issn": "9",
            "pages": "605-23",
            "other_ids": {
                "DOI": [
                    "10.1016/j.ijmedinf.2014.06.009"
                ]
            }
        },
        "BIBREF12": {
            "title": "A survey on deep learning in medical image analysis",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Litjens",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kooi",
                    "suffix": ""
                },
                {
                    "first": "BE",
                    "middle": [],
                    "last": "Bejnordi",
                    "suffix": ""
                },
                {
                    "first": "AAA",
                    "middle": [],
                    "last": "Setio",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ciompi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ghafoorian",
                    "suffix": ""
                },
                {
                    "first": "JAWM",
                    "middle": [],
                    "last": "van der Laak",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "van Ginneken",
                    "suffix": ""
                },
                {
                    "first": "Clara",
                    "middle": [
                        "I"
                    ],
                    "last": "S\u00e1nchez",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Med Image Anal",
            "volume": "42",
            "issn": "",
            "pages": "60-88",
            "other_ids": {
                "DOI": [
                    "10.1016/j.media.2017.07.005"
                ]
            }
        },
        "BIBREF13": {
            "title": "Semantic Parsing for Single-Relation Question Answering",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Yih",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Meek",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Learning semantic representations using convolutional neural networks for web search",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Mesnil",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "373-374",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Convolutional Neural Networks for Sentence Classification",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "A neural probabilistic language model",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ducharme",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Vincent",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jauvin",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "The Journal of Machine Learning Research",
            "volume": "3",
            "issn": "",
            "pages": "1137-1155",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Learning discriminative projections for text similarity measures",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Yih",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Platt",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Meek",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "247-256",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "Distributed representations of words and phrases and their compositionality",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "A comparison of word embeddings for the biomedical natural language processing",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Afzal",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rastegar-Mojarad",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kingsbury",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J Biomed Inform",
            "volume": "87",
            "issn": "",
            "pages": "12-20",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jbi.2018.09.008"
                ]
            }
        },
        "BIBREF20": {
            "title": "Corpus domain effects on distributional semantic modeling of medical terms",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pakhomov",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Finley",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "McEwan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Melton",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Bioinformatics",
            "volume": "32",
            "issn": "23",
            "pages": "3635-3644",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btw529"
                ]
            }
        },
        "BIBREF21": {
            "title": "Artificial Intelligence Learning Semantics via External Resources for Classifying Diagnosis Codes in Discharge Notes",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hsu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yeh",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J Med Internet Res",
            "volume": "19",
            "issn": "11",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.2196/jmir.8344"
                ]
            }
        },
        "BIBREF22": {
            "title": "Public health surveillance and knowing about health in the context of growing sources of health data",
            "authors": [
                {
                    "first": "LM",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "SB",
                    "middle": [],
                    "last": "Thacker",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Am J Prev Med",
            "volume": "41",
            "issn": "6",
            "pages": "636-40",
            "other_ids": {
                "DOI": [
                    "10.1016/j.amepre.2011.08.015"
                ]
            }
        },
        "BIBREF23": {
            "title": "Prospective surveillance of excess mortality due to influenza in New South Wales: feasibility and statistical approach",
            "authors": [
                {
                    "first": "DJ",
                    "middle": [],
                    "last": "Muscatello",
                    "suffix": ""
                },
                {
                    "first": "PM",
                    "middle": [],
                    "last": "Morton",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Evans",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gilmour",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Commun Dis Intell Q Rep",
            "volume": "32",
            "issn": "4",
            "pages": "435-42",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hliaoutakis",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Semantic Similarity Measures in MeSH Ontology and their application to Information Retrieval on Medline",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "Towards a framework for developing semantic relatedness reference standards",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pakhomov",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pedersen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "McInnes",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Melton",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ruggieri",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chute",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J Biomed Inform",
            "volume": "44",
            "issn": "2",
            "pages": "251-65",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jbi.2010.10.004"
                ]
            }
        },
        "BIBREF26": {
            "title": "Measures of semantic similarity and relatedness in the biomedical domain",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pedersen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pakhomov",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Patwardhan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chute",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J Biomed Inform",
            "volume": "40",
            "issn": "3",
            "pages": "288-99",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jbi.2006.06.004"
                ]
            }
        },
        "BIBREF27": {
            "title": "UMLS-Interface and UMLS-Similarity : open source software for measuring paths and semantic similarity",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "McInnes",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pedersen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pakhomov",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "AMIA Annu Symp Proc",
            "volume": "2009",
            "issn": "",
            "pages": "431-5",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "Semantic Similarity and Relatedness between Clinical Terms: An Experimental Study",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pakhomov",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "McInnes",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pedersen",
                    "suffix": ""
                },
                {
                    "first": "GB",
                    "middle": [],
                    "last": "Melton",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "AMIA Annu Symp Proc",
            "volume": "2010",
            "issn": "",
            "pages": "572-6",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gross",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wilber",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Torch",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "On the importance of initialization and momentum in deep learning",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Martens",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dahl",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Simpson",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Cornell University",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "A Survey on Transfer Learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEEE Trans Knowl Data Eng",
            "volume": "22",
            "issn": "10",
            "pages": "1345-1359",
            "other_ids": {
                "DOI": [
                    "10.1109/TKDE.2009.191"
                ]
            }
        },
        "BIBREF33": {
            "title": "Casemix Funding Optimisation: Working Together to Make the Most of Every Episode",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Uzkuraitis",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Hastings",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Torney",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Health Inf Manag",
            "volume": "39",
            "issn": "3",
            "pages": "47-49",
            "other_ids": {
                "DOI": [
                    "10.1177/183335831003900309"
                ]
            }
        },
        "BIBREF34": {
            "title": "Automatic ICD-10 classification of cancers from free-text death certificates",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Koopman",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zuccon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bergheim",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Grayson",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int J Med Inform",
            "volume": "84",
            "issn": "11",
            "pages": "956-65",
            "other_ids": {
                "DOI": [
                    "10.1016/j.ijmedinf.2015.08.004"
                ]
            }
        },
        "BIBREF35": {
            "title": "Validation of Algorithm to Identify Persons with Non-traumatic Spinal Cord Dysfunction in Canada Using Administrative Health Data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ho",
                    "suffix": ""
                },
                {
                    "first": "SJT",
                    "middle": [],
                    "last": "Guilcher",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "McKenzie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mouneimne",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Voth",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Cronin",
                    "suffix": ""
                },
                {
                    "first": "VK",
                    "middle": [],
                    "last": "Noonan",
                    "suffix": ""
                },
                {
                    "first": "SB",
                    "middle": [],
                    "last": "Jaglal",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Top Spinal Cord Inj Rehabil",
            "volume": "23",
            "issn": "4",
            "pages": "333-342",
            "other_ids": {
                "DOI": [
                    "10.1310/sci2304-333"
                ]
            }
        },
        "BIBREF36": {
            "title": "ICD-10 impact on ascertainment and accuracy of oral cleft cases as recorded by the Brazilian national live birth information system",
            "authors": [
                {
                    "first": "RL",
                    "middle": [],
                    "last": "do Nascimento",
                    "suffix": ""
                },
                {
                    "first": "EE",
                    "middle": [],
                    "last": "Castilla",
                    "suffix": ""
                },
                {
                    "first": "MDG",
                    "middle": [],
                    "last": "Dutra",
                    "suffix": ""
                },
                {
                    "first": "IM",
                    "middle": [],
                    "last": "Orioli",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Am J Med Genet A",
            "volume": "176",
            "issn": "4",
            "pages": "907-914",
            "other_ids": {
                "DOI": [
                    "10.1002/ajmg.a.38634"
                ]
            }
        },
        "BIBREF37": {
            "title": "Exploration of association rule mining for coding consistency and completeness assessment in inpatient administrative health data",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Sundararajan",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Williamson",
                    "suffix": ""
                },
                {
                    "first": "EP",
                    "middle": [],
                    "last": "Minty",
                    "suffix": ""
                },
                {
                    "first": "TC",
                    "middle": [],
                    "last": "Smith",
                    "suffix": ""
                },
                {
                    "first": "CTA",
                    "middle": [],
                    "last": "Doktorchik",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Quan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J Biomed Inform",
            "volume": "79",
            "issn": "",
            "pages": "41-47",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jbi.2018.02.001"
                ]
            }
        },
        "BIBREF38": {
            "title": "Automatic classification of diseases from free-text death certificates for real-time surveillance",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Koopman",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Karimi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "McGuire",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Muscatello",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kemp",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Truran",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Thackway",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "BMC Med Inform Decis Mak",
            "volume": "15",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/s12911-015-0174-2"
                ]
            }
        },
        "BIBREF39": {
            "title": "Automated Reconciliation of Radiology Reports and Discharge Summaries",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Koopman",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zuccon",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Wagholikar",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "O'Dwyer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Keijzers",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "AMIA Annu Symp Proc",
            "volume": "2015",
            "issn": "",
            "pages": "775-84",
            "other_ids": {
                "DOI": []
            }
        }
    }
}