{
    "paper_id": "PMC7148248",
    "metadata": {
        "title": "Context-Guided Learning to Rank Entities",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Makoto",
                "middle": [
                    "P."
                ],
                "last": "Kato",
                "suffix": "",
                "email": "mpkato@acm.org",
                "affiliation": {}
            },
            {
                "first": "Wiradee",
                "middle": [],
                "last": "Imrattanatrai",
                "suffix": "",
                "email": "wiradee@db.soc.i.kyoto-u.ac.jp",
                "affiliation": {}
            },
            {
                "first": "Takehiro",
                "middle": [],
                "last": "Yamamoto",
                "suffix": "",
                "email": "t.yamamoto@sis.u-hyogo.ac.jp",
                "affiliation": {}
            },
            {
                "first": "Hiroaki",
                "middle": [],
                "last": "Ohshima",
                "suffix": "",
                "email": "ohshima@ai.u-hyogo.ac.jp",
                "affiliation": {}
            },
            {
                "first": "Katsumi",
                "middle": [],
                "last": "Tanaka",
                "suffix": "",
                "email": "tanaka.katsumi.85e@st.kyoto-u.ac.jp",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Entity search is one of the emerging trends in major search engines [19, 32], and has been powered by large-scale knowledge bases such as DBpedia, Wikidata, and YAGO. A wide variety of entity attributes are stored in knowledge bases and have enabled search engines to support entity search queries such as \u201ceuropean countries\u201d and \u201cmovies starring emma watson\u201d.",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 71,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 73,
                    "end": 75,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "On the other hand, the current entity search systems have not supported various kinds of rankings yet, which can be found on the Web, for example, the most livable countries, innovative companies, and high-performance cameras. If such diverse rankings were integrated into entity search and explained objectively with some evidences, users could be more efficient for accomplishing complex tasks such as decision making, comparison, and planning. For example, a user is planning to visit several European countries and inputs a query \u201ceuropean countries safety\u201d to know how safe each country is. If an entity search engine provided a list of countries ranked by public safety and factors used to determine the ranking (e.g. crime rate and police budget), they would be helpful for the user to make his/her travel plan.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we propose a method for learning orders of entities using samples of ordered entities as training data and attributes of entities as features. Entity orders are expressed in several forms on the Web: comparative sentences (e.g. \u201cDiCaprio is taller than Pitt\u201d), scores (e.g. \u201c[Camera A] portrait: 9.2, landscape: 7.5, and sports: 8.5\u201d), and rankings (e.g. \u201c1st: Iceland, 2nd: Denmark, and 3rd: Austria\u201d). These expressions can be interpreted with a uniform model, i.e. a subset of entity pairs that defines an entity order, and be used as training data to learn entity orders. The learned models can be used not only to rank entities but also to explain rankings by correlated attributes. We assume that entity orders can be represented as a linear function of attributes (denoted by f), primarily because of the high explanatory capacity for users. For example, given a list of entities ordered by labeling criterion \u201csafety\u201d, (Iceland, Denmark, Austria), and their attributes such as \u201cGDP\u201d, \u201cCrime rate\u201d, and \u201cPolice budget\u201d, we learn function f(Entity) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$= +0.5$$\\end{document} (Police budget) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ -0.8$$\\end{document} (Crime rate).",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A major challenge for this problem is the lack of training data. Many Web sites do not present all the ordered entities (see Table 1). Moreover, the size of training data might not be sufficiently large for some entity classes, even if all the ordered entities are described (e.g. only 50 states in the United States). As the number of attributes should be large enough to explain diverse orders, and can be increased easily with existing techniques [11, 28], the problem of learning to rank entities can suffer from serious over-fitting problems.",
            "cite_spans": [
                {
                    "start": 451,
                    "end": 453,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 455,
                    "end": 457,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Introduction",
            "ref_spans": [
                {
                    "start": 131,
                    "end": 132,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "To cope with this essential problem, we propose a learning method referred to as context-guided learning (CGL). This method uses not only ordered entities but also contexts of labeling criteria and attributes to learn the function f. A labeling criterion refers to a textual representation to determine labels (or an order in a ranking problem). The context can provide the models with additional information, and guide learning in the correct direction by preventing over-fitting. Figure 1 illustrates how CGL is applied to a classification problem. (As can be seen later, CGL is first explained for a classification problem and later extended to a ranking problem). Our goal in this example is to learn a linear function for the labeling criterion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_1$$\\end{document} (richness), which is defined as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_1({\\mathbf{x}}) = {\\mathbf{w}}_1^T{\\mathbf{x}}$$\\end{document} (an intercept is omitted for simplicity). When we simply apply an ordinary learning algorithm, learned weights can be \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}'_1 = (1, 0)$$\\end{document} in (B) of Fig. 1, indicating that the attribute \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_1$$\\end{document} is useful for this classification. Although these weights seem reasonable as their decision boundary perfectly separates positive (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_2$$\\end{document}) and negative (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_3$$\\end{document}) examples, it is easy to anticipate that the attribute \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_1$$\\end{document} can be useless for the other cases if we know the meaning of the labeling criterion (i.e. richness) and attribute \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_1$$\\end{document} (i.e. temperature). CGL, on the other hand, incorporates contexts of the labeling criterion and attributes for making a \u201crough\u201d prediction of the ideal weights, and expects the weights \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}_1$$\\end{document} to be close to the \u201crough\u201d prediction (denoted as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{g}}_1$$\\end{document} in (C) of Fig. 1). Although the prediction based on contexts cannot be always accurate (indeed, the decision boundary of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{g}}_1$$\\end{document} fails to classify examples well), \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{g}}_1$$\\end{document} suggests that the attribute \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_1$$\\end{document} is not strongly related to the labeling criterion, and guides the learning of the weights \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}_1$$\\end{document}. Thus, the learning can be successful even if sufficient training data are not available. CGL does not require any annotations for the contexts. Alternatively, CGL learns multiple functions at the same time for learning the relationship between contexts and weights in the function f.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": [
                {
                    "start": 489,
                    "end": 490,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1848,
                    "end": 1849,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 4784,
                    "end": 4785,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "To the best of our knowledge, CGL is the first attempt to leverage contexts of labeling criteria and features directly in machine learning (ML) problems. CGL is a general ML method and can be applied not only to ranking problems but also to classification and regression problems as long as relations between labeling criteria and features are described in a particular corpus.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Our contributions in this paper can be summarized as follows: (1) we introduced the problem of learning to rank entities by using attributes as features, in order to rank entities by various criteria and precisely understand labeling criteria; (2) we proposed CGL, a general ML method using contexts of labeling criteria and features for preventing over-fitting; and (3) we conducted experiments with a wide variety of orders, and demonstrated the effectiveness of CGL in the task of learning to rank entities.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Entity ranking has been addressed in some tracks in INEX and TREC. The INEX Entity Ranking track held two tasks: entity ranking and entity list completion tasks [12\u201314]. The entity ranking task expected systems to return relevant entities in response to a given query, while the entity list completion task expected systems to return entities related to given example entities. The TREC Entity track offered related entity finding tasks, in which systems were expected to find entities related to a given entity, with the type of the target entity and nature of their relation [2\u20134]. Those tasks only expect that retrieved entities are ordered by the relatedness to given example entities, and do not expect different kinds of orders within related entities.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 164,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 165,
                    "end": 167,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 578,
                    "end": 579,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 580,
                    "end": 581,
                    "mention": "4",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Entity Ranking ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Apart from the evaluation campaigns, there are some work that addresses learning to rank entities. Kang et al. used a ranking algorithm based on a boosted tree model for finding entities related to a given query [24]. Tran et al. proposed a method of ranking entities based on salience and informativeness for timeline summarization of events [30]. Zhou et al. addressed a problem of finding entities that have a specified relation with an input entity [34]. They trained a ranker for each relation based on training queries and labeled entities by using features derived from search snippets regarding pairs of entities. Although this work and ours use contexts (or search snippets) for learning to rank entities, our rankers are built primarily on attributes of entities and does not use contexts of entity pairs. Jameel et al. proposed an entity embedding method for entity retrieval [22]. Their method is mainly based on the co-occurrence between entities and words, and does not directly model entity attributes.",
            "cite_spans": [
                {
                    "start": 213,
                    "end": 215,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 344,
                    "end": 346,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 454,
                    "end": 456,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 888,
                    "end": 890,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Entity Ranking ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Some NLP tasks are also related to our task. Iwanari et al. tackled a problem of ordering entities in terms of a given adjective by using some evidences extracted from texts [20]. Their task is similar to ours as both address entity ranking in terms of a particular labeling criterion. While their method uses contexts of labeling criteria and entities, our method uses contexts of labeling criteria and attributes of entities.",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 177,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Entity Ranking ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "The important characteristics of CGL are summarized as follows: (1) weights in the function f are learned based on labels as well as contexts regarding labeling criteria and features, and (2) multiple functions are learned at the same time to learn the relationship between the contexts and weights in the function f. Below, we review several ML methods and discuss their relationship to CGL.",
            "cite_spans": [],
            "section": "Multi-task Learning ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Multi-task learning is an approach to improving learning in each task by learning multiple tasks simultaneously [9]. CGL is considered as an instance of multi-task learning. Regularized multi-task learning, which was proposed by Evgeniou and Pontil, assumes that weights of multiple tasks are similar [15]. As explained later, their model is a special case of our model when contexts are all the same. Other models assume that weights are sampled from a common prior [10, 27, 33]. Argyriou et al. used an assumption that weights are represented in a low subspace common to multiple tasks [1]. In contrast to these methods using an assumption that all the tasks are related, some work selectively decides which tasks are related and are expected to share similar weights  [21, 25]. Similarly, CGL uses contexts to measure the similarity between tasks implicitly, and tends to estimate similar weights for similar tasks. An interesting difference between CGL and the other multi-task learning methods is that CGL still works even if any pairs of tasks are not similar. CGL only requires that some contexts are similar among multiple tasks. Thus, the applicable scope of CGL is not limited to problems targeted by existing multi-task learning methods.",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 114,
                    "mention": "9",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 302,
                    "end": 304,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 468,
                    "end": 470,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 472,
                    "end": 474,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 476,
                    "end": 478,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 589,
                    "end": 590,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 772,
                    "end": 774,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 776,
                    "end": 778,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Multi-task Learning ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Letting E be a set of entities of a particular class, we define an entity order as a total order on E, denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\preceq _{k}$$\\end{document}. Each order has a labeling criterion (or an ordering criterion in this case) denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_k$$\\end{document}. For example, labeling criteria could include \u201clivability\u201d, \u201cinnovativeness\u201d, \u201cbeauty\u201d, and \u201cperformance\u201d. A set of all \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(e_i, e_j) \\in E \\times E$$\\end{document} for which \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_i \\preceq _{k} e_j$$\\end{document} holds is called a graph1 of an entity order, denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\preceq _{k}}$$\\end{document}. Orders are usually expressed on the Web as subsets of their graphs. Thus, we can observe and use only \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G'_{\\preceq _{k}} \\subseteq G_{\\preceq _{k}}$$\\end{document} for learning entity orders. For example, a ranking of safe countries \u201c1st: Iceland, 2nd: Denmark, and 3rd: Austria\u201d implies \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G'_{\\preceq _{k}} = \\{($$\\end{document}\u201cDenmark\u201d, \u201cIceland\u201d), (\u201cAustria\u201d, \u201cIceland\u201d), (\u201cAustria\u201d, \u201cDenmark\u201d\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$)\\}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_k =$$\\end{document} \u201csafety\u201d.",
            "cite_spans": [],
            "section": "Problem Definition ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Our principal purpose is to learn a linear function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k({\\mathbf{e}}_i) = {\\mathbf{w}}_{k}^T {\\mathbf{e}}_i$$\\end{document} based on a subset of a graph \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G'_{\\preceq _{k}}$$\\end{document} for each entity order \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\preceq _{k}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{e}}_i$$\\end{document} is an M-dimensional vector representing attributes of entity \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_i \\in E$$\\end{document}, and the d-th value of the vector represents a value of attribute \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_d$$\\end{document}. We expect that the function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k$$\\end{document}\npreserves the entity order \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\preceq _{k}$$\\end{document}: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ e_i \\preceq _{k} e_j \\Rightarrow f_k({\\mathbf{e}}_i) \\le f_k({\\mathbf{e}}_j)$$\\end{document} for any \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_i, e_j \\in E$$\\end{document}, so that entities can be ranked by entity order \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\preceq _{k}$$\\end{document} with learned function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k$$\\end{document}. Moreover, attributes whose weights are non-zero are expected to explain the entity order well.",
            "cite_spans": [],
            "section": "Problem Definition ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "As we explained earlier, the key challenge of this problem is lack of training data: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$|G'_{\\preceq _{k}}|$$\\end{document} is typically small compared with the number of attributes M. For example, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M=83$$\\end{document} for countries and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M=137$$\\end{document} for cities in our experiments. Ranked lists of ten or fewer entities can provide only at most 45 entity pairs as training data, which are not considered as sufficiently large for learning. Moreover, M must be as large as possible for modeling a wide range of orders. Thus, some approaches are necessary for preventing the over-fitting problem caused by lack of training data.",
            "cite_spans": [],
            "section": "Problem Definition ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "The key idea in our work is to use data other than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G'_{\\preceq _{k}}$$\\end{document} for learning \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}_{k}$$\\end{document} effectively. One of the unique characteristics or assumptions in our problem is that textual representations for labeling criteria and attributes are available. Therefore, given a labeling criterion, it is possible to estimate a roughly appropriate weight for each attribute by leveraging the contexts regarding relations between the labeling criterion and attribute. This idea is instantiated as CGL, which is explained in the next subsection.",
            "cite_spans": [],
            "section": "Problem Definition ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "\n3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\min _{{\\mathbf{u}}, {\\mathbf{v}}_k, \\xi _{k,i}} \\Vert {\\mathbf{u}}\\Vert ^2 + \\frac{c}{K}\\sum ^{K}_{k = 1} \\Vert {\\mathbf{v}}_k\\Vert ^2 + C\\sum ^{K}_{k = 1}\\sum ^{N_k}_{i = 1} \\xi _{k, i}, \\end{aligned}$$\\end{document}subject, for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k = 1,\\ldots , K$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i=1, \\ldots , N_k$$\\end{document}, to the constraints that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_{k,i} f_k({\\mathbf{x}}_{k,i}) \\ge 1 - \\xi _{k, i},\\ \\xi _{k, i} \\ge 0$$\\end{document}, where c and C are hyper parameters.",
            "cite_spans": [],
            "section": "Problem 1 ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Slack variables \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\xi _{k,i}$$\\end{document} measure the error of the linear functions on the training data, while the other terms are regularization terms for the weights \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{u}}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{v}}_k$$\\end{document}. Hyper parameters c and C can control the effect of the contexts on the model and the sensitivity for the error on the training data: a large value for c increases the effect of the contexts, while a large value for C tends to inhibit misclassification of the training data. We learn multiple functions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k$$\\end{document} for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=1, \\ldots , K$$\\end{document} with the single objective function so that we can learn the weight \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{u}}$$\\end{document} based on the whole training data.",
            "cite_spans": [],
            "section": "Problem 1 ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "We show that Problem 1 can be solved in the same manner as would be used with the standard SVM. To this end, we first define a single function to be learned that summarizes functions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k$$\\end{document} for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=1, \\ldots , K$$\\end{document} as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F({\\mathbf{x}}, k) = f_k({\\mathbf{x}})$$\\end{document}. This function, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F: {\\mathbb R}^M \\times \\{1, \\ldots , K\\} \\rightarrow {\\mathbb R}$$\\end{document}, can be written as a linear function:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F({\\mathbf{x}}, k) = {\\mathbf{w}}^T \\psi ({\\mathbf{x}}, k), \\end{aligned}$$\\end{document}by using the following settings:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\mathbf{w}} = ({\\mathbf{u}}^T, \\sqrt{\\frac{c}{K}}{\\mathbf{v}}^T)^T,\\ \\psi ({\\mathbf{x}}, k) = ((\\varPhi _{k}{\\mathbf{x}})^T, \\underbrace{{\\mathbf 0}^T, \\ldots , {\\mathbf 0}^T}_{k-1}, \\sqrt{\\frac{K}{c}}{\\mathbf{x}}^T, \\underbrace{{\\mathbf 0}^T, \\ldots , {\\mathbf 0}^T}_{K-k})^T, \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\psi $$\\end{document} is a feature map function, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf 0}$$\\end{document} is an M-dimensional vector whose values are all zeros.",
            "cite_spans": [],
            "section": "Problem 1 ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Reassigning \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{x}}_{i}$$\\end{document} to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{x}}_{k, i'}$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_{i}$$\\end{document} to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_{k, i'}$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\xi _{i}$$\\end{document} to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\xi _{k, i'}$$\\end{document} (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i=\\sum ^{k-1}_{k'=1}N_{k'}+i'$$\\end{document}), we can reduce Problem 1 to the standard SVM problem, as follows.",
            "cite_spans": [],
            "section": "Problem 1 ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "The optimization of Problem 1 is equivalent to solving the following problem:",
            "cite_spans": [],
            "section": "Theorem 1 ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Given \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D = \\{(({\\mathbf{x}}_i, k_i), y_i)\\}^{N}_{i=1}$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N=\\sum ^{K}_{k=1} N_k$$\\end{document} such that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D = \\bigcup ^{K}_{k=1} \\left\\{ ((x_{k,i}, k), y_{k,i}) \\big | (x_{k,i}, y_{k,i}) \\in D_k \\right\\} $$\\end{document},6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\min _{{\\mathbf{w}}, \\xi _{i}} \\frac{1}{2} \\Vert {\\mathbf{w}}\\Vert ^2 + C'\\sum ^{N}_{i= 1} \\xi _{i}, \\end{aligned}$$\\end{document}subject, for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i=1, \\ldots , N$$\\end{document}, to the constraints that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_{i} F({\\mathbf{x}}_{i}, k_i) \\ge 1 - \\xi _{i},\\ \\xi _{i} \\ge 0$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C'=C/2$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\xi _i$$\\end{document} is a slack variable for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$((x_i, k_i), y_i) \\in D$$\\end{document}.",
            "cite_spans": [],
            "section": "Problem 2 ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "The norm of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}$$\\end{document} is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\Vert {\\mathbf{w}}\\Vert ^2 = \\Vert {\\mathbf{u}}\\Vert ^2 + \\frac{c}{K} \\Vert {\\mathbf{v}}\\Vert ^2$$\\end{document}. Therefore, the objective function of Problem 2 is rewritten as:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\frac{1}{2} \\left\\{ \\Vert {\\mathbf{u}}\\Vert ^2 + \\frac{c}{K} \\sum ^{K}_{k=1} \\Vert {\\mathbf{v}}_k\\Vert ^2 + C\\sum ^{N}_{i = 1} \\xi _{i} \\right\\} , \\end{aligned}$$\\end{document}which is equivalent to the objective function of Problem 1.",
            "cite_spans": [],
            "section": "Proof ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Since Problem 2 is the standard SVM problem, we can use the standard SVM dual problem for solving Problem 1. Furthermore, we can use an important characteristic of SVMs: i.e. non-linear functions can be used by means of kernels. While the linear function for classification (i.e. \n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k$$\\end{document}) cannot be a non-linear function owing to the form of the model, we can use a non-linear function for estimating the weights based on contexts (see Eq. 1). The kernel method for CGL provides us with a wide range of choices for the representation of contexts. They can be represented as vectors, sets of vectors, trees, etc. as long as the kernel function is appropriately designed for two contexts.",
            "cite_spans": [],
            "section": "Proof ::: Context-Guided Learning ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "We extend CGL to the ranking problem and explain how it can be applied to the problem of ranking entities.",
            "cite_spans": [],
            "section": "Context-Guided Learning for Ranking ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "The input for the ranking problem is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathcal D} = \\{ D_k \\}^{K}_{k=1}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_k \\subseteq {\\mathbb R}^M \\times {\\mathbb R}^M$$\\end{document}, and K is the number of labeling criteria. Labeling criterion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_k$$\\end{document} is a textual representation to determine the order for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_k$$\\end{document}: i.e. \n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$({\\mathbf{x}}_{k,i}, {\\mathbf{x}}_{k,j})$$\\end{document} in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_k$$\\end{document} indicates that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{x}}_{k,j}$$\\end{document} is higher than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{x}}_{k,i}$$\\end{document} in terms of the labeling criterion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_k$$\\end{document}. The d-th value of vectors in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_k$$\\end{document} must correspond to a particular feature and have a name denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_d$$\\end{document}. The requirements are the same as those explained in regard to CGL for classification. A ranking problem can be formalized as a learning function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k$$\\end{document} for each labeling criterion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=1, \\ldots , K$$\\end{document} such that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k({\\mathbf{x}}_{k,j}) - f_k({\\mathbf{x}}_{k,i}) \\simeq 1$$\\end{document} for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$({\\mathbf{x}}_{k,i}, {\\mathbf{x}}_{k,j})$$\\end{document} in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_k$$\\end{document}. As assumed in the classification problem, we use a linear function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k({\\mathbf{x}}_{k,i}) = {\\mathbf{w}}_k^T{\\mathbf{x}}_{k,i}$$\\end{document}.",
            "cite_spans": [],
            "section": "Context-Guided Learning for Ranking ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "It is clear that the ranking problem can be reduced to the classification problem if we redefine \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_k$$\\end{document} as follows: \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D'_k = \\left\\{ ({\\mathbf{x}}_{k, j} - {\\mathbf{x}}_{k, i}, 1) \\big | ({\\mathbf{x}}_{k,i}, {\\mathbf{x}}_{k,j}) \\in D_k \\right\\} $$\\end{document}, since \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_k({\\mathbf{x}}_{k, j} - {\\mathbf{x}}_{k, i}) = f_k({\\mathbf{x}}_{k,j}) - f_k({\\mathbf{x}}_{k,i})$$\\end{document}.",
            "cite_spans": [],
            "section": "Context-Guided Learning for Ranking ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "We can apply CGL for ranking to the problem in Problem Definition section by using vectors of entity pairs in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G'_{\\preceq _{k}}$$\\end{document} as the training data, i.e. \n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_k = \\left\\{ ({\\mathbf{e}}_{i}, {\\mathbf{e}}_{j}) | (e_{i}, e_{j}) \\in G'_{\\preceq _{k}} \\right\\} $$\\end{document}.",
            "cite_spans": [],
            "section": "Context-Guided Learning for Ranking ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Having described the learning method for the problem of ranking entities, we explain the context models used in the learning. Contexts can be a set of sentences or a set of documents regarding a labeling criterion and a feature. In this work, we describe methods of modeling contexts by using sentences retrieved from Web search results.",
            "cite_spans": [],
            "section": "Context Models ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Given labeling criterion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_k$$\\end{document} and feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_d$$\\end{document}, we create a query combining \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_k$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_d$$\\end{document} with an AND operator, and use the query to retrieve the top \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N^{(c)}$$\\end{document} search results using a particular Web search engine (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N^{(c)}=500$$\\end{document} in our experiments). We then split snippets of the search results into sentences and find sentences including both the labeling criterion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_k$$\\end{document} and the feature \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_d$$\\end{document}.",
            "cite_spans": [],
            "section": "Context Models ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "We use two basic methods for modeling sentences. One is a vector representation based on the TF-IDF weighting, and the other is a distributional representation of sentences [26]. The vector representation based on the TF-IDF weighting is sparse, and not sensitive to the order of words, but it can represent exact words appearing in the context. In contrast, the distributed representation of sentences is dense, and sensitive to the word order, but it might not retain the exact words appearing in the context.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 176,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Context Models ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Since there is no publicly available dataset for our task, we first explain our development of a dataset and its statistics.",
            "cite_spans": [],
            "section": "Data ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Various kinds of entity orders in three datasets were mined from the Web and from magazines both automatically and manually. The three datasets include City (more specifically, Japanese prefectures), Country, and Camera entities, respectively. These classes were selected primarily for the following reasons: (1) availability of a wide range of entity orders, (2) availability of attributes, and (3) diversity of statistics. The language scope of our dataset was Japanese, as we used a Japanese crowd-sourcing service in the evaluation. Entity names and attribute names were Japanese and translated into English for this paper.",
            "cite_spans": [],
            "section": "Data ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Entity orders were mined from Web pages for City and Country datasets, and from ten Japanese camera magazines for Camera dataset. The retrieved ranked lists were converted into a set of pairs for each entity orders. We excluded entity sets including less than five entities.",
            "cite_spans": [],
            "section": "Data ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Attributes for City and Country datasets were mined from tables in Web documents. We chose Web tables as a resource for obtaining attributes because (1) the extraction method can be accurate and language-independent, and (2) standardization of numerical values was not necessary as units of numerical values are usually consistent within a table. Attributes for Camera dataset were scraped from Web pages of a Japanese Web site, Kakaku.com2, which provides prices and specifications of products. All the numerical values for each attribute were normalized into [0, 1].\n\n",
            "cite_spans": [],
            "section": "Data ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Table 1 shows statistics and examples of entities, orders, and attributes. There are 158 entity orders in total. For most of the orders, we could not find all of the entities in a class in a ranking on the Web. There were many Web pages presenting the top three or ten entities for an order. Thus, the average number of entities per order is much less than the total number of entities.",
            "cite_spans": [],
            "section": "Data ::: Experiments",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We selected as baseline methods for this experiment some existing ranking methods that do not use contexts: (1) RankNet [5]: a pairwise ranking method that uses a neural network model and optimizes the cross entropy loss, (2) RankBoost [16]: application of AdaBoost [17] to pairwise preferences, (3) LinearFeature [29]: a linear feature-based model optimized by coordinate ascent, (4) LambdaMART [31]: a combination of the ranking model, LambdaRank [6], and the boosted tree model, MART [18], and (5) ListNet [7]: a listwise ranking method using a neural network model. We used these methods implemented in RankLib3. We used normalized discounted cumulative gain (nDCG@10) [23] as an evaluation metric to be optimized for some methods.",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 122,
                    "mention": "5",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 237,
                    "end": 239,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 267,
                    "end": 269,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 315,
                    "end": 317,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 397,
                    "end": 399,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 450,
                    "end": 451,
                    "mention": "6",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 488,
                    "end": 490,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 510,
                    "end": 511,
                    "mention": "7",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 674,
                    "end": 676,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Experimental Settings ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We conducted experiments using the developed dataset in the following settings. For each set of ordered entities \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\preceq _{k}}$$\\end{document}, we split entities in the set E into 50 : 50, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_\\mathrm{train}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_\\mathrm{test}$$\\end{document}, and obtained training data \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_\\mathrm{train} = \\{(e_i, e_j) | (e_i, e_j) \\in G_{\\preceq _{k}} \\wedge e_i \\in E_\\mathrm{train} \\wedge e_j \\in E_\\mathrm{train} \\}$$\\end{document} and test data \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_\\mathrm{test} = G_{\\preceq _{k}} - G_\\mathrm{train}$$\\end{document}. Our task in this experiment is to learn a model based on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_\\mathrm{train}$$\\end{document}, and to predict the pairwise preference of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_j$$\\end{document} for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(e_i, e_j) \\in G_\\mathrm{test}$$\\end{document}. We measured the accuracy defined as the fraction of correctly predicted pairwise preferences. We used five-fold cross validation on entity orders within\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_\\mathrm{train}$$\\end{document}\nof the same dataset to determine the best parameters for each method.",
            "cite_spans": [],
            "section": "Experimental Settings ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We configured CGL with the following settings. Two context models were used: TF-IDF and Distributed (distributed representation with 400 dimensional vectors). Parameters c and C were determined using the cross validation explained above. A linear kernel (Linear) and an RBF kernel (RBF) were used for the kernel in CGL.",
            "cite_spans": [],
            "section": "Experimental Settings ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Table 2 shows the accuracy in the three datasets with the standard error of the mean (SEM). CGL in any settings were better than any of the baseline methods. Among the CGL-based methods, the best method was CGL (TF-IDF, Linear), followed by CGL (Distributed, RBF). The total improvement over the best baseline method, LambdaMart, was 11.6%. According to a randomized Tukey HSD test [8]4 (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha = 0.01$$\\end{document}), the differences between CGL (TF-IDF, Linear) and all the baseline methods were found to be statistically significant, while there was no statistically significant difference across methods based on CGL.",
            "cite_spans": [
                {
                    "start": 383,
                    "end": 384,
                    "mention": "8",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "CGL (TF-IDF, Linear) achieved 8%, 11%, and 18% improvements over LambdaMART for City, Country, and Camera, respectively. We hypothesize that the quality and amount of contexts are the main factors that determine the effectiveness of CGL, based on the observation that the number of sentences used for modeling contexts per attribute was 36.0, 45.7, and 137 for City, Country, and Camera, respectively.\n\n",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We also conducted evaluation of the attributes used in the learned functions. Five attributes with the highest absolute weights for each entity order were pooled, and then presented to users in a Japanese crowd-sourcing service, Lancers5. In this evaluation, we aimed to understand to what extent the learned attributes could explain the orders. The instruction was as follows: \u201cIf you agree that there is a correlation between <labeling criterion> and <attribute>, please assign a score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$+2$$\\end{document}. If you disagree, please assign a score \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-2$$\\end{document}. If you cannot agree or disagree, please assign a score 0.\u201d Users could choose a rate from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-2$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-1$$\\end{document}, 0, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$+1$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$+2$$\\end{document}. We assigned five users for each pair of a labeling criterion and an attribute. The best CGL method, CGL (TF-IDF, Linear), was selected for this evaluation. LinearFeature was used as a baseline method, since only this method used a linear function among the baseline methods.",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Figure 2 shows the distribution of rates for five attributes with the highest absolute weights. The average rates of CGL were \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-0.455$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-0.166$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$+0.581$$\\end{document}, while those of LinearFeature were \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-0.560$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-0.204$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$+0.516$$\\end{document} for City, Country, and Camera datasets, respectively. These average rates show a high correlation with the accuracy of the models. Even though CGL could find more reasonable attributes in all of the classes than LinearFeature, their differences were small for those datasets. The average rates for City and Country datasets were negative indicating low explainability of the attributes. This is partially because some attributes only correlate to a particular labeling criterion, but were not considered as causes for increasing the criterion. Although CGL could learn a more accurate model than the baseline methods, it is still challenging to find highly explanatory attributes for a given label criterion.",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Finally, we show some examples of linear functions learned by CGL in Table 3. Most of the attributes seem explainable and can possibly affect the entity order. While the others do not seem explanatory for the labeling criteria (e.g. \u201cpopulation/family\u201d for \u201cattractiveness\u201d and \u201chighest temperature\u201d for \u201cavg. savings\u201d), they correlate well to the labeling criteria in our dataset, and are examples of attributes that were considered unreasonable in the subjective evaluation, but highly contributed to the prediction.",
            "cite_spans": [],
            "section": "Experimental Results ::: Experiments",
            "ref_spans": [
                {
                    "start": 75,
                    "end": 76,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "In this paper, we addressed the problem of learning orders of entities, by using partially observed orders as training data and attributes of entities as features. We proposed a learning method called context-guided learning (CGL) to avoid the over-fitting problem caused by lack of training data, and demonstrated the effectiveness of CGL for 158 orders in three datasets. Our future work includes theoretical analysis of CGL, application of CGL to the other problems (e.g. a fact verification task), exploration of better context models, and improvement of the efficiency of CGL for a large amount of data.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Statistics of the datasets and examples of entities, orders, and attributes.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Accuracy in the three datasets (\u00b1SEM).\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Examples of linear functions learned by CGL, in which three attributes for the highest absolute weights are shown.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: (A) Entities \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_2$$\\end{document} are rich countries, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_3$$\\end{document} is not a rich country. They have only two attributes \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_1$$\\end{document} (temperature) and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_2$$\\end{document} (GDP). (B) Every entity can be expressed as a point in a two dimensional space by their attribute values in this example. Our goal is to learn a linear function for the labeling criterion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_1$$\\end{document}, which is defined as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_1({\\mathbf{x}}) = {\\mathbf{w}}_1^T{\\mathbf{x}}$$\\end{document}. One of the possible weights that perfectly classify the training examples is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}'_1 = (1, 0)$$\\end{document}, but not necessarily effective for the other examples. (C) Contexts are used to produce a \u201crough\u201d prediction \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{g}}_1$$\\end{document} of the ideal weights. CGL determines the weights \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}_1$$\\end{document} such that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{v}}_1$$\\end{document}, the difference between \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}_1$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{g}}_1$$\\end{document}, is small and training examples are separated well. The weights \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathbf{w}}_1$$\\end{document} are expected to be effective for the other cases, since a strong correlation between richness and GDP is suggested by their contexts.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Distribution of rates for five attributes with the highest absolute weights.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Convex multi-task feature learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Argyriou",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Evgeniou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pontil",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Mach. Learn.",
            "volume": "73",
            "issn": "3",
            "pages": "243-272",
            "other_ids": {
                "DOI": [
                    "10.1007/s10994-007-5040-8"
                ]
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Overview of the INEX 2007 entity ranking track",
            "authors": [
                {
                    "first": "AP",
                    "middle": [],
                    "last": "de Vries",
                    "suffix": ""
                },
                {
                    "first": "A-M",
                    "middle": [],
                    "last": "Vercoustre",
                    "suffix": ""
                },
                {
                    "first": "JA",
                    "middle": [],
                    "last": "Thom",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Craswell",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lalmas",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Focused Access to XML Documents",
            "volume": "",
            "issn": "",
            "pages": "245-251",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "Overview of the INEX 2009 entity ranking track",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Demartini",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Iofciu",
                    "suffix": ""
                },
                {
                    "first": "AP",
                    "middle": [],
                    "last": "de Vries",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Focused Retrieval and Evaluation",
            "volume": "",
            "issn": "",
            "pages": "254-264",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Overview of the INEX 2008 entity ranking track",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Demartini",
                    "suffix": ""
                },
                {
                    "first": "AP",
                    "middle": [],
                    "last": "de Vries",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Iofciu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Advances in Focused Retrieval",
            "volume": "",
            "issn": "",
            "pages": "243-252",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "An efficient boosting algorithm for combining preferences",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Iyer",
                    "suffix": ""
                },
                {
                    "first": "RE",
                    "middle": [],
                    "last": "Schapire",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res.",
            "volume": "4",
            "issn": "",
            "pages": "933-969",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "RE",
                    "middle": [],
                    "last": "Schapire",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "J. Comput. Syst. Sci.",
            "volume": "1",
            "issn": "55",
            "pages": "119-139",
            "other_ids": {
                "DOI": [
                    "10.1006/jcss.1997.1504"
                ]
            }
        },
        "BIBREF9": {
            "title": "Greedy function approximation: a gradient boosting machine",
            "authors": [
                {
                    "first": "JH",
                    "middle": [],
                    "last": "Friedman",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Ann. Stat.",
            "volume": "29",
            "issn": "5",
            "pages": "1189-1232",
            "other_ids": {
                "DOI": [
                    "10.1214/aos/1013203451"
                ]
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Cumulated gain-based evaluation of ir techniques",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "J\u00e4rvelin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kek\u00e4l\u00e4inen",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "ACM TOIS",
            "volume": "20",
            "issn": "4",
            "pages": "422-446",
            "other_ids": {
                "DOI": [
                    "10.1145/582415.582418"
                ]
            }
        },
        "BIBREF16": {
            "title": "Learning to rank related entities in web search",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Torzec",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Neurocomputing",
            "volume": "166",
            "issn": "",
            "pages": "309-318",
            "other_ids": {
                "DOI": [
                    "10.1016/j.neucom.2015.04.004"
                ]
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Linear feature-based models for information retrieval",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Metzler",
                    "suffix": ""
                },
                {
                    "first": "WB",
                    "middle": [],
                    "last": "Croft",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Inf. Retrieval",
            "volume": "10",
            "issn": "3",
            "pages": "257-274",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-006-9019-z"
                ]
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Adapting boosting for information retrieval measures",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "CJ",
                    "middle": [],
                    "last": "Burges",
                    "suffix": ""
                },
                {
                    "first": "KM",
                    "middle": [],
                    "last": "Svore",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Inf. Retrieval",
            "volume": "13",
            "issn": "3",
            "pages": "254-270",
            "other_ids": {
                "DOI": [
                    "10.1007/s10791-009-9112-1"
                ]
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "Multiple testing in statistical analysis of systems-based information retrieval experiments",
            "authors": [
                {
                    "first": "BA",
                    "middle": [],
                    "last": "Carterette",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ACM TOIS",
            "volume": "30",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1145/2094072.2094076"
                ]
            }
        },
        "BIBREF33": {
            "title": "Multitask learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Caruana",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Mach. Learn.",
            "volume": "28",
            "issn": "1",
            "pages": "41-75",
            "other_ids": {
                "DOI": [
                    "10.1023/A:1007379606734"
                ]
            }
        }
    }
}