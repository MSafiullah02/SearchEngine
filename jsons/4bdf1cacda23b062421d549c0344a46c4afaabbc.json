{"paper_id": "4bdf1cacda23b062421d549c0344a46c4afaabbc", "metadata": {"title": "Generating Fake but Realistic Headlines Using Deep Neural Networks", "authors": [{"first": "Ashish", "middle": [], "last": "Dandekar", "suffix": "", "affiliation": {"laboratory": "", "institution": "National University of Singapore", "location": {"settlement": "Singapore", "country": "Singapore"}}, "email": "ashishdandekar@u.nus.edu"}, {"first": "Remmy", "middle": ["A M"], "last": "Zen", "suffix": "", "affiliation": {"laboratory": "", "institution": "Universitas Indonesia", "location": {"settlement": "Jakarta", "country": "Indonesia"}}, "email": ""}, {"first": "St\u00e9phane", "middle": [], "last": "Bressan", "suffix": "", "affiliation": {"laboratory": "", "institution": "National University of Singapore", "location": {"settlement": "Singapore", "country": "Singapore"}}, "email": ""}]}, "abstract": [{"text": "Social media platforms such as Twitter and Facebook implement filters to detect fake news as they foresee their transition from social media platform to primary sources of news. The robustness of such filters lies in the variety and the quality of the data used to train them. There is, therefore, a need for a tool that automatically generates fake but realistic news.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "In this paper, we propose a deep learning model that automatically generates news headlines. The model is trained with a corpus of existing headlines from different topics. Once trained, the model generates a fake but realistic headline given a seed and a topic. For example, given the seed \"Kim Jong Un\" and the topic \"Business\", the model generates the headline \"kim jong un says climate change is already making money\".", "cite_spans": [], "ref_spans": [], "section": "Abstract"}, {"text": "In order to better capture and leverage the syntactic structure of the headlines for the task of synthetic headline generation, we extend the architecture -Contextual Long Short Term Memory, proposed by Ghosh et al. -to also learn a part-of-speech model. We empirically and comparatively evaluate the performance of the proposed model on a real corpora of headlines. We compare our proposed approach and its variants using Long Short Term Memory and Gated Recurrent Units as the building blocks. We evaluate and compare the topical coherence of the generated headlines using a state-of-the-art classifier. We, also, evaluate the quality of the generated headline using a machine translation quality metric and its novelty using a metric we propose for this purpose. We show that the proposed model is practical and competitively efficient and effective.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "In the Digital News Report 2016 1 , Reuters Institute for the Study of Journalism claims that 51% of the people in their study indicate the use of social media 1 http://www.digitalnewsreport.org/survey/2016/overview-key-findings-2016/. platforms as their primary source of news. This transition of social media platforms to news sources further accentuates the issue of the trustworthiness of the news which is published on the social media platforms. In order to address this, social media platform like Facebook has already started working with five fact-checking organizations to implement a filter which can flag fake news on the platform 2 .", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Starting from the traditional problem of spam filtering to a more sophisticated problem of anomaly detection, machine learning techniques provide a toolbox to solve such a spectrum of problems. Machine learning techniques require a good quality training data for the filters to be robust and effective. To train fake news filters, they need a large amount of fake but realistic news. Fake news, which are generated by a juxtaposition of a couple of news without any context, do not lead to robust filtering. Therefore, there is a need of a tool which automatically generates a large amount of good quality fake but realistic news.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "In this paper, we propose a deep learning model that automatically generates news headlines given a seed and the context. For instance, for a seed \"obama says that\", typical news headlines generated under technology context reads \"obama says that google is having new surface pro with retina display design\" whereas the headline generated under business context reads \"obama says that facebook is going to drop on q1 profit\". For the same seed with medicine and entertainment as the topics, typical generated headlines are \"obama says that study says west africa ebola outbreak has killed million\" and \"obama says that he was called out of kim kardashian kanye west wedding\" respectively.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "We expect that the news headlines generated by the model should not only adhere to the provided context but also to conform to the structure of the sentence. In order to catch the attention of the readers, news headlines follow the structure which deviates from the conventional grammar to a certain extent. We extend the architecture of Contextual Long Short Term Memory (CLSTM), proposed by Ghosh et al. [9] , to learn the part-of-speech model for news headlines. We compare Recurrent Neural Networks (RNNs) variants towards the effectiveness of generating news headlines. We qualitatively and quantitatively compare the topical coherence and the syntactic quality of the generated headlines and show that the proposed model is competitively efficient and effective. Section 2 presents the related work. Section 3 delineates the proposed model along with some prerequisites in the neural network. We present experiments and evaluation in Sect. 4. Section 5 concludes the work by discussing the insights and the work underway.", "cite_spans": [{"start": 406, "end": 409, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Introduction"}, {"text": "In the last four-five years, with the advancement in the computing powers, neural networks have taken a rebirth. Neural networks with multiple hidden layers, dubbed as \"Deep Neural Networks\", have been applied in many fields starting from classical fields like multimedia and text analysis [11, 18, 28, 29] to more applied fields [7, 32] . Different categories of neural networks have been shown to be effective and specific to different kinds of tasks. For instance, Restricted Boltzmann Machines are widely used for unsupervised learning as well as for dimensionality reduction [13] whereas Convolutional Neural Networks are widely used for image classification task [18] .", "cite_spans": [{"start": 290, "end": 294, "text": "[11,", "ref_id": "BIBREF10"}, {"start": 295, "end": 298, "text": "18,", "ref_id": "BIBREF17"}, {"start": 299, "end": 302, "text": "28,", "ref_id": "BIBREF27"}, {"start": 303, "end": 306, "text": "29]", "ref_id": "BIBREF28"}, {"start": 330, "end": 333, "text": "[7,", "ref_id": "BIBREF6"}, {"start": 334, "end": 337, "text": "32]", "ref_id": "BIBREF31"}, {"start": 580, "end": 584, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 669, "end": 673, "text": "[18]", "ref_id": "BIBREF17"}], "ref_spans": [], "section": "Related Work"}, {"text": "Recurrent Neural Networks [28] (RNNs) are used learn the patterns in the sequence data due to their ability to capture interdependence among the observations [10, 12] . In [5] , Chung et al. show that the extensions of RNN, namely Long Short Term Memory (LSTM) [14] and Gated Recurrent Unit (GRU) [3] , are more effective than simple RNNs at capturing longer trends in the sequence data. However, they do not conclude which of these gated recurrent model is better than the other. Readers are advised to refer to [22] for an extensive survey of RNNs and their successors.", "cite_spans": [{"start": 26, "end": 30, "text": "[28]", "ref_id": "BIBREF27"}, {"start": 158, "end": 162, "text": "[10,", "ref_id": "BIBREF9"}, {"start": 163, "end": 166, "text": "12]", "ref_id": "BIBREF11"}, {"start": 172, "end": 175, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 178, "end": 190, "text": "Chung et al.", "ref_id": null}, {"start": 261, "end": 265, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 297, "end": 300, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 513, "end": 517, "text": "[22]", "ref_id": "BIBREF21"}], "ref_spans": [], "section": "Related Work"}, {"text": "Recurrent neural networks and their extensions are widely used by researchers in the domain of text analysis and language modeling. Sutskever et al. [29] have used multiplicative RNN to generate text. In [10] , Graves has used LSTM to generate text data as well as images with cursive script corresponding to the input text. Autoencoder [13] is a class of neural networks which researchers have widely used for finding latent patterns in the data. Li et al. [19] have used LSTM-autoencoder to generates text preserving the multi-sentence structure in the paragraphs. They give entire paragraph as the input to the system that outputs the text which is both semantically and syntactically closer to the input paragraph. Tomas et al. [24, 25] have proposed RNN based language models which have shown to outperform classical probabilistic language models. In [26] , Tomas et al. provide a context along with the text as an input to RNN and later predict the next word given the context of preceding text. They use LDA [2] to find topics in the text and propose a technique to compute topical features of the input which are fed to RNN along with the input. Ghosh et al. [9] have extended idea in [26] by using LSTM instead of RNN. They use the language model at the level of a word as well as at the level of a sentence and perform experiments to predict next word as well as next sentence given the input concatenated with the topic. There have been evidences of LSTM outperforming GRU for the task of language modeling [15, 16] . Nevertheless, we compare our proposed model using both of these gated recurrent building blocks. We use the simple RNN as our baseline for the comparison.", "cite_spans": [{"start": 149, "end": 153, "text": "[29]", "ref_id": "BIBREF28"}, {"start": 204, "end": 208, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 337, "end": 341, "text": "[13]", "ref_id": "BIBREF12"}, {"start": 458, "end": 462, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 732, "end": 736, "text": "[24,", "ref_id": "BIBREF23"}, {"start": 737, "end": 740, "text": "25]", "ref_id": "BIBREF24"}, {"start": 856, "end": 860, "text": "[26]", "ref_id": "BIBREF25"}, {"start": 1015, "end": 1018, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 1167, "end": 1170, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 1193, "end": 1197, "text": "[26]", "ref_id": "BIBREF25"}, {"start": 1518, "end": 1522, "text": "[15,", "ref_id": "BIBREF14"}, {"start": 1523, "end": 1526, "text": "16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Related Work"}, {"text": "Despite these applications of deep neural networks on the textual data, there are few caveats in these applications. For instance, although in [9] authors develop CLSTM which is able to generate text, they evaluate its predictive properties purely using objective metric like perplexity. The model is not truly evaluated to see how effective it is towards generating the data. In this paper, our aim is to use deep neural networks to generate the text and hence evaluate the quality of synthetically generated text against its topical coherence as well as grammatical coherence.", "cite_spans": [{"start": 143, "end": 146, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Related Work"}, {"text": "Recurrent Neural Network (RNN) is an adaptation of the standard feedforward neural network wherein connections between hidden layers form a loop. Simple RNN architecture consists of an input layer (x), a hidden layer (h), and an output layer (y). Unlike the standard feedforward networks, the hidden layer of RNN receives an additional input from the previous hidden layer. These recurrent connections give RNN the power to learn sequential patterns in the input. We use the many-to-many variant of RNN architecture which outputs n-gram given the previous n-gram as the input. For instance, given {(hello, how, are)} trigram as the input, RNN outputs {(how, are, you)} as the preceding trigram.", "cite_spans": [], "ref_spans": [], "section": "Background: Recurrent Neural Network"}, {"text": "Bengio et al. [1] show that learning the long-term dependencies using gradient descent becomes difficult because the gradients eventually either vanish or explode. The gated recurrent models, LSTM [14] and GRU [3] , alleviate these problems by adding gates and memory cells (in the case of LSTM) in the hidden layer to control the information flow. LSTM introduces three gates namely forget gate (f ), input gate (i), and output gate (o). Forget gate filters the amount of information to retain from the previous step, whereas input and output gate defines the amount of information to store in the memory cell and the amount of information to transfer to the next step, respectively. Equation 1 shows the formula to calculate the forget gate activations at a certain step t. For given layers or gates m and n, W mn denotes the weight matrix and b m is the bias vector for the respective gate. h is the activation vector for the hidden state and \u03c3(\u00b7) denotes the sigmoid function. Readers are advised to refer to [14] for the complete formulae of each gate and layer in LSTM.", "cite_spans": [{"start": 14, "end": 17, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 197, "end": 201, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 210, "end": 213, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 1013, "end": 1017, "text": "[14]", "ref_id": "BIBREF13"}], "ref_spans": [], "section": "Background: Recurrent Neural Network"}, {"text": "GRU simplifies LSTM by merging the memory cell and the hidden state, so there is only one output in GRU. It uses two gates which are update and reset gate. Update gate unifies the input gate and the forget gate in LSTM to control the amount of information from the previous hidden state. The reset gate combines the input with the previous hidden state to generate the current hidden state.", "cite_spans": [], "ref_spans": [], "section": "Background: Recurrent Neural Network"}, {"text": "Simple RNNs predict the next word solely based on the word dependencies which are learnt during the training phase. Given a certain text as a seed, the seed may give rise to different texts depending on the context. Refer to the Sect. 1 for an illustration. [9] extends the standard LSTM to Contextual Long Short Term Memory (CLSTM) model which accepts the context as an input along with the text. For example, an input pair {(where, is, your), (technology)} generates an output like {(is, your, phone)}. CLSTM is a special case of the architecture shown in Fig. 1a using LSTM as the gated recurrent model.", "cite_spans": [{"start": 258, "end": 261, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [{"start": 558, "end": 565, "text": "Fig. 1a", "ref_id": null}], "section": "Proposed Syntacto-Contextual Architecture"}, {"text": "In order to use the model for the purpose of text generation, contextual information is not sufficient to obtain a good quality output. A good quality text is coherent not only in terms of its semantics but also in terms of its syntax. By providing the syntactic information along with the text, we extend the contextual model to Syntacto-Contextual (SC) models. Figure 1b shows the general architecture of the proposed model. We encode the patterns in the syntactic meta information and input text using the gated recurrent units and, later, merge them with the context. The proposed model not only outputs text but also corresponding syntactic information. For instance, an input {(where, is, your), (adverb, verb, pronoun), (technology)} generates output like {(is, your, phone), (verb, pronoun, noun)}. Mathematically, the addition of context and syntactic information amounts to learning a few extra weight parameters. Specifically, in case of LSTM, Eq. 1 will be modified to Eqs. 2 and 3, for CLSTM and SCLSTM respectively. In Eqs. 2 and 3, p represents topic embedding and s represents embedding of the syntactic information.", "cite_spans": [], "ref_spans": [{"start": 363, "end": 372, "text": "Figure 1b", "ref_id": null}], "section": "Proposed Syntacto-Contextual Architecture"}, {"text": "For the current study, we annotate the text input with the part-of-speech tags using Penn Treebank tagset [23] . We learn the parameters of the model using stochastic gradient descent by minimizing the loss for both output text and output tags. We, also, work on a variation of the contextual architecture which does not accept topic as an input and uses conventional RNN instead of LSTM. This model is treated as the baseline against which all of the models will be compared.", "cite_spans": [{"start": 106, "end": 110, "text": "[23]", "ref_id": "BIBREF22"}], "ref_spans": [], "section": "Proposed Syntacto-Contextual Architecture"}, {"text": "For each of the model, we embed the input in a vector space. We merge the inputs by column wise concatenation of the vectors. We perform experiments using both LSTM and GRU as the gated recurrent units. The output layer is a softmax layer that represents the probability of each word or tag. We sample from that probability to get the next word and tag output.", "cite_spans": [], "ref_spans": [], "section": "Proposed Syntacto-Contextual Architecture"}, {"text": "We conduct a comparative study on five different models using a real-world News Aggregator Dataset. In the beginning of this section, we present the details of the dataset and the experimental setup for the study. We, further, describe various quality metrics which we use to evaluate the effectiveness of the models. We perform quantitative analysis using these metric and present our results. We complete the evaluation by presenting micro-analysis for a sample of generated news headlines to show the qualitative improvement observed in the task of news headline generation.", "cite_spans": [], "ref_spans": [], "section": "Experimentation and Results"}, {"text": "We use the News Aggregator dataset 3 consisting of the news headlines collected by the news aggregator from 11,242 online news hostnames, such as time.com, forbes.com, reuters.com, etc. between 10 March 2014 to 10 August 2014. The dataset contains 422,937 news articles divided into four categories, namely business, technology, entertainment, and health. We randomly select 45000 news headlines, which contain more than three words, from each category because we give trigram as the input to the models. We preprocess the data in two steps. Firstly, we remove all non alpha-numeric characters from the news titles. Secondly, we convert all the text into lower case. After the preprocessing, the data contains 4,274,380 unique trigrams and 39,461 unique words.", "cite_spans": [], "ref_spans": [], "section": "Dataset"}, {"text": "All programs are run on Linux machine with quad core 2.40 GHz Intel R Core i7 TM processor with 64 GB memory. The machine is equipped with two Nvidia GTX 1080 GPUs. Python R 2.7.6 is used as the scripting language. We use a highlevel neural network Python library, Keras [4] which runs on top of Theano [30] . We use categorical cross entropy as our loss function and use ADAM [17] as an optimizer to automatically adjust the learning rate.", "cite_spans": [{"start": 271, "end": 274, "text": "[4]", "ref_id": null}, {"start": 303, "end": 307, "text": "[30]", "ref_id": "BIBREF29"}, {"start": 377, "end": 381, "text": "[17]", "ref_id": "BIBREF16"}], "ref_spans": [], "section": "Experimental Setup"}, {"text": "We conduct experiments and comparatively evaluate five models. We refer to those models as, baseline -a simple RNN model, CLSTM -contextual architecture with LSTM as the gated recurrent model, CGRU -contextual architecture with GRU as the gated recurrent model, SCLSTM -syntactocontextual architecture with LSTM as the gated recurrent model, SCGRUsyntacto-contextual architecture with GRU as the gated recurrent model, in the rest of the evaluation. All inputs are embedded into a 200-dimensional vector space. We use recurrent layers each with 512 hidden units with 0.5 dropout rate to prevent overfitting. To control the randomness of the prediction, we set the temperature parameter in our output softmax layer to 0.4. We use the batch size of 32 to train the model until the validation error stops decreasing.", "cite_spans": [], "ref_spans": [], "section": "Experimental Setup"}, {"text": "In this section, we present different evaluation metrics that we use for the quantitative analysis. Along with purely objective quantitative metrics such as perplexity, machine translation quality metric, and topical precision, we use metrics like grammatical correctness, n-gram repetition for a finer effectiveness analysis. Additionally, we devise a novelty metric to qualitatively analyse the current use case of news headline generation.", "cite_spans": [], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "Perplexity is commonly used as the performance measure [9, 10, 15, 16] to evaluate the predictive power of a language model. Given N test data with w t as the target outputs, the perplexity is calculated by using Eq. 4, where p i wt is the probability of the target output of sample i. A good language model assigns a higher probability to the word that actually occurs in the test data. Thus, a language model with lower perplexity is a better model.", "cite_spans": [{"start": 55, "end": 58, "text": "[9,", "ref_id": "BIBREF8"}, {"start": 59, "end": 62, "text": "10,", "ref_id": "BIBREF9"}, {"start": 63, "end": 66, "text": "15,", "ref_id": "BIBREF14"}, {"start": 67, "end": 70, "text": "16]", "ref_id": "BIBREF15"}], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "As it happens, the exponent in the Eq. 4 is the approximation of cross-entropy 4 , which is the loss function we minimize to train the model, given a sequence of fixed length.", "cite_spans": [], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "Although the task under consideration of the presented work is not of a word or a topic prediction, we simply use perplexity as a purely objective baseline metric. We complement it by using various application specific measures in order to evaluate the effectiveness of the quality of the generated text.", "cite_spans": [], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "Topical coherence refers to the extent to which the generated text adheres to the desired topic. In order to evaluate the topical coherence, one requires a faithful classifier which predicts the topic of generated text. We treat the topics predicted by the classifier as the ground truth to quantitatively evaluate the topical coherence. The proposed method generates a news headline given a seed and a topic of the news. People have widely used Multinomial naive Bayes classifier to deal with text data due to independence among the words given a certain class 5 . We train a Multinomial naive Bayes classifier with Laplace smoothing on the news dataset consisting of 45000 news from each of the four categories. We hold out 20% of the data for validation. By proper tuning of the smoothing parameter, we achieve 89% validation accuracy on the news dataset. We do not use this metric for the baseline model.", "cite_spans": [], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "Taking existing text as a reference, a quality metric evaluates the effectiveness of the generated text in correspondence to the reference. Such a metric measures the closeness of the generated text to the reference text. Metrics such as BLEU [27] , Rouge [8] , NIST [21] are widely used to evaluate the quality of machine translation. All of these metrics use \"gold standard\", which is either the original text or the text written by the domain experts, to check the quality of the generated text. We use BLEU as the metric to evaluate the quality of generated text. For a generated news headline, we calculate its BLEU score by taking all the sentences in the respective topic from the dataset as the reference. Interested readers should refer to [33] for a detailed qualitative and quantitative interpretation of BLEU scores.", "cite_spans": [{"start": 243, "end": 247, "text": "[27]", "ref_id": "BIBREF26"}, {"start": 256, "end": 259, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 267, "end": 271, "text": "[21]", "ref_id": "BIBREF20"}, {"start": 749, "end": 753, "text": "[33]", "ref_id": "BIBREF32"}], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "With the motivation of the current work presented in the Sect. 1, we want the generated text from our model to be as novel as possible. So as to have a robust fake news filter, the fake news, which is used to train the model, should not be a juxtaposition of few existing news headlines. More the patterns it learns from the training data to generate a single headline, more novel is the generated headline. We define novelty of the generated output as the number of unique patterns the model learns from the training data in order to generate that output. We realize this metric by calculating longest common sentence common to the generated headline and each of the headline in the dataset. Each of these sentences stands as a pattern that the model has learned to generate the text. Novelty of a generated headline is taken as the number of unique longest common sentences.", "cite_spans": [], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "The good quality generated text should be both novel and grammatically correct. Grammatical correctness refers the judgment on whether the generated text adheres to the set of grammatical rules defined by a certain language. Researchers either employ experts for evaluation or use advanced grammatical evaluation tools which require the gold standard reference for the evaluation [6] .", "cite_spans": [{"start": 380, "end": 383, "text": "[6]", "ref_id": "BIBREF5"}], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "We use an open-source grammar and spell checker software called LanguageTool 6 to check the grammatical correctness of our generated headlines. LanguageTool uses NLP based 1516 English grammar rules to detect syntactical errors. Aside from NLP based rules, it used English specific spelling rules to detect spelling errors in the text. To evaluate grammatical correctness, we calculate the percentage of grammatically correct sentences as predicted by the LanguageTool.", "cite_spans": [], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "We find that LanguageTool only recognizes word repetition as an error. Consider a generated headline beverly hills hotel for the first in the first in the world as an example. In this headline, there is a trigram repetition -the first in -that passes LanguageTool grammatical test. Such headlines are not said to be good quality headlines. We add new rules with a regular expression to detect such repetitions. We count n-gram repetitions within a sentence for values of n greater than two.", "cite_spans": [], "ref_spans": [], "section": "Evaluation Metrics"}, {"text": "To generate the output, we need an initial trigram as a seed. We randomly pick the initial seed from the set of news headlines from the specified topic. We use windowing technique to generate the next output. We remove the first word and append the output to the back of the seed to generate the next output. The process stop when specified sentence length is generated. We generate 100 sentences for each topic in which each sentence contains 3 seed words and 10 generated words.", "cite_spans": [], "ref_spans": [], "section": "Results"}, {"text": "Quantitative Evaluation. Table 1 summarizes the quantitative evaluation of all the models using metrics described in Sect. 4.3. Scores in bold numbers denote the best value for each metric. We can see that for Contextual architecture, GRU is a better gated recurrent model. Conversely, LSTM is better for Syntacto-Contextual architecture.", "cite_spans": [], "ref_spans": [{"start": 25, "end": 32, "text": "Table 1", "ref_id": "TABREF1"}], "section": "Results"}, {"text": "For Syntacto-Contextual architecture, we only consider the perplexity of the text output to make a fair comparison with the Contextual architecture. We analyze that our Syntacto-Contextual architecture has a higher perplexity score because the model jointly minimizes both text and syntactical output losses. On the other hand, the baseline model has a low perplexity score because it simply predicts the next trigram with control on neither the context nor the syntax.", "cite_spans": [], "ref_spans": [], "section": "Results"}, {"text": "A high score on classification precision substantiates that all of these models generate headlines which are coherent with the topic label with which they are generated. We observe that all of the models achieve a competitive BLEU score. Although Contextual architecture performs slightly better in terms of BLEU score, Syntacto-Contextual architecture achieves a higher novelty score. In the qualitative evaluation, we present a more detailed comparative analysis of BLEU scores and novelty scores.", "cite_spans": [], "ref_spans": [], "section": "Results"}, {"text": "We observe that the news headlines generated by Syntacto-Contextual architecture are more grammatically correct than other models. Figure 2 shows the histogram of n-gram repetitions in the generated news headline. We see that the Syntacto-Contextual architecture gives rise to news headlines with less number of n-gram repetitions.", "cite_spans": [], "ref_spans": [{"start": 131, "end": 139, "text": "Figure 2", "ref_id": null}], "section": "Results"}, {"text": "Lastly, we have empirically evaluated, but not presented here, the time taken by different models for one epoch. CLSTM takes 2000 s for one epoch whereas SCLSTM takes 2131 s for one epoch. Despite the Syntacto-Contextual architecture being a more complex architecture than Contextual architecture, it shows that it is competitively efficient. Table 2 presents the samples of generated news from CLSTM proposed by [9] and SCLSTM, which outweighs the rest of the models in the quantitative analysis.", "cite_spans": [{"start": 413, "end": 416, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [{"start": 343, "end": 350, "text": "Table 2", "ref_id": "TABREF2"}], "section": "Results"}, {"text": "In Table 1 , we see that the Contextual architecture models receive a higher BLEU score than the proposed architecture models. BLEU score is calculated using n-gram precisions with the news headlines as the reference. It is not always necessary that the higher BLEU score leads towards a good quality text generation. Qualitative analysis of generated headlines shows that the higher BLEU score, in the most cases, is the result of the juxtaposition of the existing news headlines. For instance, consider a headline generated by CLSTM model as an example -\"justin bieber apologizes for racist joke in new york city to take on\" -which receives a BLEU score of 0.92. When we search for the same news in the dataset, we find that this generated news is a combination of two patterns from the following two headlines, \"justin bieber apologizes for racist joke\" and \"uber temporarily cuts fares in new york city to take on city cabs\". Whereas the headline generated by SCLSTM with the same seed is quite a novel headline. In the training dataset there is mention of neither Justin Bieber joking on Twitter nor joke for gay fans. Similar observation can be made with the news related to Fukushima. In the training data set there is no news headline which links Fukushima with climate change. Additionally, there is no training data which links higher growth risk to climate change as well. Thus, we observe that the headlines generated using SCLSTM are qualitatively better than CLSTM. All of the models presented in the work are probabilistic models. Text generation being a probabilistic event, on the one hand it is possible that contextual architecture generates a good quality headline at a certain occasion. For instance, we see that CLSTM also generates some good quality news headlines such as \"the fault in our stars trailer for the hunger games mockingjay part teaser\". On the other hand, it is possible that Syntacto-Contextual architecture generates some news headline with poor quality or repetitions, such as \"obama warns google apple to make android support for mobile for mobile\". In order to qualitatively analyse the novelty of generated sentence, we need to observe how likely such events occur. Figure 3 shows the boxplot of novelty numbers we calculate for each of 400 generated news headlines using different models. As discussed earlier, we want our model to generate novel news headlines. So, we prefer higher novelty scores. Although the mean novelty of all of the models lie around 24, we see that SCLSTM is more likely to generate the novel headlines. Additionally, we observe that contextual and Syntacto-Contextual architectures performs better than the baseline model.", "cite_spans": [], "ref_spans": [{"start": 3, "end": 10, "text": "Table 1", "ref_id": "TABREF1"}, {"start": 2209, "end": 2217, "text": "Figure 3", "ref_id": "FIGREF1"}], "section": "Qualitative Evaluation."}, {"text": "As mentioned in the quantitative evaluation, Contextual architecture gives rise to news headlines with a large number of n-gram repetitions. In an extreme case, CLSTM model generates the following headline, \"lorillard inc nyse wmt wal mart stores inc nyse wmt wal mart stores\", that contains 6-gram repetition. The news headline generated by CLSTM -\"Samsung sues newspaper for anti vaccine and other devices may be the best\"-exemplifies the smaller topical coherence observed for the Contextual architecture models.", "cite_spans": [], "ref_spans": [], "section": "Qualitative Evaluation."}, {"text": "In order to garner the opinion of real-world users, we use CrowdFlower 7 to conduct a crowdsource based study. In this study, we generate two news headlines using CLSTM and SCLSTM using the same seed and ask the workers to choose a more realistic headline between two. We generate such a pair of headlines for 200 different seeds. Each pair is evaluated by three workers and majority vote is used to choose the right answer. At the end of the study, 66% workers agree that SCLSTM generates more realistic headlines than CLSTM.", "cite_spans": [], "ref_spans": [], "section": "Qualitative Evaluation."}, {"text": "In [9] , Ghosh et al. proposed a deep learning model to predict the next word or sentence given the context of the input text. In this work, we adapted and extended their model towards automatic generation of news headlines. The contribution of the proposed work is two-fold. Firstly, in order to generate news headlines which are not only topically coherent but also syntactically sensible, we proposed an architecture that learns part-of-speech model along with the context of the textual input. Secondly, we performed thorough qualitative and quantitative analysis to assess the quality of the generated news headlines using existing metrics as well as a novelty metric proposed for the current application. We comparatively evaluated the proposed models with [9] and a baseline. To this end, we show that the proposed approach is competitively better and generates good quality news headlines given a seed and the topic of the interest.", "cite_spans": [{"start": 3, "end": 6, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 763, "end": 766, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Discussion and Future Work"}, {"text": "Through this work, we direct our methodology for data-driven text generation towards a \"constraint and generate\" paradigm from a more brute-force way of \"generate and test\". Quality assessment of the generated data using generative model remains an open problem in the literature [31] . We use the measure of quality, which in our case is the grammatical correctness, as an additional constraint for the model in order to generate the good quality data. The usage of POS tags as the syntactic element is mere a special case in this application. We can think of more sophisticated meta information to enrich the quality of text generation. Ontological categories can be an alternative option.", "cite_spans": [{"start": 280, "end": 284, "text": "[31]", "ref_id": "BIBREF30"}], "ref_spans": [], "section": "Discussion and Future Work"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Learning long-term dependencies with gradient descent is difficult", "authors": [{"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}, {"first": "P", "middle": [], "last": "Simard", "suffix": ""}, {"first": "P", "middle": [], "last": "Frasconi", "suffix": ""}], "year": 1994, "venue": "IEEE Trans. Neural Netw", "volume": "5", "issn": "2", "pages": "157--166", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "Latent dirichlet allocation", "authors": [{"first": "D", "middle": ["M"], "last": "Blei", "suffix": ""}, {"first": "A", "middle": ["Y"], "last": "Ng", "suffix": ""}, {"first": "M", "middle": ["I"], "last": "Jordan", "suffix": ""}], "year": 2003, "venue": "J. Mach. Learn. Res", "volume": "3", "issn": "", "pages": "993--1022", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "authors": [{"first": "K", "middle": [], "last": "Cho", "suffix": ""}, {"first": "B", "middle": [], "last": "Van Merri\u00ebnboer", "suffix": ""}, {"first": "C", "middle": [], "last": "Gulcehre", "suffix": ""}, {"first": "D", "middle": [], "last": "Bahdanau", "suffix": ""}, {"first": "F", "middle": [], "last": "Bougares", "suffix": ""}, {"first": "H", "middle": [], "last": "Schwenk", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1406.1078"]}}, "BIBREF4": {"ref_id": "b4", "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "authors": [{"first": "J", "middle": [], "last": "Chung", "suffix": ""}, {"first": "C", "middle": [], "last": "Gulcehre", "suffix": ""}, {"first": "K", "middle": [], "last": "Cho", "suffix": ""}, {"first": "Y", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.3555"]}}, "BIBREF5": {"ref_id": "b5", "title": "Better evaluation for grammatical error correction", "authors": [{"first": "D", "middle": [], "last": "Dahlmeier", "suffix": ""}, {"first": "H", "middle": ["T"], "last": "Ng", "suffix": ""}], "year": 2012, "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "volume": "", "issn": "", "pages": "568--572", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Deep learning: methods and applications. Found. Trends R Sig. Process. 7(3-4)", "authors": [{"first": "L", "middle": [], "last": "Deng", "suffix": ""}, {"first": "D", "middle": [], "last": "Yu", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "197--387", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "authors": [{"first": "G", "middle": [], "last": "Doddington", "suffix": ""}], "year": 2002, "venue": "Proceedings of the Second International Conference on Human Language Technology Research", "volume": "", "issn": "", "pages": "138--145", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Contextual LSTM (CLSTM) models for large scale NLP tasks", "authors": [{"first": "S", "middle": [], "last": "Ghosh", "suffix": ""}, {"first": "O", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "B", "middle": [], "last": "Strope", "suffix": ""}, {"first": "S", "middle": [], "last": "Roy", "suffix": ""}, {"first": "T", "middle": [], "last": "Dean", "suffix": ""}, {"first": "L", "middle": [], "last": "Heck", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1602.06291"]}}, "BIBREF9": {"ref_id": "b9", "title": "Generating sequences with recurrent neural networks", "authors": [{"first": "A", "middle": [], "last": "Graves", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1308.0850"]}}, "BIBREF10": {"ref_id": "b10", "title": "Hybrid speech recognition with deep bidirectional LSTM", "authors": [{"first": "A", "middle": [], "last": "Graves", "suffix": ""}, {"first": "N", "middle": [], "last": "Jaitly", "suffix": ""}, {"first": "A", "middle": ["R"], "last": "Mohamed", "suffix": ""}], "year": 2013, "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "volume": "", "issn": "", "pages": "273--278", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Speech recognition with deep recurrent neural networks", "authors": [{"first": "A", "middle": [], "last": "Graves", "suffix": ""}, {"first": "A", "middle": ["R"], "last": "Mohamed", "suffix": ""}, {"first": "G", "middle": [], "last": "Hinton", "suffix": ""}], "year": 2013, "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "volume": "", "issn": "", "pages": "6645--6649", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Reducing the dimensionality of data with neural networks", "authors": [{"first": "G", "middle": ["E"], "last": "Hinton", "suffix": ""}, {"first": "R", "middle": ["R"], "last": "Salakhutdinov", "suffix": ""}], "year": 2006, "venue": "Science", "volume": "313", "issn": "5786", "pages": "504--507", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Long short-term memory", "authors": [{"first": "S", "middle": [], "last": "Hochreiter", "suffix": ""}, {"first": "J", "middle": [], "last": "Schmidhuber", "suffix": ""}], "year": 1997, "venue": "Neural Comput", "volume": "9", "issn": "8", "pages": "1735--1780", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "LSTM, GRU, highway and a bit of attention: an empirical overview for language modeling in speech recognition", "authors": [{"first": "K", "middle": [], "last": "Irie", "suffix": ""}, {"first": "Z", "middle": [], "last": "T\u00fcske", "suffix": ""}, {"first": "T", "middle": [], "last": "Alkhouli", "suffix": ""}, {"first": "R", "middle": [], "last": "Schl\u00fcter", "suffix": ""}, {"first": "H", "middle": [], "last": "Ney", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "An empirical exploration of recurrent network architectures", "authors": [{"first": "R", "middle": [], "last": "Jozefowicz", "suffix": ""}, {"first": "W", "middle": [], "last": "Zaremba", "suffix": ""}, {"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 32nd ICML", "volume": "", "issn": "", "pages": "2342--2350", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Adam: A method for stochastic optimization", "authors": [{"first": "D", "middle": [], "last": "Kingma", "suffix": ""}, {"first": "J", "middle": [], "last": "Ba", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6980"]}}, "BIBREF17": {"ref_id": "b17", "title": "Imagenet classification with deep convolutional neural networks", "authors": [{"first": "A", "middle": [], "last": "Krizhevsky", "suffix": ""}, {"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "G", "middle": ["E"], "last": "Hinton", "suffix": ""}], "year": 2012, "venue": "Advances in Neural Information Processing Systems", "volume": "", "issn": "", "pages": "1097--1105", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "A hierarchical neural autoencoder for paragraphs and documents", "authors": [{"first": "J", "middle": [], "last": "Li", "suffix": ""}, {"first": "M", "middle": ["T"], "last": "Luong", "suffix": ""}, {"first": "D", "middle": [], "last": "Jurafsky", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1506.01057"]}}, "BIBREF19": {"ref_id": "b19", "title": "UCI machine learning repository", "authors": [{"first": "M", "middle": [], "last": "Lichman", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Rouge: a package for automatic evaluation of summaries", "authors": [{"first": "C", "middle": ["Y"], "last": "Lin", "suffix": ""}], "year": 2004, "venue": "Proceedings of the ACL-04 Workshop", "volume": "8", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "A critical review of recurrent neural networks for sequence learning", "authors": [{"first": "Z", "middle": ["C"], "last": "Lipton", "suffix": ""}, {"first": "J", "middle": [], "last": "Berkowitz", "suffix": ""}, {"first": "C", "middle": [], "last": "Elkan", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1506.00019"]}}, "BIBREF22": {"ref_id": "b22", "title": "Building a large annotated corpus of English: the penn treebank", "authors": [{"first": "M", "middle": ["P"], "last": "Marcus", "suffix": ""}, {"first": "M", "middle": ["A"], "last": "Marcinkiewicz", "suffix": ""}, {"first": "B", "middle": [], "last": "Santorini", "suffix": ""}], "year": 1993, "venue": "Comput. Linguist", "volume": "19", "issn": "2", "pages": "313--330", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Recurrent neural network based language model", "authors": [{"first": "T", "middle": [], "last": "Mikolov", "suffix": ""}, {"first": "M", "middle": [], "last": "Karafi\u00e1t", "suffix": ""}, {"first": "L", "middle": [], "last": "Burget", "suffix": ""}, {"first": "J", "middle": [], "last": "Cernock\u1ef3", "suffix": ""}, {"first": "S", "middle": [], "last": "Khudanpur", "suffix": ""}], "year": 2010, "venue": "In: Interspeech", "volume": "2", "issn": "", "pages": "", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "Extensions of recurrent neural network language model", "authors": [{"first": "T", "middle": [], "last": "Mikolov", "suffix": ""}, {"first": "S", "middle": [], "last": "Kombrink", "suffix": ""}, {"first": "L", "middle": [], "last": "Burget", "suffix": ""}, {"first": "J", "middle": [], "last": "\u010cernock\u1ef3", "suffix": ""}, {"first": "S", "middle": [], "last": "Khudanpur", "suffix": ""}], "year": 2011, "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "volume": "", "issn": "", "pages": "5528--5531", "other_ids": {}}, "BIBREF25": {"ref_id": "b25", "title": "Context dependent recurrent neural network language model", "authors": [{"first": "T", "middle": [], "last": "Mikolov", "suffix": ""}, {"first": "G", "middle": [], "last": "Zweig", "suffix": ""}], "year": 2012, "venue": "SLT", "volume": "12", "issn": "", "pages": "234--239", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "BLEU: a method for automatic evaluation of machine translation", "authors": [{"first": "K", "middle": [], "last": "Papineni", "suffix": ""}, {"first": "S", "middle": [], "last": "Roukos", "suffix": ""}, {"first": "T", "middle": [], "last": "Ward", "suffix": ""}, {"first": "W", "middle": ["J"], "last": "Zhu", "suffix": ""}], "year": 2002, "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics", "volume": "", "issn": "", "pages": "311--318", "other_ids": {}}, "BIBREF27": {"ref_id": "b27", "title": "Training recurrent neural networks", "authors": [{"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF28": {"ref_id": "b28", "title": "Generating text with recurrent neural networks", "authors": [{"first": "I", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "J", "middle": [], "last": "Martens", "suffix": ""}, {"first": "G", "middle": ["E"], "last": "Hinton", "suffix": ""}], "year": 2011, "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-2011)", "volume": "", "issn": "", "pages": "1017--1024", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "Theano Development Team: Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints abs/1605.02688", "authors": [], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "A note on the evaluation of generative models", "authors": [{"first": "L", "middle": [], "last": "Theis", "suffix": ""}, {"first": "A", "middle": ["V D"], "last": "Oord", "suffix": ""}, {"first": "M", "middle": [], "last": "Bethge", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1511.01844"]}}, "BIBREF31": {"ref_id": "b31", "title": "Collaborative deep learning for recommender systems", "authors": [{"first": "H", "middle": [], "last": "Wang", "suffix": ""}, {"first": "N", "middle": [], "last": "Wang", "suffix": ""}, {"first": "D", "middle": ["Y"], "last": "Yeung", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "volume": "", "issn": "", "pages": "1235--1244", "other_ids": {}}, "BIBREF32": {"ref_id": "b32", "title": "Interpreting BLEU/NIST scores: how much improvement do we need to have a better system", "authors": [{"first": "Y", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "S", "middle": [], "last": "Vogel", "suffix": ""}, {"first": "A", "middle": [], "last": "Waibel", "suffix": ""}], "year": 2004, "venue": "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC)", "volume": "", "issn": "", "pages": "2051--2054", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Springer International Publishing AG 2017 D. Benslimane et al. (Eds.): DEXA 2017, Part II, LNCS 10439, pp. 427-440, 2017. DOI: 10.1007/978-3-319-64471-4 34", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Boxplot for novelty metric", "latex": null, "type": "figure"}, "TABREF0": {"text": "Fig. 1. Contextual and syntacto-contextual architectures", "latex": null, "type": "table"}, "TABREF1": {"text": "Quantitative evaluation.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Baseline </td><td>CLSTM [9] </td><td>CGRU </td><td>SCLSTM </td><td>SCGRU\n</td></tr><tr><td>Perplexity </td><td>108.383 </td><td>119.10 </td><td>92.22 </td><td>146.93 </td><td>175.83\n</td></tr><tr><td>Topical coherence (%) - </td><td>84.25 </td><td>77.25 </td><td>94.75 </td><td>87.50\n</td></tr><tr><td>Quality (BLEU) </td><td>0.613 </td><td>0.637 </td><td>0.655 </td><td>0.633 </td><td>0.625\n</td></tr><tr><td>Novelty </td><td>21.605 </td><td>24.67 </td><td>25.21 </td><td>26.57 </td><td>25.65\n</td></tr><tr><td>Grammatical correctness (%) </td><td>28.25 </td><td>49.75 </td><td>50.75 </td><td>75.25 </td><td>69.00\n</td></tr><tr><td>n-gram Repetitions </td><td>11 </td><td>30 </td><td>12 </td><td>5 </td><td>8\n</td></tr></table></body></html>"}, "TABREF2": {"text": "Generated news headlines florida state health care system draws attention mosquitoes test positive for west africa in guinea in june to be the mosquitoes test positive for west nile virus found in storage room at home ticks and lyme disease in the us in the us are the best ticks and lyme disease rates double in autism risk in china in west Business us accuses china and russia to pay billion in billion in us in us accuses china of using internet explorer bug leaves users to change passwords wake of massive data breach of possible to buy stake in us wake of massive recall of million vehicles for ignition switch problem in china japan fukushima nuclear plant in kansas for first time in the first time japan fukushima nuclear plant linked to higher growth risk of climate change ipcc Entertainment justin bieber apologizes for racist joke in new york city to take on justin bieber apologizes for racist joke on twitter for gay fans have the fault in our stars trailer for the hunger games mockingjay part teaser the fault in our stars star chris hemsworth and elsa pataky reveal his giant practical spaceship interiors for joint venture in mexico production of star wars giant practical spaceship hits the hollywood walk of fame induction ceremony in Technologyfirst android wear watches and google be to be available in the uk first android wear watch google play edition is now available on xbox one samsung sues newspaper for anti vaccine and other devices may be the best samsung sues newspaper over facebook experiment on users with new profile feature is obama warns google glass to be forgotten on the us government issues recall obama warns google apple to make android support for mobile for mobile", "latex": null, "type": "table"}}, "back_matter": [{"text": "Acknowledgement. This work is partially supported by the National Research Foundation, Prime Ministers Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme and by the National University of Singapore under a grant from Singapore Ministry of Education for research project number T1 251RES1607. We kindly acknowledged UCI for maintaining repository of publicly available Machine Learning dataset [20] .", "cite_spans": [{"start": 449, "end": 453, "text": "[20]", "ref_id": "BIBREF19"}], "ref_spans": [], "section": "acknowledgement"}]}