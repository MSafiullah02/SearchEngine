{
    "paper_id": "70d7e7f52ea1a37688336b74d2e85a639d5d78d7",
    "metadata": {
        "title": "",
        "authors": [
            {
                "first": "Daniel",
                "middle": [],
                "last": "Graziotin",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Stuttgart",
                    "location": {
                        "country": "Germany"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [],
    "body_text": [
        {
            "text": "McDonald and Edwards [60] have subtitled their paper \"Examining the use and abuse of personality tests in software engineering. \". The authors anticipated, thirteen years ago, the issue that we attempt to address in the present submission, that is the \"the lack of progress in this [personality research in software engineering] field is due in part to the inappropriate use of psychological tests, frequently coupled with basic misunderstandings of personality theory by those who use them. \" (p. 67) .",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 25,
                    "text": "[60]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 498,
                    "end": 501,
                    "text": "67)",
                    "ref_id": "BIBREF66"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "Instances of such misconduct 3 can be observed in the results of a systematic literature review of personality research in software engineering by Cruz et al. [17] . We noted in Cruz et al. [17] results that 48% of personality studies in software engineering have employed the Myers-Brigg Type Indicator (MBTI) questionnaire, which has been shown to possess low to none reliability and validity properties [69] up to the point of being called a \"little more than an elaborate Chinese fortune cookie\" [41] .",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 30,
                    "text": "3",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 159,
                    "end": 163,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 190,
                    "end": 194,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 406,
                    "end": 410,
                    "text": "[69]",
                    "ref_id": "BIBREF68"
                },
                {
                    "start": 500,
                    "end": 504,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "Feldt et al. [25] have argued in favor of systematic studies of human aspects of software engineering. More specifically, to adopt measurement instruments coming from psychology and related fields. Graziotin et al. [33] have echoed the call seven years after but found that research on the affect of software developers had been threatened by a deep misunderstanding of related constructs and how to assess them. In particular, the authors noted that peers in software engineering tend to confuse affect-related psychological constructs such as emotions and moods with related, yet different, constructs such as motivation, commitment, and well-being.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 215,
                    "end": 219,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "Lenberg et al. [53] have conducted a systematic literature review of studies of human aspects that made use of behavioral science, calling the field behavioral software engineering. Among their results, they found that software engineering research is threatened by several knowledge gaps when performing behavioral research, and that there have been very few collaborations between software engineering and behavioral science researchers.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "Graziotin et al. [35] , meanwhile, extended their prior observations on affect to a broader view of software engineering research with a psychological perspective. Given the observation that much research in the field has misinterpreted (when not ignored) validated measurement instruments from psychology, the work offered what we can consider the sentiment for the present article, that is brief guidelines to select a theoretical framework and validated measurement instruments from psychology. Graziotin et al. [35] called the field \"psychoempirical software engineering\" but later agreed with Lenberg et al. [53] to unify the vision under \"behavioral software engineering\". Hence, the present collaboration.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 21,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 515,
                    "end": 519,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 613,
                    "end": 617,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "Our previous studies have also reported that, when a validated test from psychology is adopted from software engineering researchers, its items get modified, causing the destruction of its reliability and validity properties. This includes a thorough evaluation of the psychometric properties of candidate instruments. Gren [37] has offered a psychological test theory lens for characterizing validity and reliability in behavioral software engineering research, further enforcing our view that software engineering research that investigates any psychological construct should maintain fair psychometric properties. We agree with Gren [37] that we should \"change the culture in software engineering research from seeing tool-constructing as the holy grail of research and instead value [psychometric] validation studies higher. \" (p. 3) .",
            "cite_spans": [
                {
                    "start": 324,
                    "end": 328,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 631,
                    "end": 640,
                    "text": "Gren [37]",
                    "ref_id": null
                },
                {
                    "start": 835,
                    "end": 837,
                    "text": "3)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "A mea culpa works better than a j'accuse in further building our case, so we bring a negative example from one of our previous studies. As reported in a very recent work by Ralph et al. [71] (which we appreciate in the next paragraph), \"there is no widespread consensus about how to measure developers' productivity or the main antecedents thereof. Many researchers use simple, unvalidated productivity scales\" (p. 6). In one of the earliest works by the first author of the present paper [34] , which was published way after when the study was conducted, we compared the affect triggered by a software development task with the self-assessed productivity of individual programmers. While we were very careful to select a validated measurement instrument of emotions and to highlight how self-assessment of productivity converges to objective assessment of productivity, we used a single likert item to represent productivity. This choice was to reduce as much as possible the items of the measurement instrument, which had to be used every ten minutes. While the results of the study are not invalidated by this choice, the productivity scale itself was not validated, making the results less valuable from a psychometric perspective and, thus, our interpretation of its results. The study was also (successfully) independently replicated twice by two ICSE papers, which suffer from the same unfortunate choice.",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 190,
                    "text": "[71]",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 489,
                    "end": 493,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "We wish to refrain from being overly negative. The field of software engineering does have positive cases-excluding those from the present authors-that we can showcase here. For example, Fagerholm and Pagels [23] developed a questionnaire on lean and agile values and applied psychometric approaches to inspect the structure of value dimensions. Fagerholm [22] has also embodied psychometric approaches in his PhD dissertation by analyzing the validity of the constructs he studied. A more recent example has been offered by Ralph et al. [71] , who analyzed through a questionnaire the effects of the COVID-19 pandemic on developers' well-being and productivity. The authors constructed their measurement instrument by incorporating psychometrically validated scales on constructs such as perceived productivity, disaster preparedness, fear and resilience, ergonomics, and organizational support. Furthermore, they employed confirmatory factor analysis (which we touch upon in the present paper) to verify that the included items do indeed cluster and converge into the factors that are claimed to converge to.",
            "cite_spans": [
                {
                    "start": 208,
                    "end": 212,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 356,
                    "end": 360,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 538,
                    "end": 542,
                    "text": "[71]",
                    "ref_id": "BIBREF70"
                }
            ],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "Filling the knowledge gap: introductive guidelines to psychometric evaluation for uninitiated behavioral software engineering research",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "What we see missing is an introduction to the field of psychometrics for behavioral software engineering researchers. Such an introduction would improve their understanding of the available measurement instruments and, also, the development of new tests, allowing them to explore the human component in the software construction process more accurately.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abuse and misuse of psychological tests in software engineering research"
        },
        {
            "text": "Our overall objective is to address the lack of understanding and use of psychometrics in behavioral software engineering research.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Objective"
        },
        {
            "text": "We also hope to increase software engineering researcher's awareness and respect of theories and tools developed in established fields of the behavioral science, towards stronger methodological foundations of behavioral software engineering research.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Objective"
        },
        {
            "text": "With this paper, we contribute to the (behavioral) software engineering body of knowledge with a set of guidelines which enable a better understanding of psychological constructs in research activities. This improvement in research quality is achieved by either (1) reusing psychometrically validated measurement instruments, as well understanding why and how they are validated, or, if no such questionnaires exist, (2) developing new psychometrically validated questionnaires that are better suited for the software engineering domain.",
            "cite_spans": [
                {
                    "start": 417,
                    "end": 420,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Contribution"
        },
        {
            "text": "Our contribution is enabled by offering one theoretical deliverable and one practical companion deliverable. (1) We offer a review and synthesis of psychometric guidelines in form of textbooks, review papers, as well as empirical studies. (2) We offer a hands-on counterpart to our review by providing a fully reproducible implementation of our guidelines as R Markdown. The Standards for Educational and Psychological Testing (SEPT, American Educational Research Association et al. [3] ) is a set of gold standards in psychological testing jointly developed the American Psychological Association (APA), National Council on Measurement in Education (NCME), and the American Educational Research Association (AERA). The book defines areas and standards that should be met when developing, validating, and administering psychological tests. We adopted SEPT as a framework to guide the paper construction, for ensuring that the standards are met and that the various other references are framed in the correct context.",
            "cite_spans": [
                {
                    "start": 239,
                    "end": 242,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 483,
                    "end": 486,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Contribution"
        },
        {
            "text": "Additionally, we organized the scoping of the paper by comparing related work from the fields of psychology research.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contribution"
        },
        {
            "text": "While the present paper is not a systematic literature review or a mapping study-the discipline is so broad that entire textbooks have been written on it-we systematically framed its construction to ensure that all important topics were covered.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contribution"
        },
        {
            "text": "Several authors, e.g., Crocker and Algina [16] , Rust [82] , Singh et al. [85] , have proposed different phases for the psychometric development and evaluation of measurement instruments. Through our review, we identified 15 phases that we summarize visually in Figure 1 and outline as follows.",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 46,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 54,
                    "end": 58,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 74,
                    "end": 78,
                    "text": "[85]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [
                {
                    "start": 262,
                    "end": 270,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Scope"
        },
        {
            "text": "(1) Identification of the primary purpose for which the test scores will be employed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scope"
        },
        {
            "text": "(2) Identification of constructs, traits, and behaviors that are reflected by the purpose of the instrument. (3) Development of a test specification, delineation of the items proportion that should focus on each type of constructs, traits, and behaviors of the test. We focus mainly on the second half of psychometric activities-those with a dark background in Figure 1 -as they are the most challenging and usually uncovered in software engineering research. The first half of the activities, on questionnaire design, are covered by existing literature in software engineering (e.g., [11, 43, 48, 61, 91] ) and psychology research (e.g., [13, 65, 83, 86] ).",
            "cite_spans": [
                {
                    "start": 585,
                    "end": 589,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 590,
                    "end": 593,
                    "text": "43,",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 594,
                    "end": 597,
                    "text": "48,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 598,
                    "end": 601,
                    "text": "61,",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 602,
                    "end": 605,
                    "text": "91]",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 639,
                    "end": 643,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 644,
                    "end": 647,
                    "text": "65,",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 648,
                    "end": 651,
                    "text": "83,",
                    "ref_id": "BIBREF82"
                },
                {
                    "start": 652,
                    "end": 655,
                    "text": "86]",
                    "ref_id": "BIBREF85"
                }
            ],
            "ref_spans": [
                {
                    "start": 361,
                    "end": 369,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Scope"
        },
        {
            "text": "As a final note, the present paper, as well as any psychometric construction of measurement instruments, is not a checklist. A psychometric evaluation does not include all elements reported in this paper, as many facets of psychometrics are influenced by the research questions, study design, and data at hand. Yet, a proper psychometric evaluation requires a consideration of all elements reported in the present paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Scope"
        },
        {
            "text": "After a brief introduction on key concepts of psychometrics (section 2), that are required to understand the rest of the paper, we focus on the psychometrics of test construction, namely item review and analysis (section 3), factor analysis (section 4), statistical properties (section 5), reliability (section 6), validity (section 7), and fairness in testing and test bias (section 8). The paper ends with our recommendations for further reading (section 9) and a hands-on running example (Section 10) of a psychometric evaluation. We provide R code and generated datasets openly [32] .",
            "cite_spans": [
                {
                    "start": 582,
                    "end": 586,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Structure"
        },
        {
            "text": "The fundamental idea behind psychological testing is that what is being assessed is not a physical object, such as height and weight. We are attempting to assess a construct, that is a hypothetical entity [3, 82] . If we assess the job satisfaction of a software developer, we are not directly measuring the satisfaction of the individual. Instead, we compare the developers' score with other developers' scores or a set of established norms for job satisfaction. When comparing the satisfaction scores between developers, we are limited to seeing how the scores differentiate between satisfied and unsatisfied developers according to the knowledge and ideas we have about satisfied an dissatisfied individuals.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 208,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 209,
                    "end": 212,
                    "text": "82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "CONCEPTS 2.1 Building blocks"
        },
        {
            "text": "There are two common models of psychometrics, namely functionalist and trait [82] . Functionalist psychometrics is often occurring in educational and occupational tests; it deals with how the design of a test is determined by its application and not about the constructs being measured [36, 82] . For functionalist design, a good test is one that is able to distinguish between individuals who perform well and individuals who perform less well on a job or in schools' activities. This is also called local criterion-based validity (explained in section 7). The functionalist paradigm can be applied to most cases where a performance assessment or an evaluation are required.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 81,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 286,
                    "end": 290,
                    "text": "[36,",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 291,
                    "end": 294,
                    "text": "82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "CONCEPTS 2.1 Building blocks"
        },
        {
            "text": "Trait psychometrics attempts to manage common-sense notions such as human intelligence, personality, and affect scientifically [82] . Classic trait approach was based on the notion that intelligence is related to biological individual differences, and trait psychometric tests aimed to measure traits that would represent biological differences among people [82] .",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 131,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 358,
                    "end": 362,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "CONCEPTS 2.1 Building blocks"
        },
        {
            "text": "No matter the difference between the two schools of thoughts, they have several aspects in common, including test construction and validation methods, which differ in how validity is seen, and they are linked by the theory of true scores [39] . The theory of true scores, or latent trait theory, is governed by the formula in 1.",
            "cite_spans": [
                {
                    "start": 238,
                    "end": 242,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                }
            ],
            "ref_spans": [],
            "section": "CONCEPTS 2.1 Building blocks"
        },
        {
            "text": "where X is the observed score, T is the true score, and E is the error. There are three assumptions with the theory of true scores. (1) all errors E are random and normally distributed, (2) true scores X are uncorrelated with the errors, and (3) different measures of the observed score T on the same participants are independent from each other. Besides all issues that come with the three assumptions, the theory has been criticized with the major point being that there is arguably no such thing as a true score, and that all tests measure are abstractions of psychological constructs [56] .",
            "cite_spans": [
                {
                    "start": 588,
                    "end": 592,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "CONCEPTS 2.1 Building blocks"
        },
        {
            "text": "Elaborations and re-interpretation of the theory of true score have been proposed, among which is the statistical true score [9] . The statistical true score defines the true score T as the score we would obtain by averaging an infinite number of measures X from the same individual. With an infinite number of measures the random errors E cancel each other, leaving with the true score T . The statistical formula of the theory of true scores should not be completely new to readers of software engineering, as most quantitative methods that are in use in our field nowadays are based on it. The statistical interpretation of the theory of true score applies both to trait and functional psychometrics. A difference lies in generalization. Functional tests can only be specific to a certain context while trait tests attempt to generalize to an overall construct present in a group of individuals.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 128,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "CONCEPTS 2.1 Building blocks"
        },
        {
            "text": "Items on psychological tests can be knowledge-based or person-based [82] . Knowledge-based tests assess whether an individual performs well regarding the knowledge of certain information, including possessing skills towards certain knowledge-based construct. Debugging skills would be assessed by a knowledge-based test. Person-based tests, on the other hand, assess typical performance, or how they are represented, towards a construct. Examples of constructs related to person-based tests include personality, mood, and attitudes. Pair programming personalities would be assessed by a person-based test. Knowledge-based tests are usually uni-dimensional as they gravitate towards the notion of possessing or not possessing a certain knowledge. We can also easily rank individuals on their scores and state who ranks better. Person-based tests are usually multi-dimensional and do not allow direct ranking of individuals without some assumptions. For example, a developer could score high on extroversion. A high score on extroversion likely implies to score low on introversion but not that a developer scoring less on extroversion is a \"worse\" developer in any way.",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 72,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Test types and types of testing"
        },
        {
            "text": "A second distinction is between criterion-referenced and norm-referenced testing [30] . Criterionreferenced tests are constructed with reference to performance on a criterion that is defined as objectively as possible [4, 30] . Continuing with the example on debugging skills, if we design a test that assesses whether a developer is able to open a debugging tool and use its ten basic functionalities, and a score of 10 over 10 would mean to be able to debug software, we are constructing a criterionreferenced test. If our test was constructed instead on knowledge of basic theory of debugging and software quality, and we could administer it to several samples, we would then obtain scores that are comparable to each other. That is, we would start assessing how a group or population of software developers knows about software debugging. Obtaining group or population means and measures of deviations would allow us to evaluate how a developer performs with respect to normality. We would construct a norm-referencing test, which allows comparison with the whole population of respondents [5, 30] .",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 85,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 218,
                    "end": 221,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 222,
                    "end": 225,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1094,
                    "end": 1097,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1098,
                    "end": 1101,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Test types and types of testing"
        },
        {
            "text": "When developing a new measurement instrument, we are likely to create more items than what is really needed. Item review and item analysis are a series of methods to reduce the number of items of a measurement instrument and keep the best performing ones [82] . Item review and item analysis is a two-steps process, described in Figure 2 . First, it requires a review by experts; then, a pilot study and statistical calculations. During the first step (item review), experts in the domain of knowledge evaluate items one by one and argue for their presence in the test [82] . During the second step (item analysis), the developers of the measurement instrument calculate item facility and item discrimination. We are not describing item review here because it is a straightforward process that involves familiar methods found in systematic literature reviews and qualitative studies. The experts in the domain of knowledge discuss candidate items and argue in favor or against them, as it happens when discussing inclusion and exclusion of publications in systematic literature reviews [47] . Inter-rater reliability measures can be adopted for assessing the degree of agreement among raters. After reaching an agreement on the items to be included, a pilot study is required for an analysis of the items.",
            "cite_spans": [
                {
                    "start": 255,
                    "end": 259,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 569,
                    "end": 573,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [
                {
                    "start": 329,
                    "end": 337,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "ITEM REVIEW AND ITEM ANALYSIS"
        },
        {
            "text": "Item analysis refers to several statistical methods for the selection of items for a psychological test [49] . Two of the two most known techniques can be found in item facility and item discrimination.",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 108,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "3.1.1 Item facility. Item facility for an item, also known as item difficulty, is defined as the ratio of the number of participants who provided a wrong answer over the number of all participants to a test [49, 82] . The value of item facility is 1 if all respondents are right and 0 if all respondents are wrong. In other words, item facility for an item is the probability of obtaining the right answer for the item [82] . Of interest for test construction is the variance of an item, which is defined with the formula in 2:",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 211,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 212,
                    "end": 215,
                    "text": "82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 419,
                    "end": 423,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "where i is the item and i f i is the item facility for i. The highest possible value for var i is 0.25, and this is the case when items are neither very easy or very difficult-the variance is high, after all. When var i has small values, for example 0.047, it means that most respondents tend to reply the same way for that item, making it either extremely easy or extremely difficult.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "The variance of an item is interesting in the context of norm-reference testing, where item facility also applies, as the purpose of the test is to spread out individuals' scores as much as possible on a continuum. A larger spread is due to a larger variance, and we are interested in including items that make a contribution to the variance [49, 82] .",
            "cite_spans": [
                {
                    "start": 342,
                    "end": 346,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 347,
                    "end": 350,
                    "text": "82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "Furthermore, if an item has a high correlation to other items and has a large variance, it derives that the item makes high contribution to the total variance of a test and it will be kept in the pool of items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "Before moving to the next statistics, we would like to offer an explanation on the term facility. Facility suggests that participants to all tests can be either right or wrong on an answer. What about trait measurement, where participants are not exactly right or wrong but are assessed in terms of a psychological construct? We can calculate item facility for these cases as well. The issue relies only in the naming, because item facility was developed for knowledge-based tests first. Some scholars prefer to use the term item endorsement or item location [73] to better reflect how calculations can be done on traits. For trait measurement, it is common to have items with likert items [55] . Item facility for likert items can be calculated with the mean value of the item. If a likert item maps to the values 0 (strongly disagree) to 5 (strongly agree), the extreme values for the item will be 0 and 5 instead of 0 and 1. An item with average value of 4.8 with variance of 0.09 is a candidate for deletion, whereas an item with average value of 2.72 with variance of 3.02 is deemed interesting.",
            "cite_spans": [
                {
                    "start": 559,
                    "end": 563,
                    "text": "[73]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 690,
                    "end": 694,
                    "text": "[55]",
                    "ref_id": "BIBREF54"
                }
            ],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "3.1.2 Item discrimination. Item discrimination reflects items that behave oddly, in the sense that individuals that tend to score very high (or very low) on a test as a whole tend to be wrong (right) on the same item [82] . Such an item would possess a negative (positive) discrimination. Ideal for a test is to have items with zero discrimination [49] . On a statistical point of view, if an item is uncorrelated with the overall test score, then it is almost certainly uncorrelated with the other items and making very little contribution to the overall variance of the test [82, 85] . Therefore, we calculate item discrimination by comparing the correlation coefficient of an item score and the overall test score. If the computed correlation coefficient is 0 or below, we should consider removing the item. The same holds for trait measurement. Instead of assessing how well an item behaves with respect to the test score, we instead assess how an item is in fact measuring the overall trait in question.",
            "cite_spans": [
                {
                    "start": 217,
                    "end": 221,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 348,
                    "end": 352,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 577,
                    "end": 581,
                    "text": "[82,",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 582,
                    "end": 585,
                    "text": "85]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "Item analysis, while valuable and still in use today, is part of the so called classical test theory (CTT), which assumes that an individual's observed score is the same of a true score and an error score [90] . Modern replacements for CTT have been proposed, and the most prominent one is item response theory (IRT) [20] . IRT models build upon a function (called item response function, IRF, or item characteristics curve, ICC) that defines the probability of being right or wrong on an item [2] . IRT is outside the scope of the present paper as CTT is still in place to this day [82] and explaining IRT requires a publication on its own. Item analysis, as presented in this section, assumes that there is a single test score, meaning that a single construct is being measured. Whenever multiple constructs or a construct of multiple factors are being measured, item analysis requires to be accompanied by factor analysis [85] .",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 209,
                    "text": "[90]",
                    "ref_id": "BIBREF89"
                },
                {
                    "start": 317,
                    "end": 321,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 494,
                    "end": 497,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 583,
                    "end": 587,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 925,
                    "end": 929,
                    "text": "[85]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [],
            "section": "Item analysis"
        },
        {
            "text": "Factor analysis is one of the most widely employed psychometric tool [49, 82, 85] and it can be applied to any dataset where the number of participants is higher than the number of item scores under observation. Factor analysis is for understanding which test items \"go together\" to form factors in the data, that is constructs that we are aiming to assess [82] . At the same time, factor analysis allows to reduce the dimensionality of the problem space (i.e., reducing factors and/or associated items) and explaining the variance in the observed variables compared to underlying latent factors [50] . In case we intend to assess a single construct, factor analysis helps in identifying those items that represent the construct we are interested in, so that we can exclude the other items.",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 74,
                    "end": 77,
                    "text": "82,",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 78,
                    "end": 81,
                    "text": "85]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 357,
                    "end": 361,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 596,
                    "end": 600,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "FACTOR ANALYSIS"
        },
        {
            "text": "Factor analysis techniques are based on the notion that those constructs that we observe through our measurement instruments can be reduced to fewer latent variables which are unobservable but share a common variance [96] (see Section 2) . Factor analysis starts with the computed correlation coefficient as its first building block. A way to summarize correlation coefficients is through a correlation matrix, which is a nxn matrix of n items that displays the correlation coefficient between all n items. Given that two items correlate with each other the same, no matter their order, and that one item correlates with itself with a perfect 1.0 correlation coefficient, the tradition is to show only the lower triangle of the matrix with 1.0 as its diagonal. Table 1 provides an example correlation matrix for the items a, b, c, d, and e. A high correlation among certain items, in our case a with b and d, and b with d, indicate these items might belong to the same factor. This approach, however, lacks part of the story. Questions such as \"how do our candidate factor explain the total variance of the measurement instrument\", \"to which candidate factor does an item belong better?\" and \"how are factors related to each other\" are better answered by further analysis.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 237,
                    "text": "Section 2)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 761,
                    "end": 768,
                    "text": "Table 1",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "FACTOR ANALYSIS"
        },
        {
            "text": "There are two main factor analysis techniques, namely Exploratory Factor Analysis (EFA) and Confirmatory Factor Analysis (CFA) [50, 73, 85] . EFA attempts to uncover patterns and clusters of items by exploring predictions, while CFA attempts to confirm hypotheses on existing factors [96].",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 131,
                    "text": "[50,",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 132,
                    "end": 135,
                    "text": "73,",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 136,
                    "end": 139,
                    "text": "85]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [],
            "section": "FACTOR ANALYSIS"
        },
        {
            "text": "Exploratory factor analysis (EFA) is a family of factor analysis techniques aimed to reduce the number of items by retaining the items that are most relevant to certain factors [50] . Strictly speaking, when developing a measurement instrument, after item analysis, it is desirable to observe whether the measurements for the items tend to cluster. These clusters are likely to represent different factors that might or might not pertain to the construct being measured [49, 82] . EFA provides tools to group and select items from a correlation matrix. EFA operates on the equation in 3 for a measure x 1 [85, 96] :",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 470,
                    "end": 474,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 475,
                    "end": 478,
                    "text": "82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 605,
                    "end": 609,
                    "text": "[85,",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 610,
                    "end": 613,
                    "text": "96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "where F s are those factors grouping the items being analyzed, U s are factors that are unique to each measure, w s are loading of each item on respective factors, and e s are random measurement errors. Factor loadings are, in practice, weights that provide us with an idea of how much an item contributes to a factor [96] .",
            "cite_spans": [
                {
                    "start": 318,
                    "end": 322,
                    "text": "[96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "From the equation we derive that the variance of the constructs being measured is explained by three parts: (1) the common factors, also known as communality of a variable [21, 85, 96] (2), the influence of factors that are unique to that measure, and (3) random error, or",
            "cite_spans": [
                {
                    "start": 172,
                    "end": 176,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 177,
                    "end": 180,
                    "text": "85,",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 181,
                    "end": 184,
                    "text": "96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "Estimates for communalities of a an item are often referred to as h 2 . h 2 is the calculated proportion of variance that is free of error variance and is thus shared with other variables in a correlation matrix [21, 85, 96] . Several techniques calculate the communality of a variable by summing the squared loadings of each variable associated with a variable.",
            "cite_spans": [
                {
                    "start": 212,
                    "end": 216,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 217,
                    "end": 220,
                    "text": "85,",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 221,
                    "end": 224,
                    "text": "96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "Estimates for the unique variance, denoted as u 2 , is the proportion of variance that is associated with communalities, that is u 2 = 1 \u2212 h 2 [21, 85, 96] . Determining a value for u 2 for an item allows us to find how much specific variance can be attributed to that variable.",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 147,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 148,
                    "end": 151,
                    "text": "85,",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 152,
                    "end": 155,
                    "text": "96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "Lastly, the random error that is associated with an item is the last component of the total variance. Random error is also often called the unreliability of variance [21, 96] .",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 170,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 171,
                    "end": 174,
                    "text": "96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "Unique factors are never correlated with common factors, but common factors may or may not be correlated with each other [96] .",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 125,
                    "text": "[96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "EFA encompasses three phases [50, 73, 82, 85] , described in Figure 3 . First, we have to select a fitting procedure to estimate the factor loadings and unique variances of the model. Then, we need to define and extract a number of factors. Finally, we need to rotate the factors to be able to properly interpret the produced factor matrices.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[50,",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 34,
                    "end": 37,
                    "text": "73,",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 38,
                    "end": 41,
                    "text": "82,",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 42,
                    "end": 45,
                    "text": "85]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [
                {
                    "start": 61,
                    "end": 69,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "Many statistical programs allow to either perform all these phases separately or to perform more than one at the same time. It is not an easy task to assign a methodology to one of the three categories below. The reader is advised that some textbooks avoid our classification of phases and simply revert to a more practical set of questions, e.g., \"how to calculate factors\" and \"how many factors should we retain\", which we attempt to answer anyway.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Exploratory Factor Analysis"
        },
        {
            "text": "The most common technique for estimating the loadings and variance is called principal component analysis (PCA) [66] . PCA assumes that the communalities for the measures are equal to 1.0. That is, all the variance for a measure is explained only by the factors that are derived by the methodology, and hence there is no error variance. PCA operates on the correlation matrix, mostly on its eigenvalues, to extract factors that correlate with each other. The eigenvalue of a factor represents the variance of the variables accounted for by that factor. The lower the eigenvalue, the least the factor contributes to the variance explanation in the variables [63] . Factor weights are computed to extract the maximum possible variance, with successive factoring continuing until there is no further meaningful variance left. PCA is not a factor analysis method strictu sensu, as factor analysis does assume a presence of error variance rather than being able to explain all variance. Some advocates prefer to state that its output should be referred to as a series of components rather than factors. While less simplistic than other techniques to estimate factor loading, performing a PCA is still encouraged as a first step in EFA before performing the actual factor analysis [82] .",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 116,
                    "text": "[66]",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 657,
                    "end": 661,
                    "text": "[63]",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Factor loading."
        },
        {
            "text": "Among the proper factor analysis techniques that exist, we are interested in a widely recommended technique for estimating loadings and variance named principal axis factoring (PAF) [49, 81, 93] . PAF does not operate under the assumption that the communalities are equal to 1.0, so the diagonal of the correlation matrix (e.g., the one in Table 1 ) is substituted with estimations of communalities, h 2 . PAF estimates the communalities using different techniques (e.g., the squared multiple correlation between a measure and the rest of measures) and a covariance matrix of the items. Factors are estimated one at a time up until there is a large enough of variance accounted for in the correlation matrix. Under PAF, the ordering of the factors determines their importance in terms of fitting, e.g., the first factor accounts for as much variance as possible, followed by the second factor, and so on. Russell [81] provides a detailed description of the underlying statistical operations of PAF, that we will omit for the sake of brevity. Most statistical software provides functions to implement PAF.",
            "cite_spans": [
                {
                    "start": 182,
                    "end": 186,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 187,
                    "end": 190,
                    "text": "81,",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 191,
                    "end": 194,
                    "text": "93]",
                    "ref_id": "BIBREF92"
                },
                {
                    "start": 913,
                    "end": 917,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [
                {
                    "start": 340,
                    "end": 347,
                    "text": "Table 1",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Factor loading."
        },
        {
            "text": "Both PCA and PAF result in values assigned to candidate factors. Therefore, there has to be a strategy to extract meaningful factors. The bad news here is that there is no unique way, let alone a single proper way, to extract factors [75, 81, 85] . More than one strategy should be adopted at the same time including a sense-making analysis [21] . Several factor extraction techniques exist, which are mentioned in the cited references on the present section. We provide here those that are used most widely as well as those that are easier to apply and understand.",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 238,
                    "text": "[75,",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 239,
                    "end": 242,
                    "text": "81,",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 243,
                    "end": 246,
                    "text": "85]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 341,
                    "end": 345,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Factor extraction."
        },
        {
            "text": "Perhaps the simplest strategy to extract factors is Kaiser [45] eigenvalue-greater-than-one (K1) rule. The rule simply states that factors with eigenvalue higher than 1.0 should be retained. Kaiser's rule is quite easy to apply but it is highly controversial [15, 21, 75, 81] . First, the rule was originally designed for PCA and not for PAF or other factor analysis methods, which might make it unsuitable for methodologies that provide estimation for commonalities as diagonals for correlation matrices [15] . Second, the cut-off value for 1.0 might discriminate for factors that are just above or just below 1.0 [15] . Third, computer simulations found that K1 tends to overestimate the number of factors [21] . Yet, K1 is still the default option for some statistical software suites, making it an unfortunate de-facto main method for factor extraction [15] .",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 259,
                    "end": 263,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 264,
                    "end": 267,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 268,
                    "end": 271,
                    "text": "75,",
                    "ref_id": "BIBREF74"
                },
                {
                    "start": 272,
                    "end": 275,
                    "text": "81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 505,
                    "end": 509,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 615,
                    "end": 619,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 708,
                    "end": 712,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 857,
                    "end": 861,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Factor extraction."
        },
        {
            "text": "Cattell [10] scree test, also based on eigenvalues, foresees the plot of the eigenvalues extracted from either the correlation matrix or the reduced correlation matrix (thus making it suitable for both PCA and PAF) against the factor they are associated with, in descending order. One then inspects the curved line for a break in the values (or an elbow) up to when a substantial drop in the eigenvalues cannot be observed anymore. The break is a point at which the shape of the curve becomes horizontal. The strategy is then to keep all factors before the breaking point. The three major criticisms of this approach is that it is subjective [15] , that more than a scree might exist [89] , but also that data does not often offer a discernible scree and a conceptual analysis of the candidate factor is always required [82] .",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 642,
                    "end": 646,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 684,
                    "end": 688,
                    "text": "[89]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 820,
                    "end": 824,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Factor extraction."
        },
        {
            "text": "Revelle and Rocklin [78] proposed the Very Simple Structure (VSS) method for factor extraction that is based on assessing how the original correlation matrix can be reproduced by a simplified pattern matrix, for which only the highest loading of each item is retained (everything else set to zero) [15] . The VSS criterion to assess how well the pattern matrix performs is a number from 0 to 1, making it a goodness-of-fit measure almost of a confirmatory nature rather than an exploratory one [77] . The VSS criterion is gathered for solutions involving a number of factors that goes to 1 to a user-specified value. The strategy ends with selecting the number of factors that provides the highest VSS criterion.",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 24,
                    "text": "[78]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 298,
                    "end": 302,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 494,
                    "end": 498,
                    "text": "[77]",
                    "ref_id": "BIBREF76"
                }
            ],
            "ref_spans": [],
            "section": "Factor extraction."
        },
        {
            "text": "Finally, the method of parallel analysis (PA), introduced by Horn [42] , was found to be very robust for factor extraction [15, 21] . PA starts with the K1 concept that only factors with eigenvalue larger than 1.0 should be kept. Horn [42] has argued that the K1 rule was developed with population statistics and was thus unsuitable when sampling data. Sampling errors would then cause some components from uncorrelated variables to have eigenvalues higher than one in the population [15] . PA takes into account the proportion of variance that results from sampling rather than being able to access to the population. The way it achieves this is a constant comparison of the solution with randomly generated data [77] . PA generates a large number of matrices from random data in parallel with the real data. Factors are retained as long as they are greater than the mean eigenvalue generated from the random matrices.",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 70,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 123,
                    "end": 127,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 128,
                    "end": 131,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 235,
                    "end": 239,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 484,
                    "end": 488,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 714,
                    "end": 718,
                    "text": "[77]",
                    "ref_id": "BIBREF76"
                }
            ],
            "ref_spans": [],
            "section": "Factor extraction."
        },
        {
            "text": "The last step is to rotate the factors in the dimensional space for improving our interpretation of the results [82] .",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 116,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Factor Rotation."
        },
        {
            "text": "An unrotated output, that is the one that often results after factor extraction, maximizes the variance accounted for by the first factor, followed by the second factor, the third factor, and so on. That is, most items would load on the first factors and many of them would load on more than one factor in a substantial way.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor Rotation."
        },
        {
            "text": "Rotating factors builds on the concept that there are number of \"factor solutions\" that are mathematically equivalent to the solution found after factor extraction. By performing a rotation of the factors, we retain our solution but allow an easier interpretation. We rotate the factors to seek a so called simple structure, that is a loadings pattern such that each item loads strongly on one factor only and weakly on other factors. If the reader is interested in mathematical foundations of factor rotation, two deep overviews of factor rotation are offered by Browne [6] , Darton [19] .",
            "cite_spans": [
                {
                    "start": 571,
                    "end": 574,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 584,
                    "end": 588,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Factor Rotation."
        },
        {
            "text": "There are two families of rotations, namely orthogonal and oblique [81] . Orthogonal rotations force the assumption of independency between the factors, whereas oblique rotations allows the factors to correlate with each others. Which methodology to use is influence]d by the statistic software; for example, R psych package [76] provides \"varimax\", \"quartimax\", \"bentlerT\", \"equamax\", \"varimin\", \"geominT\", and\"bifactor\" for orthogonal rotations and \"promax\", \"oblimin\", \"simplimax\", \"bentlerQ, \"geominQ\", \"biquartimin\", and\"cluster\" for oblique rotations. Several rotation methodologies are summarized by Abdi [1] , Browne [6] , Russell [81] .",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 71,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 325,
                    "end": 329,
                    "text": "[76]",
                    "ref_id": "BIBREF75"
                },
                {
                    "start": 612,
                    "end": 615,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 625,
                    "end": 628,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 639,
                    "end": 643,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Factor Rotation."
        },
        {
            "text": "Perhaps the most known and employed [19] orthogonal rotation method is the Varimax rotation [44] . Varimax maximizes the variance (hence the name) of the squared loadings of a factor on all variables. Each factor will tend to have either large or small loadings of any particular variable. While this solution makes it rather easy to identify factors and their loading on items, the independency condition of orthogonal rotation techniques is hard to achieve. The assumption of independency of factors, especially in the context of behavioral research, belittles the value of orthogonal rotation techniques, to the point that \"we see little justification for using orthogonal rotation as a general approach to achieving solutions with simple structure\" [21, p. 283 ].",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 92,
                    "end": 96,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 753,
                    "end": 764,
                    "text": "[21, p. 283",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Factor Rotation."
        },
        {
            "text": "Oblique rotation is preferred for behavioral software engineering studies, because it is sensible to assume that behavioral, cognitive, and affective factors are separated by soft walls of independence (e.g., motivation and job satisfaction) [21, 81, 82] . If any, one would have to first conduct an investigation using oblique rotation and observe if the solution shows little to no correlation between factors and, in that case, switch to orthogonal rotation [21] . The two most employed and recommended oblique rotation techniques are Direct Oblimin (and its slight variation Direct Quartimin) and Promax, both of which perform well [21] .",
            "cite_spans": [
                {
                    "start": 242,
                    "end": 246,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 247,
                    "end": 250,
                    "text": "81,",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 251,
                    "end": 254,
                    "text": "82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 461,
                    "end": 465,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 636,
                    "end": 640,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Factor Rotation."
        },
        {
            "text": "Fabrigar et al. [21] , Russell [81] recommended to use a Promax rotation because it provides the best of both approaches. A Promax rotation first performs an orthogonal rotation (a Varimax rotation) to maximize the variance of the loadings of a factor on all the variables [81] . Then, Promax relaxes the constraint that the factors are independent between each others, turning the rotation to oblique. The advantage of this technique is that it will reveal whether factors really are uncorrelated with each other [81] .",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 20,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 31,
                    "end": 35,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 273,
                    "end": 277,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 514,
                    "end": 518,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Factor Rotation."
        },
        {
            "text": "There have been several recommendations regarding the required sample size, number of measures per factor, number of factors to retain, and interpretation of loadings [81, 85, 96] .",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 171,
                    "text": "[81,",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 172,
                    "end": 175,
                    "text": "85,",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 176,
                    "end": 179,
                    "text": "96]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Further recommendations."
        },
        {
            "text": "The recommended overall sample size as reported by Yong and Pearce [96] is at least 300 participants, with each variable that is subject to factor analysis with at least 5 to 10 observations. This recommendation has, however, low empirical validation. As reported by Russell [81] , a Monte Carlo study by MacCallum et al. [59] analyzed how different sample sizes and communalities of the variables were able to reproduce the population factor loadings. They found that with item communalities higher or equal to 0.60, results were very consistent with sample sizes as low as 60 cases. Communality levels around 0.50 required samples of 100 to 200 cases. In his review, Russell [81] also found that 39% of EFA studies involved samples of 100 or fewer cases.",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 322,
                    "end": 326,
                    "text": "[59]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 677,
                    "end": 681,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Further recommendations."
        },
        {
            "text": "On the number of measures (items) per factor, Yong and Pearce [96] report that for something to be labeled as a factor it should have at least 3 variables, especially in cases when factors receive a rotation treatment, where only a high correlation (coefficient higher than 0.70) with each other and mostly uncorrelation with other items would make them worthy of consideration. Generally speaking, the correlation coefficient for an item to belong to a factor should be 0.30 or higher [88] . Russell [81] identifies that prior work has requested at least three items per factor; however, four or more items per factor was found to be a better holistic to ensure an adequate identification of the factors. In his review he identified 25% studies with three or fewer measures per factor.",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 66,
                    "text": "[96]",
                    "ref_id": null
                },
                {
                    "start": 486,
                    "end": 490,
                    "text": "[88]",
                    "ref_id": "BIBREF87"
                },
                {
                    "start": 501,
                    "end": 505,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Further recommendations."
        },
        {
            "text": "We reported on the number of factors to retain in the Factor Extraction subsection, so we will not repeat ourselves here. There is not a recommended number and one would follow (possibly more than) one extraction method to identify the best number of factors according to the case. Tabachnick et al. [88] add that cases with missing values should be deleted to prevent overestimation of factors. Russell [81] wrote something that is worthy of mentioning for the uninitiated behavioral software engineering researchers, that is, even when constructing a new measurement instrument there is already an expectation of possible factors in the mind of the researcher. The reason is that items are developed following an investigation of prior work and/or empirical data (see section 3). That number is a good starting point to base ourselves when conducting EFA.",
            "cite_spans": [
                {
                    "start": 300,
                    "end": 304,
                    "text": "[88]",
                    "ref_id": "BIBREF87"
                },
                {
                    "start": 404,
                    "end": 408,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Further recommendations."
        },
        {
            "text": "Yong and Pearce [96] spends some further explanations on interpretation of loadings when they are produced by a statistical software. There should be few item crossloadings (i.e., split loadings, when an item loads at 0.32 or higher with two or more factors) so that each factor defines a distinct cluster of interrelated variables. There are exceptions to this that require an analysis of the items. Sometimes it is useful to retain an item that crossloads, with the assumption that it is the latent nature of the variable. Furthermore, Tabachnick et al. [88] report that, with an alpha level of 0.01, a rotated factor loading with a meaningful sample size would need to have a value of at least 0.32 for loadings as this would correspond to approximately 10% of the overlapping variance.",
            "cite_spans": [
                {
                    "start": 556,
                    "end": 560,
                    "text": "[88]",
                    "ref_id": "BIBREF87"
                }
            ],
            "ref_spans": [],
            "section": "Further recommendations."
        },
        {
            "text": "Contrary to exploratory factor analysis, confirmatory factor analysis (CFA) is for confirming a-priori hypotheses and assessing how well an hypothesized factor structure fits the obtained data [81] . A hypothesized factor could be derived from existing literature as well as data from a previous study to explore the factor structure.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 197,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Confirmatory Factor Analysis"
        },
        {
            "text": "Once the data is obtained to compare to the hypothesized factor structure, a goodness-of-fit test should be conducted. CFA requires statistical modeling that is outside the scope of this paper and the estimation of the goodness-of-fit in CFA is a long lasting debate as \"there are perhaps as many fit statistics as there are psychometricians\" ( [74, p. 31]). Russell [81] , Rust [82] , Singh et al. [85] provide several techniques for estimating the goodness-of-fit in CFA, e.g., Chi-squared test, root mean square residual (RMSR), root mean square error of approximation (RMSEA), and the standardized RMSR. Statistical software implement these techniques, including R psych package [77] . A widely employed technique for CFA is to be found in structural equation modeling (SEM), which is a family of models to fit networks of constructs [46] . MacCallum and Austin [58] provided a comprehensive review of SEM techniques in the psychological science including their applications and pitfalls.",
            "cite_spans": [
                {
                    "start": 367,
                    "end": 371,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 379,
                    "end": 383,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 399,
                    "end": 403,
                    "text": "[85]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 683,
                    "end": 687,
                    "text": "[77]",
                    "ref_id": "BIBREF76"
                },
                {
                    "start": 838,
                    "end": 842,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 866,
                    "end": 870,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [],
            "section": "Confirmatory Factor Analysis"
        },
        {
            "text": "Conducting both EFA and CFA is very expensive. When designing and validating a measurement instrument, and when obtaining a large enough sample of participants, it is common to split the sample for conducting EFA on a part of it and CFA on the remaining part [85] . Most authors, however, prefer conducting EFA only [81] and rely on future independent studies towards a better psychometrics evaluation of a tool. This is also why statistical tools, e.g., R psych package [76] provide estimates of fits for EFA as well as convenience tools to adapt the data to CFA packages, e.g., R sem [28] .",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 263,
                    "text": "[85]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 316,
                    "end": 320,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 471,
                    "end": 475,
                    "text": "[76]",
                    "ref_id": "BIBREF75"
                },
                {
                    "start": 586,
                    "end": 590,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Confirmatory Factor Analysis"
        },
        {
            "text": "We refer the reader to a prior work of ours in the behavioral software engineering domain [54] where we conducted a CFA and describe its application.",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 94,
                    "text": "[54]",
                    "ref_id": "BIBREF53"
                }
            ],
            "ref_spans": [],
            "section": "Confirmatory Factor Analysis"
        },
        {
            "text": "Assessing characteristics and performance of individuals poses several challenges when interpreting the resulting scores. One of them is that a raw score is not meaningful without understanding the test standardization characteristics. For example, a score of 38 on a debugging performance test is meaningless without knowing that 38 means to be able to open a debugger only. Furthermore, the interpretation of the results vary wildly when knowing that on average, developers score 400 on the test itself. The former issue is related to criterion-referenced standardization, the latter to norm-referenced standardization [49, 82] .",
            "cite_spans": [
                {
                    "start": 621,
                    "end": 625,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 626,
                    "end": 629,
                    "text": "82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "ITEM STATISTICAL PROPERTIES"
        },
        {
            "text": "Criterion-referenced tests assess what an individual with any score is expected to be able to do or know. Norm-referenced standardization enables to compare an individual's score to the ordered ranking of a population (also see section 2). We concentrate on norm-referenced standardization as criterion-referenced standardization is unique to a test criteria.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "ITEM STATISTICAL PROPERTIES"
        },
        {
            "text": "A first step to norm-reference a test is to order the results of all participants and rank an individual's score. Measures such as median and percentiles are useful for achieving the ranking and compare. When we can treat our data as interval scales and have it approximately following a normal distribution, we can also use the mean and the standard deviation. The standard deviation is useful for telling us how much an individual's score is above or below the mean score. Instead of reporting that an individual's score is, e.g., 13 above the mean score, it is more interesting to know that the score is 1.7 standard deviations above the mean score. Hence, we norm-standardize scores using different approaches. The remaining of this sections is modelled after Rust [82] text.",
            "cite_spans": [
                {
                    "start": 769,
                    "end": 773,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "ITEM STATISTICAL PROPERTIES"
        },
        {
            "text": "Whenever a sample approximates a normal distribution, we know that a score above average is in the upper 50% and by following the three sigma empirical rule [70] , we know that a score greater than one or two standard deviations from the mean is in the top 68% and 95% respectively. For expressing an individual's score in terms of how distant it is from the mean score, we transform the value to its Z score (also called standard score) using the formula in 4:",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 161,
                    "text": "[70]",
                    "ref_id": "BIBREF69"
                }
            ],
            "ref_spans": [],
            "section": "Standardization to Z scores"
        },
        {
            "text": "where x pc is a participant's score,x pc is the mean of all participants' scores, and s pc is the standard deviation for all participants' scores.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to Z scores"
        },
        {
            "text": "The ideal case would be to use the population mean and standard deviation. In software engineering research we lack studies estimating population characteristics (an example of norm studies was provided by [31] ), so we should either aggregate the results of some studies or gather multiple samples.",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 210,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Standardization to Z scores"
        },
        {
            "text": "An important note is that transforming scores into a Z scores does not make the scores normally distributed. This would require a normalization procedure, explained below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to Z scores"
        },
        {
            "text": "Z scores typically range between -3.00 and +3.00. The range is not always suitable for its application. A software developer could, for example, object to a Z score of -0.89.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to T scores"
        },
        {
            "text": "A T score, not to be confused with t-statistics of the Student's t-test, is a standard Z score that is scaled and shifted so that it has a mean of 50 and a standard deviation of 10. T scores typically range between 20 and 80.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to T scores"
        },
        {
            "text": "T scor e (Z pc ) = Z pc * 10 + 50",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to T scores"
        },
        {
            "text": "For transforming a Z score into a T score, we use the formula in 5. The software developer in the previous example would have a score of 41.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to T scores"
        },
        {
            "text": "Stanine and sten scores respond to the need of transforming a score to a scale from 1 to 9 (stanine) or 10 (sten) with a mean of 5 (stanine) or 5.5 (sten) and a standard deviation of 2. These scores purposely lose precision by keeping only decimal values. The conversion to stanine and sten scores follows the rules in Table 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 319,
                    "end": 326,
                    "text": "Table 2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Standardization to stanine and sten scores"
        },
        {
            "text": "The advantage of stanine and sten scores lies in their imprecision. If our non performing developer with a Z score of -0.89 was compared with other two developers having scores of -0.72 and -0.94, how meaningful would be such a tiny difference in scores? Their stanine scores are 3, 4, and 4, respectively. Their sten score would be 4. Stanine and sten scores provide clear cut-off points for easier comparisons.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to stanine and sten scores"
        },
        {
            "text": "There is an important difference between stanine and sten scores, besides their range. A stanine score of 5 represents an average score in a sample. A sten score of 5.5, which would be its average value, does not belong to its possible values. One can only claim that a stanine of 4 is in the low average while a score of 6 is in the high average. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standardization to stanine and sten scores"
        },
        {
            "text": "The standardization techniques that we presented in the previous section carry the assumption that the sample and population approximate the normal distribution. For all other cases, it is possible to normalize the data. Examples include algebraic transformation, e.g., square-rooting or log transformation, as well as graphical transformation. It is beyond the scope of the present paper to explain introductory statistical transformation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Normalization"
        },
        {
            "text": "Reliability can be seen either in terms of precision, that is the consistency of test scores across replications of the testing procedure (reliability/precision), or as a coefficient, that is the correlation between scores on two equivalent forms of the test (reliability/coefficients) [3] ). For evaluating the precision of a measurement instrument, ideal would be to have as many independent replications of the testing procedure as possible on the same very large sample. Scores are expected to generalize over alternative tests forms, contexts, raters, and over time. The reliability/precision of a measurement instrument is then assessed through the range of differences of the obtained scores. The reliability/precision of an instrument should be assessed with as many sub-groups of a population as possible. The reliability/coefficients of a measurement instrument, which we will simply call reliability from this point on, is the most common way to refer to the reliability of a test [3] . There are three categories of reliability coefficients, namely alternate-form (derived by administering alternative forms of test), test-retest (same test on different times), and internal-consistency (relationship among scores derived from individual test items during a single session).",
            "cite_spans": [
                {
                    "start": 286,
                    "end": 289,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 992,
                    "end": 995,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "RELIABILITY"
        },
        {
            "text": "We adhere to Nunnally [64] , Rust [82] classification of reliability and provide an overview or reliability faces in psychometric theory in Figure 4 .",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 26,
                    "text": "[64]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 34,
                    "end": 38,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [
                {
                    "start": 140,
                    "end": 148,
                    "text": "Figure 4",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "RELIABILITY"
        },
        {
            "text": "Several factors, as defined by American Educational Research Association et al. [3] , affect the reliability of a measurement instrument, especially adding or removing items, changing wording or intervals of items, causing variations in the constructs to be measured (e.g., using a measurement instrument for happiness to assess job satisfaction of developers), and ignoring variations in the intended populations for which the test was originally developed.",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 83,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "RELIABILITY"
        },
        {
            "text": "We now introduce the most widely employed techniques for establishing the reliability of a test.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RELIABILITY"
        },
        {
            "text": "Test-retest Parallel forms Split-half Inter-rater ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reliability Precision"
        },
        {
            "text": "Test-retest reliability, also known as test stability, is assessed when administering the measurement instrument twice to the same sample within a short interval of time. The paired set of scores for each participant is then compared with a correlation coefficient such as Pearson product-moment correlation coefficient or Spearman's rank-order correlation. A correlation coefficient of 1.00, while rare, would indicate a perfect test-retest reliability, whereas a correlation coefficient of 0.00 would indicate no test-retest reliability at all. A negative score is no good news either, and it is automatically considered as a value of 0.00.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Test-retest reliability"
        },
        {
            "text": "Test-retest reliability is not suitable for certain tests, such as those assessing knowledge or performance in general. Participants either face a learning or motivation effect from the first test session or simply improve (or worsen) their skills between sessions. Fur such cases, the parallel forms method is more suitable. The technique requires a systematic development of two versions of the same measurement instrument, namely two parallel tests, that are assessing the same construct but using different wording or items. Parallel tests for assessing debugging skills would feature the same sections and amount of items, e.g., arithmetic, logic, and syntax errors. The two tests would need different source code snippets that are, however, very similar. A trivial example would be to test for unwanted assignments inside conditions in different places and with different syntax (e.g., using if (n = foo()) in version one and if (x = y + 2) in version two). As with test-retest reliability, each participant faces both tests and a correlation coefficient can be computed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parallel forms reliability"
        },
        {
            "text": "Split-half reliability is a widely adopted and more convenient alternative to parallel forms reliability. Under this technique, a measurement instrument is split into two half-size versions. The split should be as random as possible, e.g., splitting by taking odd and even numbered items. Participants face both halves of the test and, again, a correlation coefficient can be computed. The obtained coefficient, however, is not a measure of reliability yet. The reliability of the whole measurement instrument is computed with the Spearman-Brown formula in 6.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Split-half reliability"
        },
        {
            "text": "where r hal f is the correlation of the split tests. This formula shows that the more discriminating items a test has, the higher will be its reliability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Split-half reliability"
        },
        {
            "text": "Inter-rater reliability is perhaps the most common reliability that is found in software engineering studies. Qualitative studies or systematic literature reviews and mapping studies often have different raters for evaluating the same items. The sets of rates can be assessed using a correlation coefficient. Cohen's kappa is widely used in the literature for inter-rater coefficient of two entities together with Fleiss' kappa for the inter-rater coefficient of more than two entities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Inter-rater reliability"
        },
        {
            "text": "The standard error of measurement is used for generating confidence intervals around reported scores. The score is strictly related to the reliability coefficient [82] as shown in formula 7",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 167,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Standard error of measurement"
        },
        {
            "text": "where var t est is the variance of the test scores and r t est is the reliability coefficient of the test. The standard error of measurement also provides an idea of how errors are distributed around observed scores. If the assumption that errors are distributed normally is met, one can calculate the 95% confidence interval by using the z curve value of 1.96 to construct the interval x obs \u00b1 E measur ement * 1.96. Confidence intervals could also be used to compare participants' scores. Shall one participant score fall below or above the interval, their results would differ significantly from the normality of scores.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Standard error of measurement"
        },
        {
            "text": "Validity in psychometrics is defined as \"The degree to which evidence and theory support the interpretation of test scores for proposed uses of tests.\" [3] . Psychometric validity is therefore a different (but related) concept than the one of study validity that software engineers are used to deal with [24, 68, 84, 94] . Validation in psychometric research is related to the interpretation of the test scores. For validating a test, we should gather relevant evidence for providing a sound scientific basis for the interpretation of the proposed scores.",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 155,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "[24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 309,
                    "end": 312,
                    "text": "68,",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 313,
                    "end": 316,
                    "text": "84,",
                    "ref_id": "BIBREF83"
                },
                {
                    "start": 317,
                    "end": 320,
                    "text": "94]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "VALIDITY"
        },
        {
            "text": "Rust [82] has summarized six major facets of validity in the context of psychometric tests, which we represent in Figure 5 and describe below.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 9,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [
                {
                    "start": 114,
                    "end": 122,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "VALIDITY"
        },
        {
            "text": "Face validity concerns how the items of a measurement instrument are accepted by respondents. For example, software developers expect the wording of certain items to be targeted to them instead of say, a children. Similarly, if a test presents itself to be about a certain construct, such as debugging expertise, it could cause face validity issues if it contained a personality assessment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Face validity"
        },
        {
            "text": "Content validity (sometimes called criterion validity or domain-referenced validity) concerns the extent to which a measurement instrument reflects the purpose for which the instrument is being developed. If a test was developed under the specifications of job satisfaction, but measured developers' motivation instead, it would present issues of content validity. Content validity is [82] most of the times because the form of deviation matters more than the degree of deviation.",
            "cite_spans": [
                {
                    "start": 385,
                    "end": 389,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Content validity"
        },
        {
            "text": "Predictive validity is a statistical validity defined as the correlation between the score of a measurement instrument and a score of the degree of success of the selected field. For example, the degree of success of debugging performance capability is expected to be higher with a higher programming experience. Computing a score for predictive validity is as simple as calculating a correlation value (such as Person or Spearman). According to the acceptance criterion for predictive validity, a score higher than 0.5 could be considered an acceptable predictive validity for the items. We would then feel justified in including programming experience as an item to represent the construct of debugging performance capability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Predictive validity"
        },
        {
            "text": "Concurrent validity is a statistical validity that is defined as the correlation of a new measurement instrument and existing measurement instruments for the same construct. A measurement instrument tailored to the personality of software developers ought to correlate with existing personality measurement instruments. While concurrent validity is a common measure for test validity in psychology, it is a weak criterion as the old measurement instrument itself could have a low validity. Nevertheless, concurrent validity is important for detecting low validity issues in measurement instruments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concurrent validity"
        },
        {
            "text": "Construct validity is a major validity criterion in psychometric tests. As constructs are not directly measurable, we observe the relationship between the test and the phenomena that the test attempts to represent. For example, a test that identifies highly communicative team members should have a high correlation with. . . observations of highly communicative people who have been labelled as such. The nature of construct validity is that it is cumulative over the number of available studies [82] .",
            "cite_spans": [
                {
                    "start": 497,
                    "end": 501,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Construct validity"
        },
        {
            "text": "Differential validity assesses how a measurement instrument should not correlate with measures from which it should differ, and how a measurement instrument should not correlate with measures from which it should not differ. In particular, Campbell and Fiske [8] have differentiated between two aspects of differential validity, namely convergent and discriminant validity. Rust [82] mentions a straightforward example of both. A test of mathematics reasoning should correlate positively with a test of numerical reasoning (convergent validity). However, the mathematics test should not strongly correlate positively with a test of reading comprehension, because the two construct are supposed to be different (discriminant validity). In case of a low discriminant validity, there should be an investigation of whether the correlation is a result of a wider underlying trait, say, continues Rust, general intelligence. Differential validity is overall empirically demonstrated by a discrepancy between convergent validity and discriminant validity.",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 262,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 379,
                    "end": 383,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Differential validity"
        },
        {
            "text": "Fairness is \"the quality of treating people equally or in a way that is right or reasonable\" (Cambridge [7] , online.). A test is fair when it reflects the same constructs for all participants, and its scores have the same meaning for all individuals of the population [3] . A fair test does neither advantage or disadvantage any participant through characteristics that are not related to the constructs under observation. From a participant point of view, an unfair test brings a wrong decision based on the test results. An example of test that requires fairness is an attitude or skills assessment when interviewing candidates for hire in an information technology company.",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 107,
                    "text": "[7]",
                    "ref_id": null
                },
                {
                    "start": 269,
                    "end": 272,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "FAIRNESS IN TESTING AND TEST BIAS"
        },
        {
            "text": "American Educational Research Association et al. [3] reports on several facets of fairness. Individuals should have the opportunity to maximize how they perform with respect to the constructs being assessed. Similarly, for a measurement instrument that assesses traits of participants, the test should maximize how it assesses that the constructs being measured are present among individuals. This fairness comes from how the test is administered, which should be as standardized as possible. Research articles should describe the environment for the experimental settings, how the participants were instructed, which time limits were given, and so on. Fairness also comes, on other hand, from participants themselves. Participants should be able to access the constructs as being measured without being advantaged or disadvantaged by individual characteristics. This is an issue of accessibility to a test and is also part of limiting item, test, and measurement bias.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 52,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "FAIRNESS IN TESTING AND TEST BIAS"
        },
        {
            "text": "We provide an overview or bias in psychometric theory in Figure 6 . Rust [82] provides an overview of item, test, and measurement bias. It almost feels unnecessary to state that a measurement instrument should be free from bias from age, sex, gender, and race. These cases are indeed covered by legislation to ensure fairness. In general, there are three forms of bias in tests, namely item bias, intrinsic test bias, and extrinsic test bias [82] .",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 77,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 442,
                    "end": 446,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [
                {
                    "start": 57,
                    "end": 65,
                    "text": "Figure 6",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "FAIRNESS IN TESTING AND TEST BIAS"
        },
        {
            "text": "Item bias, also known as differential item functioning, refers to bias born out of individual items of the measurement instrument. A straightforward example would be to test a European developer about coding snippets dealing with imperial system units. A more common item bias is about the wording of items. Even among native speakers, the use of idioms such as double negatives can cause confusion. Asking a developer to mark a coding snippet that is free from logic and syntax errors is clearer than asking to mark code that does not possess neither logic nor syntax errors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item bias"
        },
        {
            "text": "A systematic identification of item bias that goes beyond carefully checking an instrument is to carry out an item analysis with all possible groups of potential participants, for example men and women, or speakers of English of different levels. A comparison of the facility values (the proportion of correct answers) of each item can reveal potential item bias. For instruments that assess traits and characteristics of a group instead of function or skills, a strategy is to follow a checklist of questions that researchers and pilot participants can answer [38] . Differential item functioning (DIF) is a statistical characteristic of an item that shows potential unfairness of the item among different groups that should provide same test results otherwise [67] . A presence of DIF does not necessarily indicate bias but unexpected behavior on an item [3] . This is why, after the detection of DIF, it is important to review the root causes of the differences. Whenever DIF happens for many items of a test, a test construct or final score is potentially unfair among different groups that should provide same test results otherwise. This situation is called differential test functioning (DTF) [79] . There are three main techniques for identifying DIF, namely Mantel-Haenszel approach, item response theory (IRT) methods, and logistic regression [97].",
            "cite_spans": [
                {
                    "start": 561,
                    "end": 565,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 762,
                    "end": 766,
                    "text": "[67]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 857,
                    "end": 860,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1200,
                    "end": 1204,
                    "text": "[79]",
                    "ref_id": "BIBREF78"
                }
            ],
            "ref_spans": [],
            "section": "Item bias"
        },
        {
            "text": "Intrinsic test bias occurs when there are differences in the mean scores of two groups that are due to the characteristics of the test itself rather than difference between the groups in the constructs being measured. Measurement invariance is the desired property upon lack of which intrinsic test bias occurs. If a test for assessing the knowledge of software quality is developed in English and then administered to individuals who are not fluent in English, the measure for the construct of software quality knowledge would be contaminated by a measure of English proficiency. Differential content validity (see section 7.2) is the most severe form of intrinsic test bias as it causes lower test scores in different groups. If a measurement instrument for debugging skills has been designed by keeping in mind American software testers, any participant that is not an American software tester will likely perform worse on the test to different degrees. Rust [82] reports various statistical model proposals over the last 50 years to detect intrinsic test bias which, however, present various issues including the introduction of more unfairness near cut-off points or for certain groups of individuals. There is not a recommended way to detect intrinsic test bias other than perform item bias analysis paired with sensitivity analysis.",
            "cite_spans": [
                {
                    "start": 962,
                    "end": 966,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Intrinsic test bias"
        },
        {
            "text": "Extrinsic test bias occurs whenever unfair decision happen based on a non-biased test. These issues usually belong to tests about demographics dealing with social, economical, and political issues, so they are unlikely to belong to measurement instruments developed for the software engineering domain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extrinsic test bias"
        },
        {
            "text": "The present paper scratches the surface of psychometric theory and practice, and it is its aim to be broad rather than deep. We collect, in this section, what we consider to be next steps for a better understanding and expansion of the concepts that we have presented.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "FURTHER READING"
        },
        {
            "text": "The books written by Kline [49] , Nunnally [64] , Rust [82] provide an overall overview of psychometric theory, cover all topics mentioned in the present paper, and more. We invite in particular to compare how they present measurement theory and their views and classifications of validity and reliability. A natural followup is The Standards for Educational and Psychological Testing (SEPT, [3] ), which proposes standards that should be met in psychological testing.",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 43,
                    "end": 47,
                    "text": "[64]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 55,
                    "end": 59,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 392,
                    "end": 395,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "FURTHER READING"
        },
        {
            "text": "While our summary breaks down fundamental concepts and presents them for the unitiated researcher of behavioral software engineering, our writing can not honor enough the guidelines and recommendations for factory analysis offered by Fabrigar et al. [21] , Russell [81] , Singh et al. [85] , Yong and Pearce [96] . To those we add the work of Zumbo [97], who have explored, through data simulations, the conditions that yield reliable exploratory factor analysis with samples below 50, which is unfortunately a condition we often live with in software engineering research. Furthermore, we wish to point the reader to alternatives to factor analysis, especially for confirmatory factor analysis (CFA). Flora and Curran [27] analyzed benefits when using Robust Weighted Least Squares (Robust WLS) regression. With a Monte Carlo simulation, they have shown that robust WLS provided accurate test statistics, parameter estimates and standard errors even when the the assumption of CFA were not met. Bayesian alternatives for CFA have been proposed in the early 80s already [51] and later expanded to cover the exploratory phase as well, see, for example, the works by Conti et al. [14] , Lu et al. [57] , Muth\u00e9n and Asparouhov [62] .",
            "cite_spans": [
                {
                    "start": 250,
                    "end": 254,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 265,
                    "end": 269,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 285,
                    "end": 289,
                    "text": "[85]",
                    "ref_id": "BIBREF84"
                },
                {
                    "start": 308,
                    "end": 312,
                    "text": "[96]",
                    "ref_id": null
                },
                {
                    "start": 719,
                    "end": 723,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1195,
                    "end": 1199,
                    "text": "[57]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "text": "[62]",
                    "ref_id": "BIBREF61"
                }
            ],
            "ref_spans": [],
            "section": "FURTHER READING"
        },
        {
            "text": "Within the software engineering domain, Gren [37] has offered an alternative lens on validity and reliability of software engineering studies, also based on psychology, that we advise to read. Ralph and Tempero [72] has offered a deep overview of construct validity in software engineering through a psychological lense.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 211,
                    "end": 215,
                    "text": "[72]",
                    "ref_id": "BIBREF71"
                }
            ],
            "ref_spans": [],
            "section": "FURTHER READING"
        },
        {
            "text": "We believe that a methodology description is best complemented by a concrete example of its application. In Appendix A, we provide a complete scenario of a psychometric evaluation with the R programming language and the psych package. The evaluation follows the same structure of the present paper for ease of understanding. In the spirit of open science in software engineering [26] , we provide the running example as a replication package as well [32] . We wrote the example using R Markdown, making it fully repeatable, as well as the generated dataset, and instructions for replication with newly generated data.",
            "cite_spans": [
                {
                    "start": 379,
                    "end": 383,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 450,
                    "end": 454,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "RUNNING EXAMPLE OF PSYCHOMETRIC EVALUATION"
        },
        {
            "text": "The adoption and development of valid and reliable measurement instruments in software engineering research, whenever humans are to be evaluated, should benefit from psychology and statistics theory. This paper provides a gentle introduction to psychometric evaluation of tests, which will help the development of tests as well as a careful selection and preservation of existing tests.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CONCLUSION"
        },
        {
            "text": "After providing basic building concepts of psychometric theory, we introduced item analysis, factor analysis, standardization and normalization, reliability, validity, and fairness in testing and test bias. We also provided an implementation of a psychometric evaluation with a running example and provided all data and source code openly. We followed textbooks, method papers, and society standards for ensuring a coverage of all important steps, but we could only offer a gentle introduction and invite the reader to explore our referenced material further. Each of these steps is a universe of its own, with dozens of published artifacts related to them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CONCLUSION"
        },
        {
            "text": "Adding the steps described in this paper will increase the time required for developing measurement instruments. However, the return on investment will be incommensurable. Psychometric studies of measurement instruments will improve reliability and validity of the adopted instruments. The software engineering community must value psychometric studies more. This, however, requires a cultural change that we hope to favor with this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CONCLUSION"
        },
        {
            "text": "\"Spending an entire Ph.D. candidacy on the validation of one single measurement of a construct should be, not only approved, but encouraged. \" [37] and, we believe, should also become normal.",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 147,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "CONCLUSION"
        },
        {
            "text": "We acknowledge the support of Swedish Armed Forces, Swedish Defense Materiel Ad-ministration and Swedish Governmental Agency for Innovation Systems (VINNOVA) in the project number 2013-01199. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "ACKNOWLEDGMENTS"
        },
        {
            "text": "The following appendix (explained in section 10) is available as an always updated replication package [32] . ",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 107,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "A APPENDIX"
        },
        {
            "text": "The present document provides an executable hands-on introductory psychometric validation example written for a behavioral software engineering audience.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The present document is part of a paper: Daniel Graziotin, Per Lenberg, Robert Feldt, Stefan Wagner (2020): Behavioral Sofware Engineering: Methodological Introduction to Psychometrics. Under Review Even though it is as self-contained as possible, we recommend reading it after reading the paper. The present document is also a R Markdown file. Its text version is interactive and can be executed directly in R Studio, making it completely reproducible.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The present document contains an example, with simulated data, of the psychometric validation phases of a norm-based measurement instrument for assessing an overall construct with five sub-constructs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context"
        },
        {
            "text": "Our fictitious construct is the \"individual perception styles of source code\" that, through a literature review (or, perhaps, after a grounded theory study), we believe is mainly composed of, or highly related, to the following five constructs: code curiosity, programming paradigm flexibility, learning disposition, collaboration propensity, and comfort in novelty.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context"
        },
        {
            "text": "We develop a measurement instrument with 31 items, all represented by likert items from 1 to 5, which is self-assessed by the individual software engineer. The likert items would ideally form five likert scales, that would actually emerge after an exploratory factors analysis. However, when we develop scales we have a rough idea of the related constructs anyway. We represent here what we expect the exploratory factor analysis to show, with all items grouped by what we might see as potential factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context"
        },
        {
            "text": "\u2022 F1, \"Code curiosity\" 1 -7 items, F1_1 to F1_7 \u2022 F2 \"Programming paradigm flexibility\" -4 items, F2_1 to F1_4 \u2022 F3 \"Learning disposition\" -9 items, F3_1 to F3_9 \u2022 F4 \"Collaboration propensity\" -4 items, F4_1 to F4_4 \u2022 F5 \"Comfort in novelty\" -5 items, F5_1 to F4_7",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context"
        },
        {
            "text": "What the items actually are is not interesting for the purposes of the present document. Our team proceeds to psychometrically evaluate the measurement instrument, in particular, to reduce the number of items. Then, to validate the items belonging to the factors and, possibly, reducing the number of items agin. Finally, to offer a reasoning on statistical properties, reliability, and validity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context"
        },
        {
            "text": "The following are requirements that are typically found in a basic psychometric validation, with the exclusion of fabricatr, which is for fabricating data and we use for developing this example only. We offer a dataset with pre-populated data that allows repetition of the entire document. For discarding our provided data and simulating new data (which should conform, to a fair extent, to what we expect in the various sections), set the safeguard variable to FALSE or delete the graziotin_et_al-bse_psychometrics_example.csv file. The new dataset will behave very similarly to the provided one-after all, we used the very same code to generate it. This option will allow reproducibility only.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Requirements and options"
        },
        {
            "text": "For full repeatability, but mostly for better clarity, we recommend to start with our provided dataset. The reason is that the rest of this tutorial will provide reasoning around certain values for items that will slightly change in case a new dataset is generated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Requirements and options"
        },
        {
            "text": "The following are options that are useful for repetition and replication of our example. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Requirements and options"
        },
        {
            "text": "This section explains how we constructed the simulated dataset graziotin_et_al-bse_psychometrics_example.csv.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data simulation"
        },
        {
            "text": "This section can be safely skipped as it does not pertain to psychometric evaluation. We provide the data simulation part for full reproducibility. Up to this point we have generated a perfect dataset. All items cluster as we wish them to be. We now add noise to the data. For two items, F1_4 and F5_25, we simulate oddly worded or uninteresting items that most users rate at the maximum and minimum values, respectively. This will allow us later on to drop these items during item facility analysis. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data simulation"
        },
        {
            "text": "For brevity, as item review is mostly a review and reasoning of items, we assume that the 31 items for five factors are already the result of item review. So we proceed to perform a pilot test for item analysis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item Review and Item Analysis"
        },
        {
            "text": "We simulate a pilot study with the first pilot.test.sample.size lines of the dataframe. This is pilot.df. We call psych function describe() and start our item analysis. The function provides useful descriptive statistics for each item. Our first observation is that some items have mean and median that deviate notably from the (in our case) central value of 3. These are F1_4, F4_22, F4_23, with a mean value approaching 5, and F5_25, with a mean value of 4.04, which is high but not alarming yet. Items F1_4 and F4_23 are also those with the smallest standard deviation of 0.21, which results in a variance of 0.04. The other two items show standard deviations in line with the rest of the dataset. Furthermore, F1_4 and F4_23 have unusual values for skew and kurtosis as well, indicating that the data is strongly biased on a value (indeed, their range is from 4 to 5). We conclude that items F1_4 and F4_23 have a high item facility (item difficulty), so they are candidate for deletion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item Review and Item Analysis"
        },
        {
            "text": "As for item discrimination, being our example one for trait measurement, we calculate how the two items correlate with those that are supposedly part of their factors. Item F1_4 behaves differently to other items of its cluster as it shows near zero correlation to all items but a weak one to F1_7.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item Review and Item Analysis"
        },
        {
            "text": "Item F4_23 does not show particular differences in its cluster (we manipulated Factor 4 responses to result in a likely void factor).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item Review and Item Analysis"
        },
        {
            "text": "We conclude that only item F1_4 possesses a high discrimination and we therefore are confident in removing it from the measurement instrument. F4_23 would show high discrimination as well, but not compared to the rest of its theorized cluster, yet it has high item facility. We decide to drop it as well. We decide to not drop F4_22 yet, but the entire cluster for Factor 4 looks suspicious already.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item Review and Item Analysis"
        },
        {
            "text": "pilot.afteritemanalysis.df <pilot.df[,-which(names(pilot.df) %in% c(\"F1_4\", \"F4_23\"))]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Item Review and Item Analysis"
        },
        {
            "text": "Our psychometric evaluation study foresees an exploratory factor analysis (EFA) with an idea of items belonging to some possible constructs. While EFA does not require known factors at all (it is, after all, an exploration), we can still use it to cut down on items rather than on discovering how many factors we require. EFA will show us suggestions for factors, anyway.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor Analysis"
        },
        {
            "text": "We gather data with a new round of participants, this time aiming at a larger sample.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor Analysis"
        },
        {
            "text": "# we drop the pilot participants study.df <tail(full.df, n = NROW(full.df)pilot.test.sample.size) # we drop those columns that we identified after item analysis study.beforeefa.df <study.df[,-which(names(study.df) %in% c(\"F1_4\", \"F4_23\"))]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor Analysis"
        },
        {
            "text": "Following the structure of our paper, we show here two possibilities for factor extraction, namely the scree plot and the very simple structure, and then we provide an entire session of factor loading estimation, factor extraction, and factor rotation with principal axis factoring.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor Analysis"
        },
        {
            "text": "The scree test is based on eigenvalues. The psych package is nice enough to perform a principal component analysis (PCA) as well as of a principal axis factoring (PAF) for generating the plot.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor extraction: scree plot"
        },
        {
            "text": "We plot the scree test using our sample with 29 variables. Besides the warning that the the estimated weights are probably incorrect, which would prompt us to use a different factor extraction method anyway, we can observe that the VSS criterion does achieve its maximum with 7 factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor extraction: scree plot"
        },
        {
            "text": "However, the increase in vss1 and vss2 score decreases rapidly after four factors. This can be noted graphically as well as in the console output. The fit behaves the same. The VSS still suggests that having four factors is optimal.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor extraction: scree plot"
        },
        {
            "text": "Finally, a robust method for factor extraction is Parallel Analysis (PA), which performs a constant comparison of our solution with randomly generated data. As with the Scree Plot function, we see that psych fa.parallel function performs both a PCA and a PAF with the data. The textual output suggests already that four factors should be extracted. The graph shows our data and with calculated components (crosses) and factors (triangles). They are placed the same as with a Scree Plot. The red lines (upper one for PCA, lower one for PAF), on the other hand, represent the mean generated eingenvalues. Factors are retained as long as they are greater than the mean eigenvalue generated from the random matrices.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor extraction: Parallel Analysis"
        },
        {
            "text": "For both PCA and PAF, the generated data crosses the lines of the actual eigenvalue between 4 and 5 components and factors. This suggests that 4 factors should be extracted.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor extraction: Parallel Analysis"
        },
        {
            "text": "The R psych package makes it easy to perform loading estimation, factor extraction, and factor rotation with a single function call. We will perform here a principal axis factoring (PAF) to fully go from five factors (the initial assumption) to four factors. We show in this section how a reasoning can be conducted by observing values as returned by functions for loading, extration, and rotation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "Besides our reasoning on pure numbers, we will add elements of confirmatory factory analysis (CFA) to guide our retention of items and factors, that is, measures of fit. First, the change in chi square from factor n to factor n -1 should go from significant (there is is a bad fit) to insignificant. To the chi square test, we add other two measure of fit, namely the root mean square of the residuals (RMSR) and the root mean square error of approximation (RMSEA) index. RMSR should be as close to zero as possible while RMSEA should be below 0.05.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "It should be kept in mind that measures of fit are mere indicators in EFA, being them part of a confirmatory factor analysis (CFA) strategy mostly, and should be taken into consideration together with a sensitivity analysis.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "As suggested in our paper, we opt for Principal Axis Factoring (PAF) with a hybrid orthogonal first, oblique rotation then, namely a promax rotation. This means that we have a further criterion for factor reduction, that is reducing factors while the sum of squared loadings is below 1.00. Let us now break down the output of this function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "The function represents the candidate factors with labels PA1 to PA5. The factor numbering does not necessarily correspond to the factors we developed, that is, PA1 is not automatically assigned to items F1_x. Factors with lower numbering are those that provide the highest sum of squared loadings (SS loadings in R output) and also the variance explained.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "Let us first inspect the computed loadings (which could also be singled out with the loadings(five.factors) function.), in particular, the summary table that provides the sum of squared loadings (SS loadings) and the variance explained for each factor. A first observation is that PA5 provides a sum of squared loadings below 1, which is an indication for removal. The factor accounts also for only 0.04 variance, and the cumulative variance moves from 0.46 to 0.48 from four factors to five factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "Goodness of fit with Likelihood Chi Square is already non-significant but approaching significance levels (p = 0.052). RMSR and RMSEA are within desirable levels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "If we then move to inspect the loadings of all our single items, we see a table of our F items crossed with the candidate factors PA. Furthermore, the table provides, for each item, the communalities (h2), the unique variance (u2), and the complexity of the component loadings (com).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "We observe that the item possessing the highest load to PA5 is F4_22 with 0.536. All other items have loadings near 0. Furthermore, most of our items for a factor F have a corresponding factor PA (e.g., F1 items load mostly on factor PA2) while PA5 seems to meet all Fx items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "Another observation is on items for F4. As stated above, F4_22 is the only one with a meaningful loading on a factor, that is PA5, while F4_21 does not seem to have a meaningful loading on anything. The only remaining item, F4_24, provides a loading on PA2, which seems associated with F1items mostly. Given our previous suspicious on F4items, after an internal review, we decide to drop the F4 items entirely and test for a four factors model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor loading, extraction, and rotation: principal axis factoring"
        },
        {
            "text": "We decide to drop the F4 items entirely and test for a four factors model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Factor reduction"
        },
        {
            "text": "# we drop those columns that we identified after item analysis study.efa.df <study.beforeefa.df[,-which(names(study.beforeefa.df) %in% c(\"F4_21\", \"F4_22\", \"F4_24\"))] four.factors <fa(study.efa.df , fm = \"pa\", nfactors = 4, rotate = \"promax\") four.factors ## Factor Analysis using method = pa ## Call: fa(r = study.efa.df, nfactors = 4, rotate = \"promax\", fm = \"pa\") ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 PA2 PA3 PA4 h2 u2 com ## F1_1 -0.03 0.73 -0.11 0.00 0.568 0. 43 (rowSums(f1.df) ), F2 = scale(rowSums(f2.df)), F3 = scale(rowSums(f3.df)), F5 = scale(rowSums(f5.df)) )",
            "cite_spans": [
                {
                    "start": 506,
                    "end": 508,
                    "text": "43",
                    "ref_id": "BIBREF42"
                }
            ],
            "ref_spans": [
                {
                    "start": 509,
                    "end": 524,
                    "text": "(rowSums(f1.df)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Factor reduction"
        },
        {
            "text": "Being our measurement instrument one for norm-referencing testing and not one for assessing skills, we perform a test-retest reliability assessment. Only a smaller number of participants (reliability.sample.size) accepted to take part to a second test.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reliability"
        },
        {
            "text": "reliablity.df.first <tail(study.after.efa.df, reliability.sample.size) reliablity.df.second <tail(study.after.efa.df, reliability.sample.size) f1.first <-rowSums(reliablity.df.first[, grepl(\"F1_\" , names(reliablity.df.first))]) f2.first <-rowSums(reliablity.df.first[, grepl(\"F2_\" , names(reliablity.df.first))]) f3.first <-rowSums(reliablity.df.first[, grepl(\"F3_\" , names(reliablity.df.first))]) f5.first <-rowSums(reliablity.df.first[, grepl(\"F5_\" , names(reliablity.df.first))])",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reliability"
        },
        {
            "text": "f1.second <-round(jitter(unname(f1.first), amount = 3)) f2.second <-round(jitter(unname(f2.first), amount = 3)) f3.second <-round(jitter(unname(f3.first), amount = 3)) f5.second <-round(jitter(unname(f5.first), amount = 3))",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reliability"
        },
        {
            "text": "Test-retest reliability for F1, F2, F3, and F5 are, respectively: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reliability"
        },
        {
            "text": "Our fictitious measurement instrument is novel and has no correspondence with existing measurement instruments. Therefore, any discussion on its validity depends on the instrument itself and the reasoning we can apply to it. Therefore, the present example can not elaborate on validity measurements.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Validity"
        },
        {
            "text": "The present document presented an introduction to psychometric evaluation with R for a behavioral software engineering audience. Through phases of item review and item analysis, exploratory factor analysis, item statistical properties, and reliability and validity evaluation, we show how we started with 31 items and 5 factors to represent our fictitious \"individual perception styles of source code\" to 26 items and four factors.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We could, therefore, offer the resulting measurement instrument in a paper describing its psychometric evaluation, for other researchers to use for further evaluation, and companies to use.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Factor Rotations in Factor Analyses",
            "authors": [
                {
                    "first": "Herv\u00e9",
                    "middle": [],
                    "last": "Abdi",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Encyclopedia of Social Sciences Research",
            "volume": "",
            "issn": "",
            "pages": "792--795",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Likert or Rasch? Nothing is more applicable than good theory",
            "authors": [
                {
                    "first": "Arnold",
                    "middle": [],
                    "last": "Alphen",
                    "suffix": ""
                },
                {
                    "first": "Ruud",
                    "middle": [],
                    "last": "Halfens",
                    "suffix": ""
                },
                {
                    "first": "Arie",
                    "middle": [],
                    "last": "Hasman",
                    "suffix": ""
                },
                {
                    "first": "Tjaart",
                    "middle": [],
                    "last": "Imbos",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Journal of Advanced Nursing",
            "volume": "20",
            "issn": "",
            "pages": "196--201",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "2014. Standards for educational and psychological testing",
            "authors": [],
            "year": null,
            "venue": "National Council on Measurement in Education, and Joint Committee on Standards for Educational and Psychological Testing",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Criterion-Referenced Testing",
            "authors": [
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Berger",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Encyclopedia of Autism Spectrum Disorders",
            "volume": "",
            "issn": "",
            "pages": "823--823",
            "other_ids": {
                "DOI": [
                    "10.1007/978-1-4419-1698-3_146"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Norm-Referenced Testing",
            "authors": [
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Berger",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Encyclopedia of Autism Spectrum Disorders",
            "volume": "",
            "issn": "",
            "pages": "2063--2064",
            "other_ids": {
                "DOI": [
                    "10.1007/978-1-4419-1698-3_451"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "An Overview of Analytic Rotation in Exploratory Factor Analysis",
            "authors": [
                {
                    "first": "Michael",
                    "middle": [
                        "W"
                    ],
                    "last": "Browne",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Multivariate Behavioral Research",
            "volume": "36",
            "issn": "",
            "pages": "111--150",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Convergent and discriminant validation by the multitrait-multimethod matrix",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Donald",
                    "suffix": ""
                },
                {
                    "first": "Donald",
                    "middle": [
                        "W"
                    ],
                    "last": "Campbell",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Fiske",
                    "suffix": ""
                }
            ],
            "year": 1959,
            "venue": "Psychological Bulletin",
            "volume": "56",
            "issn": "",
            "pages": "81--105",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Logical foundations of probability",
            "authors": [
                {
                    "first": "Rudolf",
                    "middle": [],
                    "last": "Carnap",
                    "suffix": ""
                }
            ],
            "year": 1962,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "The Scree Test For The Number Of Factors",
            "authors": [
                {
                    "first": "Raymond",
                    "middle": [
                        "B"
                    ],
                    "last": "Cattell",
                    "suffix": ""
                }
            ],
            "year": 1966,
            "venue": "Multivariate Behavioral Research",
            "volume": "1",
            "issn": "",
            "pages": "245--276",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Practical Experiences in the Design and Conduct of Surveys in Empirical Software Engineering",
            "authors": [
                {
                    "first": "Marcus",
                    "middle": [],
                    "last": "Ciolkowski",
                    "suffix": ""
                },
                {
                    "first": "Oliver",
                    "middle": [],
                    "last": "Laitenberger",
                    "suffix": ""
                },
                {
                    "first": "Sira",
                    "middle": [],
                    "last": "Vegas",
                    "suffix": ""
                },
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Biffl",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Empirical methods and studies in software engineering, Gerhard Goos, Juris Hartmanis",
            "volume": "",
            "issn": "",
            "pages": "104--128",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Psychological Testing and Assessment: An Introduction to Tests and Measurement",
            "authors": [
                {
                    "first": "Ronald",
                    "middle": [
                        "Jay"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [
                        "E"
                    ],
                    "last": "Swerdlik",
                    "suffix": ""
                },
                {
                    "first": "Suzanne",
                    "middle": [
                        "M"
                    ],
                    "last": "Phillips",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Pretesting survey instruments: an overview of cognitive methods",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Collins",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Qual Life Res",
            "volume": "12",
            "issn": "",
            "pages": "229--238",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Bayesian Exploratory Factor Analysis",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Conti",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Fr\u00fchwirth-Schnatter",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Heckman",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Piatek",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "J Econom",
            "volume": "183",
            "issn": "",
            "pages": "31--57",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Determining the Number of Factors to Retain in EFA: Using the SPSS R-Menu v2.0 to Make More Judicious Estimations",
            "authors": [
                {
                    "first": "Matthew Gordon Rau",
                    "middle": [],
                    "last": "Courtney",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Practical Assessment, Research & Evaluation",
            "volume": "18",
            "issn": "",
            "pages": "1--14",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Introduction to Classical and Modern Test Theory",
            "authors": [
                {
                    "first": "Linda",
                    "middle": [],
                    "last": "Crocker",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Algina",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Forty years of research on personality in software engineering: A mapping study",
            "authors": [
                {
                    "first": "Shirley",
                    "middle": [],
                    "last": "Cruz",
                    "suffix": ""
                },
                {
                    "first": "Fabio",
                    "middle": [
                        "Q B"
                    ],
                    "last": "Da Silva",
                    "suffix": ""
                },
                {
                    "first": "Luiz Fernando",
                    "middle": [],
                    "last": "Capretz",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Computers in Human Behavior",
            "volume": "46",
            "issn": "",
            "pages": "94--113",
            "other_ids": {
                "DOI": [
                    "10.1016/j.chb.2014.12.008"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Exploring individual characteristics and programming performance: Implications for programmer selection",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "Meng",
                    "middle": [],
                    "last": "Darcy",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "314--314",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Rotation in Factor Analysis",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Darton",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "The Statistician",
            "volume": "29",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Item response theory",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Susan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Embretson",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Steven P Reise",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Evaluating the use of exploratory factor analysis in psychological research",
            "authors": [
                {
                    "first": "Duane",
                    "middle": [
                        "T"
                    ],
                    "last": "Leandre R Fabrigar",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wegener",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Robert",
                    "suffix": ""
                },
                {
                    "first": "Erin",
                    "middle": [
                        "J"
                    ],
                    "last": "Maccallum",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Strahan",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Psychological methods",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Software Developer Experience: Case Studies in Lean-Agile and Open Source Environments",
            "authors": [
                {
                    "first": "Fabian",
                    "middle": [],
                    "last": "Fagerholm",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Examining the Structure of Lean and Agile Values among Software Developers",
            "authors": [
                {
                    "first": "Fabian",
                    "middle": [],
                    "last": "Fagerholm",
                    "suffix": ""
                },
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Pagels",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Lecture Notes in Business Information Processing: Agile Processes in Software Engineering and Extreme Programming",
            "volume": "",
            "issn": "",
            "pages": "218--233",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Validity Threats in Empirical Software Engineering Research -An Initial Survey",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Feldt",
                    "suffix": ""
                },
                {
                    "first": "Ana",
                    "middle": [],
                    "last": "Magazinius",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 22nd International Conference on Software Engineering & Knowledge Engineering (SEKE'2010)",
            "volume": "",
            "issn": "",
            "pages": "374--379",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Towards individualized software engineering",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Feldt",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Torkar",
                    "suffix": ""
                },
                {
                    "first": "Lefteris",
                    "middle": [],
                    "last": "Angelis",
                    "suffix": ""
                },
                {
                    "first": "Maria",
                    "middle": [],
                    "last": "Samuelsson",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "empirical studies should collect psychometrics",
            "volume": "",
            "issn": "",
            "pages": "49--52",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Open science in software engineering",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Daniel M\u00e9ndez Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                },
                {
                    "first": "Heidi",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Seibold",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Contemporary Empirical Methods in Software Engineering",
            "volume": "",
            "issn": "",
            "pages": "479--504",
            "other_ids": {
                "arXiv": [
                    "arXiv:1712.08341"
                ]
            }
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "An empirical evaluation of alternative methods of estimation for confirmatory factor analysis with ordinal data",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "Patrick J",
                    "middle": [],
                    "last": "Flora",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Curran",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Psychological methods",
            "volume": "9",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "2017. sem: Structural Equation Models",
            "authors": [
                {
                    "first": "John",
                    "middle": [],
                    "last": "Fox",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "The Comprehensive R Archive Network. 1-79 pages",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Psychometric Properties",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Annie",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ginty",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Encyclopedia of Behavioral Medicine",
            "volume": "",
            "issn": "",
            "pages": "1563--1564",
            "other_ids": {
                "DOI": [
                    "10.1007/978-1-4419-1005-9_480"
                ]
            }
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Instructional technology and the measurement of learing outcomes: Some questions",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Glaser",
                    "suffix": ""
                }
            ],
            "year": 1963,
            "venue": "American Psychologist",
            "volume": "18",
            "issn": "",
            "pages": "519--521",
            "other_ids": {
                "DOI": [
                    "10.1037/h0049294"
                ]
            }
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "On the Unhappiness of Software Developers",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                },
                {
                    "first": "Fabian",
                    "middle": [],
                    "last": "Fagerholm",
                    "suffix": ""
                },
                {
                    "first": "Xiaofeng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Pekka",
                    "middle": [],
                    "last": "Abrahamsson",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "21st International Conference on Evaluation and Assessment in Software Engineering",
            "volume": "",
            "issn": "",
            "pages": "324--333",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Behavioral Software Engineering -Example of psychometric evaluation with R",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                },
                {
                    "first": "Per",
                    "middle": [],
                    "last": "Lenberg",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Feldt",
                    "suffix": ""
                },
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.5281/zenodo.3799603"
                ]
            }
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "The Affect of Software Developers: Common Misconceptions and Measurements",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                },
                {
                    "first": "Xiaofeng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Pekka",
                    "middle": [],
                    "last": "Abrahamsson",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "2015 IEEE/ACM 8th International Workshop on Cooperative and Human Aspects of Software Engineering",
            "volume": "",
            "issn": "",
            "pages": "123--124",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Do feelings matter? On the correlation of affects and the self-assessed productivity in software engineering",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                },
                {
                    "first": "Xiaofeng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Pekka",
                    "middle": [],
                    "last": "Abrahamsson",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of Software: Evolution and Process",
            "volume": "27",
            "issn": "",
            "pages": "467--487",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Understanding the Affect of Developers: Theoretical Background and Guidelines for Psychoempirical Software Engineering",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                },
                {
                    "first": "Xiaofeng",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Pekka",
                    "middle": [],
                    "last": "Abrahamsson",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 7th International Workshop on Social Software Engineering",
            "volume": "",
            "issn": "",
            "pages": "25--32",
            "other_ids": {
                "DOI": [
                    "10.1145/2804381.2804386"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Darwinian theory, functionalism, and the first American psychological revolution",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Christopher",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Green",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "American Psychologist",
            "volume": "64",
            "issn": "",
            "pages": "75--83",
            "other_ids": {
                "DOI": [
                    "10.1037/a0013338"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Standards of validity and the validity of standards in behavioral software engineering research",
            "authors": [
                {
                    "first": "Lucas",
                    "middle": [],
                    "last": "Gren",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Item bias review",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Ronald",
                    "suffix": ""
                },
                {
                    "first": "Jane",
                    "middle": [],
                    "last": "Hambleton",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rodgers",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "ERIC Clearinghouse on Assessment and Evaluation",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Fundamentals of item response theory",
            "authors": [
                {
                    "first": "Hariharan",
                    "middle": [],
                    "last": "Ronald K Hambleton",
                    "suffix": ""
                },
                {
                    "first": "H Jane",
                    "middle": [],
                    "last": "Swaminathan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rogers",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "The trilogy of mind: Cognition, affection, and conation",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Ernest R Hilgard",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "Journal of the History of the Behavioral Sciences",
            "volume": "16",
            "issn": "2",
            "pages": "107--117",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Personality and the fate of organizations",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Hogan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "A Rationale And Test For The Number Of Factors In Factor Analysis",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "John",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Horn",
                    "suffix": ""
                }
            ],
            "year": 1965,
            "venue": "Psychometrika",
            "volume": "30",
            "issn": "",
            "pages": "179--185",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Some lessons learned in conducting software engineering surveys in china",
            "authors": [
                {
                    "first": "Junzhong",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "Jingyue",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Reidar",
                    "middle": [],
                    "last": "Conradi",
                    "suffix": ""
                },
                {
                    "first": "Chunnian",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jianqiang",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Weibing",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement",
            "volume": "",
            "issn": "",
            "pages": "168--177",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "The varimax criterion for analytic rotation in factor analysis",
            "authors": [
                {
                    "first": "Henry",
                    "middle": [
                        "F"
                    ],
                    "last": "Kaiser",
                    "suffix": ""
                }
            ],
            "year": 1958,
            "venue": "Psychometrika",
            "volume": "23",
            "issn": "",
            "pages": "187--200",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "The Application of Electronic Computers to Factor Analysis",
            "authors": [
                {
                    "first": "Henry",
                    "middle": [
                        "F"
                    ],
                    "last": "Kaiser",
                    "suffix": ""
                }
            ],
            "year": 1960,
            "venue": "Educational and Psychological Measurement",
            "volume": "20",
            "issn": "",
            "pages": "141--151",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Structural equation modeling: Foundations and extensions",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "Kaplan",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "10",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Guidelines for performing systematic literature reviews in software engineering",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "B A Kitchenham",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Personal Opinion Surveys",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Barbara",
                    "suffix": ""
                },
                {
                    "first": "Shari",
                    "middle": [
                        "L"
                    ],
                    "last": "Kitchenham",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pfleeger",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Guide to Advanced Empirical Software Engineering",
            "volume": "",
            "issn": "",
            "pages": "63--92",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "A handbook of test construction (psychology revivals): introduction to psychometric design",
            "authors": [
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Kline",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Exploratory Factor Analysis: Theory and Application",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Gj Kootstra",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "A bayesian approach to confirmatory factor analysis",
            "authors": [
                {
                    "first": "Sik-Yum",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 1981,
            "venue": "Psychometrika",
            "volume": "46",
            "issn": "",
            "pages": "153--160",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Behavioral software engineering -guidelines for qualitative studies",
            "authors": [
                {
                    "first": "Per",
                    "middle": [],
                    "last": "Lenberg",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Feldt",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "G\u00f6ran Wallgren",
                    "suffix": ""
                },
                {
                    "first": "Inga",
                    "middle": [],
                    "last": "Tengberg",
                    "suffix": ""
                },
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Tidefors",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1712.08341"
                ]
            }
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Behavioral software engineering: A definition and systematic literature review",
            "authors": [
                {
                    "first": "Per",
                    "middle": [],
                    "last": "Lenberg",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Feldt",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [
                        "G\u00f6ran"
                    ],
                    "last": "Wallgren",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Journal of Systems and Software",
            "volume": "107",
            "issn": "",
            "pages": "15--37",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jss.2015.04.084"
                ]
            }
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "An initial analysis of software engineers' attitudes towards organizational change",
            "authors": [
                {
                    "first": "Per",
                    "middle": [],
                    "last": "Lenberg",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "G\u00f6ran Wallgren Tengberg",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Feldt",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Empirical Software Engineering",
            "volume": "22",
            "issn": "",
            "pages": "2179--2205",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "A technique for the measurement of attitudes",
            "authors": [
                {
                    "first": "Rensis",
                    "middle": [],
                    "last": "Likert",
                    "suffix": ""
                }
            ],
            "year": 1932,
            "venue": "Archives of psychology",
            "volume": "22",
            "issn": "",
            "pages": "1--55",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Objective Tests as Instruments of Psychological Theory",
            "authors": [
                {
                    "first": "Jane",
                    "middle": [],
                    "last": "Loevinger",
                    "suffix": ""
                }
            ],
            "year": 1957,
            "venue": "Psychological Reports",
            "volume": "3",
            "issn": "",
            "pages": "635--694",
            "other_ids": {
                "DOI": [
                    "10.2466/pr0.1957.3.3.635"
                ]
            }
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Bayesian Factor Analysis as a Variable-Selection Problem: Alternative Priors and Consequences",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Zh Lu",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Chow",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Loken",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Multivariate Behav Res",
            "volume": "51",
            "issn": "",
            "pages": "519--539",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Applications of structural equation modeling in psychological research",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "C"
                    ],
                    "last": "Maccallum",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Austin",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Annu Rev Psychol",
            "volume": "51",
            "issn": "",
            "pages": "201--226",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Sample size in factor analysis",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Robert",
                    "suffix": ""
                },
                {
                    "first": "Keith",
                    "middle": [
                        "F"
                    ],
                    "last": "Maccallum",
                    "suffix": ""
                },
                {
                    "first": "Shaobo",
                    "middle": [],
                    "last": "Widaman",
                    "suffix": ""
                },
                {
                    "first": "Sehee",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Psychological Methods",
            "volume": "4",
            "issn": "",
            "pages": "84--99",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Who should test whom",
            "authors": [
                {
                    "first": "Sharon",
                    "middle": [],
                    "last": "Mcdonald",
                    "suffix": ""
                },
                {
                    "first": "Helen",
                    "middle": [
                        "M"
                    ],
                    "last": "Edwards",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Commun. ACM",
            "volume": "50",
            "issn": "",
            "pages": "66--71",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Survey guidelines in software engineering: An annotated review",
            "authors": [
                {
                    "first": "Jefferson",
                    "middle": [],
                    "last": "Seide Moll\u00e9ri",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Petersen",
                    "suffix": ""
                },
                {
                    "first": "Emilia",
                    "middle": [],
                    "last": "Mendes",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Bayesian structural equation modeling: A more flexible representation of substantive theory",
            "authors": [
                {
                    "first": "Bengt",
                    "middle": [],
                    "last": "Muth\u00e9n",
                    "suffix": ""
                },
                {
                    "first": "Tihomir",
                    "middle": [],
                    "last": "Asparouhov",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Psychological Methods",
            "volume": "17",
            "issn": "",
            "pages": "313--335",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Evaluating the Use of Exploratory Factor Analysis in Developmental Disability Psychological Research",
            "authors": [
                {
                    "first": "Megan",
                    "middle": [],
                    "last": "Norris",
                    "suffix": ""
                },
                {
                    "first": "Luc",
                    "middle": [],
                    "last": "Lecavalier",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of Autism and Developmental Disorders",
            "volume": "40",
            "issn": "01",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/s10803-009-0816-2"
                ]
            }
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "Psychometric theory 3E. Tata McGraw-Hill Education",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Jum",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Nunnally",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Questionnaire Design, Interviewing and Attitude Measurement",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Oppenheim",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "On lines and planes of closest fit to systems of points in space",
            "authors": [],
            "year": 1901,
            "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science",
            "volume": "2",
            "issn": "",
            "pages": "559--572",
            "other_ids": {
                "DOI": [
                    "10.1080/14786440109462720"
                ]
            }
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Differential item functioning and item bias: Critical considerations in test fairness. Teachers College, Columbia University Working Papers",
            "authors": [
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Perrone",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "TESOL and Applied Linguistics",
            "volume": "6",
            "issn": "",
            "pages": "1--3",
            "other_ids": {}
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "Worldviews, Research Methods, and their Relationship to Validity in Empirical Software Engineering Research",
            "authors": [
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Petersen",
                    "suffix": ""
                },
                {
                    "first": "Cigdem",
                    "middle": [],
                    "last": "Gencel",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "2013 Joint Conference of the 23nd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement (IWSM-MENSURA)",
            "volume": "",
            "issn": "",
            "pages": "81--89",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "Measuring the MBTI... and coming up short",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Pittenger",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Journal of Career Planning and Employment",
            "volume": "54",
            "issn": "",
            "pages": "48--52",
            "other_ids": {}
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "The Three Sigma Rule",
            "authors": [
                {
                    "first": "Friedrich",
                    "middle": [],
                    "last": "Pukelsheim",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "The American Statistician",
            "volume": "48",
            "issn": "",
            "pages": "88--91",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Amin Milani Fard, and Rana Alkadhi. 2020. Pandemic Programming: How COVID-19 affects software developers and how their organizations can help",
            "authors": [
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Ralph",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Baltes",
                    "suffix": ""
                },
                {
                    "first": "Gianisa",
                    "middle": [],
                    "last": "Adisaputri",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Torkar",
                    "suffix": ""
                },
                {
                    "first": "Vladimir",
                    "middle": [],
                    "last": "Kovalenko",
                    "suffix": ""
                },
                {
                    "first": "Marcos",
                    "middle": [],
                    "last": "Kalinowski",
                    "suffix": ""
                },
                {
                    "first": "Nicole",
                    "middle": [],
                    "last": "Novielli",
                    "suffix": ""
                },
                {
                    "first": "Shin",
                    "middle": [],
                    "last": "Yoo",
                    "suffix": ""
                },
                {
                    "first": "Xavier",
                    "middle": [],
                    "last": "Devroey",
                    "suffix": ""
                },
                {
                    "first": "Xin",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "Minghui",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Burak",
                    "middle": [],
                    "last": "Turhan",
                    "suffix": ""
                },
                {
                    "first": "Rashina",
                    "middle": [],
                    "last": "Hoda",
                    "suffix": ""
                },
                {
                    "first": "Hideaki",
                    "middle": [],
                    "last": "Hata",
                    "suffix": ""
                },
                {
                    "first": "Gregorio",
                    "middle": [],
                    "last": "Robles",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "32",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.01127"
                ]
            }
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "Construct Validity in Software Engineering Research and Software Metrics",
            "authors": [
                {
                    "first": "Paul",
                    "middle": [],
                    "last": "Ralph",
                    "suffix": ""
                },
                {
                    "first": "Ewan",
                    "middle": [],
                    "last": "Tempero",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "An introduction to psychometric theory with applications in R. personality-project.org",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Revelle",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "An introduction to the psych package: Part II Scale construction and psychometrics",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Revelle",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "The Comprehensive R Archive Network. 1-97 pages",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "Using the psych package to generate and test structural models",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Revelle",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF75": {
            "ref_id": "b75",
            "title": "Procedures for Psychological, Psychometric, and Personality Research",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Revelle",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "How To: Use the psych package for Factor Analysis and data reduction",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Revelle",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "The Comprehensive R Archive Network. 1-96 pages",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF77": {
            "ref_id": "b77",
            "title": "Very simple structure: An alternative procedure for estimating the optimal number of interpretable factors",
            "authors": [
                {
                    "first": "William",
                    "middle": [],
                    "last": "Revelle",
                    "suffix": ""
                },
                {
                    "first": "Thomas",
                    "middle": [],
                    "last": "Rocklin",
                    "suffix": ""
                }
            ],
            "year": 1979,
            "venue": "Multivariate Behavioral Research",
            "volume": "14",
            "issn": "",
            "pages": "403--414",
            "other_ids": {}
        },
        "BIBREF78": {
            "ref_id": "b78",
            "title": "Measuring differential item and test functioning across academic disciplines",
            "authors": [
                {
                    "first": "Judith",
                    "middle": [],
                    "last": "Runnels",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Language Testing in Asia",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF79": {
            "ref_id": "b79",
            "title": "Knowledge management in software engineering",
            "authors": [
                {
                    "first": "Ioana",
                    "middle": [],
                    "last": "Rus",
                    "suffix": ""
                },
                {
                    "first": "Mikael",
                    "middle": [],
                    "last": "Lindvall",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sinha",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "IEEE software",
            "volume": "19",
            "issn": "",
            "pages": "26--38",
            "other_ids": {}
        },
        "BIBREF80": {
            "ref_id": "b80",
            "title": "Search of Underlying Dimensions: The Use (and Abuse) of Factor Analysis in Personality and Social Psychology Bulletin",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [
                        "W"
                    ],
                    "last": "Russell",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "28",
            "issn": "",
            "pages": "1629--1646",
            "other_ids": {
                "DOI": [
                    "10.1177/014616702237645"
                ]
            }
        },
        "BIBREF81": {
            "ref_id": "b81",
            "title": "Modern psychometrics : the science of psychological assessment",
            "authors": [
                {
                    "first": "John",
                    "middle": [],
                    "last": "Rust",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF82": {
            "ref_id": "b82",
            "title": "Asking Questions About Behavior: Cognition, Communication, and Questionnaire Construction",
            "authors": [
                {
                    "first": "Norbert",
                    "middle": [],
                    "last": "Schwarz",
                    "suffix": ""
                },
                {
                    "first": "Daphna",
                    "middle": [],
                    "last": "Oyserman",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "American Journal of Evaluation",
            "volume": "22",
            "issn": "",
            "pages": "127--160",
            "other_ids": {
                "DOI": [
                    "10.1177/109821400102200202"
                ]
            }
        },
        "BIBREF83": {
            "ref_id": "b83",
            "title": "Views on internal and external validity in empirical software engineering",
            "authors": [
                {
                    "first": "Janet",
                    "middle": [],
                    "last": "Siegmund",
                    "suffix": ""
                },
                {
                    "first": "Norbert",
                    "middle": [],
                    "last": "Siegmund",
                    "suffix": ""
                },
                {
                    "first": "Sven",
                    "middle": [],
                    "last": "Apel",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Software Engineering (ICSE), 2015 IEEE/ACM 37th IEEE International Conference on",
            "volume": "1",
            "issn": "",
            "pages": "9--19",
            "other_ids": {
                "DOI": [
                    "10.1109/ICSE.2015.24"
                ]
            }
        },
        "BIBREF84": {
            "ref_id": "b84",
            "title": "Norms for Test Construction",
            "authors": [
                {
                    "first": "Kamlesh",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "Mohita",
                    "middle": [],
                    "last": "Junnarkar",
                    "suffix": ""
                },
                {
                    "first": "Jasleen",
                    "middle": [],
                    "last": "Kaur",
                    "suffix": ""
                },
                {
                    "first": "Kamlesh",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "Mohita",
                    "middle": [],
                    "last": "Junnarkar",
                    "suffix": ""
                },
                {
                    "first": "Jasleen",
                    "middle": [],
                    "last": "Kaur",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Measures of Positive Psychology",
            "volume": "",
            "issn": "",
            "pages": "17--34",
            "other_ids": {}
        },
        "BIBREF85": {
            "ref_id": "b85",
            "title": "Eliciting salient beliefs in research on the theory of planned behaviour: The effect of question wording",
            "authors": [
                {
                    "first": "Stephen",
                    "middle": [],
                    "last": "Sutton",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [
                        "P"
                    ],
                    "last": "French",
                    "suffix": ""
                },
                {
                    "first": "Susie",
                    "middle": [
                        "J"
                    ],
                    "last": "Hennings",
                    "suffix": ""
                },
                {
                    "first": "Jo",
                    "middle": [],
                    "last": "Mitchell",
                    "suffix": ""
                },
                {
                    "first": "Nicholas",
                    "middle": [
                        "J"
                    ],
                    "last": "Wareham",
                    "suffix": ""
                },
                {
                    "first": "Simon",
                    "middle": [],
                    "last": "Griffin",
                    "suffix": ""
                },
                {
                    "first": "Wendy",
                    "middle": [],
                    "last": "Hardeman",
                    "suffix": ""
                },
                {
                    "first": "Ann",
                    "middle": [
                        "Louise"
                    ],
                    "last": "Kinmonth",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Current Psychology",
            "volume": "22",
            "issn": "",
            "pages": "234--251",
            "other_ids": {
                "DOI": [
                    "10.1007/s12144-003-1019-1"
                ]
            }
        },
        "BIBREF86": {
            "ref_id": "b86",
            "title": "Sharing knowledge in knowledge-intensive firms",
            "authors": [
                {
                    "first": "Juani",
                    "middle": [],
                    "last": "Swart",
                    "suffix": ""
                },
                {
                    "first": "Nicholas",
                    "middle": [],
                    "last": "Kinnie",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Human resource management journal",
            "volume": "13",
            "issn": "",
            "pages": "60--75",
            "other_ids": {}
        },
        "BIBREF87": {
            "ref_id": "b87",
            "title": "Using multivariate statistics",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Barbara",
                    "suffix": ""
                },
                {
                    "first": "Linda",
                    "middle": [
                        "S"
                    ],
                    "last": "Tabachnick",
                    "suffix": ""
                },
                {
                    "first": "Jodie",
                    "middle": [
                        "B"
                    ],
                    "last": "Fidell",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ullman",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF88": {
            "ref_id": "b88",
            "title": "Uses of factor analysis in counseling psychology research",
            "authors": [
                {
                    "first": "Howard",
                    "middle": [
                        "E"
                    ],
                    "last": "Tinsley",
                    "suffix": ""
                },
                {
                    "first": "Diane",
                    "middle": [
                        "J"
                    ],
                    "last": "Tinsley",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "Journal of Counseling Psychology",
            "volume": "34",
            "issn": "",
            "pages": "414--424",
            "other_ids": {}
        },
        "BIBREF89": {
            "ref_id": "b89",
            "title": "Classical Test Theory in Historical Perspective",
            "authors": [
                {
                    "first": "Ross",
                    "middle": [
                        "E"
                    ],
                    "last": "Traub",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Educational Measurement: Issues and Practice",
            "volume": "16",
            "issn": "",
            "pages": "8--14",
            "other_ids": {
                "DOI": [
                    "10.1111/j.1745-3992.1997.tb00603.x"
                ]
            }
        },
        "BIBREF90": {
            "ref_id": "b90",
            "title": "Challenges in survey research",
            "authors": [
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                },
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Mendez",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Felderer",
                    "suffix": ""
                },
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Graziotin",
                    "suffix": ""
                },
                {
                    "first": "Marcos",
                    "middle": [],
                    "last": "Kalinowski",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Contemporary Empirical Methods in Software Engineering",
            "volume": "",
            "issn": "",
            "pages": "95--127",
            "other_ids": {
                "arXiv": [
                    "arXiv:1908.05899"
                ]
            }
        },
        "BIBREF91": {
            "ref_id": "b91",
            "title": "The psychology of computer programming",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gerald",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Weinberg",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "",
            "volume": "932633420",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF92": {
            "ref_id": "b92",
            "title": "Common Factor Analysis Versus Principal Component Analysis: Differential Bias in Representing Model Parameters",
            "authors": [
                {
                    "first": "Keith",
                    "middle": [
                        "F"
                    ],
                    "last": "Widaman",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Multivariate Behavioral Research",
            "volume": "28",
            "issn": "",
            "pages": "263--311",
            "other_ids": {
                "DOI": [
                    "10.1207/s15327906mbr2803_1"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Steps for developing a psychological tests. Phases with dark background are uncommon in software engineering research and are covered in the present paper.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Construction of an initial pool of items. (5) Review of the items. (6) Conduction of a pilot test with the revised items. (7) Execution of an item analysis to possibly reduce the number of items. (8) Evaluation of an exploratory factor analysis to possibly reduce and group items into components or factors. (9) Execution of a field test of the items with a larger, representative sample. (10) Determination of statistical properties of item scores. (11) Design and execution of reliability studies. (12) Design and execution of validity studies. (13) Evaluation of fairness in testing and test bias. (14) Development of guidelines for administering, scoring, and interpreting test scores.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Phases for exploratory factor analysis",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Reliability in psychometric theory",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig. 5. Validity in psychometric theory",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Bias in psychometric theory",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Claes Wohlin, Per Runeson, Martin H\u00f6st, Magnus C. Ohlsson, Bj\u00f6rn Regnell, and Anders Wessl\u00e9n. 2012. Experimentation in Software Engineering. Springer Berlin Heidelberg, Berlin, Heidelberg. [95] Marvin Wyrich, Daniel Graziotin, and Stefan Wagner. 2019. A theory on individual characteristics of successful coding challenge solvers. PeerJ Computer Science 5 (2019), e173. [96] An Gie Yong and Sean Pearce. 2013. A Beginner's Guide to Factor Analysis: Focusing on Exploratory Factor Analysis. Tutorials in Quantitative Methods for Psychology 9, 2 (2013), 79-94. [97] Bruno D Zumbo. 2007. Three generations of DIF analyses: Considering where it has been, where it is now, and where it is going. Language assessment quarterly 4, 2 (2007), 223-233.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Requirements and options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Data simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Item Review and Item Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Factor Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Item statistical properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "if (!require(\"fabricatr\")){ install.packages(\"fabricatr\", dep = T) } ## Loading required package: fabricatr require(\"fabricatr\") if (!require(\"psych\")){ install.packages(\"psych\", dep = T) } ## Loading required package: psych require(\"psych\") if (!require(\"knitr\")){ install.packages(\"knitr\", dep = T) } ## Loading required package: knitr require(\"knitr\") knitr::opts_chunk$set(echo = TRUE, cache = FALSE, width = 50)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "if (!safeguard) { # 31 items belonging to 5 clusters items.initialset <-c(rep(1, 7), # cluster 1 rep(2, 4), # cluster 2 rep(3, 9), # cluster 3 rep(4, 4), # cluster 4 rep(5, 7)) # cluster 5 likert.brakes <-c(-Inf,-1.5,-0.5, 0.5, 1.5, Inf)likert.values <-c(\"1\", \"2\", \"3\", \"4\", \"5\") counter <-0 var.names <unlist(lapply(items.initialset, function(x) { paste0(\"F\", x, \"_\", counter <<-counter + 1) })) normalized.response <-draw_normal_icc(clusters = items.initialset, ICC = sample(seq(0.4, 0.7, by = 0.05), 1), sd = sample(seq(0.5, 0.9, by = 0.1), 1)) ordered.response <-draw_ordered(x = normalized.response, breaks = likert.brakes, break_labels = likert.values) full.df <-rbind(ordered.response) colnames(full.df) <-var.names for (i in seq(1, overall.sample.size -1)) { normalized.response <-draw_normal_icc(clusters = items.initialset, ICC = sample(seq(0.4, 0.7, by = 0.05), 1), sd = sample(seq(0.5, 0.9, by = 0.1), 1)) ordered.response <-draw_ordered(x = normalized.response, breaks = likert.brakes, break_labels = likert.values) full.df <-rbind(full.df, ordered.response) } # each row is a participant id, from 1 to overall.sample.size rownames(full.df) <-seq(1, overall.sample.size) }",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "cor(pilot.df[, grepl(\"F1_\" , names(pilot.df))], method = c(\"kendall\")) pilot.df[, grepl(\"F4_\" , names(pilot.df))], method = c(\"kendall\"))",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "point starts with the fifth component for PCA and with the fifth factor for PAF, suggesting that 4 is the number of factors to extract.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Parallel analysis suggests that the number of factors = 4 and the number of components = 4",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Analysis using method = pa ## Call: fa(r = study.beforeefa.df, nfactors = 5, rotate = \"promax\", fm = \"pa\") ## Standardized loadings (pattern matrix) based upon correlation matrix F4_21 -0.16 -0.01 -0.13 0.19 0.06 0.089 0.91 3of the hypothesis that 5 factors are sufficient. ## ## The degrees of freedom for the null model are 406 and the objective function was 14.23 with Chi Sq ## The degrees of freedom for the model are 271 and the objective function was 2.98 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.05 ## ## The harmonic number of observations is 119 with the empirical chi square 167.43 with prob < 1 ## The total number of observations was 119 with Likelihood Chi Square = 310.07 with prob < 0.051 ## ## Tucker Lewis Index of factoring reliability = 0.946 ## RMSEA index = 0.034 and the 90 % confidence intervals are 0 0.052 ## BIC = -985.07 ## Fit based upon off diagonal values = 0.97 ## Measures of factor score adequacy ## PA1 PA2 PA3 PA4 PA5 ## Correlation of (regression)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "0.64 -0.19 0.08 0.08 0.519 0.48 1.2",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "unname(f1.first), unname(f1.second)) ## [1] 0.9462781 print(\"F2\") ## [1] \"F2\" cor(unname(f2.first), unname(f2.second)) ## [1] 0.8248873 print(\"F3\") ## [1] \"F3\" cor(unname(f3.first), unname(f3.second))",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "Example correlation matrix for items a, b, c, d, e",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Conversion from Z scores to Stanine and Sten scores",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "# set to FALSE to regenerate the dataset safeguard <-TRUE && file.exists( graziotin_et_al-bse_psychometrics_example.csv ) overall.sample.size = 142 pilot.test.sample.size = 23 reliability.sample.size = 48",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "For other three items, F4_21 to F4_23, we get even more malicious. Factor 4 only had 4 candidates for a representative cluster. We generate values for these three items again such that they do not likely belong to the cluster 'F4' anymore, but they do not provide clear extreme values that would get caught during item analysis.",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Number of FactorsVery Simple Structure Fit Very Simple Structure of our fictitious five factor model vss.test.five ## ## Very Simple Structure of Very Simple Structure of our fictitious five factor model ## Call: vss(x = study.beforeefa.df, n = 7, title = \"Very Simple Structure of our fictitious five factor ## VSS complexity 1 achieves a maximimum of 0.81 with 7 factors ## VSS complexity 2 achieves a maximimum of 0.84 with 7 factors ## ## The Velicer MAP achieves a minimum of 0.02 with 4 factors ## BIC achieves a minimum of -1071.41 with 4 factors ## Sample Size adjusted BIC achieves a minimum of -135.64 with 4 factors ## ## Statistics by number of factors",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "pa.test.five <fa.parallel(study.beforeefa.df) Parallel Analysis Scree Plots Factor/Component Number eigenvalues of principal components and factor analysis PC Actual Data PC Simulated Data PC Resampled Data FA Actual Data FA Simulated Data FA Resampled Data",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "factors.after.efa.df <data.frame( F1 = rowSums(f1.df), F2 = rowSums(f2.df), F3 = rowSums(f3.df), F5 = rowSums(f5.df) ) describe(factors.after.efa.df) Confidence intervals for F1, F2, F3, and F5 print(\"Standardized scores available in factors.after.efa.standard.df but not shown here.\") ## [1] \"Standardized scores available in factors.after.efa.standard.df but not shown here.\" factors.after.efa.standard.df <data.frame( F1 = scale",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "An alternative for factor extraction is the Very Simple Structure, or VSS, which can be used for extracting factors before their rotation.We plot the structure using our initial five factor model with 29 items. As written in our paper, the VSS criterion requires a user specified number of factors, which should be above the target number of factors. We specify seven factors using the n = 7 parameter.vss.test.five <vss(study.beforeefa.df, title = \"Very Simple Structure of our fictitious five factor model\", n = 7)## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, : ## The estimated weights for the factor scores are probably incorrect. Try a ## different factor score estimation method. The four factor model seems an improvement over the previous one. The variance explained has not decreased (it actually increases to 0.51), and each factor explains at minimum 16% of the variance. Where we did not improve was the Likelihood Chi Square test, which is now significant (p = 0.04). RMSR and RMSEA have not changed significantly are are still within desirable levels.If we inspect the single items and their loadings on factors, however, the situation is improved significantly. All our developed items for a factor F load majorly on a single cluster PA found by the PAF analysis.The only exception appears to be F5_25, which seems to not load on any factor whatsoever. After reviewing 13 the item, we decide to remove it and run the EFA again.# we drop those columns that we identified after item analysis study.efa.bis.df <study.efa.df[,-which(names(study.efa.df) %in% c(\"F5_25\"))] four.factors.bis <fa(study.efa.bis.df , fm = \"pa\", nfactors = 4, rotate = \"promax\") four.factors.bis With the revised four factor model, the total variance explained rises to 0.53, which is another improvement.It holds that all our developed items for a factor F load majorly on a single cluster PA found by the PAF analysis. In particular, our factors sorted for their importance (variance explained) are F3 (PA1), F1 (PA2), F5 (PA3), and F2 (PA4). We are satisfied with the resulting measurement instrument so far.We improved on Likelihood Chi Square test, which is now back to non-significant (p = 0.087). RMSR and RMSEA have not changed significantly are are still within desirable levels. We decide to stop our EFA at this point and use the present form of our measurement instrument, with 4 factors and 26 items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        },
        {
            "text": "We started with 31 items, which in our fictitious example were already a reduction, after item review, of an original measurement instrument with five factors. Thanks to item analysis and exploratory factor analysis, we were able to reduce the measurement instrument to 26 items (a 16% reduction) and four factors. All this with an improved confidence in what we can conclude from interpreting the results of employing the measurement instrument. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The resulting measurement instrument"
        }
    ]
}