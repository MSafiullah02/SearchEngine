{
    "paper_id": "PMC4890616",
    "metadata": {
        "title": "Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning",
        "authors": [
            {
                "first": "Hoo-Chang",
                "middle": [],
                "last": "Shin",
                "suffix": "",
                "email": "hoochang.shin@nih.gov",
                "affiliation": {}
            },
            {
                "first": "Holger",
                "middle": [
                    "R."
                ],
                "last": "Roth",
                "suffix": "",
                "email": "holger.roth@nih.gov",
                "affiliation": {}
            },
            {
                "first": "Mingchen",
                "middle": [],
                "last": "Gao",
                "suffix": "",
                "email": "mingchen.gao@nih.gov",
                "affiliation": {}
            },
            {
                "first": "Le",
                "middle": [],
                "last": "Lu",
                "suffix": "",
                "email": "le.lu@nih.gov",
                "affiliation": {}
            },
            {
                "first": "Ziyue",
                "middle": [],
                "last": "Xu",
                "suffix": "",
                "email": "ziyue.xu@nih.gov",
                "affiliation": {}
            },
            {
                "first": "Isabella",
                "middle": [],
                "last": "Nogues",
                "suffix": "",
                "email": "isabella.nogues@nih.gov",
                "affiliation": {}
            },
            {
                "first": "Jianhua",
                "middle": [],
                "last": "Yao",
                "suffix": "",
                "email": "jyao@cc.nih.gov",
                "affiliation": {}
            },
            {
                "first": "Daniel",
                "middle": [],
                "last": "Mollura",
                "suffix": "",
                "email": "molluradj@cc.nih.gov",
                "affiliation": {}
            },
            {
                "first": "Ronald",
                "middle": [
                    "M."
                ],
                "last": "Summers",
                "suffix": "",
                "email": "rms@nih.gov",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e., ImageNet [1], [2]) and the recent revival of deep convolutional neural networks (CNN) [3], [4]. For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5], [4]. Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes. The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6], [7], e.g., PASCAL [8] and medical image categorization [9]\u2013[12]. However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 143,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 145,
                    "end": 148,
                    "mention": "[2]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 217,
                    "end": 220,
                    "mention": "[3]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 222,
                    "end": 225,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 405,
                    "end": 408,
                    "mention": "[5]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 410,
                    "end": 413,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 480,
                    "end": 483,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 758,
                    "end": 761,
                    "mention": "[6]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 763,
                    "end": 766,
                    "mention": "[7]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 781,
                    "end": 784,
                    "mention": "[8]",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 818,
                    "end": 821,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 822,
                    "end": 822,
                    "mention": "",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 822,
                    "end": 822,
                    "mention": "",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 822,
                    "end": 826,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "There are currently three major techniques that successfully employ CNNs to medical image classification: 1) training the \u201cCNN from scratch\u201d [13]\u2013[17]; 2) using \u201coff-the-shelf CNN\u201d features (without retraining the CNN) as complementary information channels to existing hand-crafted image features, for chest X-rays [10] and CT lung nodule identification [9], [12]; and 3) performing unsupervised pre-training on natural or medical images and fine-tuning on medical target images using CNN or other types of deep learning models [18]\u2013[21]. A decompositional 2.5D view resampling and an aggregation of random view classification scores are used to eliminate the \u201ccurse-of-dimensionality\u201d issue in [22], in order to acquire a sufficient number of training image samples.",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 145,
                    "mention": "[13]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 146,
                    "end": 146,
                    "mention": "",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 146,
                    "end": 146,
                    "mention": "",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 146,
                    "end": 146,
                    "mention": "",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 146,
                    "end": 150,
                    "mention": "[17]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 315,
                    "end": 319,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 354,
                    "end": 357,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 359,
                    "end": 363,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 528,
                    "end": 532,
                    "mention": "[18]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 533,
                    "end": 533,
                    "mention": "",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 533,
                    "end": 533,
                    "mention": "",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 533,
                    "end": 537,
                    "mention": "[21]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 695,
                    "end": 699,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Previous studies have analyzed three-dimensional patch creation for LN detection [23], [24], atlas creation from chest CT [25] and the extraction of multi-level image features [26], [27]. At present, there are several extensions or variations of the decompositional view representation introduced in [22], [28], such as: using a novel vessel-aligned multi-planar image representation for pulmonary embolism detection [29], fusing unregistered multiview for mammogram analysis [16] and classifying pulmonary peri-fissural nodules via an ensemble of 2D views [12].",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 85,
                    "mention": "[23]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 87,
                    "end": 91,
                    "mention": "[24]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 122,
                    "end": 126,
                    "mention": "[25]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 176,
                    "end": 180,
                    "mention": "[26]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 182,
                    "end": 186,
                    "mention": "[27]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 300,
                    "end": 304,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 306,
                    "end": 310,
                    "mention": "[28]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 417,
                    "end": 421,
                    "mention": "[29]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 476,
                    "end": 480,
                    "mention": "[16]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 557,
                    "end": 561,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Although natural images and medical images differ significantly, conventional image descriptors developed for object recognition in natural images, such as the scale-invariant feature transform (SIFT) [30] and the histogram of oriented gradients (HOG) [31], have been widely used for object detection and segmentation in medical image analysis. Recently, ImageNet pre-trained CNNs have been used for chest pathology identification and detection in X-ray and CT modalities [10], [9], [12]. They have yielded the best performance results by integrating low-level image features (e.g., GIST [32], bag of visual words (BoVW) and bag-of-frequency [12]). However, the fine-tuning of an ImageNet pre-trained CNN model on medical image datasets has not yet been exploited.",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 205,
                    "mention": "[30]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 252,
                    "end": 256,
                    "mention": "[31]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 472,
                    "end": 476,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 478,
                    "end": 481,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 483,
                    "end": 487,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 588,
                    "end": 592,
                    "mention": "[32]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 642,
                    "end": 646,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we exploit three important, but previously under-studied factors of employing deep convolutional neural networks to computer-aided detection problems. Particularly, we explore and evaluate different CNN architectures varying in width (ranging from 5 thousand to 160 million parameters) and depth (various numbers of layers), describe the effects of varying dataset scale and spatial image context on performance, and discuss when and why transfer learning from pre-trained ImageNet CNN models can be valuable. We further verify our hypothesis by inheriting and adapting rich hierarchical image features [5], [33] from the large-scale ImageNet dataset for computer aided diagnosis (CAD). We also explore CNN architectures of the most studied seven-layered \u201cAlexNet-CNN\u201d [4], a shallower \u201cCifar-CNN\u201d [22], and a much deeper version of \u201cGoogLeNet-CNN\u201d [33] (with our modifications on CNN structures). This study is partially motivated by recent studies [34], [35] in computer vision. The thorough quantitative analysis and evaluation on deep CNN [34] or sparsity image coding methods [35] elucidate the emerging techniques of the time and provide useful suggestions for their future stages of development, respectively.",
            "cite_spans": [
                {
                    "start": 618,
                    "end": 621,
                    "mention": "[5]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 623,
                    "end": 627,
                    "mention": "[33]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 784,
                    "end": 787,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 813,
                    "end": 817,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 864,
                    "end": 868,
                    "mention": "[33]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 965,
                    "end": 969,
                    "mention": "[34]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 971,
                    "end": 975,
                    "mention": "[35]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "mention": "[34]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1096,
                    "end": 1100,
                    "mention": "[35]",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification are studied in this work. On mediastinal LN detection, we surpass all currently reported results. We obtain 86% sensitivity on 3 false positives (FP) per patient, versus the prior state-of-art sensitivities of 78% [36] (stacked shallow learning) and 70% [22] (CNN), as prior state-of-the-art. For the first time, ILD classification results under the patient-level five-fold cross-validation protocol (CV5) are investigated and reported. The ILD dataset [37] contains 905 annotated image slices with 120 patients and 6 ILD labels. Such sparsely annotated datasets are generally difficult for CNN learning, due to the paucity of labeled instances.",
            "cite_spans": [
                {
                    "start": 371,
                    "end": 375,
                    "mention": "[36]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 411,
                    "end": 415,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 610,
                    "end": 614,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Evaluation protocols and details are critical to deriving significant empirical findings [34]. Our experimental results suggest that different CNN architectures and dataset re-sampling protocols are critical for the LN detection tasks where the amount of labeled training data is sufficient and spatial contexts are local. Since LN images are more flexible than ILD images with respect to resampling and reformatting, LN datasets may be more readily augmented by such image transformations. As a result, LN datasets contain more training and testing data instances (due to data auugmentation) than ILD datasets. They nonetheless remain less comprehensive than natural image datasets, such as ImageNet. Fine-tuning ImageNet-trained models for ILD classification is clearly advantageous and yields early promising results, when the amount of labeled training data is highly insufficient and multi-class categorization is used, as opposed to the LN dataset's binary class categorization. Another significant finding is that CNNs trained from scratch or fine-tuned from ImageNet models consistently outperform CNNs that merely use off-the-shelf CNN features, in both the LN and ILD classification problems. We further analyze, via CNN activation visualizations, when and why transfer learning from non-medical to medical images in CADe problems can be valuable.",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 93,
                    "mention": "[34]",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We use the publicly available dataset from [22], [41]. There are 388 mediastinal LNs labeled by radiologists in 90 patient CT scans, and 595 abdominal LNs in 86 patient CT scans. To facilitate comparison, we adopt the data preparation protocol of [22], where positive and negative LN candidates are sampled with the fields-of-view (FOVs) of 30 mm to 45 mm, surrounding the annotated and detected LN centers (obtained by a candidate generation process). More precisely, [22], [41], [36] follow a coarse-to-fine CADe scheme, partially inspired by [42], which operates with \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$\\sim 100\\%$\\end{document} detection recalls at the cost of approximately 40 false or negative LN candidates per patient scan. In this work, positive and negative LN candidate are first sampled up to 200 times with translations and rotations. Afterwards, negative LN samples are randomly re-selected at a lower rate close to the total number of positives. LN candidates are randomly extracted from fields-of-view (FOVs) spanning 35 mm to 128 mm in soft-tissue window \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$[-100, 200~{\\rm HU}]$\\end{document}. This allows us to capture multiple spatial scales of image context [43], [44]). The samples are then rescaled to a \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$64\\times 64~{\\rm pixel}$\\end{document} resolution via B-spline interpolation. A few examples of LNs with axial, coronal, and sagittal views encoded in RGB color images [22] are shown in Fig. 1.\n\n",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 47,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 49,
                    "end": 53,
                    "mention": "[41]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 247,
                    "end": 251,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 469,
                    "end": 473,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 475,
                    "end": 479,
                    "mention": "[41]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 481,
                    "end": 485,
                    "mention": "[36]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 545,
                    "end": 549,
                    "mention": "[42]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "mention": "[43]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "mention": "[44]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 2065,
                    "end": 2069,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Datasets ::: Datasets and Related Work",
            "ref_spans": [
                {
                    "start": 2083,
                    "end": 2089,
                    "mention": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Unlike the heart or the liver, lymph nodes have no pre-determined anatomic orientation. Hence, the purely random image resampling (with respect to scale, displacement and orientation) and reformatting (the axial, coronal, and sagittal views are in any system randomly resampled coordinates) is a natural choice, which also happens to yield high CNN performance. Although we integrate three channels of information from three orthogonal views for LN detection, the pixel-wise spatial correlations between or among channels are not necessary. The convolutional kernels in the lower level CNN architectures can learn the optimal weights to linearly combine the observations from the axial, coronal, and sagittal channels by computing their dot-products. Transforming axial, coronal, and sagittal representations to RGB also facilitates transfer learning from CNN models trained on ImageNet.\n\n",
            "cite_spans": [],
            "section": "Thoracoabdominal Lymph Node Datasets ::: Datasets and Related Work",
            "ref_spans": []
        },
        {
            "text": "This learning representation (i.e., \u201cbuilt-in CNN\u201d) is flexible, in that it naturally combines multiple sources or channels of information. In the recent literature [45], even heterogeneous class-conditional probability maps can be combined with raw images to improve performance. This set-up is similar to that of other works in computer vision, such as [46], where heterogeneous image information channels are jointly fed into the CNN convolutional layers for high-accuracy human parsing and segmentation. Finally, if there are correlations among CNN input channels, one may observe the corresponding correlated patterns in the learned filters.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 169,
                    "mention": "[45]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 355,
                    "end": 359,
                    "mention": "[46]",
                    "ref_id": "BIBREF40"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Datasets ::: Datasets and Related Work",
            "ref_spans": []
        },
        {
            "text": "In summary, the assumption that there are or must be pixel-wise spatial correlations among input channels does not apply to the CNN model representation. For other medical imaging problems, such as pulmonary embolism detection [29], in which orientation can be constrained along the attached vessel axis, vessel-aligned multi-planar image representation (MPR) is more effective than randomly aligned MPR.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 231,
                    "mention": "[29]",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Datasets ::: Datasets and Related Work",
            "ref_spans": []
        },
        {
            "text": "We utilize the publicly available dataset of [37]. It contains 905 image slices from 120 patients, with six lung tissue types annotations containing at least one of the following: healthy (NM), emphysema (EM), ground glass (GG), fibrosis (FB), micronodules (MN) and consolidation (CD) (Fig. 3). At the slice level, the objective is to classify the status of \u201cpresence/absence\u201d of any of the six ILD classes for an input axial CT slice [40]. Characterizing an arbitrary CT slice against any possible ILD type, without any manual ROI (in contrast to [38], [39]), can be useful for large-scale patient screening. For slice-level ILD classification, we sampled the slices 12 times with random translations and rotations. After this, we balanced the numbers of CT slice samples for the six classes by randomly sampling several instances at various rates. For patch-based classification, we sampled up to 100 patches of size 64\u00d764 from each ROI. This dataset is divided into five folds with disjoint patient subsets. The average number of CT slices (training instances) per fold is small, as shown in Table I. Slice-level ILD classification is a very challenging task where CNN models need to learn from very small numbers of training examples and predict ILD labels on unseen patients.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 435,
                    "end": 439,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 548,
                    "end": 552,
                    "mention": "[38]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 554,
                    "end": 558,
                    "mention": "[39]",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Interstitial Lung Disease Dataset ::: Datasets and Related Work",
            "ref_spans": [
                {
                    "start": 286,
                    "end": 292,
                    "mention": "Fig. 3",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1095,
                    "end": 1102,
                    "mention": "Table I",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "In the publicly available ILD dataset, very few CT slices are labeled as normal or healthy. The remaining CT slices cannot\n\n\nbe simply classified as normal, because many ILD disease regions or slices have not yet been labeled. ILD [37] is a partially labeled database; this is one of its main limitations. Research is being conducted to address this issue. In particular, [47] has proposed to fully label the ILD dataset pixel-wise via proposed segmentation label propagation.",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 235,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 372,
                    "end": 376,
                    "mention": "[47]",
                    "ref_id": "BIBREF41"
                }
            ],
            "section": "Interstitial Lung Disease Dataset ::: Datasets and Related Work",
            "ref_spans": []
        },
        {
            "text": "To leverage the CNN architectures designed for color images and to transfer CNN parameters pre-trained on ImageNet, we transform all gray-scale axial CT slice images via three CT window ranges: lung window range \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$[-1400, -200~{\\rm HU}]$\\end{document}, high-attenuation range \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$[-160, 240~{\\rm HU}]$\\end{document}, and low-attenuation range \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$[-1400; -950~{\\rm HU}]$\\end{document}. We then encode the transformed images into RGB channels (to be aligned with the input channels of CNN models [4], [33] pre-trained from natural image datasets [1]). The low-attenuation CT window is useful for visualizing certain texture patterns of lung diseases (especially emphysema). The usage of different CT attenuation channels improves classification results over the usage of a single CT windowing channel, as demonstrated in [40]. More importantly, these CT windowing processes do not depend on the lung segmentation, which instead is directly defined in the CT HU space. Fig. 4 shows a representative example of lung, high-attenuation, and low-attenuation CT windowing for an axis lung CT slice.",
            "cite_spans": [
                {
                    "start": 1193,
                    "end": 1196,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "mention": "[33]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1243,
                    "end": 1246,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Interstitial Lung Disease Dataset ::: Datasets and Related Work",
            "ref_spans": [
                {
                    "start": 1665,
                    "end": 1671,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF7"
                }
            ]
        },
        {
            "text": "As observed in [40], lung segmentation is crucial to holistic slice-level ILD classification. We empirically compare performance in two scenarios with a rough lung segmentation.11This can be achieved by segmenting the lung using simple label-fusion methods [48] In the first case, we overlay the target image slice with the average lung mask among the training folds. In the second, we perform simple morphology operations to obtain the lung boundary. In order to retain information from the inside of the lung, we apply Gaussian smoothing to the regions outside of the lung boundary. There is no significant difference between two setups. Due to the high precision of CNN based image processing, highly accurate lung segmentation is not necessary. The localization of ILD regions within the lung is simultaneously learned through selectively weighted CNN reception fields in the deepest convolutional layers during the classification based CNN training [49], [50]. Some areas outside of the lung appear in both healthy or diseased images. CNN training learns to ignore them by setting very small filter weights around the corresponding regions (Fig. 13). This observation is validated by [40].",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 257,
                    "end": 261,
                    "mention": "[48]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 954,
                    "end": 958,
                    "mention": "[49]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 960,
                    "end": 964,
                    "mention": "[50]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Interstitial Lung Disease Dataset ::: Datasets and Related Work",
            "ref_spans": [
                {
                    "start": 1146,
                    "end": 1153,
                    "mention": "Fig. 13",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Interstitial Lung Disease Dataset ::: Datasets and Related Work",
            "ref_spans": []
        },
        {
            "text": "CifarNet, introduced in [5], was the state-of-the-art model for object recognition on the Cifar10 dataset, which consists of 32\u00d732 images of 10 object classes. The objects are normally centered in the images. Some example images and class categories from the Cifar10 dataset are shown in Fig. 7. CifarNet has three convolution layers, three pooling layers, and one fully-connected layer. This CNN architecture, also used in [22] has about 0.15 million free parameters. We adopt it as a baseline model for the LN detection.\n\n\n",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 27,
                    "mention": "[5]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 424,
                    "end": 428,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Cifarnet ::: Convolutional Neural Network Architectures ::: Methods",
            "ref_spans": [
                {
                    "start": 288,
                    "end": 294,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF10"
                }
            ]
        },
        {
            "text": "The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. This success has revived the interest in CNNs [3] in computer vision. ImageNet consists of 1.2 million 256\u00d7256 images belonging to 1000 categories. At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model. More details about the ImageNet dataset will be discussed in Section III-B. AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters. AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 45,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 247,
                    "end": 250,
                    "mention": "[3]",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Alexnet ::: Convolutional Neural Network Architectures ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The GoogLeNet model proposed in [33], is significantly more complex and deep than all previous CNN architectures. More importantly, it also introduces a new module called \u201cInception\u201d, which concatenates filters of different sizes and dimensions into a single new filter (refer to Fig. 6). Overall, GoogLeNet has two convolution layers, two pooling layers, and nine \u201cInception\u201d layers. Each \u201cInception\u201d layer consists of six convolution layers and one pooling layer. An illustration of an \u201cInception\u201d layer \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$({\\tt inception3a})$\\end{document} from GoogLeNet is shown in Fig. 6. GoogLeNet is the current state-of-the-art CNN architecture for the ILSVRC challenge, where it achieved 5.5% top-5 classification error on the ImageNet challenge, compared to AlexNet's 15.3% top-5 classification error.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 36,
                    "mention": "[33]",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Googlenet ::: Convolutional Neural Network Architectures ::: Methods",
            "ref_spans": [
                {
                    "start": 280,
                    "end": 286,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 804,
                    "end": 810,
                    "mention": "Fig. 6",
                    "ref_id": "FIGREF9"
                }
            ]
        },
        {
            "text": "ImageNet [1] has more than 1.2 million 256\u00d7256 images categorized under 1000 object class categories. There are more than 1000 training images per class. The database is organized according to the WordNet [55] hierarchy, which currently contains only nouns in 1000 object categories. The image-object labels are obtained largely through crowd-sourcing, e.g., Amazon Mechanical Turk, and human inspection. Some examples of object categories in ImageNet are \u201csea snake\u201d, \u201csandwich\u201d, \u201cvase\u201d, \u201cleopard\u201d, etc. ImageNet is currently the largest image dataset among other standard datasets for visual recognition. Indeed, the Caltech101, Caltech256 and Cifar10 dataset merely contain \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$60000~32\\times 32$\\end{document} images and 10 object classes. Furthermore, due to the large number (1000+) of object classes, the objects belonging to each ImageNet class category can be occluded, partial and small, relative to those in the previous public image datasets. This significant intra-class variation poses greater challenges to any data-driven learning system that builds a classifier to fit given data and generalize to unseen data. For comparison, some example images of Cifar10 dataset and ImageNet images in the \u201ctennis ball\u201d class category are shown in Fig. 7. The ImageNet dataset is publicly available, and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has become the standard benchmark for large-scale object recognition.",
            "cite_spans": [
                {
                    "start": 9,
                    "end": 12,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 205,
                    "end": 209,
                    "mention": "[55]",
                    "ref_id": "BIBREF50"
                }
            ],
            "section": "Imagenet: Large Scale Annotated Natural Image Dataset ::: Methods",
            "ref_spans": [
                {
                    "start": 1484,
                    "end": 1490,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF10"
                }
            ]
        },
        {
            "text": "When learned from scratch, all the parameters of CNN models are initialized with random Gaussian distributions and trained for 30 epochs with the mini-batch size of 50 image instances. Training convergence can be observed within 30 epochs. The other hyperparameters are momentum: 0.9; weight decay: 0.0005; (base) learning rate: 0.01, decreased by a factor of 10 at every 10 epochs. We use the Caffe framework [56] and NVidia K40 GPUs to train the CNNs.",
            "cite_spans": [
                {
                    "start": 410,
                    "end": 414,
                    "mention": "[56]",
                    "ref_id": "BIBREF51"
                }
            ],
            "section": "Training Protocols and Transfer Learning ::: Methods",
            "ref_spans": []
        },
        {
            "text": "AlexNet and GoogLeNet CNN models can be either learned from scratch or fine-tuned from pre-trained models. Girshick et al. [6] find that, by applying ImageNet pre-trained AlexNet to PASCAL dataset [8], performances of semantic 20-class object detection and segmentation tasks significantly improve over previous methods that use no deep CNNs. AlexNet can be fine-tuned on the PASCAL dataset to surpass the performance of the ImageNet pre-trained AlexNet, although the difference is not as significant as that between the CNN and non-CNN methods. Similarly, [57], [58] also demonstrate that better performing deep models are learned via CNN transfer learning from ImageNet to other datasets of limited scales.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 126,
                    "mention": "[6]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 197,
                    "end": 200,
                    "mention": "[8]",
                    "ref_id": "BIBREF71"
                },
                {
                    "start": 557,
                    "end": 561,
                    "mention": "[57]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 563,
                    "end": 567,
                    "mention": "[58]",
                    "ref_id": "BIBREF53"
                }
            ],
            "section": "Training Protocols and Transfer Learning ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Our hypothesis on CNN parameter transfer learning is the following: despite the disparity between natural images and natural images, CNNs comprehensively trained on the large scale well-annotated ImageNet may still be transferred to make medical image recognition tasks more effective. Collecting and annotating large numbers of medical images still poses significant challenges. On the other hand, the mainstream deep CNN architectures (e.g., AlexNet and GoogLeNet) contain tens of millions of free parameters to train, and thus require sufficiently large numbers of labeled medical images.",
            "cite_spans": [],
            "section": "Training Protocols and Transfer Learning ::: Methods",
            "ref_spans": []
        },
        {
            "text": "For transfer learning, we follow the approach of [57], [6] where all CNN layers except the last are fine-tuned at a learning rate 10 times smaller than the default learning rate. The last fully-connected layer is random initialized and freshly trained, in order to accommodate the new object categories in our CADe applications. Its learning rate is kept at the original 0.01. We denote the models with random initialization or transfer learning as AlexNet-RI and AlexNet-TL, and GoogLeNet-RI and GoogLeNet-TL. We found that the transfer learning strategy yields the best performance results. Determining the optimal learning rate for different layers is challenging, especially for very deep networks such as GoogLeNet.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 53,
                    "mention": "[57]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 55,
                    "end": 58,
                    "mention": "[6]",
                    "ref_id": "BIBREF55"
                }
            ],
            "section": "Training Protocols and Transfer Learning ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We also perform experiments using \u201coff-the-shelf\u201d CNN features of AlexNet pre-trained on ImageNet and training only the final classifier layer to complete the new CADe classification tasks. Parameters in the convolutional and fully connected layers are fixed and are used as deep image extractors, as in [10], [9], [12]. We refer to this model as AlexNet-ImNet in the remainder of the paper. Note that [10], [9], [12] train support vector machines and random forest classifiers using ImageNet pre-trained CNN features. Our simplified implementation is intended to determine whether fine-tuning the \u201cend-to-end\u201d CNN network is necessary to improve performance, as opposed to merely training the final classification layer. This is a slight modification from the method described in [10], [9], [12].",
            "cite_spans": [
                {
                    "start": 304,
                    "end": 308,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 310,
                    "end": 313,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 315,
                    "end": 319,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 402,
                    "end": 406,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 408,
                    "end": 411,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 413,
                    "end": 417,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 781,
                    "end": 785,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 787,
                    "end": 790,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 792,
                    "end": 796,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Training Protocols and Transfer Learning ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Finally, transfer learning in CNN representation, as empirically verified in previous literature [59]\u2013[61], [11], [62] can be effective in various cross-modality imaging settings (RGB images to depth images [59], [60], natural images to general CT and MRI images [11], and natural images to neuroimaging [61] or ultrasound [62] data). More thorough theoretical studies on cross-modality imaging statistics and transferability will be needed for future studies.",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 101,
                    "mention": "[59]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 102,
                    "end": 102,
                    "mention": "",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 102,
                    "end": 106,
                    "mention": "[61]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 108,
                    "end": 112,
                    "mention": "[11]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 114,
                    "end": 118,
                    "mention": "[62]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 207,
                    "end": 211,
                    "mention": "[59]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 213,
                    "end": 217,
                    "mention": "[60]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 263,
                    "end": 267,
                    "mention": "[11]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 304,
                    "end": 308,
                    "mention": "[61]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 323,
                    "end": 327,
                    "mention": "[62]",
                    "ref_id": "BIBREF58"
                }
            ],
            "section": "Training Protocols and Transfer Learning ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We train and evaluate CNNs using three-fold cross-validation (folds are split into disjoint sets of patients), with the different CNN architectures described above. In testing, each LN candidate has multiple random 2.5D views tested by CNN classifiers to generate LN class probability scores. We follow the random view aggregation by averaging probabilities, as in [22].",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 369,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "We first sample the LN image patches at a \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$64\\times 64~{\\rm pixel}$\\end{document} resolution. We then up-sample the \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$64\\times 64~{\\rm pixel}$\\end{document} LN images via bi-linear interpolation to \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$256\\times 256~{\\rm pixels}$\\end{document}, in order to accommodate AlexNet-RI-L, AlexNet-TL-H, GoogLeNet-RI-H and GoogLeNet-TL-H. For the modified AlexNet-RI-L at (64\u00d764) pixel resolution, we reduce the number of first layer convolution filters from 96 to 64 and reduce the stride from 4 to 2. For the modified GoogLeNet-RI (64\u00d764), we decrease the number of first layer convolution filters from 64 to 32, the pad size from 3 to 2, the kernel size from 7 to 5, stride from 2 to 1 and the stride of the subsequent pooling layer from 2 to 1. We slightly reduce the number of convolutional filters in order to accommodate the smaller input image sizes of target medical image datasets [22], [37], while preventing over-fitting. This eventually improves performance on patch-based classification. CifarNet is used in [22] to detect LN samples of \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$32\\times 32\\times 3$\\end{document} images. For consistency purposes, we down-sample \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$64\\times 64\\times 3$\\end{document} resolution LN sample images to the dimension of \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$32\\times 32\\times 3$\\end{document}.",
            "cite_spans": [
                {
                    "start": 1585,
                    "end": 1589,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1591,
                    "end": 1595,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1716,
                    "end": 1720,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "Results for lymph node detection in the mediastinum and abdomen are reported in Table II. FROC curves are illustrated in Fig. 8. The area-under-the-FROC-curve (AUC) and true positive rate (TPR, recall or sensitivity) at three false positives per patient (TPR/3FP) are used as performance metrics. Of the nine investigated CNN models, CifarNet, AlexNet-ImNet and GoogLeNet-RI-H generally yielded the least competitive detection accuracy results. Our LN datasets are significantly more complex (i.e., display much larger within-class appearance variations), especially due to the extracted fields-of-view (FOVs) of (35 mm-128 mm) compared to (30 mm-45 mm) in [22], where CifarNet is also employed. In this experiment, CifarNet is under-trained with respect to our enhanced LN datasets, due to its limited input resolution and parameter complexity. The inferior performance of AlexNet-ImNet implies that using the pre-trained ImageNet CNNs alone as \u201coff-the-shelf\u201d deep image feature extractors may not be optimal or adequate for mediastinal and abdominal LN detection tasks. To complement \u201coff-the-shelf\u201d CNN features, [10], [9], [12] all add and integrate various other hand-crafted image features as hybrid inputs for the final CADe classification.\n\n\n\n",
            "cite_spans": [
                {
                    "start": 657,
                    "end": 661,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1123,
                    "end": 1126,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 121,
                    "end": 127,
                    "mention": "Fig. 8",
                    "ref_id": "FIGREF11"
                },
                {
                    "start": 80,
                    "end": 88,
                    "mention": "Table II",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "GoogLeNet-RI-H performs poorly, as it is susceptible to over-fitting. No sufficient data samples are available to train GoogLeNet-RI-H with random initialization. Indeed, due to GoogLeNet-RI-H's complexity and 22-layer depth, million-image datasets may be required to properly train this model. However, GoogLeNet-TL-H significantly improves upon GoogLeNet-RI-H (0.81 versus 0.61 TPR/3FP in mediastinum; 0.70 versus 0.48 TPR/3FP in abdomen). This indicates that transfer learning offers a much better initialization of CNN parameters than random initialization. Likewise, AlexNet-TL-H consistently outperforms AlexNet-RI-H, though by smaller margins (0.81 versus 0.79 TPR/3FP in mediastinum; 0.69 versus 0.67 TPR/3FP in abdomen). This is also consistent with the findings reported for ILD detection in Table III and Fig. 11.\n\n\n\n",
            "cite_spans": [],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 816,
                    "end": 823,
                    "mention": "Fig. 11",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 802,
                    "end": 811,
                    "mention": "Table III",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "GoogLeNet-TL-H yields results similar to AlexNet-TL-H's for the mediastinal LN detection, and slightly outperforms Alex-Net-H for abdominal LN detection. AlexNet-RI-H exhibits less severe over-fitting than GoogLeNet-RI-H. We also evaluate a simple ensemble by averaging the probability scores from five CNNs: AlexNet-RI-H, AlexNet-TL-H, AlexNet-RI-H, GoogLeNet-TL-H and GoogLeNet-RI-L. This combined ensemble outputs the classification accuracies matching or slightly exceeding the best performing individual CNN models on the mediastinal or abdominal LN detection tasks, respectively.",
            "cite_spans": [],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "Many of our CNN models achieve notably better (FROC-AUC and TPR/3FP) results than the previous state-of-the-art models [36] for mediastinal LN detection: GoogLeNet-RI-L obtains an \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}${\\rm AUC}=0.95$\\end{document} and 0.85 TPR/3FP, versus \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}${\\rm AUC}=0.92$\\end{document} and 0.70 TPR/3FP [22] and 0.78 TPR/3FP [36] which uses stacked shallow learning. This difference lies in the fact that annotated lymph node segmentation masks are required to learn a mid-level semantic boundary detector [36], whereas CNN approaches only need LN locations for training [22]. In abdominal LN detection, [22] obtains the best trade-off between its CNN model complexity and sampled data configuration. Our best performing CNN model is GoogLeNet-TL (256\u00d7256) which obtains an \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}${\\rm AUC}=0.92$\\end{document} and 0.70 TPR/3FP.",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 123,
                    "mention": "[36]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 754,
                    "end": 758,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 776,
                    "end": 780,
                    "mention": "[36]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 957,
                    "end": 961,
                    "mention": "[36]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "The main difference between our dataset preparation protocol and that from [22] is a more aggressive extraction of random views within a much larger range of FOVs. The usage of larger FOVs to capture more image spatial context is inspired by deep zoom-out features [44] that improve semantic segmentation. This image sampling scheme contributes to our best reported performance results in both mediastinal LN detection (in this paper) and automated pancreas segmentation [45]. As shown in Fig. 1, abdominal LNs are surrounded by many other similar looking objects. Meanwhile, mediastinal LNs are more easily distinguishable, due to the images' larger spatial contexts. Finally, from the perspective of the data-model trade-off: \u201cDo We Need More Training Data or Better Models?\u201d [51], more abdomen CT scans from distinct patient populations need to be acquired and annotated, in order to take full advantage of deep CNN models of high capacity. Nevertheless, deeper and wider CNN models (e.g., GoogLeNet-RI-L and GoogLeNet-TL-H versus Cifar-10 [22]) have shown improved results in the mediastinal LN detection.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 265,
                    "end": 269,
                    "mention": "[44]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 471,
                    "end": 475,
                    "mention": "[45]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 778,
                    "end": 782,
                    "mention": "[51]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 489,
                    "end": 495,
                    "mention": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Fig. 9 provides examples of misclassified lymph nodes (in axial view) (both false negatives (Left) and false positives(Right)), from the Abdomen and Mediastinum datasets. The overall reported LN detection results are clinically significant, as indicated in [63].",
            "cite_spans": [
                {
                    "start": 257,
                    "end": 261,
                    "mention": "[63]",
                    "ref_id": "BIBREF59"
                }
            ],
            "section": "Thoracoabdominal Lymph Node Detection ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 6,
                    "mention": "Fig. 9",
                    "ref_id": "FIGREF12"
                }
            ]
        },
        {
            "text": "The CNN models evaluated in this experiment are 1) AlexNet-RI (training from scratch on the ILD dataset with random initialization); 2) AlexNet-TL (with transfer learning from [4]); 3) AlexNet-ImNet: pre-trained ImageNet-CNN model [4] with only the last cost function layer retrained from random initialization, according to the six ILD classes (similar to [9] but without using additional hand-crafted non-deep feature descriptors, such as GIST and BoVW); 4) GoogLeNet-RI (random initialization); 5) GoogLeNet-TL (GoogLeNet with transfer learning from [33]). All ILD images (patches of 64\u00d764 and CT axial slices of 512\u00d7512) are re-sampled to a fixed dimension of \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$256\\times 256~{\\rm pixels}$\\end{document}.",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 179,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 231,
                    "end": 234,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 357,
                    "end": 360,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 553,
                    "end": 557,
                    "mention": "[33]",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Interstitial Lung Disease Classification ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "We evaluate the ILD classification task with five-fold CV on patient-level split, as it is more informative for real clinical performance than LOO. The classification accuracy rates for interstitial lung disease detection are shown in Table III. Two sub-tasks on ILD patch and slice classifications are conducted. In general, patch-level ILD classification is less challenging than slice-level classification, as far more data samples can be sampled from the manually annotated ROIs (up to 100 image patches per ROI), available from [37]. From Table III, all five deep models evaluated obtain comparable results within the range of classification accuracy rates \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$[0.74,0.76]$\\end{document}. Their averaged model achieves a slightly better accuracy of 0.79.",
            "cite_spans": [
                {
                    "start": 533,
                    "end": 537,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                }
            ],
            "section": "Interstitial Lung Disease Classification ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 235,
                    "end": 244,
                    "mention": "Table III",
                    "ref_id": null
                },
                {
                    "start": 544,
                    "end": 553,
                    "mention": "Table III",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "F1-scores [38], [39], [54] and the confusion matrix (Table V) for patch-level ILD classification using GoogLeNet-TL under five-fold cross-validation (we denote as Patch-CV5) are also computed. F1-scores are reported on patch classification only (\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$32\\times 32~{\\rm pixel}$\\end{document} patches extracted from manual ROIs) [38], [39], [54], as shown in Table IV. Both [38] and [39] use the evaluation protocol of \u201cleave-one-patient-out\u201d (LOO), which is arguably much easier and not directly comparable to 10-fold CV [54] or our Patch-CV5. In this study, we classify six ILD classes by adding a consolidation (CD) class to five classes of healthy (normal\u2014NM), emphysema (EM), ground glass (GG), fibrosis (FB), and micronodules (MN) in [38], [39], [54]. Patch-CV10 [54] and Patch-CV5 report similar medium to high F-scores. This implies that the ILD dataset (although one of the mainstream public medical image datasets) may not adequately represent ILD disease CT lung imaging patterns, over a population of only 120 patients. Patch-CV5 yields higher F-scores than [54] and classifies the extra consolidation (CD) class. At present, the most pressing task is to drastically expand the dataset or to explore across-dataset deep learning on the combined ILD and LTRC datasets [64].\n\n\n",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 14,
                    "mention": "[38]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 16,
                    "end": 20,
                    "mention": "[39]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 22,
                    "end": 26,
                    "mention": "[54]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 557,
                    "end": 561,
                    "mention": "[38]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 563,
                    "end": 567,
                    "mention": "[39]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 569,
                    "end": 573,
                    "mention": "[54]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 602,
                    "end": 606,
                    "mention": "[38]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 611,
                    "end": 615,
                    "mention": "[39]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 750,
                    "end": 754,
                    "mention": "[54]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 968,
                    "end": 972,
                    "mention": "[38]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 974,
                    "end": 978,
                    "mention": "[39]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 980,
                    "end": 984,
                    "mention": "[54]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "mention": "[54]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "mention": "[54]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1507,
                    "end": 1511,
                    "mention": "[64]",
                    "ref_id": "BIBREF60"
                }
            ],
            "section": "Interstitial Lung Disease Classification ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 53,
                    "end": 60,
                    "mention": "Table V",
                    "ref_id": null
                },
                {
                    "start": 587,
                    "end": 595,
                    "mention": "Table IV",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "Recently, Gao et al. [40] have argued that a new CADe protocol on holistic classification of ILD diseases directly, using axial CT slice attenuation patterns and CNN, may be more realistic for clinical applications. We refer to this as slice-level classification, as image patch sampling from manual ROIs can be completely avoided (hence, no manual ROI inputs will be provided). The experimental results in [40] are conducted with a patient-level hard split of 100 (training) and 20 (testing). The method's testing F-scores (i.e., Slice-Test) are given in Table IV. Note that the F-scores in [40] are not directly comparable to our results, due to different evaluation criteria. Only Slice-Test is evaluated and reported in [40], and we find that F-scores can change drastically from different rounds of the five-fold CV.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 25,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 407,
                    "end": 411,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 592,
                    "end": 596,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 724,
                    "end": 728,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Interstitial Lung Disease Classification ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 556,
                    "end": 564,
                    "mention": "Table IV",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "While it is a more practical CADe scheme, slice-level CNN learning [40] is very challenging, as it is restricted to only 905 CT image slices with tagged ILD labels. We only benchmark the slice-level ILD classification results in this section. Even with the help of data augmentation (described in Section II), the classification accuracy of GoogLeNet-TL from Table III is only 0.57. However, transfer learning from ImageNet pre-trained model is consistently beneficial, as evidenced by AlexNet-TL (0.46) versus AlexNet-RI (0.44), and GoogLeNet-TL (0.57) versus GoogLeNet-RI (0.41). It especially prevents GoogLeNet from over-fitting on the limited CADe datasets. Finally, when the cross-validation is conducted by randomly splitting the set of all 905 CT axial slices into five folds, markedly higher F-scores are obtained (Slice-Random in Table IV). This further validates the claim that the dataset poorly generalizes ILDs for different patients. Fig. 10 shows examples of misclassified ILD patches (in axial view), with their ground truth labels and inaccurately classified labels.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 71,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Interstitial Lung Disease Classification ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 949,
                    "end": 956,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 359,
                    "end": 368,
                    "mention": "Table III",
                    "ref_id": null
                },
                {
                    "start": 840,
                    "end": 848,
                    "mention": "Table IV",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "No existing work has reached the performance requirements for a realistic clinical setting [40], in which simple ROI-guided image patch extraction and classification (which requires manual ROI selection by clinicians) is implemented. The main goal of this paper is to investigate the three factors (CNN architectures, dataset characteristics and transfer learning) that affect performance on a specific medical image analysis problem and to ultimately deliver clinically relevant results. For ILD classification, the most critical performance bottlenecks are the challenge of cross-dataset learning and the limited patient population size. We attempt to overcome these obstacles by merging the ILD [37] and LTRC datasets. Although the ILD [37] and LTRC datasets [64] (used in [19]) were generated and annotated separately, they contain many common disease labels. For instance, the ILD disease classes emphysema (EM), ground glass (GG), fibrosis (FB), and micronodules (MN) belong to both datasets, and thus can be jointly trained/tested to form a larger and unified dataset.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 95,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 698,
                    "end": 702,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 739,
                    "end": 743,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 762,
                    "end": 766,
                    "mention": "[64]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 776,
                    "end": 780,
                    "mention": "[19]",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Interstitial Lung Disease Classification ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "Adapting fully convolutional CNN or FCNN to parse every pixel location in the ILD lung CT images or slices, or adapting other methods from CNN based semantic image segmentation using PASCAL or ImageNet, may improve accuracy and efficiency. However, current FCNN approaches [65], [66] lack adequate spatial resolution in their directly output label space. A segmentation label propagation method was recently proposed [47] to provide full pixel-wise labeling of the ILD data images. In this work, we sample image patches from the slice using the ROIs for the ILD provided in the dataset, in order to be consistent with previous methods in patch-level [38], [39], [54] and slice-level classification [40].",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 277,
                    "mention": "[65]",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 279,
                    "end": 283,
                    "mention": "[66]",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 417,
                    "end": 421,
                    "mention": "[47]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 650,
                    "end": 654,
                    "mention": "[38]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 656,
                    "end": 660,
                    "mention": "[39]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 662,
                    "end": 666,
                    "mention": "[54]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 698,
                    "end": 702,
                    "mention": "[40]",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Interstitial Lung Disease Classification ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "OverFeat is described in [67] as an integrated framework for using CNN for classification, localization and detection. Its architecture is similar to that of AlexNet, but contains far more parameters (e.g., 1024 convolution filters in both \u201cconv4\u201d and \u201cconv5\u201d layers compared to 384 and 256 convolution kernels in the \u201cconv4\u201d and \u201cconv5\u201d layers of AlexNet), and operates more densely (e.g., smaller kernel size of 2 in \u201cpool2\u201d layer \u201cpool5\u201d compared to the kernel size 3 in \u201cpool2\u201d and \u201cpool5\u201d of AlexNet) on the input image. Overfeat is the winning model of the ILSVRC 2013 in detection and classification tasks.",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 29,
                    "mention": "[67]",
                    "ref_id": "BIBREF63"
                }
            ],
            "section": "Overfeat ::: Evaluation of Five CNN Models Using ILD Classification ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "The VGGNet architecture is introduced in [68], where it is designed to significantly increase the depth of the existing CNN architectures with 16 or 19 layers. Very small 3\u00d73 size convolutional filters are used in all convolution layers with a convolutional stride of size 1, in order to reduce the number of parameters in deeper networks. Since VGGNet is substantially deeper than the other CNN models, VGGNet is more susceptible to the vanishing gradient problem [69]\u2013[71]. Hence, the network may be more difficult to train. Training the network requires far more memory and computation time than AlexNet. We use the 16 layer variant as our default VGGNet model in our study.",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 45,
                    "mention": "[68]",
                    "ref_id": "BIBREF64"
                },
                {
                    "start": 465,
                    "end": 469,
                    "mention": "[69]",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 470,
                    "end": 470,
                    "mention": "",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 470,
                    "end": 474,
                    "mention": "[71]",
                    "ref_id": "BIBREF68"
                }
            ],
            "section": "Vggnet ::: Evaluation of Five CNN Models Using ILD Classification ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "The classification accuracy results for ILD slice and patch level classification of five CNN architectures (CifarNet, AlexNet, Overfeat, VGGNet and GoogLeNet) are shown in Table VI. Based on the analysis in Section IV-B, transfer learning is only used for the slice level classification task. From Table VI, quantitative classification accuracy rates increase as the CNN model becomes more complex (CifarNet, AlexNet, Overfeat, VGGNet and GoogLeNet, in ascending order), for both ILD slice and patch level classification problems. The reported results validate our assumption that OverFeat's and VGGNet's performance levels fall between AlexNet's and GoogLeNet's (this observation is consistent with the computer vision findings). CifarNet is designed for images with smaller dimensions (32\u00d732 images), and thus is not catered to classification tasks involving 256\u00d7256 images.\n\n",
            "cite_spans": [],
            "section": "Vggnet ::: Evaluation of Five CNN Models Using ILD Classification ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 172,
                    "end": 180,
                    "mention": "Table VI",
                    "ref_id": null
                },
                {
                    "start": 298,
                    "end": 306,
                    "mention": "Table VI",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "To investigate the performance difference between five-fold cross-validation (CV) in Section IV-B and leave-one-patient-out (LOO) validation, this experiment is performed under the LOO protocol. By comparing results in Table III (CV-5) to those in Table VI (LOO), one can see that LOO's quantitative performances are remarkably better than CV-5's. For example, in ILD slice-level classification, the accuracy level drastically increases from 0.46 to 0.867 using AlexNet-TL, and from 0.57 to 0.902 for GoogLeNet-TL.",
            "cite_spans": [],
            "section": "Vggnet ::: Evaluation of Five CNN Models Using ILD Classification ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 219,
                    "end": 228,
                    "mention": "Table III",
                    "ref_id": null
                },
                {
                    "start": 248,
                    "end": 256,
                    "mention": "Table VI",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "CNN training is implemented with the Caffe [56] deep learning framework, using a NVidia K40 GPU on Ubuntu 14.04 Linux OS. All models are trained for up to 90 epochs with early stopping criteria, where a model snapshot with low validation loss is taken for the final model. Other hyper-parameters are fixed as follows: momentum: 0.9; weight decay: 0.0005; and a step learning rate schedule with base learning rate of 0.01, decreased by a factor of 10 every 30 epochs. The image batch size is set to 128, except for GoogLeNet's (64) and VGG-16's (32), which are the maximum batch sizes that can fit in the NVidia K40 GPU with 12GB of memory capacity. Table VII illustrates the training time and memory requirements of the five CNN architectures on ILD patch-based classification up to 90 epochs.\n\n",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 47,
                    "mention": "[56]",
                    "ref_id": "BIBREF51"
                }
            ],
            "section": "Vggnet ::: Evaluation of Five CNN Models Using ILD Classification ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 649,
                    "end": 658,
                    "mention": "Table VII",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "Medical datasets are often \u201cbiased\u201d, in that the number of healthy samples is much larger than the number of diseased instances, or that the numbers of images per class are uneven. In ILD dataset, the number of fibrosis samples is about 3.5 times greater than the number of emphysema samples. The number of non-LNs is \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$3\\sim 4$\\end{document} times greater than the number of LNs in lymph node detection. Different sampling or resampling rates are routinely applied to both ILD and LN detection to balance the data sample number or scale per class, as in [22]. We refer this as \u201cEqual Prior\u201d. If we use the same sampling rate, that will lead to a \u201cBiased Prior\u201d across different classes.",
            "cite_spans": [
                {
                    "start": 789,
                    "end": 793,
                    "mention": "[22]",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Training With \u201cequal Prior\u201d vs.\u201cbiased Prior\u201d ::: Evaluations and Discussions",
            "ref_spans": []
        },
        {
            "text": "Without loss of generality, after GoogLeNet is trained on the training sets under \u201cEqual\u201d or \u201cBiased\u201d priors, we compare its classification results on the balanced validation sets. Evaluating a classifier on a biased validation set will cause unfair assessment of its performance. For instance, a classifier that predicts every image patch as \u201cnon-LN\u201d will still achieve a 70% accuracy rate on a biased set with 3.5 times as many non-LN samples as LN samples. The classification accuracy results of GoogLeNet trained under two configurations are shown in Table VIII. Overall, it achieves lower accuracy results when trained with a \u201cbiased prior\u201d in both tasks, and the accuracy difference for ILD patch-based classification is small.\n\n",
            "cite_spans": [],
            "section": "Training With \u201cequal Prior\u201d vs.\u201cbiased Prior\u201d ::: Evaluations and Discussions",
            "ref_spans": [
                {
                    "start": 555,
                    "end": 565,
                    "mention": "Table VIII",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "In Fig. 12, the first layer convolution filters from five different CNN architectures are visualized. We notice that without transfer learning [57], [6], somewhat blurry filters are learned (AlexNet-RI (256\u00d7256), AlexNet-RI (64\u00d764), GoogLeNet-RI (256\u00d7256) and GoogLeNet-RI (64\u00d764)). However, in AlexNet-TL (256\u00d7256), many higher orders of contrast- or edge-preserving patterns (that enable capturing image appearance details) are evidently learned through fine-tuning from ImageNet. With a smaller input resolution, AlexNet-RI (64\u00d764) and GoogLeNet-RI (64\u00d764) can learn image contrast filters to some degree; whereas, GoogLeNet-RI (256\u00d7256) and AlexNet-RI (256\u00d7256) have over-smooth low-level filters throughout.\n\n",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 147,
                    "mention": "[57]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 149,
                    "end": 152,
                    "mention": "[6]",
                    "ref_id": "BIBREF55"
                }
            ],
            "section": "Thoracoabdominal LN Detection ::: Analysis Via CNN Learning Traces and Lulvisualization",
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "mention": "Fig. 12",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "We focus on analyzing visual CNN optimization traces and activations from the ILD dataset, as its slice-level setting is most similar to ImageNet's. Indeed, both datasets use full-size images. The traces of the training loss, validation loss and validation accuracy of AlexNet-RI and AlexNet-TL, are shown in Fig. 11. For AlexNet-RI in Fig. 11(a), the training loss significantly decreases as the number of training epochs increases, while the validation loss notably increases and the validation accuracy does not improve much before reaching a plateau. With transfer learning and fine-tuning, much better and consistent performances of training loss, validation loss and validation accuracy traces are obtained (see Fig. 11(b)). We begin the optimization problem\u2014that of fine-tuning the ImageNet pre-trained CNN to classify a comprehensive set of images\u2014by initializing the parameters close to an optimal solution. One could compare this process to making adults learn to classify ILDs, as opposed to babies. During the process, the validation loss, having remained at lower values throughout, achieves higher final accuracy levels than the validation loss on a similar problem with random initialization. Meanwhile, the training losses in both cases decrease to values near zero. This indicates that both AlexNet-RI and AlexNet-TL over-fit on the ILD dataset, due to its small instance size. The quantitative results in Table III indicate that AlexNet-TL and GoogLeNet-TL have consistently better classification accuracies than AlexNet-RI and GoogLeNet-RI, respectively.",
            "cite_spans": [],
            "section": "ILD Classification ::: Analysis Via CNN Learning Traces and Lulvisualization",
            "ref_spans": [
                {
                    "start": 309,
                    "end": 316,
                    "mention": "Fig. 11",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 336,
                    "end": 346,
                    "mention": "Fig. 11(a)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 718,
                    "end": 728,
                    "mention": "Fig. 11(b)",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1423,
                    "end": 1432,
                    "mention": "Table III",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "The last pooling layer (pool-5) activation maps of the ImageNet pre-trained AlexNet [4] (analogical to AlexNet-ImNet) and AlexNet-TL, obtained by processing two input images of Figs. 2(b), 2(c), are shown in Figs. 13(a), 13(b). The last pooling layer activation map summarizes the entire input image by highlighting which relative locations or neural reception fields relative to the image are activated. There are a total of 256 (6\u00d76) reception fields in AlexNet [4]. Pooling units where the relative image location of the disease region is present in the image are highlighted with green boxes. Next, we reconstruct the original ILD images using the process of de-convolution, back-propagating with convolution and un-pooling from the activation maps of the chosen pooling units [72]. From the reconstructed images (Fig. 13 bottom), we observe that with fine-tuning, AlexNet-TL detects and localizes objects of interest (ILD disease regions depicted in in Figs. 2(b) and 2(c)) better than AlexNet-ImNet. The filters shown in Fig. 13 that better localize regions on the input images (Figs. 2(b) and 2(c)) respectively, produce relatively higher activations (in the top 5%) among all 512 reception field responses in the fine-tuned AlexNet-TL model. As observed in [73], the final CNN classification score can not be driven solely by a single strong activation in the receptions fields, but often by a sparse set of high activations (i.e., varying selective or sparse activations per input image).\n\n",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 87,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 464,
                    "end": 467,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 781,
                    "end": 785,
                    "mention": "[72]",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 1265,
                    "end": 1269,
                    "mention": "[73]",
                    "ref_id": "BIBREF70"
                }
            ],
            "section": "ILD Classification ::: Analysis Via CNN Learning Traces and Lulvisualization",
            "ref_spans": [
                {
                    "start": 177,
                    "end": 187,
                    "mention": "Figs. 2(b)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 189,
                    "end": 193,
                    "mention": "2(c)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 208,
                    "end": 219,
                    "mention": "Figs. 13(a)",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 221,
                    "end": 226,
                    "mention": "13(b)",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 818,
                    "end": 825,
                    "mention": "Fig. 13",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 958,
                    "end": 968,
                    "mention": "Figs. 2(b)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 973,
                    "end": 977,
                    "mention": "2(c)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1027,
                    "end": 1034,
                    "mention": "Fig. 13",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1085,
                    "end": 1095,
                    "mention": "Figs. 2(b)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "mention": "2(c)",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "We summarize our findings as follows.\n\u2022Deep CNN architectures with 8, even 22 layers [4], [33], can be useful even for CADe problems where the available training datasets are limited. Previously, CNN models used in medical image analysis applications have often been \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$2\\sim 5$\\end{document} orders of magnitude smaller.\u2022The trade-off between using better learning models and using more training data [51] should be carefully considered when searching for an optimal solution to any CADe problem (e.g., mediastinal and abdominal LN detection).\u2022Limited datasets can be a bottleneck to further advancement of CADe. Building progressively growing (in scale), well annotated datasets is at least as crucial as developing new algorithms. This has been accomplished, for instance, in the field of computer vision. The well-known scene recognition problem has made tremendous progress, thanks to the steady and continuous development of Scene-15, MIT Indoor-67, SUN-397 and Place datasets [58].\u2022Transfer learning from the large scale annotated natural image datasets (ImageNet) to CADe problems has been consistently beneficial in our experiments. This sheds some light on cross-dataset CNN learning in the medical image domain, e.g., the union of the ILD [37] and LTRC datasets [64], as suggested in this paper.\u2022Finally, applications of off-the-shelf deep CNN image features to CADe problems can be improved by either exploring the performance-complementary properties of hand-crafted features [10], [9], [12], or by training CNNs from scratch and better fine-tuning CNNs on the target medical image dataset, as evaluated in this paper.",
            "cite_spans": [
                {
                    "start": 85,
                    "end": 88,
                    "mention": "[4]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 90,
                    "end": 94,
                    "mention": "[33]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 635,
                    "end": 639,
                    "mention": "[51]",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "mention": "[58]",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 1483,
                    "end": 1487,
                    "mention": "[37]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1506,
                    "end": 1510,
                    "mention": "[64]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 1722,
                    "end": 1726,
                    "mention": "[10]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1728,
                    "end": 1731,
                    "mention": "[9]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 1733,
                    "end": 1737,
                    "mention": "[12]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Findings and Future Directions",
            "ref_spans": []
        },
        {
            "text": "In this paper, we exploit and extensively evaluate three important, previously under-studied factors on deep convolutional neural networks (CNN) architecture, dataset characteristics, and transfer learning. We evaluate CNN performance on two different computer-aided diagnosis applications: thoraco-abdominal lymph node detection and interstitial lung disease classification. The empirical evaluation, CNN model visualization, CNN performance analysis, and conclusive insights can be generalized to the design of high performance CAD systems for other medical imaging tasks.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "FIGREF0": {
            "text": "Fig.\u00a01.: Some examples of abdominal and mediastinal lymph nodes sampled on axial (ax), coronal (co), and sagittal (sa) views, with four different fields-of-views (30 mm: orange; 45 mm: red; 85 mm: green; 128 mm: blue) surrounding lymph nodes.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig.\u00a010.: Visual examples of misclassified ILD 64\u00d764 patches (in axial view), with their ground truth labels and inaccurately classified labels.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig.\u00a011.: Traces of training and validation loss (blue and green lines) and validation accuracy (orange lines) during (a) training alexnet from random initialization and (b) fine-tuning from imagenet pre-trained cnn, for ILD classification.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig.\u00a012.: Visualization of first layer convolution filters of CNNs trained on abdominal and mediastinal LNs in RGB color, from random initialization (alexnet-RI (256\u00d7256), alexnet-RI (64\u00d764), googlenet-RI (256\u00d7256) and googlenet-RI (64\u00d764)) and with transfer learning (alexnet-TL (256\u00d7256)).",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig.\u00a013.: Visualization of the last pooling layer (pool-5) activations (top). Pooling units where the relative image location of the disease region is located in the image are highlighted with green boxes. The original images reconstructed from the units are shown in the bottom [72]. The examples in (a) and (b) are computed from the input ILD images in Figs. 2(b) and 2(c), respectively.",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Fig.\u00a02.: Some examples of CT image slices with six lung tissue types in the ILD dataset [37]. Disease tissue types are located with dark orange arrows. (a): healthy; (b): emphysema; (c): ground glass; (d): fibrosis; (e): micronodules; (f): consolidation.",
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Fig.\u00a03.: Some examples of \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}$64\\times 64~{\\rm pixel}$\\end{document} CT image patches for (a) Nm, (b) Em, (c) Gg, (d) Fb, (e) MN (f) CD.",
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Fig.\u00a04.: An example of lung/high-attenuation/low-attenuation CT windowing for an axis lung CT slice. We encode the lung/high-attenuation/low-attenuation CT windowing into red/green/blue channels.",
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Fig.\u00a05.: A simplified illustration of the CNN architectures used. Googlenet [33] contains two convolution layers, three pooling layers, and nine inception layers. Each of the inception layer of googlenet consists of six convolution layers and one pooling layer.",
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Fig.\u00a06.: Illustration of \\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym} \n\\usepackage{amsfonts} \n\\usepackage{amssymb} \n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n}{}${\\tt inception3a}$\\end{document} layer of googlenet. Inception layers of googlenet consist of six convolution layers with different kernel sizes and one pooling layer.",
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Fig.\u00a07.: Some examples of cifar10 dataset and some images of \u201ctennis ball\u201d class from imagenet dataset. Images of cifar10 dataset are small (32\u00d732) images with object of the image class category in the center. Images of imagenet dataset are larger (256\u00d7256), where object of the image class category can be small, obscure, partial, and sometimes in a cluttered environment.",
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Fig.\u00a08.: FROC curves averaged on three-fold CV for the abdominal (left) and mediastinal (right) lymph nodes using different CNN models.",
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Fig.\u00a09.: Examples of misclassified lymph nodes (in axial view) of both false negatives (left) and false positives (right). Mediastinal LN examples are shown in the upper row, and abdominal LN examples in the bottom row.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "ImageNet: A large-scale hierarchical image database",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "248-255",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Chest pathology detection using deep learning with non-medical training",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Bar",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Diamant",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Greenspan",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE 12th Int. Symp. Biomed. Imag.",
            "volume": "",
            "issn": "",
            "pages": "294-297",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Interleaved text/image deep mining on a large-scale radiology image database",
            "authors": [
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "1090-1099",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Automatic classification of pulmonary peri-fissural nodules in computed tomography using an ensemble of 2d views and a convolutional neural network out-of-the-box",
            "authors": [
                {
                    "first": "F.",
                    "middle": [],
                    "last": "Ciompi",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Med. Image Anal.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "The multimodal brain tumor image segmentation benchmark (BRATS)",
            "authors": [
                {
                    "first": "B.",
                    "middle": [],
                    "last": "Menze",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Reyes",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Van Leemput",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Med. Imag.",
            "volume": "34",
            "issn": "10",
            "pages": "1993-2024",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Brain tumor grading based on neural networks and convolutional neural networks",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. 37th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.",
            "volume": "",
            "issn": "",
            "pages": "699-702",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Multi-scale convolutional neural networks for lung nodule classification",
            "authors": [
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "F.",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IPMI",
            "volume": "",
            "issn": "",
            "pages": "588-599",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Unregistered multiview mammogram analysis with pre-trained deep learning models",
            "authors": [
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Carneiro",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Nascimento",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [
                        "P."
                    ],
                    "last": "Bradley",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": "652-660",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Automatic coronary calcium scoring in cardiac CT angiography using convolutional neural networks",
            "authors": [
                {
                    "first": "J.",
                    "middle": [
                        "M."
                    ],
                    "last": "Wolterink",
                    "suffix": ""
                },
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Leiner",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [
                        "A."
                    ],
                    "last": "Viergever",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "I\u0161gum",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": "589-596",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Schlegl",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Ofner",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Langs",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Medical Comput. Vision: Algorithms for Big Data",
            "volume": "",
            "issn": "",
            "pages": "82-93",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "Mapping visual features to semantic profiles for retrieval in medical imaging",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Hofmanninger",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Langs",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "457-465",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "ImageNet large scale visual recognition challenge",
            "authors": [
                {
                    "first": "O.",
                    "middle": [],
                    "last": "Russakovsky",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ArXiv:1409.0575",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Combining multiple dynamic models and deep learning architectures for tracking the left ventricle endocardium in ultrasound data",
            "authors": [
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Carneiro",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Nascimento",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "35",
            "issn": "11",
            "pages": "2592-2607",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Deep learning based imaging data completion for improved brain disease diagnosis",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Improving computer-aided detection using convolutional neural networks and random view aggregation",
            "authors": [
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Med. Imag.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Automatic detection and segmentation of lymph nodes from CT data",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Barbu",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Trans. Med. Imag.",
            "volume": "31",
            "issn": "2",
            "pages": "240-250",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Lymph node detection and segmentation in chest CT data using discriminative learning and a spatial prior",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Feulner",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [
                        "K."
                    ],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Hammon",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Hornegger",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Comaniciu",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Med. Image Anal.",
            "volume": "17",
            "issn": "2",
            "pages": "254-270",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Mediastinal atlas creation from 3-d chest computed tomography images: Application to automated detection and station mapping of lymph nodes",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Feuerstein",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Med. Image Anal.",
            "volume": "16",
            "issn": "1",
            "pages": "63-74",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Devarakota",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Vikal",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Medical Comput. Vision. Large Data in Medical Imaging",
            "volume": "",
            "issn": "",
            "pages": "161-174",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "Effective 3d object detection and regression using probabilistic segmentation features in CT images",
            "authors": [
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Bi",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Salganicoff",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "1049-1056",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "Accurate polyp segmentation for 3d CT colonography using multi-staged probabilistic binary learning and compositional model",
            "authors": [
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE Comput. Vis. Pattern Recognit. Conf.",
            "volume": "",
            "issn": "",
            "pages": "1-8",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Computer-aided pulmonary embolism detection using a novel vessel-aligned multi-planar image representation and convolutional neural networks",
            "authors": [
                {
                    "first": "N.",
                    "middle": [],
                    "last": "Tajbakhsh",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [
                        "B."
                    ],
                    "last": "Gotway",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Gradient-based learning applied to document recognition",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "LeCun",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Haffner",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proc. IEEE",
            "volume": "86",
            "issn": "11",
            "pages": "2278-2324",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "Distinctive image features from scale-invariant keypoints",
            "authors": [
                {
                    "first": "D.",
                    "middle": [
                        "G."
                    ],
                    "last": "Lowe",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Int. J. Comput. Vis.",
            "volume": "60",
            "issn": "2",
            "pages": "91-110",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Histograms of oriented gradients for human detection",
            "authors": [
                {
                    "first": "N.",
                    "middle": [],
                    "last": "Dalal",
                    "suffix": ""
                },
                {
                    "first": "B.",
                    "middle": [],
                    "last": "Triggs",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
            "volume": "1",
            "issn": "",
            "pages": "886-893",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "Small codes and large image databases for recognition",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "1-8",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "Going deeper with convolutions",
            "authors": [
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "1-9",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "Return of the devil in the details: Delving deep into convolutional nets",
            "authors": [
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Chatfield",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Br. Mach. Vis. Conf.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "The devil is in the details: An evaluation of recent feature encoding methods",
            "authors": [
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Chatfield",
                    "suffix": ""
                },
                {
                    "first": "V.",
                    "middle": [
                        "S."
                    ],
                    "last": "Lempitsky",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Br. Mach. Vis. Conf.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "Leveraging mid-level semantic boundary cues for computer-aided lymph node detection",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Seff",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Barbu",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                },
                {
                    "first": "H.-C.",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [
                        "M."
                    ],
                    "last": "Summers",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "Building a reference multimedia database for interstitial lung diseases",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Depeursinge",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Comput. Med. Imaging and. Grap.",
            "volume": "36",
            "issn": "3",
            "pages": "227-238",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "Feature-based image patch approximation for lung tissue classification",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [
                        "D."
                    ],
                    "last": "Feng",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Med. Imag.",
            "volume": "32",
            "issn": "4",
            "pages": "797-808",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "Large margin local estimate with applications to medical image classification",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Med. Imag.",
            "volume": "34",
            "issn": "6",
            "pages": "1362-1377",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "ImageNet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [
                        "E."
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proc. NIPS",
            "volume": "",
            "issn": "",
            "pages": "1097-1105",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "Holistic classification of CT attenuation patterns for interstitial lung diseases via deep convolutional neural networks",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. MICCAI First Workshop Deep Learn. Med. Image Anal.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "2d view aggregation for lymph node detection using a shallow hierarchy of linear classifiers",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Seff",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": "544-552",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "Coarse-to-fine classification via parametric and nonparametric models for computer-aided diagnosis",
            "authors": [
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "X.",
                    "middle": [],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "ACM Conf. CIKM",
            "volume": "",
            "issn": "",
            "pages": "2509-2512",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "Learning hierarchical features for scene labeling",
            "authors": [
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Farabet",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Couprie",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Najman",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "LeCun",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "35",
            "issn": "8",
            "pages": "1915-1929",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "Feedforward semantic segmentation with zoom-out features",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Mostajabi",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Yadollahpour",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Shakhnarovich",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ArXiv Preprint ArXiv:1412.0774",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "DeepOrgan: Multi-level deep convolutional networks for automated pancreas segmentation",
            "authors": [
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "Human parsing with contextualized convolutional neural network",
            "authors": [
                {
                    "first": "X.",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. IEEE Int. Conf. Comput. Vis.",
            "volume": "",
            "issn": "",
            "pages": "1386-1394",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "Segmentation label propagation using deep convolutional neural networks and dense conditional random field",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Nogues",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [
                        "R."
                    ],
                    "last": "Summers",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Mollura",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. IEEE Int. Conf. Comput. Vis.",
            "volume": "",
            "issn": "",
            "pages": "1377-1385",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "Multi-atlas segmentation with joint label fusion",
            "authors": [
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "35",
            "issn": "3",
            "pages": "611-623",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF43": {
            "title": "Is object localization for free?\u2014Weakly-supervised learning with convolutional neural networks",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Oquab",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Laptev",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "685-694",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Learning multiple layers of features from tiny images",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "Learning and transferring mid-level image representations using convolutional neural networks",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Oquab",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Bottou",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Laptev",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Josef",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "1717-1724",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "Do we need more training data or better models for object detection?",
            "authors": [
                {
                    "first": "X.",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Vondrick",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Fowlkes",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Br. Mach. Vis. Conf.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "Mitosis detection in breast cancer histology images with deep neural networks",
            "authors": [
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Ciresan",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Giusti",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Gambardella",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "Deep convolutional neural networks for multi-modality isointense infant brain image segmentation",
            "authors": [
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "NeuroImage",
            "volume": "108",
            "issn": "",
            "pages": "214-224",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF49": {
            "title": "Medical image classification with convolutional neural network",
            "authors": [
                {
                    "first": "Q.",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. 13th Int. Conf. Control Automat. Robot. Vis.",
            "volume": "",
            "issn": "",
            "pages": "844-848",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF50": {
            "title": "Wordnet: A lexical database for English",
            "authors": [
                {
                    "first": "G.",
                    "middle": [
                        "A."
                    ],
                    "last": "Miller",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Commun. ACM",
            "volume": "38",
            "issn": "11",
            "pages": "39-41",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF51": {
            "title": "Caffe: Convolutional architecture for fast feature embedding",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ACM Multimedia",
            "volume": "2",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF52": {
            "title": "CNN features off-the-shelf: An astounding baseline for recognition",
            "authors": [
                {
                    "first": "A.",
                    "middle": [
                        "S."
                    ],
                    "last": "Razavian",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Azizpour",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Sullivan",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Carlsson",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops",
            "volume": "",
            "issn": "",
            "pages": "512-519",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF53": {
            "title": "Learning deep features for scene recognition using places database",
            "authors": [
                {
                    "first": "B.",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Lapedriza",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Torralba",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Oliva",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. NIPS",
            "volume": "",
            "issn": "",
            "pages": "487-495",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF54": {
            "title": "Learning rich features from RGB-D images for object detection and segmentation",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Arbel\u00e1ez",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. Eur. Conf. Comput. Vis.",
            "volume": "",
            "issn": "",
            "pages": "345-360",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF55": {
            "title": "Region-based convolutional networks for accurate object detection and semantic segmentation",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Donahue",
                    "suffix": ""
                },
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "38",
            "issn": "1",
            "pages": "142-158",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF56": {
            "title": "Indoor scene understanding with RGB-D images: Bottom-up segmentation, object detection and semantic segmentation",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Arbel\u00e1ez",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Comput. Vis.",
            "volume": "112",
            "issn": "2",
            "pages": "133-149",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF57": {
            "title": "Natural image bases to represent neuroimaging data",
            "authors": [
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Ayhan",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Maida",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proc. ICML",
            "volume": "",
            "issn": "",
            "pages": "987-994",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF58": {
            "title": "Automatic fetal ultrasound standard plane detection using knowledge transferred recurrent neural networks",
            "authors": [
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. MICCAI",
            "volume": "",
            "issn": "",
            "pages": "507-514",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF59": {
            "title": "Performance assessment of retroperitoneal lymph node computer-assisted detection using random forest and deep convolutional neural network learning algorithms in tandem",
            "authors": [
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "102nd Annu. Meet. Radiol. Soc. N. Am.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF60": {
            "title": "The lung tissue research consortium: An extensive open database containing histological, clinical, and radiological data to study chronic lung disease",
            "authors": [
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Holmes III",
                    "suffix": ""
                },
                {
                    "first": "B.",
                    "middle": [],
                    "last": "Bartholmai",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Karwoski",
                    "suffix": ""
                },
                {
                    "first": "V.",
                    "middle": [],
                    "last": "Zavaletta",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Robb",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "MICCAI Open Sci. Workshop",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF61": {
            "title": "Fully convolutional networks for semantic segmentation",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "E.",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.",
            "volume": "",
            "issn": "",
            "pages": "3431-3440",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF62": {
            "title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs",
            "authors": [
                {
                    "first": "L.-C.",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Kokkinos",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [
                        "L."
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. ICLR",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF63": {
            "title": "Overfeat: Integrated recognition, localization and detection using convolutional networks",
            "authors": [
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Sermanet",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. ICLR",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF64": {
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. ICLR",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF65": {
            "title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Int. J. Uncertainty, Fuzziness Knowl.-Based Syst.",
            "volume": "6",
            "issn": "02",
            "pages": "107-116",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF66": {
            "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
            "authors": [
                {
                    "first": "K.",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X.",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.",
            "volume": "37",
            "issn": "9",
            "pages": "1904-1916",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF67": {
            "title": "A fast learning algorithm for deep belief nets",
            "authors": [
                {
                    "first": "G.",
                    "middle": [
                        "E."
                    ],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Osindero",
                    "suffix": ""
                },
                {
                    "first": "Y.-W.",
                    "middle": [],
                    "last": "Teh",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Neural Comput.",
            "volume": "18",
            "issn": "7",
            "pages": "1527-1554",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF68": {
            "title": "Learning long-term dependencies with gradient descent is difficult",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Simard",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Frasconi",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "IEEE Trans. Neural Netw.",
            "volume": "5",
            "issn": "2",
            "pages": "157-166",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF69": {
            "title": "Visualizing and understanding con volutional networks",
            "authors": [
                {
                    "first": "M.",
                    "middle": [
                        "D."
                    ],
                    "last": "Zeiler",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. Eur. Conf. Comput. Vis.",
            "volume": "",
            "issn": "",
            "pages": "818-833",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF70": {
            "title": "Analyzing the performance of multilayer neural networks for object recognition",
            "authors": [
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Agrawal",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Malik",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. Eur. Conf. Comput. Vis.",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF71": {
            "title": "The pascal visual object classes challenge: A retrospective",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Everingham",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Comput. Vis.",
            "volume": "111",
            "issn": "1",
            "pages": "98-136",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF72": {
            "title": "Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans",
            "authors": [
                {
                    "first": "B.",
                    "middle": [],
                    "last": "van Ginneken",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Setio",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Jacobs",
                    "suffix": ""
                },
                {
                    "first": "F.",
                    "middle": [],
                    "last": "Ciompi",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Proc. IEEE 12th Int. Symp. Biomed. Imag.",
            "volume": "",
            "issn": "",
            "pages": "286-289",
            "other_ids": {
                "DOI": []
            }
        }
    }
}