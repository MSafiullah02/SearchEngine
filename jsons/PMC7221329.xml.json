{
    "paper_id": "PMC7221329",
    "metadata": {
        "title": "Extracting Possibly Representative COVID-19 Biomarkers from X-ray Images with Deep Learning Approach and Image Data Related to Pulmonary Diseases",
        "authors": [
            {
                "first": "Ioannis",
                "middle": [
                    "D."
                ],
                "last": "Apostolopoulos",
                "suffix": "",
                "email": "ece7216@upnet.gr",
                "affiliation": {}
            },
            {
                "first": "Sokratis",
                "middle": [
                    "I."
                ],
                "last": "Aznaouridis",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Mpesiana",
                "middle": [
                    "A."
                ],
                "last": "Tzani",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "The Coronavirus (COVID-19) is perhaps the greatest challenge of mankind in the twenty-first century. The development of the disease, its transmission, and the increased mortality in a number of countries, make it imperative to develop treatment, but also to protect health care and society from the transmission of the disease.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Therefore, remote control of the disease, including diagnosis, early quarantine, and follow-up, is essential. Artificial intelligence can contribute to the above perspectives. Recent studies claim to achieve precise results regarding the automatic detection of the disease from thoracic X-ray scans [1\u20133]. Although the research is limited due to the absence of large scale image data, the first results are encouraging and necessitate further investigation and research.",
            "cite_spans": [
                {
                    "start": 300,
                    "end": 301,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 302,
                    "end": 303,
                    "mention": "3",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Although the diagnosis is increasingly becoming a rapid process, the financial issues arising from the cost of diagnostic tests concern both states and patients, especially in countries with private health systems, or restricted access health systems due to prohibitive prices.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "During the first months of 2020, there has been an increase in publicly available patient data, including X-ray images. Possible patterns and knowledge mined from the X-ray scans may constitute a possible pipeline for the diagnosis of COVID-19.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "\u03a4he development of deep learning applications enables the researchers to perform a rapid and deep analysis on the X-ray scans. Deep Learning is a combination of Machine Learning methods mainly focused on the automatic feature extraction and classification from images, while its applications are broadly met in medical image detection, segmentation, and classification tasks. Machine learning and Deep Learning have become established disciplines in applying artificial intelligence to mine, analyze, and recognize patterns from data. Reclaiming the advances of those fields to the benefit of clinical decision making and computer-aided systems is increasingly becoming nontrivial, as new data emerge [4].",
            "cite_spans": [
                {
                    "start": 702,
                    "end": 703,
                    "mention": "4",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Deep Learning for pattern recogtition in images is a learning method, wherein deep Convolutional Neural Networks (CNN) are utilized for automatic mass feature extraction, achieved by the process called convolution [5]. Each layer involves a transformation of the data into a higher and more abstract level. Higher layers (i.e., deep layers) of portrayal enhance parts of the information that are significant for segregation and smother unimportant attributes. Due to the unlimited parameters mined during this process, several methods have been proposed to achieve dimensionality reduction, such as Pooling [5].",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 216,
                    "mention": "5",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 608,
                    "end": 609,
                    "mention": "5",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Motivated by the recent and relative research, in this study, we focus on circumventing two vital issues arisen in the detection of COVID-19 from X-ray scans. The first issue is related to the methodology of the experimental setups. In essence, the researches have demonstrated that the detection of COVID-19 is achievable, but this conclusion derives from an analysis based on incomplete data. \u03a4he models proposed are powerful in classifying images between only three classes (viral and bacterial pneumonia, COVID-19, normal). This, unfortunately, does not demonstrate the existence of a clear fingerprint of the Coronavirus in X-ray images, firstly, due to the insignificant database size and, secondly, due to the fact that the fingerprints of other pulmonary diseases have not been compared. The second issue is related to the flaws of Deep Learning and is often referred to as the issue of interpretability [6]. In short, the algorithm is not transparent, thereby a radiologist cannot supervise and know which factors or indices were utilized by the model to reach to a decision.",
            "cite_spans": [
                {
                    "start": 913,
                    "end": 914,
                    "mention": "6",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To further evaluate the methodology of Deep Learning, we perform an experiment utilizing six of the most common pulmonary diseases, including that of COVID-19. In this way, the capabilities of the method in distinguishing between the various diseases is evaluated. Besides, the dataset of the particular experiment is significant, including approximately 450 cases of COVID-19. To contribute to the latter referred issue (i.e., the interpretability), we perform three different experiments altering the mining methods to inspect the variance of the extracted features. Specifically, the state-of-the-art CNN called Mobile Net (v2) is employed to extract features from the images in three different ways, as follows: (a) training from scratch, (b) feature extraction via transfer learning (or of-the-self-features), and (c) hybrid feature extraction via fine-tuning. Those methods are explained in Sect. 2.2.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Due to the absence of a complete X-ray dataset containing not only common pneumonia or other diseases, but also cases of diagnosed COVID-19, the final dataset of this experiment is a combination of X-rays corresponding to common pulmonary diseases recorded during the last years and confirmed COVID-19 cases recorded from January 2020 to March 2020.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The results of the present research further enhance the research to date. In particular, it is highlighted that with the strategy of training from scratch, the CNN succeeds in mining significant image features, discovered solely in the particular X-ray images. Based on these characteristics (features), 88% accuracy in classification of the relative diseases and ~ 99% accuracy in diagnosis of COVID is achieved. This may prove that these features are Biomarkers and need further analysis, as they may be gene or other signatures.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "For the creation of the dataset, the research focused on obtaining X-rays corresponding to confirmed cases infected by the virus SARS-COV-2. Through extensive research, a collection of 455 well-visualized, confirmed pathological X-ray images was created. The final collection includes selected X-rays from a repository created by Dr. Cohen [7], and publically available medical image repositories, such as the Radiological Society of North America (RSNA), Radiopaedia, and the Italian Society of Medical and Interventional Radiology (SIRM). The latter association released a publically available COVID-19 dataset [8], which was also incorporated.",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 342,
                    "mention": "7",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 614,
                    "end": 615,
                    "mention": "8",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "COVID-19 X-ray Images ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "To train and evaluate the classification method in more complex conditions, a collection of conventional bacterial and viral pneumonia X-ray scans was added to the dataset. This collection is available on the Internet by Kermany et al. [9]. A selection of 910 related X-ray images, which were randomly selected, was incorporated into the dataset.",
            "cite_spans": [
                {
                    "start": 237,
                    "end": 238,
                    "mention": "9",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Common Bacterial and Viral Pneumonia X-ray Images ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "It is impossible to investigate the performance of any classification method in detecting the COVID-19 disease unless other pulmonary diseases are incorporated. For this reason, the final dataset includes selected X-ray scans corresponding to other pulmonary abnormalities.",
            "cite_spans": [],
            "section": "Pulmonary Diseases Detected from X-ray Scans ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The National Institutes of Health (NIH) X-ray dataset was exploited, which is referred to as NIH dataset for the particular experiment, and comprises 112.120 frontal-view X-ray images of 30.805 unique patients with the text-mined fourteen disease image labels [10].",
            "cite_spans": [
                {
                    "start": 261,
                    "end": 263,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Pulmonary Diseases Detected from X-ray Scans ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Those images are extracted from the clinical PACS database at the National Institutes of Health Clinical Center in America. The corresponding diseases were mined from the associated radiological reports using natural language processing. The labels contain fourteen common thoracic pathologies include Atelectasis, Consolidation, Infiltration, Pneumothorax, Edema, Emphysema, Fibrosis, Effusion, Pneumonia, Pleural thickening, Cardiomegaly, Nodule, Mass and Hernia. This dataset is significantly more representative of the real patient population distributions and realistic clinical diagnosis challenges, than any previous chest X-ray datasets. The dataset comes with annotated metadata info, consisting of several risk associated factors.",
            "cite_spans": [],
            "section": "Pulmonary Diseases Detected from X-ray Scans ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "A significant limitation of this dataset is the labeling policy, which may raise some concerns. More specifically, the medical reports were analyzed by an automatic text-mining model, which assigned the corresponding labels according to its text-mining procedure. However, as the authors claim, \u201cthere would be some erroneous labels, but the Natural Language Processing (NLP) labeling accuracy is estimated to be > 90%\u201d [10].",
            "cite_spans": [
                {
                    "start": 421,
                    "end": 423,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Pulmonary Diseases Detected from X-ray Scans ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "For the particular experiment, the following disease cases were selected: (a) Pulmonary Edema, (b) Pleural effusion, (c) Chronic obstructive pulmonary disease, (d) Pulmonary fibrosis. The selection was based on the significance and frequency of those diseases. Analytically, 293 images representing the Pulmonary Edema, 311 images representing the Pleural Effusion, 315 images representing the Chronic Obstructive Pulmonary Disease (COPD), and 280 images representing the Pulmonary Fibrosis were randomly chosen from the collection.",
            "cite_spans": [],
            "section": "Pulmonary Diseases Detected from X-ray Scans ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The X-ray images were rescaled to a size of 200 \u00d7 200. For the images of different pixel ratios and to avoid distortion, black background of 200 \u00d7 200 pixels was added to achieve a complete transformation. Low contrast images or images containing parts of the whole thoracic X-ray scan were excluded.",
            "cite_spans": [],
            "section": "Image Pre-processing and Data Augmentation ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "During the training process, slight augmentations were applied to the images. Data augmentation is mandatory to generate the necessary diversity aiding to the generalization capabilities of the CNNs [11]. Specifically, the images are randomly rotated by a maximum of 10\u00b0 and randomly shifted horizontally or vertically by a maximum of 20 pixels towards any direction. In this way, the CNN learns to be robust to position and orientation variance.",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 202,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Image Pre-processing and Data Augmentation ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The present collection of data faces some limitations, which have to be mentioned. Firstly, a relatively small sample of COVID-19-infected cases is incorporated. Besides, this sample may derive from patient cases with severe symptoms, the analysis of which was mandatory. Cases with slight symptoms are missing from the current public collections, which is due to the policy of protecting people (and society) who have mild symptoms of the disease, and are immediately quarantined without further examination.",
            "cite_spans": [],
            "section": "Data Limitations ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Secondly, the pneumonia incidence samples are older recorded samples and do not represent pneumonia images from patients with suspected Coronavirus symptoms, while the clinical conditions are missing.",
            "cite_spans": [],
            "section": "Data Limitations ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Thirdly, further data related to demographic characteristics and other potential predisposing or risk factors are not available, and this impedes a holistic approach and examination beyond the medical image.",
            "cite_spans": [],
            "section": "Data Limitations ::: Dataset of the Study ::: Methods",
            "ref_spans": []
        },
        {
            "text": "There are currently major techniques that successfully employ CNNs to medical image classification, by extracting features, as follows: (a) training the CNN from scratch, (b) employing a pre-trained CNN, which is called Transfer Learning [12], and (c) a hybrid method, which also a Transfer Learning method, while it adopts the former strategies by tuning the trainability of specific layers of the CNN; this method is called Fine-Tuning [13].",
            "cite_spans": [
                {
                    "start": 239,
                    "end": 241,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 439,
                    "end": 441,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Learning Strategies for Feature Extraction ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The first strategy may be adopted either by developing a novel CNN architecture, or by employing the architecture of a successful CNN. In this study, we employ a state-of-the-art CNN architecture, to follow each of the strategies.",
            "cite_spans": [],
            "section": "Learning Strategies for Feature Extraction ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The second and the third strategy are means of Transfer Learning. Transfer learning is a machine learning method wherein a model developed for a specific task is reused for another task. There are two categories to perform the Transfer Learning, i.e. the of-the-self feature extraction and Fine-Tuning.",
            "cite_spans": [],
            "section": "Learning Strategies for Feature Extraction ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The off-the-self strategy is an approach utilizing the weights of the Convolutional layers, which are defined from the source task (the initial training of another domain) without re-training the network [14]. Extracting such features is usually fast and this approach requires only the addition of a classifier to perform the classification of those features with respect to their significance in the particular task.",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 207,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Learning Strategies for Feature Extraction ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The Fine-Tuning strategy involves utilizing a network initialized with pre-trained weights and partially re-training it on the target task. In the context of deep learning, fine-tuning a deep network is a common strategy to learn both task-specific deep features, and retain the methodology to extract global features met in every image, such as shapes. Usually, the Fine-Tuning strategy allows more trainable weights at the top of the network (i.e., the final steps), due to the fact that those convolutional layers extract more abstract and high-level information, compared to the first layers wherein local features are learned. In the particular experiment, we gradually allow more layers to be trainable, by defining six experimental cases, referred to as Fine-Tuning (e.g., 11), where the number in the parenthesis corresponds to the number of trainable convolutional blocks.",
            "cite_spans": [],
            "section": "Learning Strategies for Feature Extraction ::: Methods",
            "ref_spans": []
        },
        {
            "text": "For the classification task, the state-of-the-art CNN called Mobile Net [15] was employed. Mobile Net has been recently utilized for the same classification task by Apostolopoulos [3]. In their work, the authors demonstrated the superiority of Mobile Net in reducing the False Negatives for the detection of COVID-19, compared to other famous CNNs. Besides, this CNN introduces a fewer number of parameters compared to other CNNs, which makes it appropriate for swift training.",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 75,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 181,
                    "end": 182,
                    "mention": "3",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "The State-of-the-Art CNN Called Mobile Net ::: Method for the Extraction of Possibly Significant Biomarkers ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The MobileNet [15] model is based on depthwise separable convolutions [16], which is a form of convolutions transforming a conventional convolution into a depthwise convolution [16] and a 1 \u00d7 1 convolution, which is commonly known pointwise convolution [16]. This procedure reduces the number of parameters drastically.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 17,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 71,
                    "end": 73,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 178,
                    "end": 180,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 254,
                    "end": 256,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "The State-of-the-Art CNN Called Mobile Net ::: Method for the Extraction of Possibly Significant Biomarkers ::: Methods",
            "ref_spans": []
        },
        {
            "text": "To the top of the Mobile Net v2, a Global Average Pooling [17] layer was added, which drastically reduces the issue of overfitting [18]. The extracted image features are inserted into a Neural Network of 2500 nodes to distinguish between the irrelevant and the significant ones. To further aid to the overfitting reduction, the weights of each feature are normalized utilizing a Batch Normalization layer [19], while we independently zero out the 50% of the outputs of neurons at random, via a Dropout layer [20]. An overview of the method is illustrated in Fig. 1.",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 61,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 132,
                    "end": 134,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 406,
                    "end": 408,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 509,
                    "end": 511,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "The State-of-the-Art CNN Called Mobile Net ::: Method for the Extraction of Possibly Significant Biomarkers ::: Methods",
            "ref_spans": [
                {
                    "start": 563,
                    "end": 564,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The intention of the particular study is not only to achieve a high classification accuracy, but to achieve this by training the CNN from scratch. This strategy is preferable to transfer learning to evaluate the significance of the features extracted from the precise images, while it is not depending on features already learned by the pre-trained model, the initial training of which, was performed utilizing non-medical images.",
            "cite_spans": [],
            "section": "The State-of-the-Art CNN Called Mobile Net ::: Method for the Extraction of Possibly Significant Biomarkers ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Based on the results, the extracted features may be evaluated to conclude that they may constitute real image biomarkers related to various diseases.",
            "cite_spans": [],
            "section": "The State-of-the-Art CNN Called Mobile Net ::: Method for the Extraction of Possibly Significant Biomarkers ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We performed a set of three different experiments employing the same CNN (Mobile Net v2), but altering the learning strategy. The following strategies are evaluated: (a) Transfer Learning with of-the-self features, (b) Transfer Learning with Fine-Tuning, and (c) Training from scratch, which, in this experiment, is a latent form of the Transfer Learning, since we only borrow the architecture of the Mobile Net and not the learned parameters. The experiments were performed utilizing a single GPU setup (NVIDIA GeForce RTX 2060 Super) using the Keras library [21] and TensorFlow [22] as backend.",
            "cite_spans": [
                {
                    "start": 561,
                    "end": 563,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 581,
                    "end": 583,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Experiment Setup ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The training and evaluation procedure was performed with ten fold-cross-validation. During this procedure, the dataset is randomly split to ten folds, nine of which are utilized for training the model, and the remaining fold is hidden and used to test the performance and the confidence of the predictions after the training. This process is repeated in a way that every fold is utilized as the test set. This increases the computational cost but enhances the significance of the result. The final accuracy is obtained by calculating the mean accuracy derived from each testing fold.",
            "cite_spans": [],
            "section": "Experiment Setup ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The metrics, based upon which the evaluation of the performance is made, are the overall 7-class accuracy, and the accuracy corresponding to the 2-class classification (COVID-19 vs. non-COVID-19).",
            "cite_spans": [],
            "section": "Metrics ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Besides, to focus on the performance of COVID-19 detection, the following values are recorded: (a) Correctly predicted COVID-19 cases (True Positives), (b) Correctly predicted non-COVID-19 cases (True Negatives), (c) Incorrectly predicted COVID-19 cases (False Positives), and (d) Incorrectly predicted non-COVID-19 cases (False Negatives). Based on those values, the Sensitivity and Specificity of the test are calculated by the following equations:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Sensitivity = \\frac{True \\,positives}{{True\\,positives + False\\,negatives}}$$\\end{document}Sensitivity=TruepositivesTruepositives+Falsenegatives2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Specificity = \\frac{True\\, negatives}{{True\\, negatives + False \\,positives}}$$\\end{document}Specificity=TruenegativesTruenegatives+FalsepositivesFor the particular experiments and given that there is a class imbalance issue, the most reliable metric is that of the 7-class accuracy, while given that this accuracy is high, the second most vital metric is that of Specificity. This is due to the importance of correctly identifying the actual non-COVID-19 cases (True Negatives).",
            "cite_spans": [],
            "section": "Metrics ::: Methods",
            "ref_spans": []
        },
        {
            "text": "In Table 1, the accuracy, sensitivity, and Specificity of the first strategy are given. The reader should recall that the 2-class accuracy refers to the case where the labels are \u201cCOVID-19\u201d and \u201cNon-COVID-19\u201d. Besides, the sensitivity and the Specificity refer to the 2-class measurement.",
            "cite_spans": [],
            "section": "Results of the Of-the-Self-features Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 9,
                    "end": 10,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Due to the class imbalance, the metric of Specificity was approaching 100% and was not mentioned in Table 1, as it is not a meaningful measurement when reaching those values. The same issue is valid for the 2-class accuracy but was mentioned in Table 1 for comparisons. The confusion matrix for each class is presented in Table 2.",
            "cite_spans": [],
            "section": "Results of the Of-the-Self-features Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 106,
                    "end": 107,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 251,
                    "end": 252,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 328,
                    "end": 329,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "The confusion matrix corresponding to the COVID-19 class vs. all the classes is presented in Table 3.",
            "cite_spans": [],
            "section": "Results of the Of-the-Self-features Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 99,
                    "end": 100,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Based on the results, it is confirmed that the particular strategy is not effective in extracting useful features to distinguish possible underlying information from the X-rays related to the COVID-19 disease. Besides, a bias towards the non-COVID-19 cases is observed in Table 3, which makes the strategy not appropriate for the particular task.",
            "cite_spans": [],
            "section": "Results of the Of-the-Self-features Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 278,
                    "end": 279,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "In Table 4, the accuracy, the Sensitivity, and the Specificity of the second strategy are given. The reader should recall that several adjustments for fine-tuning are tested in the particular section, which are discussed in Sect. 2.2. The number defining each experimental case, refers to the number of blocks made trainable during the experiment, e.g., \u201cFine-Tuning 3\u201d corresponds to 3 trainable blocks starting from the top of the CNN. The reader should also recall that the values for the accuracy are the mean values of the accuracies obtained at each fold from the tenfold-cross-validation procedure.",
            "cite_spans": [],
            "section": "Results of the Fine-Tuning Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 9,
                    "end": 10,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "As it is observed in Table 4, the strategy of fine-tuning obtains different results. This is explained by the fact that we gradually allow more layers to be trainable, thus approaching close to the strategy of training from scratch, which obtains the best results, as it is presented in Sect. 3.3. Hence, the confusion matrixes are not provided due to insignificance and limitations of space.",
            "cite_spans": [],
            "section": "Results of the Fine-Tuning Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 27,
                    "end": 28,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "In Table 5, the accuracy, Sensitivity, and Specificity of the specific strategy are presented.",
            "cite_spans": [],
            "section": "Results of the Training-from-Scratch Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 9,
                    "end": 10,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "In Table 5, it is observed that training from scratch outperforms the other strategies in terms of every metric, obtaining a remarkable 2-class accuracy of 99.18% and a high 7-class accuracy of 86.66%. The reader should recall that 2-class accuracy refers to the effectiveness of distinguishing the COVID-19 cases from every other case, including both abnormal and normal cases. In Table 6, the confusion matrix for the 7-class task is presented.",
            "cite_spans": [],
            "section": "Results of the Training-from-Scratch Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 9,
                    "end": 10,
                    "mention": "5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 388,
                    "end": 389,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "Several outcomes are to be highlighted in Table 6. Firstly, out of 455 COVID-19 cases, 443 cases were correctly identified, while only 2 cases were mistakenly classified as normal. Secondly, out of the 1341 normal cases, only 1 case was mistakenly identified. For the rest of the pulmonary abnormalities, there is a diversity, which may derive from the fact that the different pathogens embody seals that are difficult to distinguish from the X-rays and confuse the CNN.",
            "cite_spans": [],
            "section": "Results of the Training-from-Scratch Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 48,
                    "end": 49,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "The confusion matrix corresponding to the COVID-19 class vs. all the classes is presented in Table 7.",
            "cite_spans": [],
            "section": "Results of the Training-from-Scratch Strategy ::: Results",
            "ref_spans": [
                {
                    "start": 99,
                    "end": 100,
                    "mention": "7",
                    "ref_id": "TABREF6"
                }
            ]
        },
        {
            "text": "The classification obtains an excellent trade-off between the corresponding True Positives, False Positives, True Negatives, and False Negatives.",
            "cite_spans": [],
            "section": "Results of the Training-from-Scratch Strategy ::: Results",
            "ref_spans": []
        },
        {
            "text": "The particular research focuses on discovering possible image biomarkers from X-ray images. These biomarkers may be significantly related to the COVID-19 disease.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "While Deep Learning extracts a massive amount of high-dimensional features from images, it is possible that some of those features behave as actual image biomarkers. The reader may be confused by the difference between a feature and an image biomarker. Therefore, we briefly describe the difference between them. A feature is a specific characteristic of an image, either well-defined in the literature or yet to be defined as to its importance. With Deep Learning, it is possible to extract millions of related features. The extracted features' importance to the specific task is questionable. The majority of those features may be irrelevant to the desired outcome, or the desired subject of study and are rejected by the automatic classification performed after the convolutional layers of a CNN. The biomarkers are quantitative markers of confirmed significance and are not limited to the image features [23]. Generally, the ability of Deep Learning for image biomarker extraction is questionable due to the issue of the interpretability.",
            "cite_spans": [
                {
                    "start": 909,
                    "end": 911,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "This study suggests that it may be possible to discover new reliable biomarkers from X-ray images due to the fact that a high classification accuracy was achieved. Since the CNNs and the Neural Networks lay on the evaluation of millions of parameters to classify the significant features, some of those features may actually be image biomarkers leading to a reliable result. This horizon in to be investigated in future research, possibly exploring other approaches, such as Radiomics [24].",
            "cite_spans": [
                {
                    "start": 486,
                    "end": 488,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "\u039fne factor that underpins the conclusion mentioned above, is the comparison between the various image feature mining strategies. In particular, it is demonstrated that those strategies do not mine the same features. This can be easily interpreted, since with strategies of Transfer Learning with of-the-self-features and Transfer Learning with fine-tuning, the ability of the CNN to extract significant features depends on factors related to the initial training. The initial training was mandatory to be performed on images of a completely different nature due to the absence of large-scale data. However, despite the fact that the latter strategies have excellent performances in other medical image classification tasks [12, 25], in the particular experiment, they were underperforming. This may suggest that with the training from scratch, essential features related to the pulmonary abnormalities have been mined, which may constitute relevant Biomarkers.",
            "cite_spans": [
                {
                    "start": 724,
                    "end": 726,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 728,
                    "end": 730,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "In future studies, some issues of the present study can be circumvented. A more in-depth analysis, in particular, requires much more patient data, particularly those suffering from COVID-19.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "A more promising approach for future studies would concentrate on identifying patients infected by COVID-19, but showing mild symptoms, although those symptoms may not be visualized correctly on X-rays, or may not be visualized at all.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "It is of vital importance to establish models capable of distinguishing between a more significant numbers of pulmonary diseases, possibly including that of SARS. Also, despite the fact that the appropriate treatment is not determined solely from an X-ray image [26], an initial screening of the cases would be useful, not in the type of treatment, but in the timely application of quarantine measures in the positive samples, until a complete examination and specific treatment or follow-up procedure are followed.",
            "cite_spans": [
                {
                    "start": 263,
                    "end": 265,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "The contribution of this work is twofold. Firstly, low-cost, rapid, and automatic detection of the COVID-19 disease was achieved, utilizing a significantly large sample of several pulmonary infections. It was demonstrated that the various infections may be distinguished by a computer-aided diagnostic system, utilizing deep features extracted by Deep Learning methods. This strategy may be beneficial for medical-decision assisting tools to provide a second opinion in challenging cases. It could be also applied to achieve an intact first assessment of the likelihood of disease in patients with either suspected or no symptoms. Besides, the advantage of automatic detection of COVID-19 from either medical image lies in the reduction of exposure of nursing and medical staff to the outbreak.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "Secondly, the study suggests that future research should be conducted to investigate the possible behavior of the extracted features as Biomarkers, since there is sufficient evidence, based on the particular results. The three training strategies were employed to evaluate the significance of the extracted features of each strategy. The strategy of allowing the deep CNN to learn new characteristics and features from the specific images resulted in an excellent performance over the other approaches, wherein features are extracted based on mined knowledge from non-medical images, or images unrelated to pulmonary diseases. This underlines the uniqueness of the extracted features and marks them as possible biomarkers.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1: Accuracy, sensitivity, and specificity for the of-the-self-features strategy\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2: Confusion Matrix for the 7-class classification employing transfer learning with of-the-self features\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3: Confusion matrix for the 2-class classification employing transfer learning with of-the-self features\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4: Accuracy, sensitivity, and specificity for the different cases of the fine-tuning strategy\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5: Accuracy, sensitivity, and specificity when the training from scratch strategy was followed\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6: Confusion matrix for the 7-class classification employing the strategy of training Mobile Net v2 from scratch\n",
            "type": "table"
        },
        "TABREF6": {
            "text": "Table 7: Confusion matrix for the 2-class classification employing the strategy of training Mobile Net v2 from scratch\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1: Overview of the feature extraction process",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "The effectiveness of data augmentation in image classification using deep learning",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Perez",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Convolutional Neural Networks",
            "volume": "11",
            "issn": "",
            "pages": "1-8",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning",
            "authors": [
                {
                    "first": "H-C",
                    "middle": [],
                    "last": "Shin",
                    "suffix": ""
                },
                {
                    "first": "HR",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Nogues",
                    "suffix": ""
                },
                {
                    "first": "RM",
                    "middle": [],
                    "last": "Summers",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Transactions on Medical Imaging",
            "volume": "35",
            "issn": "5",
            "pages": "1285-1298",
            "other_ids": {
                "DOI": [
                    "10.1109/TMI.2016.2528162"
                ]
            }
        },
        "BIBREF4": {
            "title": "Interactive medical image segmentation using deep learning with image-specific fine tuning",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "MA",
                    "middle": [],
                    "last": "Zuluaga",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pratt",
                    "suffix": ""
                },
                {
                    "first": "PA",
                    "middle": [],
                    "last": "Patel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aertsen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Transactions on Medical Imaging",
            "volume": "37",
            "issn": "7",
            "pages": "1562-1573",
            "other_ids": {
                "DOI": [
                    "10.1109/TMI.2018.2791721"
                ]
            }
        },
        "BIBREF5": {
            "title": "Glioma grading on conventional MR images: A deep learning study with transfer learning",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "L-F",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "H-Y",
                    "middle": [],
                    "last": "Nan",
                    "suffix": ""
                },
                {
                    "first": "Y-C",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Frontiers in Neuroscience",
            "volume": "12",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.3389/fnins.2018.00804"
                ]
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "Overfitting and use of mismatched cohorts in deep learning models: Preventable design limitations",
            "authors": [
                {
                    "first": "SR",
                    "middle": [],
                    "last": "Mummadi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Al-Zubaidi",
                    "suffix": ""
                },
                {
                    "first": "PY",
                    "middle": [],
                    "last": "Hahn",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "American Journal of Respiratory and Critical Care Medicine",
            "volume": "198",
            "issn": "4",
            "pages": "544-545",
            "other_ids": {
                "DOI": [
                    "10.1164/rccm.201802-0350LE"
                ]
            }
        },
        "BIBREF10": {
            "title": "How does batch normalization help optimization?",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Santurkar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tsipras",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ilyas",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Madry",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "2483-2493",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "A theoretically grounded application of dropout in recurrent neural networks",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gal",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ghahramani",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "1019-1027",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "What are biomarkers?",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Strimbu",
                    "suffix": ""
                },
                {
                    "first": "JA",
                    "middle": [],
                    "last": "Tavel",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Current Opinion in HIV and AIDS",
            "volume": "5",
            "issn": "6",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1097/COH.0b013e32833ed177"
                ]
            }
        },
        "BIBREF16": {
            "title": "Radiomics analysis of pulmonary nodules in low-dose CT for early detection of lung cancer",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "JH",
                    "middle": [],
                    "last": "Oh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Riyahi",
                    "suffix": ""
                },
                {
                    "first": "C-J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Medical Physics",
            "volume": "45",
            "issn": "4",
            "pages": "1537-1549",
            "other_ids": {
                "DOI": [
                    "10.1002/mp.12820"
                ]
            }
        },
        "BIBREF17": {
            "title": "Implementing machine learning in radiology practice and research",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kohli",
                    "suffix": ""
                },
                {
                    "first": "LM",
                    "middle": [],
                    "last": "Prevedello",
                    "suffix": ""
                },
                {
                    "first": "RW",
                    "middle": [],
                    "last": "Filice",
                    "suffix": ""
                },
                {
                    "first": "JR",
                    "middle": [],
                    "last": "Geis",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "American Journal of Roentgenology",
            "volume": "208",
            "issn": "4",
            "pages": "754-760",
            "other_ids": {
                "DOI": [
                    "10.2214/AJR.16.17224"
                ]
            }
        },
        "BIBREF18": {
            "title": "The COVID-19 epidemic",
            "authors": [
                {
                    "first": "TP",
                    "middle": [],
                    "last": "Velavan",
                    "suffix": ""
                },
                {
                    "first": "CG",
                    "middle": [],
                    "last": "Meyer",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Tropical Medicine & International Health",
            "volume": "25",
            "issn": "3",
            "pages": "278-280",
            "other_ids": {
                "DOI": [
                    "10.1111/tmi.13383"
                ]
            }
        },
        "BIBREF19": {
            "title": "Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks",
            "authors": [
                {
                    "first": "ID",
                    "middle": [],
                    "last": "Apostolopoulos",
                    "suffix": ""
                },
                {
                    "first": "TA",
                    "middle": [],
                    "last": "Mpesiana",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Physical and Engineering Sciences in Medicine",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1007/s13246-020-00865"
                ]
            }
        },
        "BIBREF20": {
            "title": "Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Greenspan",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Van Ginneken",
                    "suffix": ""
                },
                {
                    "first": "RM",
                    "middle": [],
                    "last": "Summers",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Transactions on Medical Imaging",
            "volume": "35",
            "issn": "5",
            "pages": "1153-1159",
            "other_ids": {
                "DOI": [
                    "10.1109/TMI.2016.2553401"
                ]
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Deep learning",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Visual interpretability for deep learning: A survey",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S-C",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Frontiers of Information Technology & Electronic Engineering",
            "volume": "19",
            "issn": "1",
            "pages": "27-39",
            "other_ids": {
                "DOI": [
                    "10.1631/FITEE.1700808"
                ]
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "Identifying medical diagnoses and treatable diseases by image-based deep learning",
            "authors": [
                {
                    "first": "DS",
                    "middle": [],
                    "last": "Kermany",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Goldbaum",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "CCS",
                    "middle": [],
                    "last": "Valentim",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "SL",
                    "middle": [],
                    "last": "Baxter",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Cell",
            "volume": "172",
            "issn": "5",
            "pages": "1122-1131.e9",
            "other_ids": {
                "DOI": [
                    "10.1016/j.cell.2018.02.010"
                ]
            }
        }
    }
}