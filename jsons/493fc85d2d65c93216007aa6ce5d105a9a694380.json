{
    "paper_id": "493fc85d2d65c93216007aa6ce5d105a9a694380",
    "metadata": {
        "title": "Identification of diagnostic markers for tuberculosis by proteomic fingerprinting of serum Webappendix: Supplementary methods",
        "authors": []
    },
    "abstract": [],
    "body_text": [
        {
            "text": "A supervised learning algorithm is tasked to find a decision function capable of assigning the correct label for a set of input/output pairs of examples, called the training data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The ability of the decision function to predict correct labels for unseen samples (test data) is know as its generalization. Current machine learning methods such as support vector machines (SVM) aim to optimize this property. 1 The generalisation of a classifier is dependent on a set of parameters (model) that must be chosen to optimise performance. For this purpose we adopted a grid search strategy in which a range of parameter values are discretized and tested using cross-validation.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 228,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "The SVM 2,3 maps its inputs to a high or even infinite, dimensional feature space. The output of the SVM is then a linear thresholded function of the mapped inputs in the feature space, which may be nonlinear in the original input space. The mapping is accomplished by a user-selected reproducing kernel function ) , ( K x x ! where x and x! are input vectors. The kernel function must satisfy Mercer's conditions. 4 Well-known examples of kernels include the Gaussian",
            "cite_spans": [
                {
                    "start": 415,
                    "end": 416,
                    "text": "4",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "where the parameter ! determines the width;",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "where d determines the degree. When d = 1 it is called the linear kernel and corresponds to the identity map of the input data. A trained SVM classifier has the form . In our study the SLP did not provide an optimal discriminative function, giving an accuracy of 86.5% in the independent test set (table 3 and (Table 3 and Fig 1A) , but relied on AdaBoost 9 boosting to achieve such levels of generalisation 9 (Table 3) . We used AdaBoost with 100 iterations for the ADTree and C4.5 classifiers, and boosting with a maximum of 10 iterations for the non-commercial version of the C5.0 classifier.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 297,
                    "end": 309,
                    "text": "(table 3 and",
                    "ref_id": null
                },
                {
                    "start": 310,
                    "end": 318,
                    "text": "(Table 3",
                    "ref_id": null
                },
                {
                    "start": 323,
                    "end": 330,
                    "text": "Fig 1A)",
                    "ref_id": null
                },
                {
                    "start": 410,
                    "end": 419,
                    "text": "(Table 3)",
                    "ref_id": null
                }
            ],
            "section": ""
        },
        {
            "text": "The Pearson correlation coefficient is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mass peak cluster selection"
        },
        {
            "text": "where k X is the random variable corresponding to the th k component of sample input vectors",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mass peak cluster selection"
        },
        {
            "text": "x and Y is the random variable of output labels. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mass peak cluster selection"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "An Introduction to Support Vector Machines and other kernel-based learning methods",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Cristianini",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shawe-Taylor",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A training algorithm for optimal margin classifiers",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "E"
                    ],
                    "last": "Boser",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "M"
                    ],
                    "last": "Guyon",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [
                        "N"
                    ],
                    "last": "Vapnik",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Proceedings of the fifth annual workshop on Computational Learning Theory",
            "volume": "",
            "issn": "",
            "pages": "144--152",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Theory of reproducing kernels",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Aronszajn",
                    "suffix": ""
                }
            ],
            "year": 1950,
            "venue": "Trans Amer Math Soc",
            "volume": "68",
            "issn": "",
            "pages": "337--404",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Principles of Neurodynamics",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Rosenblatt",
                    "suffix": ""
                }
            ],
            "year": 1962,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "5: Programs for Machine Learning",
            "authors": [
                {
                    "first": "Jr",
                    "middle": [
                        "C4"
                    ],
                    "last": "Quinlan",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "The alternating decision tree learning algorithm",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mason",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Proceedings of the Sixteenth International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "124--133",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Experiments with a New Boosting Algorithm",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Schapire",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Thirteenth International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "148--156",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "a sample class label i y is denoted by i \u0177 .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "and b. Typically, many of the \u03b1s will be zero. Those that are non-zero are called support vectors and are used to define a separation hyperplane in the transformed feature space. Training a SVM is a convex (quadratic) optimisation problem not subject to local minima, unlike a multi-layer perceptron. We trained soft-margin SVMs which are practicable when data are noisy. In this case the algorithm also minimises the distance of incorrectly classified examples to the margin by adjusting a penalty value, C, called the soft-margin parameter.The single layer perceptron 5 (SLP) is an artificial neural network with one output neuron that computes a linear combination of the values given by the input layer.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "fig 1A,blue square).The multi-layer perceptron 6 (MLP) is a generalisation of the SLP with intermediate layers of hidden neurons. It tackles the problem of non-linearly separable classes by allowing the neurons to process their inputs with a sigmoid function on the this network the weights are learned by a back-propagation algorithm which is a gradient descent rule to minimise the error given by ! showed similar generalisation performance to SLP, classifying with an accuracy of 86.5%(table 3 andfigure 1A, orange diamond).A decision tree learns to classify a dataset of samples D=[X,Y] by aggregating their features within a set of nodes organised in a binary tree structure. To find the tree structure, sample features are tested according to their discriminative power using a splitting criterion: < where T is any test that produces a binary partition of dataset D. In the C4.5 (ref.7 ) classifier the test thresholds are evaluated by an informationthe class to which the sample belongs and z is the number of outcomes of the testT. An iterative algorithm places nodes with increasing information gain from the root to the leaves of the tree. The final tree might be pruned in order to get a more compact representation of the classifier. A testing set sample can be classified by testing its mass peak values against those in the nodes of the tree following a path from the root to a leaf with a classification output. The C5.0 algorithm is an extended version of C4.5 that winnows irrelevant features and incorporates variable misclassification costs (http://www.rulequest.com/). The Alternating Decision Tree 8 (ADTree) is a tree with additional nodes for predicting values that are summed over a classification path and the final output is the sign of this sum. In the TB vs. control dataset (",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "to value m/z of the mass cluster k of sample i, i y is the class label for sample i and m is the number of samples.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "the ADTree and the C4.5 classifiers achieved accuracies of 92.3% and 91.0%, respectively",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}