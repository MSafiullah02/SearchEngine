{
    "paper_id": "ecb81dc0d3d43988f8e8ce41b4b7075e6b939595",
    "metadata": {
        "title": "Context-Guided Learning to Rank Entities",
        "authors": [
            {
                "first": "Makoto",
                "middle": [
                    "P"
                ],
                "last": "Kato",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "PRESTO",
                    "location": {
                        "region": "Tsukuba",
                        "country": "Japan"
                    }
                },
                "email": "mpkato@acm.org"
            },
            {
                "first": "Wiradee",
                "middle": [],
                "last": "Imrattanatrai",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Kyoto University",
                    "location": {
                        "settlement": "Kyoto",
                        "country": "Japan"
                    }
                },
                "email": "wiradee@db.soc.i.kyoto-u.ac.jp"
            },
            {
                "first": "Takehiro",
                "middle": [],
                "last": "Yamamoto",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Hyogo",
                    "location": {
                        "settlement": "Kobe",
                        "country": "Japan"
                    }
                },
                "email": "t.yamamoto@sis.u-hyogo.ac.jp"
            },
            {
                "first": "Hiroaki",
                "middle": [],
                "last": "Ohshima",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Hyogo",
                    "location": {
                        "settlement": "Kobe",
                        "country": "Japan"
                    }
                },
                "email": "ohshima@ai.u-hyogo.ac.jp"
            },
            {
                "first": "Katsumi",
                "middle": [],
                "last": "Tanaka",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Kyoto University",
                    "location": {
                        "settlement": "Kyoto",
                        "country": "Japan"
                    }
                },
                "email": "tanaka.katsumi.85e@st.kyoto-u.ac.jp"
            }
        ]
    },
    "abstract": [
        {
            "text": "We propose a method for learning entity orders, for example, safety, popularity, and livability orders of countries. We train linear functions by using samples of ordered entities as training data, and attributes of entities as features. An example of such functions is f (Entity) = +0.5 (Police budget) \u22120.8 (Crime rate), for ordering countries in terms of safety. As the size of training data is typically small in this task, we propose a machine learning method referred to as context-guided learning (CGL) to overcome the over-fitting problem. Exploiting a large amount of contexts regarding relations between the labeling criteria (e.g. safety) and attributes, CGL guides learning in the correct direction by estimating a roughly appropriate weight for each attribute by the contexts. This idea was implemented by a regularization approach similar to support vector machines. Experiments were conducted with 158 kinds of orders in three datasets. The experimental results showed high effectiveness of the contextual guidance over existing ranking methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Entity search is one of the emerging trends in major search engines [19, 32] , and has been powered by large-scale knowledge bases such as DBpedia, Wikidata, and YAGO. A wide variety of entity attributes are stored in knowledge bases and have enabled search engines to support entity search queries such as \"european countries\" and \"movies starring emma watson\".",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 72,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 73,
                    "end": 76,
                    "text": "32]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "On the other hand, the current entity search systems have not supported various kinds of rankings yet, which can be found on the Web, for example, the most livable countries, innovative companies, and high-performance cameras. If such diverse rankings were integrated into entity search and explained objectively with some evidences, users could be more efficient for accomplishing complex tasks such as decision making, comparison, and planning. For example, a user is planning to visit several European countries and inputs a query \"european countries safety\" to know how safe each country is. If an entity search engine provided a list of countries ranked by public safety and factors used to determine the ranking (e.g. crime rate and police budget), they would be helpful for the user to make his/her travel plan. Contexts are used to produce a \"rough\" prediction g1 of the ideal weights. CGL determines the weights w1 such that v1, the difference between w1 and g1, is small and training examples are separated well. The weights w1 are expected to be effective for the other cases, since a strong correlation between richness and GDP is suggested by their contexts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we propose a method for learning orders of entities using samples of ordered entities as training data and attributes of entities as features. Entity orders are expressed in several forms on the Web: comparative sentences (e.g. \"DiCaprio is taller than Pitt\"), scores (e.g. \"[Camera A] portrait: 9.2, landscape: 7.5, and sports: 8.5\"), and rankings (e.g. \"1st: Iceland, 2nd: Denmark, and 3rd: Austria\"). These expressions can be interpreted with a uniform model, i.e. a subset of entity pairs that defines an entity order, and be used as training data to learn entity orders. The learned models can be used not only to rank entities but also to explain rankings by correlated attributes. We assume that entity orders can be represented as a linear function of attributes (denoted by f ), primarily because of the high explanatory capacity for users. For example, given a list of entities ordered by labeling criterion \"safety\", (Iceland, Denmark, Austria), and their attributes such as \"GDP\", \"Crime rate\", and \"Police budget\", we learn function f (Entity) = +0.5 (Police budget) \u22120.8 (Crime rate).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A major challenge for this problem is the lack of training data. Many Web sites do not present all the ordered entities (see Table 1 ). Moreover, the size of training data might not be sufficiently large for some entity classes, even if all the ordered entities are described (e.g. only 50 states in the United States). As the number of attributes should be large enough to explain diverse orders, and can be increased easily with existing techniques [11, 28] , the problem of learning to rank entities can suffer from serious over-fitting problems.",
            "cite_spans": [
                {
                    "start": 451,
                    "end": 455,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 456,
                    "end": 459,
                    "text": "28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [
                {
                    "start": 125,
                    "end": 132,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "To cope with this essential problem, we propose a learning method referred to as context-guided learning (CGL). This method uses not only ordered entities but also contexts of labeling criteria and attributes to learn the function f . A labeling criterion refers to a textual representation to determine labels (or an order in a ranking problem). The context can provide the models with additional information, and guide learning in the correct direction by preventing over-fitting. Figure 1 illustrates how CGL is applied to a classification problem. (As can be seen later, CGL is first explained for a classification problem and later extended to a ranking problem). Our goal in this example is to learn a linear function for the labeling criterion l 1 (richness), which is defined as f 1 (x) = w T 1 x (an intercept is omitted for simplicity). When we simply apply an ordinary learning algorithm, learned weights can be w 1 = (1, 0) in (B) of Fig. 1 , indicating that the attribute a 1 is useful for this classification. Although these weights seem reasonable as their decision boundary perfectly separates positive (e 1 and e 2 ) and negative (e 3 ) examples, it is easy to anticipate that the attribute a 1 can be useless for the other cases if we know the meaning of the labeling criterion (i.e. richness) and attribute a 1 (i.e. temperature). CGL, on the other hand, incorporates contexts of the labeling criterion and attributes for making a \"rough\" prediction of the ideal weights, and expects the weights w 1 to be close to the \"rough\" prediction (denoted as g 1 in (C) of Fig. 1 ). Although the prediction based on contexts cannot be always accurate (indeed, the decision boundary of g 1 fails to classify examples well), g 1 suggests that the attribute a 1 is not strongly related to the labeling criterion, and guides the learning of the weights w 1 . Thus, the learning can be successful even if sufficient training data are not available. CGL does not require any annotations for the contexts. Alternatively, CGL learns multiple functions at the same time for learning the relationship between contexts and weights in the function f .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 483,
                    "end": 491,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 946,
                    "end": 952,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1583,
                    "end": 1589,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "To the best of our knowledge, CGL is the first attempt to leverage contexts of labeling criteria and features directly in machine learning (ML) problems. CGL is a general ML method and can be applied not only to ranking problems but also to classification and regression problems as long as relations between labeling criteria and features are described in a particular corpus.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our contributions in this paper can be summarized as follows: (1) we introduced the problem of learning to rank entities by using attributes as features, in order to rank entities by various criteria and precisely understand labeling criteria; (2) we proposed CGL, a general ML method using contexts of labeling criteria and features for preventing over-fitting; and (3) we conducted experiments with a wide variety of orders, and demonstrated the effectiveness of CGL in the task of learning to rank entities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We review related work on entity ranking and discuss the difference between CGL and existing ML methods, in particular, multi-task learning methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Entity ranking has been addressed in some tracks in INEX and TREC. The INEX Entity Ranking track held two tasks: entity ranking and entity list completion tasks [12] [13] [14] . The entity ranking task expected systems to return relevant entities in response to a given query, while the entity list completion task expected systems to return entities related to given example entities. The TREC Entity track offered related entity finding tasks, in which systems were expected to find entities related to a given entity, with the type of the target entity and nature of their relation [2] [3] [4] . Those tasks only expect that retrieved entities are ordered by the relatedness to given example entities, and do not expect different kinds of orders within related entities.",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 165,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 166,
                    "end": 170,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 171,
                    "end": 175,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 585,
                    "end": 588,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 589,
                    "end": 592,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 593,
                    "end": 596,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Entity Ranking"
        },
        {
            "text": "Apart from the evaluation campaigns, there are some work that addresses learning to rank entities. Kang et al. used a ranking algorithm based on a boosted tree model for finding entities related to a given query [24] . Tran et al. proposed a method of ranking entities based on salience and informativeness for timeline summarization of events [30] . Zhou et al. addressed a problem of finding entities that have a specified relation with an input entity [34] . They trained a ranker for each relation based on training queries and labeled entities by using features derived from search snippets regarding pairs of entities. Although this work and ours use contexts (or search snippets) for learning to rank entities, our rankers are built primarily on attributes of entities and does not use contexts of entity pairs. Jameel et al. proposed an entity embedding method for entity retrieval [22] . Their method is mainly based on the co-occurrence between entities and words, and does not directly model entity attributes.",
            "cite_spans": [
                {
                    "start": 212,
                    "end": 216,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 344,
                    "end": 348,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 455,
                    "end": 459,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 890,
                    "end": 894,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Entity Ranking"
        },
        {
            "text": "Some NLP tasks are also related to our task. Iwanari et al. tackled a problem of ordering entities in terms of a given adjective by using some evidences extracted from texts [20] . Their task is similar to ours as both address entity ranking in terms of a particular labeling criterion. While their method uses contexts of labeling criteria and entities, our method uses contexts of labeling criteria and attributes of entities.",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Entity Ranking"
        },
        {
            "text": "The important characteristics of CGL are summarized as follows: (1) weights in the function f are learned based on labels as well as contexts regarding labeling criteria and features, and (2) multiple functions are learned at the same time to learn the relationship between the contexts and weights in the function f . Below, we review several ML methods and discuss their relationship to CGL.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-task Learning"
        },
        {
            "text": "Multi-task learning is an approach to improving learning in each task by learning multiple tasks simultaneously [9] . CGL is considered as an instance of multi-task learning. Regularized multi-task learning, which was proposed by Evgeniou and Pontil, assumes that weights of multiple tasks are similar [15] . As explained later, their model is a special case of our model when contexts are all the same. Other models assume that weights are sampled from a common prior [10, 27, 33] . Argyriou et al. used an assumption that weights are represented in a low subspace common to multiple tasks [1] . In contrast to these methods using an assumption that all the tasks are related, some work selectively decides which tasks are related and are expected to share similar weights [21, 25] . Similarly, CGL uses contexts to measure the similarity between tasks implicitly, and tends to estimate similar weights for similar tasks. An interesting difference between CGL and the other multi-task learning methods is that CGL still works even if any pairs of tasks are not similar. CGL only requires that some contexts are similar among multiple tasks. Thus, the applicable scope of CGL is not limited to problems targeted by existing multi-task learning methods.",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 115,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 302,
                    "end": 306,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 469,
                    "end": 473,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 474,
                    "end": 477,
                    "text": "27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 478,
                    "end": 481,
                    "text": "33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 484,
                    "end": 504,
                    "text": "Argyriou et al. used",
                    "ref_id": null
                },
                {
                    "start": 591,
                    "end": 594,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 774,
                    "end": 778,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 779,
                    "end": 782,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Multi-task Learning"
        },
        {
            "text": "In this section, we first explain the problem of learning to rank entities from samples of ordered entities with attributes. We then introduce CGL, apply it to our problem, and explain some approaches to modeling contexts for CGL.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "Letting E be a set of entities of a particular class, we define an entity order as a total order on E, denoted by k . Each order has a labeling criterion (or an ordering criterion in this case) denoted by l k . For example, labeling criteria could include \"livability\", \"innovativeness\", \"beauty\", and \"performance\". A set of all (e i , e j ) \u2208 E \u00d7 E for which e i k e j holds is called a graph 1 of an entity order, denoted by G k . Orders are usually expressed on the Web as subsets of their graphs. Thus, we can observe and use only G k \u2286 G k for learning entity orders. For example, a ranking of safe countries \"1st: Iceland, 2nd: Denmark, and 3rd: Austria\" implies G k = {(\"Denmark\", \"Iceland\"), (\"Austria\", \"Iceland\"), (\"Austria\", \"Denmark\")} and l k = \"safety\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "Our principal purpose is to learn a linear function f k (e i ) = w T k e i based on a subset of a graph G k for each entity order k , where e i is an M -dimensional vector representing attributes of entity e i \u2208 E, and the d-th value of the vector represents a value of attribute a d . We expect that the function f k preserves the entity order k :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "for any e i , e j \u2208 E, so that entities can be ranked by entity order k with learned function f k . Moreover, attributes whose weights are non-zero are expected to explain the entity order well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "As we explained earlier, the key challenge of this problem is lack of training data: |G k | is typically small compared with the number of attributes M . For example, M = 83 for countries and M = 137 for cities in our experiments. Ranked lists of ten or fewer entities can provide only at most 45 entity pairs as training data, which are not considered as sufficiently large for learning. Moreover, M must be as large as possible for modeling a wide range of orders. Thus, some approaches are necessary for preventing the over-fitting problem caused by lack of training data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "The key idea in our work is to use data other than G k for learning w k effectively. One of the unique characteristics or assumptions in our problem is that textual representations for labeling criteria and attributes are available. Therefore, given a labeling criterion, it is possible to estimate a roughly appropriate weight for each attribute by leveraging the contexts regarding relations between the labeling criterion and attribute. This idea is instantiated as CGL, which is explained in the next subsection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Definition"
        },
        {
            "text": "We introduce CGL, our proposed learning method that leverages contexts of labeling criteria and features. We begin with CGL for classification problems and then extend it to be used for ranking problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "The input for a classification problem is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": ", K is the number of labeling criteria, and N k is the number of examples for the k-th labeling criteria. Labeling criterion l k is a textual representation to determine values for y k,i . For example, if x k,i represents a feature of a city and y k,i = +1 if the city is a metropolitan city, the labeling criterion l k could be \"metropolitan city\". Another example can be found in Fig. 1 . The d-th value of a vector should correspond to a particular feature and have a name denoted by a d . Example names include \"population\" and \"GDP\".",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 382,
                    "end": 388,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Context-Guided Learning"
        },
        {
            "text": "The requirements for CGL are summarized as follows: (1) A labeling criterion l k is expressed in language, (2) Features A = {a d } M d=1 are expressed in language, and (3) There is a corpus including contexts regarding relations between labeling criteria and feature names. It is not necessary that all the labeling criteria and feature are expressed in language. In contrast to multi-task learning, CGL does not require that tasks (or labeling criteria in CGL) are similar.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "A classification problem can be formalized as learning function f k for each labeling criterion k = 1, . . . , K such that f k (x k,i ) y k,i . To solve this problem, we use a linear function f k (x k,i ) = w T k x k,i . Letting c k,d represent contexts for labeling criterion l k and feature a d \u2208 A, we can use the contexts for estimating w k as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "where w k,d is the d-th value of w k , \u03c6 is a feature map function that transforms a context to a vector, and u is a weight vector that does not depend on labeling criteria. The equation above indicates that the weight for the labeling criterion l k and feature a d is estimated by their context c k,d and an intercept v k,d . Equation 1 is generalization of w k,d = z d + v k,d in the regularized multi-task learning [15] , where z d is a weight common to multiple tasks. Equation 1 is reduced to their model if all the contexts are the same. If contexts for two labeling criteria are similar, or equivalently, labeling criteria are similar, w k,d tends to be similar for these labeling criteria. This property is similar to some multi-task learning methods [21, 25] .",
            "cite_spans": [
                {
                    "start": 418,
                    "end": 422,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 759,
                    "end": 763,
                    "text": "[21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 764,
                    "end": 767,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "Based on Eq. 1, w k can be expressed as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "where v k = (v k,1 , . . . , v k,M ), g k = \u03a6 T k u, and \u03a6 k = (\u03c6(c k,1 ), . . . , \u03c6(c k,M )). This equation is illustrated in (C) of Fig. 1 . We expect that the \"rough\" prediction g k can be given by contexts of labeling criterion l k , and ideal weights are close to g k ; in other words, v k is not large.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 134,
                    "end": 140,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Context-Guided Learning"
        },
        {
            "text": "We propose to learn the linear function using a regularization approach similar to support vector machines (SVMs) and the regularized multi-task learning [15] . The optimization problem is shown below:",
            "cite_spans": [
                {
                    "start": 154,
                    "end": 158,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "subject, for k = 1, . . . , K and i = 1, . . . , N k , to the constraints that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "where c and C are hyper parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "Slack variables \u03be k,i measure the error of the linear functions on the training data, while the other terms are regularization terms for the weights u and v k . Hyper parameters c and C can control the effect of the contexts on the model and the sensitivity for the error on the training data: a large value for c increases the effect of the contexts, while a large value for C tends to inhibit misclassification of the training data. We learn multiple functions f k for k = 1, . . . , K with the single objective function so that we can learn the weight u based on the whole training data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "We show that Problem 1 can be solved in the same manner as would be used with the standard SVM. To this end, we first define a single function to be learned that summarizes functions f k for k = 1, . . . , K as F (x, k) = f k (x). This function, F : R M \u00d7 {1, . . . , K} \u2192 R, can be written as a linear function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "by using the following settings:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "where \u03c8 is a feature map function, and 0 is an M -dimensional vector whose values are all zeros. Reassigning x i to x k,i , y i to y k,i , and \u03be i to \u03be k,i (i = k\u22121 k =1 N k + i ), we can reduce Problem 1 to the standard SVM problem, as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning"
        },
        {
            "text": "subject, for i = 1, . . . , N, to the constraints that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. The optimization of Problem 1 is equivalent to solving the following problem:"
        },
        {
            "text": "Proof. The norm of w is w 2 = u 2 + c K v 2 . Therefore, the objective function of Problem 2 is rewritten as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. The optimization of Problem 1 is equivalent to solving the following problem:"
        },
        {
            "text": "which is equivalent to the objective function of Problem 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. The optimization of Problem 1 is equivalent to solving the following problem:"
        },
        {
            "text": "Since Problem 2 is the standard SVM problem, we can use the standard SVM dual problem for solving Problem 1. Furthermore, we can use an important characteristic of SVMs: i.e. non-linear functions can be used by means of kernels. While the linear function for classification (i.e. f k ) cannot be a nonlinear function owing to the form of the model, we can use a non-linear function for estimating the weights based on contexts (see Eq. 1). The kernel method for CGL provides us with a wide range of choices for the representation of contexts. They can be represented as vectors, sets of vectors, trees, etc. as long as the kernel function is appropriately designed for two contexts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. The optimization of Problem 1 is equivalent to solving the following problem:"
        },
        {
            "text": "We extend CGL to the ranking problem and explain how it can be applied to the problem of ranking entities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning for Ranking"
        },
        {
            "text": "The input for the ranking problem is D = {D k } K k=1 , where D k \u2286 R M \u00d7 R M , and K is the number of labeling criteria. Labeling criterion l k is a textual representation to determine the order for D k : i.e. (x k,i , x k,j ) in D k indicates that x k,j is higher than x k,i in terms of the labeling criterion l k . The d-th value of vectors in D k must correspond to a particular feature and have a name denoted by a d . The requirements are the same as those explained in regard to CGL for classification. A ranking problem can be formalized as a learning function f k for each labeling criterion k = 1, .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning for Ranking"
        },
        {
            "text": "As assumed in the classification problem, we use a linear function f k (x k,i ) = w T k x k,i . It is clear that the ranking problem can be reduced to the classification problem if we redefine D k as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning for Ranking"
        },
        {
            "text": "We can apply CGL for ranking to the problem in Problem Definition section by using vectors of entity pairs in G k as the training data, i.e. D k = (e i , e j )|(e i , e j ) \u2208 G k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context-Guided Learning for Ranking"
        },
        {
            "text": "Having described the learning method for the problem of ranking entities, we explain the context models used in the learning. Contexts can be a set of sentences or a set of documents regarding a labeling criterion and a feature. In this work, we describe methods of modeling contexts by using sentences retrieved from Web search results. Given labeling criterion l k and feature a d , we create a query combining l k and a d with an AND operator, and use the query to retrieve the top N (c) search results using a particular Web search engine (N (c) = 500 in our experiments). We then split snippets of the search results into sentences and find sentences including both the labeling criterion l k and the feature a d .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Context Models"
        },
        {
            "text": "We use two basic methods for modeling sentences. One is a vector representation based on the TF-IDF weighting, and the other is a distributional representation of sentences [26] . The vector representation based on the TF-IDF weighting is sparse, and not sensitive to the order of words, but it can represent exact words appearing in the context. In contrast, the distributed representation of sentences is dense, and sensitive to the word order, but it might not retain the exact words appearing in the context.",
            "cite_spans": [
                {
                    "start": 173,
                    "end": 177,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Context Models"
        },
        {
            "text": "This section explains data used in the experiment, describes experimental settings, and shows the experimental results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Since there is no publicly available dataset for our task, we first explain our development of a dataset and its statistics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "Various kinds of entity orders in three datasets were mined from the Web and from magazines both automatically and manually. The three datasets include City (more specifically, Japanese prefectures), Country, and Camera entities, respectively. These classes were selected primarily for the following reasons: (1) availability of a wide range of entity orders, (2) availability of attributes, and (3) diversity of statistics. The language scope of our dataset was Japanese, as we used a Japanese crowd-sourcing service in the evaluation. Entity names and attribute names were Japanese and translated into English for this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "Entity orders were mined from Web pages for City and Country datasets, and from ten Japanese camera magazines for Camera dataset. The retrieved ranked lists were converted into a set of pairs for each entity orders. We excluded entity sets including less than five entities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "Attributes for City and Country datasets were mined from tables in Web documents. We chose Web tables as a resource for obtaining attributes because (1) the extraction method can be accurate and language-independent, and (2) standardization of numerical values was not necessary as units of numerical LambdaMART [31] 0 Table 1 shows statistics and examples of entities, orders, and attributes. There are 158 entity orders in total. For most of the orders, we could not find all of the entities in a class in a ranking on the Web. There were many Web pages presenting the top three or ten entities for an order. Thus, the average number of entities per order is much less than the total number of entities.",
            "cite_spans": [
                {
                    "start": 312,
                    "end": 316,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [
                {
                    "start": 319,
                    "end": 326,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Data"
        },
        {
            "text": "We selected as baseline methods for this experiment some existing ranking methods that do not use contexts: (1) RankNet [5] : a pairwise ranking method that uses a neural network model and optimizes the cross entropy loss, (2) Rank-Boost [16] : application of AdaBoost [17] to pairwise preferences, (3) Linear-Feature [29] : a linear feature-based model optimized by coordinate ascent, (4) LambdaMART [31] : a combination of the ranking model, LambdaRank [6] , and the boosted tree model, MART [18] , and (5) ListNet [7] : a listwise ranking method using a neural network model. We used these methods implemented in RankLib 3 . We used normalized discounted cumulative gain (nDCG@10) [23] as an evaluation metric to be optimized for some methods.",
            "cite_spans": [
                {
                    "start": 120,
                    "end": 123,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 238,
                    "end": 242,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 269,
                    "end": 273,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 318,
                    "end": 322,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 401,
                    "end": 405,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 455,
                    "end": 458,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 494,
                    "end": 498,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 517,
                    "end": 520,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 684,
                    "end": 688,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "We conducted experiments using the developed dataset in the following settings. For each set of ordered entities G k , we split entities in the set E into 50:50, E train and E test , and obtained training data G train = {(e i , e j )|(e i , e j ) \u2208 G k \u2227 e i \u2208 E train \u2227 e j \u2208 E train } and test data G test = G k \u2212 G train . Our task in this experiment is to learn a model based on G train , and to predict the pairwise preference of e i and e j for (e i , e j ) \u2208 G test . We measured the accuracy defined as the fraction of correctly predicted pairwise preferences. We used five-fold cross validation on entity orders within E train of the same dataset to determine the best parameters for each method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Settings"
        },
        {
            "text": "We configured CGL with the following settings. Two context models were used: TF-IDF and Distributed (distributed representation with 400 dimensional vectors). Parameters c and C were determined using the cross validation explained above. A linear kernel (Linear) and an RBF kernel (RBF) were used for the kernel in CGL. Table 2 shows the accuracy in the three datasets with the standard error of the mean (SEM). CGL in any settings were better than any of the baseline methods. Among the CGL-based methods, the best method was CGL (TF-IDF, Linear), followed by CGL (Distributed, RBF). The total improvement over the best baseline method, LambdaMart, was 11.6%. According to a randomized Tukey HSD test [8] 4 (\u03b1 = 0.01), the differences between CGL (TF-IDF, Linear) and all the baseline methods were found to be statistically significant, while there was no statistically significant difference across methods based on CGL.",
            "cite_spans": [
                {
                    "start": 702,
                    "end": 705,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 320,
                    "end": 327,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Settings"
        },
        {
            "text": "CGL (TF-IDF, Linear) achieved 8%, 11%, and 18% improvements over LambdaMART for City, Country, and Camera, respectively. We hypothesize that the quality and amount of contexts are the main factors that determine the effectiveness of CGL, based on the observation that the number of sentences used for modeling contexts per attribute was 36.0, 45.7, and 137 for City, Country, and Camera, respectively. We also conducted evaluation of the attributes used in the learned functions. Five attributes with the highest absolute weights for each entity order were pooled, and then presented to users in a Japanese crowd-sourcing service, Lancers 5 . In this evaluation, we aimed to understand to what extent the learned attributes could explain the orders. The instruction was as follows: \"If you agree that there is a correlation between <labeling criterion> and <attribute>, please assign a score +2. If you disagree, please assign a score \u22122. If you cannot agree or disagree, please assign a score 0.\" Users could choose a rate from \u22122, \u22121, 0, +1, and +2. We assigned five users for each pair of a labeling criterion and an attribute. The best CGL method, CGL (TF-IDF, Linear), was selected for this evaluation. LinearFeature was used as a baseline method, since only this method used a linear function among the baseline methods. Figure 2 shows the distribution of rates for five attributes with the highest absolute weights. The average rates of CGL were \u22120.455, \u22120.166, and +0.581, while those of LinearFeature were \u22120.560, \u22120.204, +0.516 for City, Country, and Camera datasets, respectively. These average rates show a high correlation with the accuracy of the models. Even though CGL could find more reasonable attributes in all of the classes than LinearFeature, their differences were small for those datasets. The average rates for City and Country datasets were negative indicating low explainability of the attributes. This is partially because some attributes only correlate to a particular labeling criterion, but were not considered as causes for increasing the criterion. Although CGL could learn a more accurate model than the baseline methods, it is still challenging to find highly explanatory attributes for a given label criterion.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1327,
                    "end": 1335,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Finally, we show some examples of linear functions learned by CGL in Table 3 . Most of the attributes seem explainable and can possibly affect the entity order. While the others do not seem explanatory for the labeling criteria (e.g. \"population/family\" for \"attractiveness\" and \"highest temperature\" for \"avg. savings\"), they correlate well to the labeling criteria in our dataset, and are examples of attributes that were considered unreasonable in the subjective evaluation, but highly contributed to the prediction.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 69,
                    "end": 76,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "In this paper, we addressed the problem of learning orders of entities, by using partially observed orders as training data and attributes of entities as features. We proposed a learning method called context-guided learning (CGL) to avoid the over-fitting problem caused by lack of training data, and demonstrated the effectiveness of CGL for 158 orders in three datasets. Our future work includes theoretical analysis of CGL, application of CGL to the other problems (e.g. a fact verification task), exploration of better context models, and improvement of the efficiency of CGL for a large amount of data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Convex multi-task feature learning",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Argyriou",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Evgeniou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pontil",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Mach. Learn",
            "volume": "73",
            "issn": "3",
            "pages": "243--272",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Overview of the TREC 2010 entity track",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Serdyukov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "De Vries",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Overview of the TREC 2011 entity track",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Serdyukov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "De Vries",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Overview of the TREC 2009 entity track",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "De Vries",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Serdyukov",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Westerveld",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Learning to rank using gradient descent",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Burges",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "ICML",
            "volume": "",
            "issn": "",
            "pages": "89--96",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Learning to rank with nonsmooth cost functions",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Burges",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ragno",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "NIPS",
            "volume": "",
            "issn": "",
            "pages": "193--200",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Learning to rank: from pairwise approach to listwise approach",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "F"
                    ],
                    "last": "Tsai",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "ICML",
            "volume": "",
            "issn": "",
            "pages": "129--136",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Multiple testing in statistical analysis of systems-based information retrieval experiments",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "A"
                    ],
                    "last": "Carterette",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ACM TOIS",
            "volume": "30",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Multitask learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Caruana",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Mach. Learn",
            "volume": "28",
            "issn": "1",
            "pages": "41--75",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Bayesian multitask learning with latent hierarchies",
            "authors": [
                {
                    "first": "Iii",
                    "middle": [],
                    "last": "Daum\u00e9",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "135--142",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Extraction and approximation of numerical attributes from the web",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Davidov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rappoport",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1308--1317",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Overview of the INEX 2007 entity ranking track",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "De Vries",
                    "suffix": ""
                },
                {
                    "first": "A.-M",
                    "middle": [],
                    "last": "Vercoustre",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Thom",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Craswell",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lalmas",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "INEX 2007",
            "volume": "4862",
            "issn": "",
            "pages": "245--251",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-540-85902-4_22"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Overview of the INEX 2009 entity ranking track",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Demartini",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Iofciu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "De Vries",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "INEX 2009",
            "volume": "6203",
            "issn": "",
            "pages": "254--264",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-14556-8_26"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Overview of the INEX 2008 entity ranking track",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Demartini",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "De Vries",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Iofciu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "INEX 2008",
            "volume": "5631",
            "issn": "",
            "pages": "243--252",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-03761-0_25"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Regularized multi-task learning",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Evgeniou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pontil",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "KDD",
            "volume": "",
            "issn": "",
            "pages": "109--117",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "An efficient boosting algorithm for combining preferences",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Iyer",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Schapire",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "J. Mach. Learn. Res",
            "volume": "4",
            "issn": "",
            "pages": "933--969",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Schapire",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "J. Comput. Syst. Sci",
            "volume": "1",
            "issn": "55",
            "pages": "119--139",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Greedy function approximation: a gradient boosting machine",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Friedman",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Ann. Stat",
            "volume": "29",
            "issn": "5",
            "pages": "1189--1232",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Named entity recognition in query",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "SIGIR",
            "volume": "",
            "issn": "",
            "pages": "267--274",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Ordering concepts based on common attribute intensity",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Iwanari",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Yoshinaga",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kaji",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Nishina",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Toyoda",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kitsuregawa",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IJCAI",
            "volume": "",
            "issn": "",
            "pages": "3747--3753",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Clustered multi-task learning: a convex formulation",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jacob",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Vert",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "R"
                    ],
                    "last": "Bach",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "NIPS",
            "volume": "",
            "issn": "",
            "pages": "745--752",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Member: Max-margin based embeddings for entity retrieval",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jameel",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Bouraoui",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Schockaert",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "SIGIR",
            "volume": "",
            "issn": "",
            "pages": "783--792",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Cumulated gain-based evaluation of ir techniques",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "J\u00e4rvelin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kek\u00e4l\u00e4inen",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "ACM TOIS",
            "volume": "20",
            "issn": "4",
            "pages": "422--446",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Learning to rank related entities in web search",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Torzec",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Neurocomputing",
            "volume": "166",
            "issn": "",
            "pages": "309--318",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Learning task grouping and overlap in multi-task learning. In: ICML",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "Iii",
                    "middle": [],
                    "last": "Daum\u00e9",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1383--1390",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Distributed representations of sentences and documents",
            "authors": [
                {
                    "first": "Q",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ICML",
            "volume": "",
            "issn": "",
            "pages": "1188--1196",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Learning a meta-level prior for feature relevance from multiple related tasks",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "I"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Chatalbashev",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Vickrey",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Koller",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "ICML",
            "volume": "",
            "issn": "",
            "pages": "489--496",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Numerical relation extraction with minimal supervision",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Madaan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "R"
                    ],
                    "last": "Mausam",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Ramakrishnan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sarawagi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2764--2771",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Linear feature-based models for information retrieval",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Metzler",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "B"
                    ],
                    "last": "Croft",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Inf. Retrieval",
            "volume": "10",
            "issn": "3",
            "pages": "257--274",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Balancing novelty and salience: Adaptive learning to rank entities for timeline summarization of high-impact events",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "A"
                    ],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Nieder\u00e9e",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kanhabua",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Gadiraju",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anand",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1201--1210",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Adapting boosting for information retrieval measures",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Burges",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Svore",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Inf. Retrieval",
            "volume": "13",
            "issn": "3",
            "pages": "254--270",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Building taxonomy of Web search intents for name entity queries",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1001--1010",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Learning gaussian processes from multiple tasks",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Tresp",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Schwaighofer",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "ICML",
            "volume": "",
            "issn": "",
            "pages": "1012--1019",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Learning to rank from distant supervision: exploiting noisy redundancy for relational entity search",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "C C"
                    ],
                    "last": "Change",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "ICDE",
            "volume": "",
            "issn": "",
            "pages": "829--840",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "(A) Entities e1 and e2 are rich countries, and e3 is not a rich country. They have only two attributes a1 (temperature) and a2 (GDP). (B) Every entity can be expressed as a point in a two dimensional space by their attribute values in this example. Our goal is to learn a linear function for the labeling criterion l1, which is defined as f1(x) = w T 1 x. One of the possible weights that perfectly classify the training examples is w 1 = (1, 0), but not necessarily effective for the other examples. (C)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "usually consistent within a table. Attributes for Camera dataset were scraped from Web pages of a Japanese Web site, Kakaku.com 2 , which provides prices and specifications of products. All the numerical values for each attribute were normalized into [0, 1].",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Statistics of the datasets and examples of entities, orders, and attributes.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Examples of linear functions learned by CGL, in which three attributes for the highest absolute weights are shown.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Fig. 2. Distribution of rates for five attributes with the highest absolute weights.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgments. This work was supported by JSPS KAKENHI Grant Numbers JP16H02906, JP17H00762, JP18H03243, and JP18H03244, and JST PRESTO Grant Number JPMJPR1853, Japan.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}