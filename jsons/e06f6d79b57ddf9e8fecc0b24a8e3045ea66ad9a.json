{
    "paper_id": "e06f6d79b57ddf9e8fecc0b24a8e3045ea66ad9a",
    "metadata": {
        "title": "On Biomedical Named Entity Recognition: Experiments in Interlingual Transfer for Clinical and Social Media Texts",
        "authors": [
            {
                "first": "Zulfat",
                "middle": [],
                "last": "Miftahutdinov",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Chemoinformatics and Molecular Modeling Laboratory",
                    "institution": "Kazan Federal University",
                    "location": {
                        "settlement": "Kazan",
                        "country": "Russia"
                    }
                },
                "email": "zulfatme@gmail.com"
            },
            {
                "first": "Ilseyar",
                "middle": [],
                "last": "Alimova",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Chemoinformatics and Molecular Modeling Laboratory",
                    "institution": "Kazan Federal University",
                    "location": {
                        "settlement": "Kazan",
                        "country": "Russia"
                    }
                },
                "email": "alimovailseyar@gmail.com"
            },
            {
                "first": "Elena",
                "middle": [],
                "last": "Tutubalina",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Chemoinformatics and Molecular Modeling Laboratory",
                    "institution": "Kazan Federal University",
                    "location": {
                        "settlement": "Kazan",
                        "country": "Russia"
                    }
                },
                "email": "tutubalinaev@gmail.com"
            }
        ]
    },
    "abstract": [
        {
            "text": "Although deep neural networks yield state-of-the-art performance in biomedical named entity recognition (bioNER), much research shares one limitation: models are usually trained and evaluated on English texts from a single domain. In this work, we present a fine-grained evaluation intended to understand the efficiency of multilingual BERT-based models for bioNER of drug and disease mentions across two domains in two languages, namely clinical data and user-generated texts on drug therapy in English and Russian. We investigate the role of transfer learning (TL) strategies between four corpora to reduce the number of examples that have to be manually annotated. Evaluation results demonstrate that multi-BERT shows the best transfer capabilities in the zero-shot setting when training and test sets are either in the same language or in the same domain. TL reduces the amount of labeled data needed to achieve high performance on three out of four corpora: pretrained models reach 98-99% of the full dataset performance on both types of entities after training on 10-25% of sentences. We demonstrate that pretraining on data with one or both types of transfer can be effective.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Drugs and diseases play a central role in many areas of biomedical research and healthcare. A large part of the biomedical research has focused on scientific abstracts in English; see a good overview of the field in [9] . In contrast to the biomedical literature, research into the processing of electronic health records (EHRs) and user-generated texts (UGTs) about drug therapy has not reached the same level of maturity. The bottleneck of modern supervised models for named entity recognition (NER) is the human effort needed to annotate sufficient training examples for each language or domain. Moreover, state of the art text processing models may perform extremely poorly under domain shift [14] . Recent advances in neural networks, especially deep contextualized word representations via language models [1, 4, 12, 16] and Transformer-based architectures [19] , offer new opportunities to improve NER models in the biomedical field.",
            "cite_spans": [
                {
                    "start": 216,
                    "end": 219,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 697,
                    "end": 701,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 812,
                    "end": 815,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 816,
                    "end": 818,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 819,
                    "end": 822,
                    "text": "12,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 823,
                    "end": 826,
                    "text": "16]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 863,
                    "end": 867,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we take the task a step further from existing monolingual research in a single domain [2, 3, 6, 12, 13, 20, 22] by exploring multilingual transfer between EHRs and UGTs in different languages. Our goal is not to outperform state of the art models on each dataset separately, but to ask whether we can transfer knowledge from a high-resource language, such as English, to a lowresource one, e.g., Russian, for NER of biomedical entities. Our transfer learning strategy involves pretraining the multilingual cased BERT [4] on one corpus and transferring the learned weights to initialize training on a gold-standard corpus in another language or domain. In this work, we seek to answer the following research questions: RQ1: How well does a BERT-based NER model trained on one corpus works for the detection of drugs and diseases from another language or domain in the zero-shot setting? RQ2: Given a small number of training examples, can the NER model perform as well as a model trained on much larger datasets? RQ3: Will transfer learning help achieve more stable performance on a varying size of training data?",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 103,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 104,
                    "end": 106,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 107,
                    "end": 109,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 110,
                    "end": 113,
                    "text": "12,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 114,
                    "end": 117,
                    "text": "13,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 118,
                    "end": 121,
                    "text": "20,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 122,
                    "end": 125,
                    "text": "22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 531,
                    "end": 534,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "All experiments are carried out on 4 datasets: English corpora CADEC [10] and n2c2 [7] , a dataset of EHRs in Russian [17] , and our novel dataset of UGTs in Russian. All three existing corpora share an entity of interest with our corpus. To our knowledge, this is the first work exploring the interlingual transfer ability of multilingual BERT on bioNER on two domains in English and Russian.",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 83,
                    "end": 86,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 118,
                    "end": 122,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Each corpus is characterized by two parameters: (i) language: English (EN) or Russian (RU); (ii) domain: electronic health records (EHRs) or user-generated texts (UGTs). A statistical summary of the datasets is presented in Table 1 . Since all corpora have different annotation schemes for disease-related entities, and these subtypes are highly imbalanced in the corpus, we join them into a single primary type named Disease. Further, we unify the names of four datasets according to their characteristics. [10] contains medical forum posts taken from AskaPatient.com about 12 drugs of two categories: Diclofenac and Lipitor. Medical students and computer scientists annotated the dataset. The agreement between four annotators computed on a set of 55 user posts was approximately 78% for Diclofenac and 95% for Lipitor posts.",
            "cite_spans": [
                {
                    "start": 508,
                    "end": 512,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [
                {
                    "start": 224,
                    "end": 231,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Data"
        },
        {
            "text": "Our Dataset of UGTs (RU UGT). We utilized and annotated user posts in Russian from a publicly accessible source Otzovik.com; we note that we have obtained all reviews without accessing password-protected information. Four annotators from the I.M. Sechenov First Moscow State Medical University and the department of pharmacology of the Kazan Federal University were asked to read the review and highlight all spans of text including drug names and patient's health conditions experienced before/during/after the drug use. The agreement between two annotators computed on a set of 100 posts was 72%.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CADEC (EN UGT). CSIRO Adverse Drug Event Corpus"
        },
        {
            "text": "Shelmanov et al. [17] created a corpus of Russian clinical notes from a multi-disciplinary pediatric center. The authors extended an annotation scheme from the CLEF eHealth 2014 Task 2.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 21,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Russian EHRs (RU EHR)."
        },
        {
            "text": "This corpus consists of de-identified EHRs [7] . Two independent annotators annotated each record in the dataset and a third annotator resolved conflicts. For both EHR corpora, the agreement rates were not provided.",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 46,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "n2c2 (EN EHR)."
        },
        {
            "text": "For NER, we utilize BERT with a softmax layer over all possible tags as the output. Word labels are encoded with the BIO tag scheme. The model was trained on a sentence level. Due to space constraints, we refer to [4, 12] for more details. In particular, we use BERT base , Multilingual Cased (Multi-BERT), which is pretrained on 104 languages and has 12 heads, 12 layers, 768 hidden units per layer, and a total of 110M parameters. All models were trained without fine-tuning or explicit selection of parameters. The loss function became stable (without significant decreases) after 35-40 epochs. We use Adam optimizer with polynomial decay to update the learning rate on each epoch with warm-up steps in the beginning. As baselines, we utilized LSTM-CRF with default settings from the Saber library [5] and BioBERT [12] . For LSTM-CRF, we adopted (i) 200-dim. word2vec embeddings trained on 2.5M of health-related posts in English [18] and (ii) 300-dim. word2vec embeddings trained on the Russian National Corpus [11] .",
            "cite_spans": [
                {
                    "start": 214,
                    "end": 217,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 218,
                    "end": 221,
                    "text": "12]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 801,
                    "end": 804,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 817,
                    "end": 821,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 933,
                    "end": 937,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Models"
        },
        {
            "text": "We randomly split each of the datasets into 70% training set and 30% test set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Evaluation"
        },
        {
            "text": "We trained a total of 720 models on one machine with 8 NVIDIA P40 GPUs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Evaluation"
        },
        {
            "text": "The training of all models took approximately 96 h. We compare all models in terms of precision (P), recall (R), and F1-score (F) on the test sets with exactly matching criteria via a CoNLL script. Our experiments are available at https:// github.com/dartrevan/multilingual experiments. Table 2 shows the in-corpus (IC) performance of Multi-BERT with BioBERT and LSTM-CRF when trained and tested on the same corpus. On all datasets, BERT-based models achieve the best scores over LSTM-CRF based on word embeddings. The difference in the performance of BioBERT and Multi-BERT is not statistically significant; we measured significance with the two-tailed t-test (p \u2264 0.05). All models achieve much higher performance for the detection of drugs rather than diseases; it can be explained by boundary problems in multi-word expressions (see the av. length in Table 1 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 287,
                    "end": 294,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 855,
                    "end": 862,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experiments and Evaluation"
        },
        {
            "text": "Transfer. To answer RQ1, we trained Multi-BERT on one corpus and then applied it to another language/domain in a zero-shot fashion, i.e., without further training. Results of the out-of-corpus (OOC) performance of Multi-BERT are presented in Table 3 . For drug recognition, the best generalizability is achieved when training on EHRs and evaluated on UGTs in English. For OOC performance on the EN UGT corpus, the model reaches F1-scores of 77.08% and 36.31% when trained on the EN EHR and RU UGT corpora, respectively, while IC reaches the F1-score of 84.88%. We note that the number of sentences in the EN EHR corpus is nine times higher than in the EN UGT corpus. 78% of Drug tokens in the EN UGT corpus are presented in the EN EHR set (see Table 4 ). For OOC performance on the RU UGT corpus, the model achieves F1-scores of 26.31% and 34.78% when trained on the EN UGT and EN EHR corpora, respectively, while the IC performance is F1-score of 60.45%. For disease recognition, Multi-BERT generalizes much worse to corpora other than it was trained on. For OOC performance on the RU UGT corpus, the model achieves F1-scores of 24.12% and 30.86% when trained on the EN UGT and RU EHR corpora, respectively, while the IC performance is F1-score of 49.35%. For OOC performance on the EN UGT corpus, the model obtains F1scores of 37.94% and 4.32% when trained on the RU UGT and EN EHR corpora, respectively, while the IC performance is F1-score of 67.25%. One possible explanation might be that there are well-known differences in layperson language and professional medical terms.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 242,
                    "end": 249,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 744,
                    "end": 751,
                    "text": "Table 4",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Zero-Shot"
        },
        {
            "text": "Few-Shot Transfer. Transfer learning aims to solve the problem on a \"target\" dataset using knowledge learned from a \"source\" dataset [5, 15, 21] . In the transfer learning setting, the BERT-based NER model was pretrained on one of three \"source\" datasets (see Table 2 for the IC performance of these models). To answer RQ2 and RQ3, we begin with a random sampling of 50 sentences from a \"target\" training set, train the pretrained model on this subsampled dataset, and test it on the \"target\" test set. Next, we increase the sample size by 50 sentences of the \"target\" training set and repeat the described procedure, doing so up to 2000 sentences of the training set. In each round, we train from scratch to avoid overfitting, as suggested in [8] .",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 136,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 137,
                    "end": 140,
                    "text": "15,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 141,
                    "end": 144,
                    "text": "21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 744,
                    "end": 747,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 260,
                    "end": 267,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Zero-Shot"
        },
        {
            "text": "For each pretraining setup, we record the size of the subset when the model achieves at least 99% of the F1-measure achieved on the full dataset. Results for the RU UGT, RU EHR, and EN UGT datasets are given in Table 5 and Fig. 1 . Multi-BERT pretrained on the EN UGT set and trained on 2000 sentences from the EN EHR corpus (2.81% of the full corpus) obtains 92% F1 and 76% F1 of the full dataset performance on drugs and diseases respectively. As shown in Table 5 and Fig. 1 , models with transfer knowledge outperform the models without the pretraining phase even in cases when both domain and language shifts between \"source\" and \"target\" sets. Using the transfer learning strategy could require up to 550 sentences less than training from scratch. In particular, models require only 10% and 23% of the EN UGT and RU URT corpora respectively to achieve results as good as full dataset performances. We believe that this observation is very crucial for low resource languages and new domains (e.g., social media, clinical trials). We observe that the performance of models with pretraining setup trained on the different numbers of sentences becomes more stable in terms of deviations between F1-scores (see Fig. 1 ). ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 211,
                    "end": 218,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 223,
                    "end": 229,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 458,
                    "end": 465,
                    "text": "Table 5",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 470,
                    "end": 476,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1211,
                    "end": 1217,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Zero-Shot"
        },
        {
            "text": "We studied the task of recognition of drug and disease mentions in English and a low-resource language in the biomedical area, using a newly collected Russian corpus of user reviews about drugs (RU UGT) with 3,624 manually annotated entities. We ask: can additional pretraining on an existing dataset be helpful for bioNER performance of multilingual BERT-based NER model on a new dataset with a small number of labeled examples if the domain, the language, or both shift between these datasets? Our study consisted of over 720 models trained on different subsets of two corpora in English and two corpora in Russian. For each language, we experimented with the clinical domain, i.e., electronic health records, and the social media domain, i.e., reviews about drug therapy. As expected, models with pretraining on data in the same language or the same domain obtain better results in zero-shot or few-shot settings. To our surprise, we found that pretraining on data with two shifts can be effective. The model with the best pretraining achieves 99% of the full dataset performance using only 23.56% of the training data on our RU URT corpus, while the model with pretraining on data with two shifts (the EN EHR set) used 26.1% of the training data. The model without pretraining achieves similar results on the RU URT corpus using 31.97% of the training set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        },
        {
            "text": "We foresee three directions for future work. First, transfer learning and multitask strategies on three and more domains remain to be explored. Second, a promising research direction is the evaluation of multilingual BERT on a broad set of entities. Third, future research will focus on the creation of fine-grained entity types in our corpus of Russian reviews that can help in finding associations between drugs and adverse drug reactions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Publicly available clinical BERT embeddings",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Alsentzer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
            "volume": "",
            "issn": "",
            "pages": "72--78",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "A neural network multi-task learning approach to biomedical named entity recognition",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Crichton",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pyysalo",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chiu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Korhonen",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "BMC Bioinform",
            "volume": "18",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "D3NER: biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "H"
                    ],
                    "last": "Dang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "Q"
                    ],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "T"
                    ],
                    "last": "Vu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Bioinformatics",
            "volume": "34",
            "issn": "20",
            "pages": "3539--3546",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "W"
                    ],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Toutanova",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "volume": "1",
            "issn": "",
            "pages": "4171--4186",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Towards reliable named entity recognition in the biomedical domain",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Giorgi",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Bader",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Bioinformatics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Fully contextualized biomedical NER",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Sarkar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Gattu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Azzopardi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Stein",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Fuhr",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Mayr",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hauff",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ECIR 2019",
            "volume": "11438",
            "issn": "",
            "pages": "117--124",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-15719-7_15"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "n2c2 shared task on adverse drug events and medication extraction in electronic health records",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Henry",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Buchan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Filannino",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Stubbs",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Uzuner",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J. Am. Med. Inform. Assoc",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Active learning with partial feedback",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "C"
                    ],
                    "last": "Lipton",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Anandkumar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1802.07427"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Community challenges in biomedical text mining over 10 years: success, failure and the future",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "C"
                    ],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Brief. Bioinform",
            "volume": "17",
            "issn": "1",
            "pages": "132--144",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Cadec: a corpus of adverse drug event annotations",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Karimi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Metke-Jimenez",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kemp",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "J. Biomed. Inform",
            "volume": "55",
            "issn": "",
            "pages": "73--81",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Size vs. structure in training corpora for word embedding models: araneum russicum maximum and Russian National Corpus",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kutuzov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kunilovskaya",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "AIST 2017",
            "volume": "10716",
            "issn": "",
            "pages": "47--58",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "BioBERT: pre-trained biomedical language representation model for biomedical text mining",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Bioinformatics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Deep neural models for medical concept normalization in user-generated texts",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Miftahutdinov",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Tutubalina",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
            "volume": "",
            "issn": "",
            "pages": "393--399",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "ScispaCy: fast and robust models for biomedical natural language processing",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Neumann",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Beltagy",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ammar",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1902.07669"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "A survey on transfer learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEEE Trans. Knowl. Discov. Data Eng",
            "volume": "22",
            "issn": "10",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Peters",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "1",
            "issn": "",
            "pages": "2227--2237",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Information extraction from clinical texts in Russian",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shelmanov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Smirnov",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Vishneva",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Computational Linguistics and Intellectual Technologies: Papers from the Annual International Conference",
            "volume": "14",
            "issn": "",
            "pages": "537--549",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Using semantic analysis of texts for the identification of drugs with similar therapeutic effects",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "V"
                    ],
                    "last": "Tutubalina",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Russ. Chem. Bull",
            "volume": "66",
            "issn": "11",
            "pages": "2180--2189",
            "other_ids": {
                "DOI": [
                    "10.1007/s11172-017-2000-8"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "HUNER: improving biomedical NER with pretraining",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Weber",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "M\u00fcnchmeyer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Rockt\u00e4schel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Habibi",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Leser",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Bioinformatics",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A survey of transfer learning",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Khoshgoftaar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "J. Big Data",
            "volume": "3",
            "issn": "1",
            "pages": "1--40",
            "other_ids": {
                "DOI": [
                    "10.1186/s40537-016-0043-6"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A neural multi-task learning framework to jointly model medical named entity recognition and normalization",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "volume": "33",
            "issn": "",
            "pages": "817--824",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Performance of Multi-BERT models with pre-training on the source dataset (a corpus's name in a legend) or without pre-training (\"No pretrain\" line) for the EN UGT, RU UGT, RU EHR datasets. Y-axis: F1-scores for detection of Drug or Disease mentions, X-axis: the number of sentences used for training.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Summary statistics of four datasets. Summary of each dataset includes the number of Drug and Disease entities, the number of documents and sentences, the average length of a document (in sentences), the average length of a sentence (in tokens), the average length of a Drug/Disease entity (in tokens).",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "In-corpus (IC) performance of multi-BERT with comparison to BiLSTM-CRF and BioBERT, measured by Precision, Recall, and F1score with an exact matching criteria.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Out",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Summary",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Summary of the number of training sentences needed to achieve 99% of the full dataset performance with Multi-BERT with pretraining.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We thank Sergey Nikolenko for helpful discussions. This research was supported by the Russian Science Foundation grant # 18-11-00284.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments."
        }
    ]
}