{
    "paper_id": "17a7bccd1d6f13d76838f88b379591cb75be20fd",
    "metadata": {
        "title": "JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation",
        "authors": [
            {
                "first": "Yu-Huan",
                "middle": [],
                "last": "Wu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Shang-Hua",
                "middle": [],
                "last": "Gao",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jie",
                "middle": [],
                "last": "Mei",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jun",
                "middle": [],
                "last": "Xu",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Deng-Ping",
                "middle": [],
                "last": "Fan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Chao-Wei",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Ming-Ming",
                "middle": [],
                "last": "Cheng",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Recently, the novel coronavirus 2019 has caused a pandemic disease over 200 countries, influencing billions of humans. To control the infection, the first and key step is to identify and separate the infected people. But due to the lack of Reverse Transcription Polymerase Chain Reaction (RT-PCR) tests, it is essential to discover suspected COVID-19 patients via CT scan analysis by radiologists. However, CT scan analysis is usually time-consuming, requiring at least 15 minutes per case. In this paper, we develop a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID-19 diagnosis. To train our JCS system, we construct a large scale COVID-19 Classification and Segmentation (COVID-CS) dataset, with 144,167 CT images of 400 COVID-19 patients and 350 uninfected cases. 3,855 CT images of 200 patients are annotated with fine-grained pixel-level labels, lesion counts, infected areas and locations, benefiting various diagnosis aspects. Extensive experiments demonstrate that, the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0% and a specificity of 93.0% on the classification test set, and 78.3% Dice score on the segmentation test set, of our COVID-CS dataset. The online demo of our JCS diagnosis system will be available soon.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "C ORONAVIRUS disease 2019, or COVID-19, is an epidemic disease caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2). It outbreaks around the world in a short period of time, and has caused 1,439,516 confirmed cases and 85,711 confirmed deaths as of April 10th 2020. COVID-19 pushes the health systems of over 200 countries to the brink of collapse due to the lack of medical supplies and staffs, and thus has been declared as a pandemic by the World Health Organization [1] . Current golden standard diagnostic method for COVID-19 cases is via viral nucleic acid detection using Reverse Transcription Polymerase Chain Reaction (RT-PCR) [2] . However, the shortage of RT-PCR test kits around the world [3] makes this golden standard test indeed as precious as gold. Besides, this process needs cumbersome operations in highly controlled environment, usually taking about 4 hours [4] to receive the test results, limiting its spread popularization [5] . What's more, the false negative cases of RT-PCR tests are the potential thread to public wellness. To hinder the terrific infection of COVID-19, medical radiology imaging is employed as an ultra-fast alternative for discovering the rapidly growing suspected or asymptomatic cases. This is based on the fact that the clinical signs of chest X-rays for most COVID-19 patients suffer from lung infection [6] . The work of [7] demonstrates that CT scan tests exhibit higher sensitivity than the RT-PCR ones. This point is further validated by [8] , in which CT scans and RT-PCR tests obtain the sensitivity of 98% and 71%, respectively. However, the diagnosis duration is still the major limitation of CT scan tests: even experienced radiologists need about 21.5 minutes [9] to analyze the test results of each case.",
            "cite_spans": [
                {
                    "start": 492,
                    "end": 495,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 658,
                    "end": 661,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 723,
                    "end": 726,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 900,
                    "end": 903,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 968,
                    "end": 971,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1375,
                    "end": 1378,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1513,
                    "end": 1516,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1741,
                    "end": 1744,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Thanks to the powerful discriminative ability of deep convolutional neural networks (CNNs), artificial intelligence (AI) technologies are reforming the medical imaging community. Deep CNNs are usually trained on large scale datasets to demonstrate their capability. However, most of existing CT scan datasets for COVID-19 [5] , [10] - [12] could not meet this demand, as they contain at most hundreds of CT images from tens of cases. Besides, most of the current COVID-19 datasets only provide the patient-level labels (i.e., class labels) of indicating whether the person is infected or not, and lack of fine-grained pixel-level annotations. Thus, CNN models trained with these datasets usually neglect the valuable information for explaining the final predictions. Despite several CT scan diagnosis systems [8] , [13] - [17] have been established for testing the suspected COVID-19 cases, most of them suffer from two drawbacks: 1) they are trained on small scale datasets and thus not robust enough for versatile COVID-19 infections; 2) they perform classification based on the black box CNNs, while lacking the explainable transparency to assist the doctors during the medical diagnosis.",
            "cite_spans": [
                {
                    "start": 322,
                    "end": 325,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 328,
                    "end": 332,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 335,
                    "end": 339,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 809,
                    "end": 812,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 815,
                    "end": 819,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 822,
                    "end": 826,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "To largely alleviate the above-mentioned drawbacks, in this work, we 1) construct a large scale COVID-CS dataset with both patient-level and pixel-level annotations and 2) propose a Joint Classification and Segmentation (JCS) based diagnosis system, to provide explainable diagnosis results for medical staffs fighting with COVID-19. Specifically, we utilize the collected COVID-CS dataset that contains thousands of CT images from hundreds of COVID-19 cases to train our JCS system for better diagnosis performance. As illustrated in Figure 1 , our JCS diagnosis system firstly identifies the suspected COVID-19 patients by a classification model, and provide the diagnosis explanations via activation mapping techniques [18] . Then, our system is feasible to discover the locations and areas of the COVID-19 infection in lung radiography via fine-grained image segmentation techniques. With the explainable classification results and corresponding fine-grained lesion segmentation, our JCS system largely simplifies and accelerates the diagnosis process for radiologists or other medical staffs. As shown in Table II surpassing previous state-of-the-art segmentation methods. The remaining paper is organized as follows. In \u00a7II, we briefly summarize the related works. In \u00a7III, we present our COVID-CS dataset with our labeling procedures in detail. In \u00a7IV, we introduce the developed diagnosis system for recognizing and analysing the COVID-19 cases. Extensive experiments are conducted in \u00a7V to evaluate the performance of our system on COVID-19 recognition, with in-depth analysis.",
            "cite_spans": [
                {
                    "start": 722,
                    "end": 726,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 535,
                    "end": 543,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1110,
                    "end": 1118,
                    "text": "Table II",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u00a7VI concludes this work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Currently, over a million people are infected by COVID-19. But their CT scans are usually private and could not be publicly accessed. To facilitate the development of diagnosis systems, several COVID-19 related datasets are publicly released by Table I . One X-ray dataset from Cohen et al. [10] contains overall 122 frontal view X-rays, including 100 images of COVID-19 cases, 11 SARS images and 11 other pneumonia images. The COVID-CT dataset from [5] has 746 CT scan images, with 349 images from COVID-19 patients and 397 from non-COVID-19 cases. All the images in these datasets are collected from public websites and/or COVID-19 related papers on medRxiv, bioRxiv, and journals, etc. CTs containing COVID-19 abnormalities are selected by reading the figure captions in the papers. Some other resources of COVID-19 dataset are PLXR [11] and CTSeg [12] , which contains 98 and 110 CT scan images cases, respectively. These datasets are in a small scale and lack of diversity, since they often contain at most hundreds of images from tens of cases. To fully exploit the power of deep CNNs, it is extremely essential to construct a large scale dataset for the training of deep CNNs in accurate and robust COVID-19 systems.",
            "cite_spans": [
                {
                    "start": 291,
                    "end": 295,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 450,
                    "end": 453,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 836,
                    "end": 840,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 851,
                    "end": 855,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 245,
                    "end": 252,
                    "text": "Table I",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "A. Existing Accessible COVID-19 Datasets"
        },
        {
            "text": "Most of current medical imaging systems are developed for common diseases that exist for many years, e.g., tuberculosis [19] . These developed systems can be directly modified to attenuate the COVID-19 outbreak. The doctors find that the chest X-rays of COVID-19 patients exhibiting certain abnormalities in the radiography. Based on ResNet-50 [20] , COVID-ResNet [21] is proposed to differentiate three different types COVID-19 infections from the normal pneumonia individuals. On 1531 chest X-ray images, Zhang et al. proposed a deep anomaly detection system for COVID-19 screening, achieving 96.0% sensitivity and 70.65% specificity. Yang et al. [22] proposed a system to evaluate the images of 102 volunteers, with a sensitivity of 83.3% and specificity of 94.0%. The system developed by Li et al. [23] identifies 78 COVID-19 patients with a sensitivity of 82.6% and a specificity of 100.0% by using the axial and coronal-view of lung CT severity index (CTSI). Chung et al. [14] confirmed via collected from 21 patients that, visual inspection helps to identify the COVID-19 cases and predict the severity via the overall lung total severity score (LTSS). Bernheim et al. [15] analyzed the 121 COVID-19 patients, and carried on a visual check by experienced radiologist to categorize them as early, intermediate and late cases. Wang et al. [16] found that the COVID-19 disease will be severe during 6-11 days from the infection, based on a study on 366 CT scans of 90 patients. Shi et al. [17] developed an imaging assisted diagnosis procedure to detect the COVID-19 caused pneumonia. Fang et al. [8] examined 81 patients by procedure based on the CTSI, and obtained a sensitivity of 98.0%, contrast to the sensitivity of 71.0% by RT-PCR. Zhou et al. [24] implemented the examination using the non-contrast CTSI of 62 COVID-19 patients, confirming that the CT assisted evaluation shows better detection accuracy in progressive stage confirmed to the early stage. Despite their success on small set of samples, these COVID-19 diagnosis systems have not been tested by large scale samples, and could not provide useful diagnostic evidence during their diagnostic inference. As far as we know, the work of [25] is the only one that extracts infected region via pixel-level segmentation. But the segmentation is performed via the watershed transform techniques [26] with coarse results and limited accuracy. In this work, we propose a diagnosis system by integrating learning based classification and segmentation networks, to provide explainable diagnostic evidence for doctors and improve the user-interactive aspects of the diagnosis process.",
            "cite_spans": [
                {
                    "start": 120,
                    "end": 124,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 344,
                    "end": 348,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 364,
                    "end": 368,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 649,
                    "end": 653,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 802,
                    "end": 806,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 978,
                    "end": 982,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1601,
                    "end": 1604,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 2207,
                    "end": 2211,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 2361,
                    "end": 2365,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "B. COVID-19 Diagnosis Systems"
        },
        {
            "text": "Ever since the release of ImageNet dataset [27] , deep convolutional neural networks (CNNs) are becoming the workhorse for image classification tasks with improving performance. Representative deep classifiers, e.g., AlexNet [28] , VGGNet [29] , ResNet [20] , DenseNet [30] , and Res2Net [31] , have been widely employed as the feature extractors for other computer vision tasks, such as image segmentation [32] , salient object detection [33] , face recognition [34] , aerial images analysis [35] , feature matching [36] , and image restoration [37] , etc. Despite the impressive representation ability of these classifiers, the classification process is in a black box, providing no explanation of the predicted results.",
            "cite_spans": [
                {
                    "start": 43,
                    "end": 47,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 225,
                    "end": 229,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 253,
                    "end": 257,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 269,
                    "end": 273,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 288,
                    "end": 292,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 407,
                    "end": 411,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 439,
                    "end": 443,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 463,
                    "end": 467,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 493,
                    "end": 497,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 517,
                    "end": 521,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 546,
                    "end": 550,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "C. Deep Classification and Segmentation Methods"
        },
        {
            "text": "Image segmentation tackles the problem of pixel-level predictions. Semantic segmentation aims to distinguish the stuffs from each other [38] . Representative work in this area include the DeepLab [39] - [41] and the MobileNet [42] . Instance segmentation focuses on discriminating foreground objects in the image [31] . Panoptic segmentation [43] integrates the semantic-level and instance-level segmentation, and considers both stuff-level and object-level predictions. UNet [44] is a widely employed network for medical image segmentation analysis. It is further extended to 3D U-Net [45] , TernausNet [46] , and UNet++ [47] with promising performance on versatile image segmentation tasks. In this work, we develop a novel COVID-19 diagnosis system by integrating deep-based image classification and segmentation techniques.",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 140,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 196,
                    "end": 200,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 203,
                    "end": 207,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 226,
                    "end": 230,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 313,
                    "end": 317,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 342,
                    "end": 346,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 476,
                    "end": 480,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 586,
                    "end": 590,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 604,
                    "end": 608,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 622,
                    "end": 626,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "C. Deep Classification and Segmentation Methods"
        },
        {
            "text": "Data acts as a basis role in the deep-based AI diagnosis system. Currently, there are few publicly available COVID-19 datasets with either large scale samples or fine-grained pixellevel labeling. To fill in this gap, we construct a new COVID-19 Classification and Segmentation (COVID-CS) dataset. In this section, we present the data collection, professional labeling and statistics of our dataset. Fig. 2 shows some examples of our COVID-CS dataset. Fig. 4 presents diverse information in the segmentation set of our COVID-CS dataset. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 399,
                    "end": 405,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 451,
                    "end": 457,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "III. OUR COVID-CS DATASET"
        },
        {
            "text": "To protect the patients' privacy, we omit their personal information in our dataset construction. We collected 144,167 CT scan images from 750 cases, 400 of which are positive cases of COVID-19 and the other 350 cases are negative, all confirmed by RT-PCR tests. As previous studies [48] did, we do not take the community acquired pneumonia (CAP) patients (see Fig. 2 ) into consideration. All involved patients underwent standard chest CT scans. The CT scanners include BrightSpeed, Discovery CT750 HD, LightSpeed VCT, LightSpeed16, Revolution CT from GE Medical Systems, Aquilion ONE from Toshiba, and uCT 780 from United Imaging Healthcare. The numbers of cases from different scanners are summarized in Table III . The thickness of reconstructed CT slices ranges from 0.75mm to 1.25mm (percentage from 1.0% to 67.0%, please refer to Fig. 3 for more details).",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 287,
                    "text": "[48]",
                    "ref_id": "BIBREF47"
                }
            ],
            "ref_spans": [
                {
                    "start": 361,
                    "end": 367,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 707,
                    "end": 716,
                    "text": "Table III",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 837,
                    "end": 843,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "A. Data Collection"
        },
        {
            "text": "We provide two aspects of labels for the collected CT scan images in our COVID-CS dataset, so as to implement joint classification and segmentation tasks. As mentioned above, our dataset is divided into 400 COVID-19 cases and 350 uninfected cases. For the segmentation task, we perform professional labeling through the following strategies:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Professional Labeling"
        },
        {
            "text": "\u2022 In order to save their labeling time, the radiologists only select at most 30 discrete CT scan images for each patient, in which the infections are observed for further annotation. In this step, our goal is to label every infected area with pixel-level annotations. \u2022 To generate high-quality annotations, we first invite a radiologist to mark as many infected areas as possible based on his/her clinical experience. Then we invite another senior radiologist to refine the labeling marks several times for cross-validation. Some inaccurate labels are fixed after this step. By implementing the above annotation procedures, we finally obtain 3,855 pixel-level labeled CT scan images of 200 COVID-19 patients with a resolution of 512\u00d7512. 64,771 CT images of the other 200 COVID-19 patients are without pixel-level annotation due to the shortage of radiologists, but such data will be used in classification test. As can be seen in the last three columns of Fig. 2 , our COVID-CS dataset covers different levels, i.e., mild, medium, and severe, of COVID-19 cases.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 958,
                    "end": 964,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "B. Professional Labeling"
        },
        {
            "text": "Age. The 400 COVID-19 patients (175 males and 225 females) range from 14 to 89 years, with an average age of 48.9 years. Fig. 3 shows the distribution of ages, the counts of samples in age ranges, and the gender percentages. Lesion count. As shown in Fig. 5 (a) , we illustrate the distribution of lesion counts. We observe that the lesion count distributes from 1 to 10 in each CT scan image. Infected areas. We plot the widths and heights of the infected areas in Fig. 5 (b) . The ranges of width and height are 7 \u223c 191 and 8 \u223c 271, respectively, showing diverse distributions. Location. We also show the relationship between each infected area and the corresponding central location (x0, y0) in Fig. 5 (c) . As can be seen, the normalized infected areas range from the smallest size (35/28452 pixels) to the largest size (28452/28452 pixels). It also shows that, in our COVID-CS dataset, the infected areas are evenly distributed in diverse locations, which are also evenly distributed in lungs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 121,
                    "end": 127,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 251,
                    "end": 261,
                    "text": "Fig. 5 (a)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 466,
                    "end": 476,
                    "text": "Fig. 5 (b)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 698,
                    "end": 708,
                    "text": "Fig. 5 (c)",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "C. Dataset Statistics"
        },
        {
            "text": "IV. OUR COVID-19 DIAGNOSTIC SYSTEM Our JCS system consists of an explainable classification model to identify the COVID-19 infected cases and a seg- mentation model to discover the infected areas. The classifier is trained on large amount of images with low-cost patientlevel annotations. And the segmentation model is trained with accurately annotated CT images, performing fine-grained lesion segmentation. By integrating the two models, our JCS system provides informative diagnosis results for COVID-19.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Dataset Statistics"
        },
        {
            "text": "Owing to the strong representation ability of CNNs, the COVID-19 infections can be predicted through only patientlevel supervised training. To this end, we propose a classification model to endow our JCS diagnosis system the capability of discriminating the COVID-19 patients.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Explainable Classification Model"
        },
        {
            "text": "1) Diagnosing COVID-19 via Classification: Predicting whether the suspected patient is COVID-19 positive or not is basically a binary classification task based on his/her CT scan images. The designing of novel classification model is not our focus, here we build our classifier based on the Res2Net network [31] . As a powerful network, it has the capability of multi-scale representation. The last layer is modified as a fullyconnected layer with two channels, to output the probability of COVID-19 infection or not. If the probability of infected channel is larger than that of the uninfected one, the patient is diagnosed as COVID-19 positive, and vice versa. For each patient, the CT images are sent to classification model one by one. If the number of infected CT images is larger than a threshold, the patient is diagnosed as COVID-19 positive.",
            "cite_spans": [
                {
                    "start": 307,
                    "end": 311,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "A. Explainable Classification Model"
        },
        {
            "text": "2) Explanation by Activation Mapping: As the diagnosis process of CNN classification is in a black box, we employ the activation mapping [18] to increase the explainable transparency of our COVID-19 diagnosis system on its predictions. The last convolutional layer of the classification network is followed by a global average pooling (GAP) layer and a fully-connected layer. Through the GAP layer, our classification model downsamples the feature size from (H, W ) to (1, 1), and thus lost the spatial representation ability. Through activation mapping [18] , our system finds the response region of the prediction result, through the hypothesis that the gradient of regions in features before GAP layer is consistent with the evidence for prediction. The feature map before GAP layer contains both high-level semantic and location information. Each channel corresponds to the activation for different semantic cues. The activation mapping is obtained through the gradients of the predicted probability to the feature map. Specifically, given the prediction of COVID-19 branch y p and the feature map X before GAP, the weight for the k-th channel of X is calculated as:",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 141,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 554,
                    "end": 558,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "A. Explainable Classification Model"
        },
        {
            "text": "where X k i,j is the value at position (i, j) in the k-th channel of feature map X. Larger gradients in Eqn. (1) produce larger weight of the activation mapping for a certain channel. The activation mapping for a COVID-19 case is computed as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Explainable Classification Model"
        },
        {
            "text": "As shown in Fig. 8 , the activation mapping accurately locates the infected areas of COVID-19 patients, providing explainable results for the prediction of our JCS system.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "A. Explainable Classification Model"
        },
        {
            "text": "3) Alleviating Data Bias by Image Mixing: By utilizing our explainable classification model, our system can be trained only with patient-level annotation. However, since CT images are from multiple sources, the classifier may be possibly trained to overfit unwanted areas (e.g., the area outside the lesion), as been observed via the activation mapping. Therefore, we propose to utilize the image mixing technique [49] and help the classifier focus on the lesion areas of COVID-19 cases. The CT images from different sources and the corresponding patient-level annotations are mixed during training. Specifically, for two randomly sampled CT images x i and x j (i = j) and corresponding labels\u0177 i and\u0177 j , the newly mixed sample and the corresponding label are written as:",
            "cite_spans": [
                {
                    "start": 414,
                    "end": 418,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "A. Explainable Classification Model"
        },
        {
            "text": "where \u03bb \u2208 [0, 1] is a random number generated in Beta distribution, i.e., \u03bb \u223c Beta(\u03b1, \u03b1). With mixed samples, our classification model is trained to focus more on the decisive lesion areas of COVID-19 cases, rather than the bias in data source. Also, the mixing process weakens the confidence of labels, and thus alleviating our system from overfitting. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Explainable Classification Model"
        },
        {
            "text": "Our segmentation model aims at discovering the exact lesion areas from the CT images of COVID-19 patients. Fig. 6 shows the architecture of our segmentation model. 1) Encoder-Decoder Architecture: Our segmentation model consists of an encoder and a decoder. Encoder. The encoder is based on the VGG-16 [29] backbone, without the last two fully-connected layers. It has five VGG blocks defined as {E 1 , E 2 , E 3 , E 4 , E 5 }, respectively. The VGG-16 backbone is first fed with the CT images, and produces multi-scale feature maps from the last layers of the five VGG blocks. To downsize the input feature map by half, the front of each block (except the first one) is a max pooling function with a stride of 2. The feature map produced by the block E 1 contains the finest features with the highest resolution, while the feature map by the block E 5 is coarsest with lowest resolution. To achieve better performance, we propose an Enhanced Feature Module (EFM, will be introduced in \u00a7IV-B2) for our encoder to improve its representational power. The EFM module is added after the last layer conv5 3 in the block E 5 . It consists of two Grouped Atrous Modules (GAM) to extract stronger feature maps with larger receptive fields. The GAM module generates an extra smaller feature map, which is of half size compared to the coarsest feature map of the VGG-16 backbone. It also enhances the representational power of the feature map produced by the block E 5 . Hence, our encoder produces six levels of feature maps {1, 2, 4, 8, 16 , 32}, respectively. As we employ a Ushape encoder-decoder architecture [50] , all these six feature maps will be used in the decoder, as will be introduced later. Decoder. Our decoder has five side-outputs with 5 different sizes. Here, we do not predict the side-output from the coarsest feature map with stride of 32, and thus no side-output matches the size of the coarsest feature map M 6 E . In our decoder, we propose an Attentive Feature Fusion (AFF, will be introduced in \u00a7IV-B3) strategy to aggregate the feature maps from different stages and predict the side-output of each stage. Our AFF emphasizes the significance of the top-level feature map, and utilizes the attention mechanism to filter useful features from the bottom feature map. The last output with the same resolution of the CT image input will be chosen as the final prediction.",
            "cite_spans": [
                {
                    "start": 302,
                    "end": 306,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 1604,
                    "end": 1608,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [
                {
                    "start": 107,
                    "end": 113,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1516,
                    "end": 1531,
                    "text": "{1, 2, 4, 8, 16",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "B. Accurate Segmentation Model"
        },
        {
            "text": "2) Enhanced Feature Module: The proposed EFM module is added after the last layer of E 5 in the VGG-16 encoder. It consists of two sequential GAM modules, and a max pooling function between them. As shown in Fig. 7 (a) , the first layer of the GAM module is a 1 \u00d7 1 convolution layer to expand the channels of the feature map. Then the feature map is equally divided into 4 groups. Different from the trivial group convolution, we deploy the atrous convolution with different atrous rates to the 4 groups so as to derive a more abundant feature map with various receptive fields. To fully exploit useful features, we adopt the Squeeze-Excitation (SE) unit [51] in our network. That is, each channel of the feature map is multiplied a channel factor calculated by a SE block, which consists of two linear layers followed by a sigmoid function. We set the reduction rate in the SE block as 4. To reduce the output channels by half, we add an 1 \u00d7 1 convolution layer after the SE block. At last, we use a 3 \u00d7 3 convolution layer, in which the number of channels equals to that of input feature map, as the transition layer to the next module.",
            "cite_spans": [
                {
                    "start": 656,
                    "end": 660,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [
                {
                    "start": 208,
                    "end": 218,
                    "text": "Fig. 7 (a)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "B. Accurate Segmentation Model"
        },
        {
            "text": "3) Attentive Feature Fusion: Traditional fusion strategy of top-down decoders [50] , [52] treats the input feature maps equally. To better aggregate the feature maps, we propose an Attentive Feature Fusion (AFF) strategy. In our AFF fusion strategy, the feature map with smaller size is more valued. As shown in Fig. 7 (b) , the input feature maps M i E and M i+1 D in current stage are reduced to half size via 1 \u00d7 1 convolution layers. Then the reduced M i+1 D is up-sampled by bilinear interpolation to output a double sized feature map. We concatenate the two outputs together, and apply the SE block (also used in GAM) to produce an enhanced feature map. This feature map is then concatenated with the feature map of doubly up-sampled output in previous stage. After the concatenation we use another SE block to enhance the feature map again. After each SE block we use a 3 \u00d7 3 convolution layer, with the same number of channels as the input, as the transition layer. An 1\u00d71 convolution layer with a single neuron will be used to predict one feature map as the side-output of the current stage.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 85,
                    "end": 89,
                    "text": "[52]",
                    "ref_id": "BIBREF51"
                }
            ],
            "ref_spans": [
                {
                    "start": 312,
                    "end": 322,
                    "text": "Fig. 7 (b)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "B. Accurate Segmentation Model"
        },
        {
            "text": "Although the final prediction is only from the last side-output, we apply the deep supervision strategy [53] to all side-outputs with different sizes. For each side-output, we up-sample it to the size of the ground-truth map, and compute the sum of the standard binary cross-entropy loss and the Dice loss [54] as follows:",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 108,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 306,
                    "end": 310,
                    "text": "[54]",
                    "ref_id": "BIBREF53"
                }
            ],
            "ref_spans": [],
            "section": "4) Deep Supervision Loss:"
        },
        {
            "text": "where the binary cross-entropy loss is averaged among all H \u00d7 W pixels, p i,j is the confidence score at pixel (i, j) calculated by a sigmoid function, and means the elementwise production. P and G are predicted map and groundtruth map, respectively, while P 1 and G 1 denote the corresponding 1 norms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4) Deep Supervision Loss:"
        },
        {
            "text": "An explainable classifier or accurate segmentation model itself could not fully implement comprehensive functions for COVID-19 diagnosis. Comparing to the segmentation model, our classifier is trained with CT images from both COVID-19 infected and uninfected cases, benefiting from more training data with lower annotation costs. Although our classifier can provide explainable lesion location of COVID-19 through activation mapping techniques, it cannot perform accurate and complete lesion segmentation. To this end, our segmentation model further provides complementary analysis by discovering the complete lesions in lung and estimate the severity of the COVID-19 patients. But annotating vast segmentation labels by experienced radiologists is prohibitively expensive. To integrate their advantages for better application, we develop a diagnosis system for COVID-19 via joint explainable classification and segmentation models. In practice, our classification model will first predict whether the CT images of a suspected case to be COVID-19 positive or not. If the prediction is positive, the suspected case is very likely to be infected by COVID-19. Then our segmentation model will be performed on the CT images for in-depth analysis, and discover the whole infected areas in each CT image.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Joint Diagnosis"
        },
        {
            "text": "In our JCS system, the classification and segmentation models are trained separately. For the classification model, we train it with a batch-size of 256 on 4 GPUs. The CT images are resized into 224 \u00d7 224 for computational efficiency. We adopt the SGD optimizer with the initial learning rate of 0.1, which is divided by 10 in every 30 epochs. The classifier is trained with 100 epochs. For data augmentation, we use the random horizontal flip and random crop, and the image mixing technique [49] to alleviate the data bias. The \u03b1 in Beta distribution of image mixing is set as 0.5. For the segmentation model, the number of CT images in each mini-batch is always 4, and the size of the input CT images is unchanged as 512 \u00d7 512. The backbone of our segmentation model is pretrained on ImageNet [27] . The atrous rates of four atrous convolutions in two sequential GAMs are {1, 3, 6, 9} and {1, 2, 3, 4}, respectively. The initial learning rate is 2.5 \u00d7 10 \u22125 . We adopt the poly learning rate policy that the actual learning rate will be multiplied by a factor (1 \u2212 cur iter max iter ) power , where the power is 0.9. The segmentation model is trained with 21000 iterations. We employ the Adam [55] optimizer, and set \u03b2 1 , \u03b2 2 as 0.9 and 0.999 respectively. For data augmentation, we use random horizontal flip and random crop.",
            "cite_spans": [
                {
                    "start": 492,
                    "end": 496,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 795,
                    "end": 799,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1195,
                    "end": 1199,
                    "text": "[55]",
                    "ref_id": "BIBREF54"
                }
            ],
            "ref_spans": [],
            "section": "D. Implementation Details"
        },
        {
            "text": "Training/Test Protocol. For the segmentation task, our training set contains 2,794 images from 150 COVID-19 patients and the test set has 1,061 images from other 50 COVID-19 cases. For the classification task, the training set contains the 2,794 images from the 150 COVID-19 infected cases in the segmentation set. In addition, we randomly pick 150 uninfected cases with 7,500 CT images as negative cases for training. The test set contains the 64,711 images of the other randomly selected 200 infected cases and the 68,041 images from 200 uninfected cases. Evaluation Metrics. For the classification task, we adopt the widely used metrics of specificity and sensitivity as suggested by [19] . For the segmentation task, we use two standard metrics, i.e., Dice score [56] and Intersection over Union (IoU). To provide more comprehensive evaluation, we further use the enhanced alignment measure (E \u03c6 ) [57] . Comparison methods. On classification task, we compare our classification model with or without image mixing technique [49] . On segmentation task, to provide in-depth evaluation of our JCS model, we compare it with versatile cutting-edge models, i.e., the UNet [50] for medical imaging and the DSS [33] , PoolNet [58] , and EGNet [59] for saliency detection.",
            "cite_spans": [
                {
                    "start": 687,
                    "end": 691,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 767,
                    "end": 771,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 902,
                    "end": 906,
                    "text": "[57]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1208,
                    "end": 1212,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "text": "[59]",
                    "ref_id": "BIBREF58"
                }
            ],
            "ref_spans": [],
            "section": "V. EXPERIMENTS A. Experimental Settings"
        },
        {
            "text": "Performance on explainable classification. Fig. 8 shows the visualization of activation mapping of our classification model trained with or without image mixing [49] . The activation mapping (AM) of our classification model trained with random horizontal flip and random crop (i.e., the \"AM origin\" in Fig. 8 ) not only covers the lesion areas, but also presents unrelated areas. This indicates that the classification model is biased to non-lesion areas. By introducing the image mixing technique [49] , the AM of our classification model provides more accurate locations of the lesion areas (the \"AM mixing\" in Fig. 8 ). During the inference, AM assists the medical staffs using our JCS system to judge whether the prediction is correct or not. When the number of CT images from a suspected patient is larger than a threshold, the patient is diagnosed as COVID-19 positive. Changing the threshold enables our model to achieve a trade-off between sensitivity and specificity. Table IV shows the results of the classification model under different thresholds on the test set of our COVID-CS dataset. One can see that our model is very robust to the changing of thresholds, and achieves a sensitivity of 95.0% and a specificity of 93.0% when the threshold is 25. However, AM could not provide accurate segmentation of lesion areas (if have). Subsequently, we further employ our segmentation model to discover the lesion areas in the CT images of COVID-19 patients. Comparison on segmentation performance. Table V lists the quantitative comparisons of 4 cutting-edge methods and our model on segmentation. It can be seen that the proposed [58] and EGNet [59] obtains comparable results on the three metrics. U-Net [50] performs better than DSS [33] in terms of IoU, though they are comparable on the Dice score. Fig. 9 shows the qualitative results of the comparison methods. One can see that the other competitors produce inaccurate or even wrong predictions of the lesion areas in the CT images of mild, medium and severe COVID-19 infections. But our segmentation model correctly discovers the whole lesion areas on all levels of COVID-19 infections.",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 165,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 498,
                    "end": 502,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 1652,
                    "end": 1656,
                    "text": "[59]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1742,
                    "end": 1746,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [
                {
                    "start": 43,
                    "end": 49,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 302,
                    "end": 308,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 613,
                    "end": 619,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 977,
                    "end": 985,
                    "text": "Table IV",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1504,
                    "end": 1511,
                    "text": "Table V",
                    "ref_id": null
                },
                {
                    "start": 1810,
                    "end": 1816,
                    "text": "Fig. 9",
                    "ref_id": null
                }
            ],
            "section": "B. Results"
        },
        {
            "text": "To further study its stability, we perform statistical analysis of our segmentation model on our segmentation test set. Fig. 10 CT image GT Ours PoolNet [58] EGNet [59] DSS [33] U-Net [50] (a) shows the correlation between the Dice score of our model and the infected areas of CT images. Note that the CT images with infected area \u2265 8000mm 2 are not plotted in Fig. 10 (a) , since they only occupy 1.0% of all CT images in terms of quantity. We observe that 95.0% CT images have the Dice scores in [0.6, 1], while the other 3.3% CT images are with Dice scores between [0.1, 0.6) and recognized as bad cases. Only 1.7% CT images suffer from Dice score of less than 0.1, and they are taken as failure cases. We also explore the relationship between the lesion count of each slice and the Dice score from a different perspective. As shown in Fig. 10 (b) , the probability distribution of Dice score is little affected by the number of lesion counts in a CT image. The medium dice score is above 0.8 for 4 different cases of lesion counts, and the 95.0% confidence interval lies in [0. 5, 1] . We also observe that the lesion count of failure cases is \u2264 2. The consistently promising performance on segmenting lesion areas and the low probability (1.7%) of failure confirm the stability of our segmentation model. Diagnosis time. The speed test of JCS system is on a single RTX 2080Ti. Assuming each suspected case has 300 CT images, the classification model in JCS only costs about 1s to ensure whether infected. If infected, The segmentation model will spend 18.0s on fine-grained lesion segmentation. Hence, JCS system costs 19s for each case. Note that the complete RT-PCR test and radiologist CT diagnosis cost about 4 hours and 21.5 minutes respectively.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 157,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 164,
                    "end": 168,
                    "text": "[59]",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 173,
                    "end": 177,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 1082,
                    "end": 1084,
                    "text": "5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1085,
                    "end": 1087,
                    "text": "1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 120,
                    "end": 127,
                    "text": "Fig. 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 361,
                    "end": 372,
                    "text": "Fig. 10 (a)",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 839,
                    "end": 850,
                    "text": "Fig. 10 (b)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "B. Results"
        },
        {
            "text": "To facilitate the training of strong CNN models for COVID-19 diagnosis, in this paper, we systematically constructed a large scale COVID-19 Classification and Segmentation (COVID-CS) dataset. We also developed a Joint Classification and Segmentation (JCS) system for COVID-19 diagnosis. In our system, the classification model identified whether the suspected patient is COVID-19 positive or not, along with convincing visual explanations. It obtained a 95.0% sensitivity and 93.0% specificity on the classification test set of our COVID-CS dataset. To provide complementary pixel-level prediction, we implemented a segmentation model to discover fine-grained lesion areas in the CT images of COVID-19 patients. Comparing to the competing methods, e.g., PoolNet [58] , our segmentation model achieved an improvement of 0.078 on Dice metric. Our JCS system is also very stable. On our segmentation test set, it failed only on 1.7% images and obtained Dice scores between [0.6, 1] for 95.0% of images. The online demo on COVID-19 diagnosis will be available soon.",
            "cite_spans": [
                {
                    "start": 762,
                    "end": 766,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                }
            ],
            "ref_spans": [],
            "section": "VI. CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Coronavirus disease (covid-19) outbreak situation",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "A deep learning algorithm using ct images to screen for corona virus disease",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Testing backlog linked to shortage of chemicals needed for covid-19 test",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "G"
                    ],
                    "last": "Malone",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Development of a laboratory-safe and low-cost detection protocol for sars-cov-2 of the coronavirus disease 2019 (covid-19)",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Won",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Choi",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Covid-ct-dataset: a ct scan dataset about covid-19",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.13865"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Covid-19 screening on chest x-ray images using deep learning based anomaly detection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Correlation of chest ct and rt-pcr testing in coronavirus disease 2019 (covid-19) in china: a report of 1014 cases",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ai",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lv",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Sensitivity of chest ct for covid-19: comparison to rt-pcr",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "The battle against coronavirus disease 2019 (covid-19): Emergency management and infection control in a radiology department",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of the American College of Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Covid-19 image data collection",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Cohen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Morrison",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Dao",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Covid-19 patients lungs x ray images 10000",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sajid",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Covid-19 ct segmentation dataset",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "B"
                    ],
                    "last": "Jenssen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Artificial intelligence distinguishes covid-19 from community acquired pneumonia on chest ct",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Ct imaging features of 2019 novel coronavirus (2019-ncov)",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chung",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bernheim",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Mei",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cui",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "A"
                    ],
                    "last": "Fayad",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jacobi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "295",
            "issn": "",
            "pages": "202--207",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Relationship to duration of infection",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bernheim",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Mei",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [
                        "A"
                    ],
                    "last": "Fayad",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Diao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jacobi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chung",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "19",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Temporal changes of ct findings in 90 patients with covid-19 pneumonia: A longitudinal study",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Radiological findings from 81 patients with covid-19 pneumonia in wuhan, china: a descriptive study",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Alwalid",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "The Lancet Infect Disease",
            "volume": "20",
            "issn": "4",
            "pages": "425--434",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "R"
                    ],
                    "last": "Selvaraju",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cogswell",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vedantam",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Parikh",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Batra",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Rethinking computer-aided tuberculosis diagnosis",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y.-H",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ban",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Covid-resnet: A deep learning framework for screening of covid19 from radiographs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Farooq",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hafeez",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "ArXiv",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Chest ct severity score: An imaging tool for assessing severe covid-19",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhen",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Radiology: Cardiothoracic Imaging",
            "volume": "2",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Ct image visual quantitative evaluation and clinical classification of coronavirus disease (covid-19)",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "European Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Coronavirus disease 2019: initial chest ct findings",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "European Radiology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Harmony-search and otsu based system for coronavirus disease (covid-19) detection using lung ct scan images",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Rajinikanth",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dey",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N J"
                    ],
                    "last": "Raj",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Hassanien",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "C"
                    ],
                    "last": "Santosh",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "S M"
                    ],
                    "last": "Raja",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "The watershed transform: Definitions, algorithms and parallelization strategies",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Roerdink",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Meijster",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Fundamenta Informaticae",
            "volume": "",
            "issn": "1",
            "pages": "187--228",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Imagenet: A large-scale hierarchical image database",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "L.-J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "248--255",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Imagenet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Adv. Neural Inform. Process. Syst",
            "volume": "",
            "issn": "",
            "pages": "1097--1105",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Very deep convolutional networks for large-scale image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Simonyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Learn. Represent",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Densely connected convolutional networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Van Der Maaten",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "Q"
                    ],
                    "last": "Weinberger",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Res2net: A new multi-scale backbone architecture",
            "authors": [
                {
                    "first": "S.-H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "X.-Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "M.-H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "",
            "issn": "",
            "pages": "1--1",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Hfs: Hierarchical feature selection for efficient image segmentation",
            "authors": [
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bian",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Torr",
                    "suffix": ""
                },
                {
                    "first": "S.-M",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "867--882",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Deeply supervised salient object detection with short connections",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Borji",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "41",
            "issn": "4",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Regularface: Deep face recognition via exclusive regularization",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "1136--1144",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Vecroad: Point-based iterative graph exploration for road graphs extraction",
            "authors": [
                {
                    "first": "Y.-Q",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "S.-H",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "X.-Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "An evaluation of feature matchers for fundamental matrix estimation",
            "authors": [
                {
                    "first": "J.-W",
                    "middle": [],
                    "last": "Bian",
                    "suffix": ""
                },
                {
                    "first": "Y.-H",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Reid",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Brit. Mach. Vis. Conf",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Noisy-as-clean: learning unsupervised denoising from the corrupted image",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.06878"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Searching for mobilenetv3",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sandler",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                },
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Vasudevan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [
                        "V"
                    ],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Kokkinos",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Murphy",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "L"
                    ],
                    "last": "Yuille",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
            "authors": [
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Papandreou",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schroff",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Eur. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L.-C",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Schroff",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hua",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Yuille",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Fei-Fei",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kalenichenko",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Weyand",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Andreetto",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "04",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1704.04861"
                ]
            }
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Panoptic segmentation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kirillov",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Rother",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Image. Comput. Comput. Assist",
            "volume": "",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Learning dense volumetric segmentation from sparse annotation",
            "authors": [
                {
                    "first": "\u00d6",
                    "middle": [],
                    "last": "I\u00e7ek",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Abdulkadir",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lienkamp",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Image. Comput. Comput. Assist. Interv",
            "volume": "",
            "issn": "",
            "pages": "424--432",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for image segmentation",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Iglovikov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shvets",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Unet++: Redesigning skip connections to exploit multiscale features in image segmentation",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M R"
                    ],
                    "last": "Siddiquee",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Tajbakhsh",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Medical Imaging",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Large-scale screening of covid-19 from community acquired pneumonia using infection size-aware classification",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sui",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.09860"
                ]
            }
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "mixup: Beyond empirical risk minimization",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Cisse",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "N"
                    ],
                    "last": "Dauphin",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lopezpaz",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int. Conf. Learn. Represent",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ronneberger",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Med. Image. Comput. Comput. Assist. Interv",
            "volume": "",
            "issn": "",
            "pages": "234--241",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Squeeze-and-excitation networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "7132--7141",
            "other_ids": {}
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Feature pyramid networks for object detection",
            "authors": [
                {
                    "first": "T.-Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hariharan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Belongie",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "2117--2125",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Deeply-supervised nets",
            "authors": [
                {
                    "first": "C.-Y",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Gallagher",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Tu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Artificial intelligence and statistics",
            "volume": "",
            "issn": "",
            "pages": "562--570",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Milletari",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Navab",
                    "suffix": ""
                },
                {
                    "first": "S.-A",
                    "middle": [],
                    "last": "Ahmadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Int. Conf. 3D Vision",
            "volume": "",
            "issn": "",
            "pages": "565--571",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. Conf. Learn. Represent",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Lung infection quantification of covid-19 in ct images with deep learning",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Shan+",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gao+",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xue",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2003.04655"
                ]
            }
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Enhanced-alignment Measure for Binary Foreground Map Evaluation",
            "authors": [
                {
                    "first": "D.-P",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Borji",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int. Joint Conf",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "A simple poolingbased design for real-time salient object detection",
            "authors": [
                {
                    "first": "J.-J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Conf. Comput. Vis. Pattern Recog",
            "volume": "",
            "issn": "",
            "pages": "3917--3926",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Egnet: Edge guidance network for salient object detection",
            "authors": [
                {
                    "first": "J.-X",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "J.-J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D.-P",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M.-M",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int. Conf. Comput. Vis",
            "volume": "",
            "issn": "",
            "pages": "8779--8788",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "This work was supported in part by the Major Project for New Generation of AI under Grant No. 2018AAA0100400, NSFC (61922046), and Tianjin Natural Science Foundation (18ZXZNGX00110). (Corresponding author: M.-M. Cheng) Y.-H. Wu is with the TKLNDST, College of Computer Science, Nankai University, and also with the InferVision. (E-mail: wuyuhuan@mail.nankai.edu.cn) S.-H. Gao, J. Mei, J. Xu, D.-P. Fan, and M.-M. Cheng are with the TKLNDST, College of Computer Science, Nankai University. (E-mail: shgao@mail.nankai.edu.cn, meijie0507@gmail.com, csjunxu@nankai.edu.cn, dengpfan@gmail.com, cmm@nankai.edu.cn) C.-W. Zhao is with the InferVision. (E-mail: zchaowei@infervision.com) Illustration of our JCS diagnosis system for COVID-19. Our JCS system will perform the segmentation diagnosis only if the classification model reports positive COVID-19 predictions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Examples of our COVID-CS dataset, including CT scan images and labels of a normal person (1 st column), two community acquired pneumonia (CAP) cases (2 nd and 3 rd columns), and three COVID-19 patients from mild to severe (4 th \u223c 6 th columns).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The age, gender, and slice thickness distribution of the COVID-19 patients in our COVID-CS dataset. Zoom in for details.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Illustration of diverse information about infected areas (in pixels), location (x0,y0), position (left, up), and width/height of infected areas in our COVID-CS dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Statistics of the segmentation set (200 COVID-19 cases) in our COVID-CS dataset. (a) Lesion count distribution. (b) The distribution of width & height of the infected areas. (c) The relationship between the infected areas and their locations.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Architecture of our segmentation model. EFM indicates the Enhanced Feature Module ( \u00a7IV-B2). AFF refers to the Attentive Feature Fusion strategy ( \u00a7IV-B3). We apply the deep supervision to train our segmentation model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Proposed (a) GAM and (b) AFF for the segmentation network. In AFF, M i+1 D will be replaced with M 6 E if i = 5. Cubes represent three dimensional feature maps, while rectangles mean feature vectors.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Visualizations of activation mapping (AM). AM origin (mixing) means the AM of models trained without (with) image mixing technique[49].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Qualitative comparisons of different methods on our segmentation test set. The first, second, and third rows show the comparison results on CT images with different lesion areas, from mild, medium, and severe COVID-19 patients, respectively. Statistical analysis for our segmentation model on our segmentation test set. (a) The relationship between the infected area of each CT image and the corresponding Dice score. (b) The relationship between the lesion count and the corresponding probability distribution of the Dice score.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": ", our JCS system needs only 19.0 seconds for each case, much faster than the RT-PCR tests and CT scan analysis by experienced radiologists.In summary, our contributions are mainly three-folds: We develop a novel COVID-19 diagnosis system to perform Joint explainable Classification and accurate lesion Segmentation (JCS), showing clear superiority over previous systems. \u2022 On our COVID-CS dataset, our JCS system achieves 95.0% sensitivity and 93.0% specificity on COVID-19 classification, and 78.3% Dice score on segmentation,",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "SUMMARY OF DIFFERENT DATASETS (UPDATED ON 2020/4/10).Table II AVERAGE TIME OF COVID-19 DIAGNOSIS BY DIFFERENT METHODS.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "THE CT SCANNERS AND NUMBERS OF POSITIVE CASES.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "SENSITIVITY AND SPECIFICITY OF OUR CLASSIFICATION MODEL UNDER DIFFERENT THRESHOLDS. WE SET THE THRESHOLD AS 25 IN THE FINAL SETTING. QUANTITATIVE RESULTS ON OUR SEGMENTATION TEST SET. \" \u2020 \" INDICATES THE MODEL WITH MULTI-SCALE TRAINING.model achieves the best result on all three metrics. It obtains improvements of 0.078, 0.093 and 0.085 on Dice score, IoU, and E \u03c6 over the second best PoolNet[58], respectively. With the multi-scale data augmentation strategy, our boosted JCS \u2020 obtains further improvements of 0.008, 0.013, and 0.001 on the Dice score, IoU, and E \u03c6 , respectively. Besides, PoolNet",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}