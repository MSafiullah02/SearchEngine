{
    "paper_id": "71713446acc477e72697358dea56651db9de707c",
    "metadata": {
        "title": "Accelerating Antimicrobial Discovery with Controllable Deep Generative Models and Molecular Dynamics",
        "authors": [
            {
                "first": "Payel",
                "middle": [],
                "last": "Das",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Tom",
                "middle": [],
                "last": "Sercu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Kahini",
                "middle": [],
                "last": "Wadhawan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Inkit",
                "middle": [],
                "last": "Padhi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Sebastian",
                "middle": [],
                "last": "Gehrmann",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Harvard John A. Paulson School of Engineering and Applied Sciences",
                    "location": {
                        "postCode": "02138",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Flaviu",
                "middle": [],
                "last": "Cipcigan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "The Hartree Centre STFC Laboratory",
                    "institution": "IBM Research Europe",
                    "location": {
                        "postCode": "WA4 4AD",
                        "settlement": "Warrington",
                        "country": "UK"
                    }
                },
                "email": ""
            },
            {
                "first": "Vijil",
                "middle": [],
                "last": "Chenthamarakshan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Hendrik",
                "middle": [],
                "last": "Strobelt",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "MIT-IBM Watson AI Lab",
                    "location": {
                        "postCode": "02142",
                        "settlement": "Cambridge",
                        "region": "MA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Cicero Dos",
                "middle": [],
                "last": "Santos",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Pin-Yu",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Yi",
                "middle": [
                    "Yan"
                ],
                "last": "Yang",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jeremy",
                "middle": [],
                "last": "Tan",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "James",
                "middle": [],
                "last": "Hedrick",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Almaden Research Center",
                    "location": {
                        "postCode": "95120",
                        "settlement": "San Jose",
                        "region": "CA",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Jason",
                "middle": [],
                "last": "Crain",
                "suffix": "",
                "affiliation": {
                    "laboratory": "The Hartree Centre STFC Laboratory",
                    "institution": "IBM Research Europe",
                    "location": {
                        "postCode": "WA4 4AD",
                        "settlement": "Warrington",
                        "country": "UK"
                    }
                },
                "email": ""
            },
            {
                "first": "Aleksandra",
                "middle": [],
                "last": "Mojsilovic",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "IBM Thomas J Watson Research Center",
                    "location": {
                        "postCode": "10598",
                        "settlement": "Yorktown Heights",
                        "region": "NY",
                        "country": "USA"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "De novo therapeutic design is challenged by a vast chemical repertoire and multiple constraints such as high broad-spectrum potency and low toxicity. We propose CLaSS (Controlled Latent attribute Space Sampling) a novel and efficient computational method for attribute-controlled generation of molecules, which leverages guidance from classifiers trained on an informative latent space of molecules modeled using a deep generative autoencoder. We further screen the generated molecules by using a set of deep learning classifiers in conjunction with novel physicochemical features derived from highthroughput molecular simulations. The proposed approach is employed for designing non-toxic antimicrobial peptides (AMPs) with strong broad-spectrum potency, which are emerging drug candidates for tackling antibiotic resistance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Synthesis and wet lab testing of only twenty designed sequences identified two novel and minimalist AMPs with high potency against diverse Gram-positive and Gram-negative pathogens, including the hard-to-treat multidrug-resistant K. pneumoniae, as well as low in vitro and in vivo toxicity. The proposed approach thus presents a viable path for faster discovery of potent and selective broad-spectrum antimicrobials with a higher success rate than state-of-the-art methods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "De novo drug design remains a cost and time-intensive process: It typically requires more than ten years and $2-3 B USD for a new drug to reach the market, and the failure rate is nearly 90% (1) . Rational methods for novel drug design, both in cerebro and in silico, heavily rely upon structure-activity relationship (SAR) studies. Such methods struggle with the prohibitively large molecular space, complex structure-function relationships, and multiple competing constraints such as activity, toxicity, synthesis cost, and stability associated with the design task. Recently, artificial intelligence (AI) methods, in particular, statistical learning and optimization-based approaches, have shown promise in designing novel and chemically plausible small-and macromolecules. In particular, deep learning-based architectures, such as neural language models as well as deep generative neural networks, have emerged as a popular choice process (2) (3) (4) (5) (6) (7) (8) (9) .",
            "cite_spans": [
                {
                    "start": 191,
                    "end": 194,
                    "text": "(1)",
                    "ref_id": null
                },
                {
                    "start": 943,
                    "end": 946,
                    "text": "(2)",
                    "ref_id": null
                },
                {
                    "start": 947,
                    "end": 950,
                    "text": "(3)",
                    "ref_id": null
                },
                {
                    "start": 951,
                    "end": 954,
                    "text": "(4)",
                    "ref_id": null
                },
                {
                    "start": 955,
                    "end": 958,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 959,
                    "end": 962,
                    "text": "(6)",
                    "ref_id": null
                },
                {
                    "start": 963,
                    "end": 966,
                    "text": "(7)",
                    "ref_id": null
                },
                {
                    "start": 967,
                    "end": 970,
                    "text": "(8)",
                    "ref_id": null
                },
                {
                    "start": 971,
                    "end": 974,
                    "text": "(9)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Probabilistic autoencoders (10, 11) , a powerful class of deep generative models, have been used for this design task, which learn a bidirectional mapping of the input molecules (and their attributes) to a continuous latent space.",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "text": "(10,",
                    "ref_id": null
                },
                {
                    "start": 32,
                    "end": 35,
                    "text": "11)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To achieve targeted generation, earlier deep neural net-based approaches have often limited the learning to a fixed library of molecules with desired attributes, to restrict the exhaustive search to a defined section of the chemical space. Such an approach can affect the novelty as well as the validity of the generated molecules, as the fixed library represents a small portion of the combinatorial molecular space (12) . Alternative methods include Bayesian optimization (BO) on a learned latent space (4), reinforcement learning (RL) (13, 14) , or semi-supervised learning (SS) (15) . However, those approaches require surrogate model fitting (as in BO), optimal policy learning (as in RL), or minimizing attribute-specific loss objectives (as in SS), which suffers from additional computational complexity. As a result, controlling attribute(s) of designed molecules efficiently continues to remain a non-trivial task. In this study, we propose a novel and efficient computational framework for targeted generation and screening of molecules with desired properties. We demonstrate the efficiency of the proposed method on antimicrobial peptide (AMP) design problem. Antimicrobial peptides are emerging drug candidates for tackling antibiotic resistance, one of the biggest threats in global health, food security, and development. Patients at a higher risk from drug-resistant pathogens are also more vulnerable to illness from viral lung infections like influenza, severe acute respiratory syndrome (SARS), and COVID-19. Drug-resistant diseases claim 700,000 lives a year globally (16) , which is expected to rise to 10 million deaths per year by 2050 based on current trend (17) . Of particular concern is multidrug-resistant Gram-negative bacteria (18). Antimicrobial peptides are typically 12-50 amino acids long and produced by multiple higher-order organisms to combat invading microorganisms. Due to their exceptional structural and functional variety (19) , promising activity, and low tendency to induce (or even reduce) resistance, natural AMPs have been proposed as promising alternatives to traditional antibiotics and as potential next-generation antimicrobial agents (20) . Most reported antimicrobials are cationic and amphiphilic in nature, and properties thought to be crucial for insertion into and disruption of bacterial membrane (20) . Although several antimicrobial peptides are in clinical trials (20) , the future design of novel AMP therapeutics requires minimizing the high production cost due to longer sequence length, proteolytic degradation, poor solubility, and off-target toxicity. A rational path for resolving these problems is to design short peptides as a minimal physical model (21, 22) that captures the high selectivity of natural AMPs. That is, maximizing antimicrobial activity, while minimizing toxicity towards the host.",
            "cite_spans": [
                {
                    "start": 417,
                    "end": 421,
                    "text": "(12)",
                    "ref_id": null
                },
                {
                    "start": 538,
                    "end": 542,
                    "text": "(13,",
                    "ref_id": null
                },
                {
                    "start": 543,
                    "end": 546,
                    "text": "14)",
                    "ref_id": null
                },
                {
                    "start": 582,
                    "end": 586,
                    "text": "(15)",
                    "ref_id": null
                },
                {
                    "start": 1588,
                    "end": 1592,
                    "text": "(16)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1682,
                    "end": 1686,
                    "text": "(17)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1965,
                    "end": 1969,
                    "text": "(19)",
                    "ref_id": null
                },
                {
                    "start": 2187,
                    "end": 2191,
                    "text": "(20)",
                    "ref_id": null
                },
                {
                    "start": 2356,
                    "end": 2360,
                    "text": "(20)",
                    "ref_id": null
                },
                {
                    "start": 2426,
                    "end": 2430,
                    "text": "(20)",
                    "ref_id": null
                },
                {
                    "start": 2721,
                    "end": 2725,
                    "text": "(21,",
                    "ref_id": null
                },
                {
                    "start": 2726,
                    "end": 2729,
                    "text": "22)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To tackle these challenges, the proposed in silico design framework combines novel attributecontrolled generative models, deep learning, and physics-driven learning. For targeted generation, we propose Conditional Latent (attribute) Space Sampling -CLaSS (Fig. 2 ) that leverages guidance from attribute classifier(s) trained on the latent space of the system of interest and uses a rejection sampling scheme for generating molecules with desired attributes.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 255,
                    "end": 262,
                    "text": "(Fig. 2",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "CLaSS is fundamentally novel, fast, efficient, embarrassingly parallelizable, and easily repurposable in comparison to existing machine learning algorithms for targeted generation. To encourage novelty and validity of designed sequences, we performed CLaSS on the latent space of a deep generative autoencoder that was trained on a larger dataset consisting of all known peptide sequences, instead of a limited number of known antimicrobials. Extensive analyses showed that the resulting latent space is biologically more meaningful. As a result, the antimicrobial peptides generated from this informative space are more novel, diverse, valid, and optimized.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To account for additional key requirements, such as broad-spectrum nature and low toxicity, we further provide an efficient in silico screening method that uses deep learning classifiers augmented with high-throughput physics-driven molecular simulations (Fig. 1) . To our knowledge, this is the first computational approach for antimicrobial design that explicitly accounts for broad-spectrum potency and low-toxicity, and performs experimental verification of those properties. Synthesis of 20 candidate sequences (from a pool of \u223c 90,000 generated sequences) that passed the screening enabled discovering two novel and minimalist peptides with experimentally validated strong antimicrobial activity against diverse pathogens, including a hard-to-treat multidrug-resistant Gram-negative K. pneumoniae. Importantly, both sequences demonstrated low in vitro hemolysis (HC50) and in vivo lethal (LD50) toxicity. Circular dichroism experiments further revealed the amphiphilic helical topology of the two novel cationic AMPs, while all-atom simulations show a distinct mode of lipid membrane interaction. The present strategy, therefore, provides an efficient approach for discovering novel, broad-spectrum and low-toxic antimicrobials with a much higher (10%) success rate and faster (48 days) pace, relative to existing methods (<1% and 2-4 years) (23) .",
            "cite_spans": [
                {
                    "start": 1348,
                    "end": 1352,
                    "text": "(23)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 255,
                    "end": 263,
                    "text": "(Fig. 1)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "For modeling the peptide latent space, we used generative models based on a deep autoencoder (10, 11) composed of two neural networks, an encoder and a decoder. The encoder q \u03c6 (z|x) parameterized with \u03c6 learns to map the input x to a variational distribution, and the decoder p \u03b8 (x|z) parameterized with \u03b8 aims to reconstruct the input x given the latent vector z from the learned distribution, as illustrated in Fig. 2A . Variational Autoencoder (VAE), the most popular model in this family (11) , assumes latent variable z \u223c p(z) and follows a simple prior (e.g. Gaussian) distribution. And the decoder then produces a distribution over sequences given the continuous representation z. Thus, the generative process is specified as:",
            "cite_spans": [
                {
                    "start": 93,
                    "end": 97,
                    "text": "(10,",
                    "ref_id": null
                },
                {
                    "start": 98,
                    "end": 101,
                    "text": "11)",
                    "ref_id": null
                },
                {
                    "start": 494,
                    "end": 498,
                    "text": "(11)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 415,
                    "end": 422,
                    "text": "Fig. 2A",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Peptide autoencoder"
        },
        {
            "text": "p(x) = p(z)p \u03b8 (x|z)dz where we integrate out the latent variable. However, the VAE that aims to minimize Kullback-Leibler (KL) distance between the encoded training distribution and prior reportedly suffers from ignoring the latent z-information during decoding (24) . This issue is addressed in Wasserstein Autoencoders (WAE) (25) by minimizing the optimal transport distance or Wasserstein distance between distributions (26). Within the VAE/WAE framework, the peptide generation is formulated as a density modeling problem, i.e. estimating p(x) where x are short variable-length strings of amino acids. The density estimation procedure has to assign a high likelihood to known peptides. Therefore, the model generalization implies that plausible novel peptides can be generated from regions with a high probability density under the model.",
            "cite_spans": [
                {
                    "start": 263,
                    "end": 267,
                    "text": "(24)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Peptide autoencoder"
        },
        {
            "text": "Peptide sequences are presented as text strings composed of 20 natural amino acid characters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Peptide autoencoder"
        },
        {
            "text": "Only sequences with length \u2264 25 were considered for model training and generation, as short AMPs are desired.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Peptide autoencoder"
        },
        {
            "text": "However, instead of learning a model only over known AMP sequences, one can learn a model overall short peptide sequences reported in the UniProt database (27) -an extensive database of protein/peptide sequences that may or may not have an annotation. For example, the number of annotated AMP sequences is \u223c 9000, and peptide sequences in Uniprot is \u223c 1.7M, when a sequence length up to 50 is considered. Therefore, we learn a density model of overall known peptides sequences in this work. The fact also inspires this approach, that unsupervised representation learning by pre-training on a large corpus has recently led to impressive results for downstream tasks in text and speech (28) (29) (30) (31) , as well as in protein biology (32, 33) .",
            "cite_spans": [
                {
                    "start": 684,
                    "end": 688,
                    "text": "(28)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 689,
                    "end": 693,
                    "text": "(29)",
                    "ref_id": null
                },
                {
                    "start": 694,
                    "end": 698,
                    "text": "(30)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 699,
                    "end": 703,
                    "text": "(31)",
                    "ref_id": null
                },
                {
                    "start": 736,
                    "end": 740,
                    "text": "(32,",
                    "ref_id": null
                },
                {
                    "start": 741,
                    "end": 744,
                    "text": "33)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Peptide autoencoder"
        },
        {
            "text": "Additionally, in contrast to similar models for protein sequence generation (34), we do not restrict ourselves to learning the density associated with a single protein family or a specific 3D fold. Instead, we learn a global model, overall known short peptide sequences expressed in different organisms. This global approach should enable meaningful density modeling across multiple families, the interpolation between them, better learning of the \"grammar\" of plausible peptides, and exploration beyond known antimicrobial templates, as shown next.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Peptide autoencoder"
        },
        {
            "text": "The advantage of training a WAE (instead of a VAE) on peptide sequences is evident from the reported evaluation metrics in Supplementary Information (SI) Table S1 (26). We also observed high reconstruction accuracy and diversity of generated sequences, when the WAE was trained on all peptide sequences, instead of only on AMP sequences (SI Table S1 ). Next, we analyzed the information content of the peptide WAE, inspired by recent investigations in natural language processing. By using the so-called \"probing\" methods, it has been shown that encoded sentences can retain much linguistic information (35) . In a similar vein, we investigated if the evolutionary relationships between sequences (36) are captured by their encodings in the latent z-space, as the evolutionary information is known to specify the biological function and fold of peptide sequences. Fig. 3A reveals a negative correlation (Pearson correlation coefficient = \u22120.63) between evolutionary similarities and Euclidean distances in the z-space of the WAE model, suggesting that WAE intrinsically captures the evolutionary relationship within the peptide space. The VAE latent space fails to capture such a relation.",
            "cite_spans": [
                {
                    "start": 603,
                    "end": 607,
                    "text": "(35)",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [
                {
                    "start": 341,
                    "end": 349,
                    "text": "Table S1",
                    "ref_id": null
                },
                {
                    "start": 864,
                    "end": 871,
                    "text": "Fig. 3A",
                    "ref_id": null
                }
            ],
            "section": "Peptide autoencoder"
        },
        {
            "text": "With the end-goal of a conditional generation of novel peptide sequences, it is crucial to ensure that the learned encoding in the z-space retains identifiable information about functional attributes of the original sequence. Specifically, we investigate whether the space is linearly separable into different attributes, such that sampling from a specific region of that space yields consistent and controlled generations. For this purpose, we trained linear classifiers for binary (yes/no) functional attribute prediction using the z encodings of sequences (Fig. 2B) . Probing the z-space modeled by the WAE uncovers that the space is indeed linearly separable into different functional attributes, as evident from the test accuracy of binary logistic classifiers presented in Fig. 3B (26) . Results demonstrate a performance on par with AMP classifiers reported in literature (37, 38) or trained in-house (Fig. 3B ) that have access to the original sequences (instead of using latent features) and involve highly non-linear (neural net-based) models. However, on toxicity classification, a much lower accuracy was found, when compared to similar sequence-level deep classifiers (39) (also see Fig. 3B and SI Table S2 ) that report accuracy as high as 90%. These results imply that some attributes, such as toxicity, are more challenging to predict from the learned latent peptide representation; one possible reason can be higher class imbalance in training data (26).",
            "cite_spans": [
                {
                    "start": 879,
                    "end": 883,
                    "text": "(37,",
                    "ref_id": null
                },
                {
                    "start": 884,
                    "end": 887,
                    "text": "38)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 559,
                    "end": 568,
                    "text": "(Fig. 2B)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 779,
                    "end": 791,
                    "text": "Fig. 3B (26)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 908,
                    "end": 916,
                    "text": "(Fig. 3B",
                    "ref_id": null
                },
                {
                    "start": 1196,
                    "end": 1203,
                    "text": "Fig. 3B",
                    "ref_id": null
                },
                {
                    "start": 1211,
                    "end": 1219,
                    "text": "Table S2",
                    "ref_id": null
                }
            ],
            "section": "Peptide autoencoder"
        },
        {
            "text": "We also investigated the smoothness of the latent space by analyzing the sequences generated along a linear interpolation vector in the z-space between two distant training sequences ( Fig. 3C ). Evolutionary similarity, functional attributes (AMP and Toxic class probabilities), as well as several physicochemical properties including aromaticity, charge, and hydrophobic moment (indicating amphiphilicity of a helix) change smoothly during the interpolation. These results are encouraging, as the WAE latent space trained on the much larger amount of unlabeled data appears to carry significant structure in terms of functional, physicochemical, and evolutionary Figure 3C also demonstrates that it is possible to identify sequence(s) during linear interpolation that is visibly different from both endpoint sequences, indicating the potential of the learned latent space for novel sequence generation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 185,
                    "end": 192,
                    "text": "Fig. 3C",
                    "ref_id": null
                }
            ],
            "section": "Peptide autoencoder"
        },
        {
            "text": "For controlled generation, we aim to control a set of binary (yes/no) attributes of interest such as antimicrobial function and/or toxicity. We propose CLaSS -Conditional Latent (attribute) Space Sampling for this purpose. CLaSS leverages attribute classifiers directly trained on the peptide z-space, as those, can capture important attribute information (Fig. 3B ). Let us formalize that there are n different (and possibly independent) binary attributes of interest a \u2208 {0, 1} n = [a 1 , a 2 , . . . , a n ], each attribute is only available (labeled) for a small and possibly disjoint subset of the dataset. Since functional annotation of peptide sequences is expensive, current databases typically represent a small (\u2248 100 \u2212 10000) subset of the unlabeled corpus. We posit that all plausible datapoints have those attributes, albeit mostly without label annotation. Therefore, the data distribution implicitly is generated as",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 356,
                    "end": 364,
                    "text": "(Fig. 3B",
                    "ref_id": null
                }
            ],
            "section": "CLaSS for controlled sequence generation"
        },
        {
            "text": "where the distribution over the (potentially huge) discrete set of attribute combinations p(a) is integrated out, and for each attribute combination the set of possible sequences is specified as p(x|a). The goal now is to sample conditionally p(x|a t ) for a specified target attribute combination a t . This task was approached through CLaSS (Fig. 2C) , which makes the assumption that attribute conditional density factors as follows: p(x|a t ) = E z [p(z|a t )p(x|z)]. We sample p(z|a t ) approximately using rejection sampling from models in the latent z-space appealing to Bayes rule and p(a t |z) modeled by the attribute classifiers ( Fig. 2B -C) (26). Since CLaSS only employs simple attribute predictor models and rejection sapling from models of z-space, it is a simple and efficient forward-only screening method. It does not require any complex optimization over latent space, when compared to existing methods for controlled generation, e.g. Bayesian optimization (40) , reinforcement learning (13, 14) , or semi-supervised generative models (15) . CLaSS is easily repurposable and embarrassingly parallelizable at the same time and does not need defining a starting point in the latent space.",
            "cite_spans": [
                {
                    "start": 977,
                    "end": 981,
                    "text": "(40)",
                    "ref_id": null
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "text": "(13,",
                    "ref_id": null
                },
                {
                    "start": 1012,
                    "end": 1015,
                    "text": "14)",
                    "ref_id": null
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "text": "(15)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 343,
                    "end": 352,
                    "text": "(Fig. 2C)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 642,
                    "end": 649,
                    "text": "Fig. 2B",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "CLaSS for controlled sequence generation"
        },
        {
            "text": "Since the Toxicity classifier trained on latent features appears weaker (Fig. 3B) , antimicrobial function (yes/no) was used as the sole condition for controlling the sampling from the latent peptide space. Generated antimicrobial candidates were then screened for toxicity using the sequence-level classifier during post-generation filtering.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 72,
                    "end": 81,
                    "text": "(Fig. 3B)",
                    "ref_id": null
                }
            ],
            "section": "CLaSS for controlled sequence generation"
        },
        {
            "text": "To check the similarity of CLaSS-generated AMP sequences with training data, we performed a BLAST sequence similarity (or homology) search. We analyzed the Expect value (E-value) for the matches with the highest alignment score. E-value indicates statistical (aka. biological) significance of the match between the query and sequences from a database of a particular size. Larger E-value indicates a higher chance that the similarity between the hit and the query is merely a coincidence, i.e. the query is not homologous or related to the hit. Typically E-values \u2264 0.001 when querying Uniprot nr database of size \u223c 220 M are used to infer homology (41) . Since our training database is \u223c 1000 times smaller than Uniprot, an E-value of \u2264 10 \u22126 can be used for indicating homology. As shown in SI Table S3 , about 14% of generated sequences show an E-value of \u2265 10, and another 36% have an E-value > 1, when considering the match with the highest alignment score, indicating insignificant similarity between generated and training sequences. If only the alignments with score > 20 are considered, the average E-value is found to be > 2, further implying the non-homologous nature of generated sequences. Similar criteria have also been used for detecting novelty of designed short antimicrobials (42) . CLaSS-generated AMPs are also more diverse, as the unique (i.e. found only once in an ensemble of sequences) k-mers (k = 3-6) are more abundant compared to training sequences (SI Figure S1 ). These results highlight the ability of the present approach to generate minimalist AMP sequences that are, on average, highly novel with respect to training data, as well as diverse among themselves.",
            "cite_spans": [
                {
                    "start": 649,
                    "end": 653,
                    "text": "(41)",
                    "ref_id": null
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "text": "(42)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 796,
                    "end": 804,
                    "text": "Table S3",
                    "ref_id": "TABREF6"
                },
                {
                    "start": 1481,
                    "end": 1490,
                    "text": "Figure S1",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Features of CLaSS-generated AMPs"
        },
        {
            "text": "Distributions of key molecular features implicated in antimicrobial nature, such as amino acid composition, charge, hydrophobicity (H), and hydrophobic moment (\u00b5H), were compared between the training and generated AMPs, as illustrated in Fig. 4A -D. Additional features are reported in SI Figure S1 . CLaSS-generated AMP sequences show distinct character: Specifically, those are richer in R, L, S, Q, and C, whereas A, G, D, H, N, and W content is reduced, in comparison to training antimicrobial sequences (Fig. 4A) . We also present the most abundant k-mers (k=3, 4) in SI Figure S1 , suggesting that the most frequent 3 and 4-mers are K and Lrich in both generated and training AMPs, the frequency being higher in generated sequences. Generated AMPs are characterized by global net positive charge and aromaticity somewhere in between unlabeled and AMP-labeled training sequences, while the hydrophobic moment is comparable to that of known AMPs (Fig. 4B -D and SI Figure S1 ). These trends imply that the generated antimicrobials are still cationic and can form a putative amphiphilic \u03b1-helix, similar to the majority of known antimicrobials. Interestingly, they also exhibit a moderately higher hydrophobic ratio and an aliphatic index compared to training sequences (SI Figure S1 ). These observations highlight the distinct physicochemical nature of the CLaSS-generated AMP sequences, as a result of the semi-supervised nature of our learning paradigm, that might help in their therapeutic application. For example, lower aromaticity and higher aliphatic index are known to induce better oxidation susceptibility and higher heat stability in short peptides (43) , while lower hydrophobicity is associated with reduced toxicity (44) .",
            "cite_spans": [
                {
                    "start": 1665,
                    "end": 1669,
                    "text": "(43)",
                    "ref_id": null
                },
                {
                    "start": 1735,
                    "end": 1739,
                    "text": "(44)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 238,
                    "end": 245,
                    "text": "Fig. 4A",
                    "ref_id": null
                },
                {
                    "start": 289,
                    "end": 298,
                    "text": "Figure S1",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 508,
                    "end": 517,
                    "text": "(Fig. 4A)",
                    "ref_id": null
                },
                {
                    "start": 576,
                    "end": 585,
                    "text": "Figure S1",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 950,
                    "end": 958,
                    "text": "(Fig. 4B",
                    "ref_id": null
                },
                {
                    "start": 969,
                    "end": 978,
                    "text": "Figure S1",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1277,
                    "end": 1286,
                    "text": "Figure S1",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Features of CLaSS-generated AMPs"
        },
        {
            "text": "To screen the \u223c90,000 CLaSS-generated AMP sequences, we first used an independent set of binary (yes/no) sequence-level deep neural net-based classifiers that screens for antimicrobial function, broad-spectrum efficacy, presence of secondary structure, as well as toxicity (See Table S2 ). 163 candidates passed this screening, which were then subjected to coarse-grained Molecular Dynamics (CGMD) simulations of peptide-membrane interactions. The computational efficiency of these simulations makes them an attractive choice for high-throughput and physically-inspired filtering of peptide sequences.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 278,
                    "end": 286,
                    "text": "Table S2",
                    "ref_id": null
                }
            ],
            "section": "In silico post-generation screening"
        },
        {
            "text": "Since there exists no standardized protocol for screening antimicrobial candidates using molecular simulations, we performed a set of control simulations of known sequences with or without antimicrobial activity. From those control runs, we found for the first time that the variance of the number of contacts between positive residues and membrane lipids is predictive of antimicrobial activity (Fig. 5) : Specifically, the contact variance differentiates between high potency AMPs and non-antimicrobial sequences with a sensitivity of 88% and specificity of 63% (26). Physically, this feature can be interpreted as measuring the robust binding tendency of a peptide sequence to the model membrane. Therefore, we used the contact variance cutoff of 2 for further filtering of the 163 generated AMPs that passed the classifier screening.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 396,
                    "end": 404,
                    "text": "(Fig. 5)",
                    "ref_id": null
                }
            ],
            "section": "Figs. 1 and 3 as well as SI"
        },
        {
            "text": "A final set of 20 CLaSS-generated AMP sequences that passed the contact variance-based screening mentioned above, along with their simulated and physico-chemical characteristics, are reported in SI Tables S4 and S5. Those sequences were tested in the wet lab for antimicrobial activity, as measured using minimum inhibitory concentration (MIC, lower the better) against Gram-positive S. aureus and Gram-negative E. coli (SI Table S6 ). 11 generated non-AMP sequences were also screened for antimicrobial activity (SI Table S7 ). None of the designed non-AMP sequences showed MIC values that are low enough to be considered as antimicrobials, implying that our approach is not prone to false-negative predictions.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 424,
                    "end": 432,
                    "text": "Table S6",
                    "ref_id": null
                },
                {
                    "start": 517,
                    "end": 525,
                    "text": "Table S7",
                    "ref_id": null
                }
            ],
            "section": "Wet lab characterization"
        },
        {
            "text": "Among the 20 AI-designed AMP candidates, two sequences, YLRLIRYMAKMI-CONH2 (YI12, 12 amino acids) and FPLTWLKWWKWKK-CONH2 (FK13, 13 amino acids), were identified to be the best with the lowest MIC values ( Fig. 6A and SI Table S6 ). Both peptides are positively charged and have a nonzero hydrophobic moment (SI Table S5 ), indicating their cationic and amphiphilic nature in line with known antimicrobials. These peptides were further evaluated against the more difficult-to-treat Gram-negative P. aeruginosa, A. baummannii, as well as a multi-drug resistant Gram-negative K. pnuemoniae. As listed in Fig. 6 , both YI12 and FK13 showed potent broad-spectrum antimicrobial activity with comparable MIC values.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 206,
                    "end": 213,
                    "text": "Fig. 6A",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 221,
                    "end": 229,
                    "text": "Table S6",
                    "ref_id": null
                },
                {
                    "start": 312,
                    "end": 320,
                    "text": "Table S5",
                    "ref_id": null
                },
                {
                    "start": 602,
                    "end": 608,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Wet lab characterization"
        },
        {
            "text": "We further performed in vitro and in vivo testing for toxicity. Based on activity measure at 50% hemolysis (HC 50 ) and lethal dose (LD 50 ) toxicity values ( Fig. 6A and Fig. S2 ), both peptides appear biocompatible (as the HC 50 and LD 50 values are much higher than MIC values), FK13 being more biocompatible than YI12. More importantly, the LD 50 values of both peptides compare favorably with that of polymyxin B (20.5 mg/kg) (45) , which is a clinically used antimicrobial drug for treatment of antibiotic-resistant Gram-negative bacterial infection.",
            "cite_spans": [
                {
                    "start": 431,
                    "end": 435,
                    "text": "(45)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 159,
                    "end": 178,
                    "text": "Fig. 6A and Fig. S2",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Wet lab characterization"
        },
        {
            "text": "To investigate the novelty of YI12 and FK13 with respect to training sequences, we analyzed the sequence similarity metrics returned by the BLAST homology search in detail ( Fig. 6B and SI Figure S2 ), in line with earlier works (42, 46) . Similarity metrics include alignment score, E-value, percentage of alignment coverage, percentage of identity, percentage of positive matches or similarity, and percentage of alignment gap (indicating the presence of additional amino acids). BLAST searching with an E-value threshold of 10 against the training database did not reveal any match for YI12, suggesting that there exists no statistically significant match of YI12. Therefore, we further searched for related sequences of YI12 in the much larger Uniprot database consisting of \u223c 223.5 M non-redundant sequences, only a fraction of which was included in our model training. The closest match to YI12 shows an E-value of 2.9 with 75% identity, 83% similarity, a gap of 1 at a query coverage of 92%, which is an 11 residue segment from the bacterial EAL domain-containing protein (Fig. 6B ). This result suggests that YI12 is significantly high in novelty, even when all protein sequences in Uniprot are considered.",
            "cite_spans": [
                {
                    "start": 229,
                    "end": 233,
                    "text": "(42,",
                    "ref_id": null
                },
                {
                    "start": 234,
                    "end": 237,
                    "text": "46)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 174,
                    "end": 181,
                    "text": "Fig. 6B",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 189,
                    "end": 198,
                    "text": "Figure S2",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1079,
                    "end": 1087,
                    "text": "(Fig. 6B",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Novelty of YI12 and FK13"
        },
        {
            "text": "We also performed a BLAST search of YI12 against the PATSEQ database that contains \u223c 65.5 M patented peptides and still received a minimum E-value of 1.66. The sequence nearest to YI12 from PATSEQ is an eight amino acid long segment from a 79 amino acid long human protein, which has with 87.5% similarity and only 66.7% coverage, further confirming YI12's high novelty.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Novelty of YI12 and FK13"
        },
        {
            "text": "FK13 shows less than 75% identity, a gap in the alignment, and 85% query coverage to its closest match in the training database, implying FK13 is also novel (Fig. 6B ). YI12 is more novel than FK13, though. The closest match of FK13 in the training database is a syn- since Tryptophan (W) is susceptible to oxidation in air. Lower W-content has also been implicated in improving in vivo peptide stability (49) . Taken together, these results illustrate that CLaSS on latent peptide space modeled by the WAE is able to generate novel and optimal antimicrobial sequences by efficiently learning the complicated sequence-function relationship in peptides and exploiting that knowledge for controlled exploration. When combined with subsequent in silico screening, novel and optimal lead candidates with experimentally confirmed high broad-spectrum efficacy and selectivity are identified at a success rate of 10%. The whole cycle (from database curation to wet lab confirmation) took 48 days in total and a single iteration ( Fig.   1 ).",
            "cite_spans": [
                {
                    "start": 405,
                    "end": 409,
                    "text": "(49)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 157,
                    "end": 165,
                    "text": "(Fig. 6B",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 1023,
                    "end": 1031,
                    "text": "Fig.   1",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Novelty of YI12 and FK13"
        },
        {
            "text": "Peptides were further experimentally characterized using CD spectroscopy (26). Both YI12 and FK13 showed random coil-like structure in water, but formed \u03b1-helix in 20% SDS buffer ( Fig. S2) , consistent with structure classifier predictions (26). From CD spectra, \u03b1-helicity of YI12 appears stronger than that of FK13, in line with its stronger hydrophobic moment (SI Table   S5 ). In summary, physicochemical analyses and CD spectroscopy together suggest that cationic nature and amphiphilic helical topology are the underlying factors inducing antimicrobial nature in YI12 and FK13.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 181,
                    "end": 189,
                    "text": "Fig. S2)",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 368,
                    "end": 378,
                    "text": "Table   S5",
                    "ref_id": null
                }
            ],
            "section": "Structural and mechanistic analyses of YI12 and FK13"
        },
        {
            "text": "We also performed all-atom explicit water simulations (26) of these two sequences in the presence of a lipid membrane starting from an \u03b1-helical structure, as seen in CD experiments ( Fig. S2 ). Different membrane binding mechanisms were observed for the two sequences, as illustrated in Fig. 6 . YI12 embeds into the membrane by using positively charged N-terminal Arginine (R) residues. While FK13 embeds either with N-terminal Phenylalanine (F) or with C-terminal Tryptophan (W) and Lysine (K). These results provide mechanistic insights into different modes of action adopted by YI12 and FK13 during the early stages of membrane interaction.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 184,
                    "end": 191,
                    "text": "Fig. S2",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 288,
                    "end": 294,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Structural and mechanistic analyses of YI12 and FK13"
        },
        {
            "text": "Learning implicit interaction rule(s) of complex molecular systems is a major goal of artificial intelligence (AI) research. This direction is critical for designing new molecules/materials with specific structural and/or functional requirements, one of the most anticipated and highly needed applications. Antimicrobial peptides considered here represent an archetypal system for molecular discovery problems. They exhibit a near-infinite and mostly unexplored chemical repertoire, a well-defined chemical palette (natural amino acids), as well as potentially conflicting or opposing design objectives, and is of high importance due to the global increase in antibiotic resistance and a depleted antibiotic discovery pipeline. Recent work has shown that deep learning can be used to help screen libraries of existing chemicals for antibiotic properties (50) .",
            "cite_spans": [
                {
                    "start": 854,
                    "end": 858,
                    "text": "(50)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "A number of recent studies have also used AI methods for de novo design of antimicrobial peptides and provided experimental validation (9, 12, (51) (52) (53) (54) . However, to our knowledge, the present work provides for the first time a fully automated computational framework that combines novel controllable generative modeling, deep learning, and physics-driven learning for designing broad-spectrum potent and selective AMP sequences and experimentally validating them for the set of desired attributes. Wet lab results confirmed the efficiency of the proposed approach for designing novel and optimized sequences with a very modest number of candidate compounds synthesized and tested. The present design approach in this proof-of-concept study yielded a 10% success rate and a rapid turnaround of 48 days, as opposed to a 1% success rate and a timeline of 2-4 years required for antimicrobial lead generation (23) . The generative modeling approach presented here can be tuned for not only generating novel candidates, but also for designing novel combination therapies and antibiotic adjuvants, to further advance antibiotic treatments.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 138,
                    "text": "(9,",
                    "ref_id": null
                },
                {
                    "start": 139,
                    "end": 142,
                    "text": "12,",
                    "ref_id": null
                },
                {
                    "start": 143,
                    "end": 147,
                    "text": "(51)",
                    "ref_id": null
                },
                {
                    "start": 148,
                    "end": 152,
                    "text": "(52)",
                    "ref_id": null
                },
                {
                    "start": 153,
                    "end": 157,
                    "text": "(53)",
                    "ref_id": null
                },
                {
                    "start": 158,
                    "end": 162,
                    "text": "(54)",
                    "ref_id": null
                },
                {
                    "start": 917,
                    "end": 921,
                    "text": "(23)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "Since CLaSS is a generic approach, it is suitable for a variety of controlled generation tasks and can handle multiple controls simultaneously. The method is simple to implement, fast, efficient, and scalable, as it does not require any optimization over the latent space. CLaSS has additional advantages regarding repurposability, as adding a new constraint requires a simple predictor training. Therefore, future directions of this work will explore the effect of additional relevant constraints, such as the induced resistance and fine-grained strain-specificity, on the designed AMPs using the approach presented here. Finally, the AI models will be further optimized in an iterative manner by using the feedback from simulations and/or experiments in an active learning framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion and Conclusions"
        },
        {
            "text": "To learn meaningful continuous latent representations from sequences without supervision signal, the Variational Autoencoder (VAE) (11) family has emerged as a principled and successful method. The data distribution p(x) over samples x is represented as the marginal of a joint distribution p(x, z) that factors out as p(z)p \u03b8 (x|z). The prior p(z) is a simple smooth distribution, while p \u03b8 (x|z) is the decoder that maps a point in latent z-space to a distribution in x data space. The exact inference of the hidden variable z for a given input x would require integration over the full latent space:",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 135,
                    "text": "(11)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "To avoid this computational burden, the inference is approximated through an inference neural network or encoder q \u03c6 (z|x).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "Our implementation follows (55) , where both encoder and decoder are single-layer LSTM recurrent neural networks (56) , and the encoder specifies a diagonal Gaussian distribution, i.e.",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "text": "(55)",
                    "ref_id": null
                },
                {
                    "start": 113,
                    "end": 117,
                    "text": "(56)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "The basis for auto-encoder training is optimization of an objective consisting of the sum of a reconstruction loss and a regularization constraint loss term:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "In the standard VAE objective (11) , reconstruction loss L rec (\u03b8, \u03c6) is based on the negative log likelihood of the training sample, and the constraint L c (\u03c6) uses D KL , the Kullback-Leibler divergence:",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 34,
                    "text": "(11)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "for a single sample. This exact objective is derived from a lower bound on the data likelihood; hence this objective is called the ELBO (Evidence Lower Bound). With the standard VAE, we observed the same posterior collapse as detailed for natural language in the literature (24), meaning q(z|x) \u2248 p(z) such that no meaningful information is encoded in z space. Further extensions include \u03b2-VAE that adds a multiplier \"weight\" hyperparameter \u03b2 on the regularization term, and \u03b4-VAE that encourages the D KL term to be close to a nonzero \u03b4, etc., to tackle the issue of posterior collapse. However, finding the right setting that serves as a workaround for the posterior collapse is tricky within these VAE variants.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "Therefore, many variations within the VAE family have been recently proposed, such as Wasserstein Autoencoder (WAE) (25, 57) and Adversarial Autoencoder (AAE) (58) .",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "text": "(25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 121,
                    "end": 124,
                    "text": "57)",
                    "ref_id": null
                },
                {
                    "start": 159,
                    "end": 163,
                    "text": "(58)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "WAE factors an optimal transport plan through the encoder-decoder pair, on the constraint",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "This is relaxed to an objective similar to L VAE above. However, in the WAE objective (25),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "to be close to the prior p(z). We enforce the constraint by penalizing maximum mean discrepancy (59) with random features approximation of the radial basis function (60):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "In WAE training with maximum mean discrepancy (MMD) or with a discriminator, we found a benefit of regularizing the encoder variance as in the literature (57, 61) . ",
            "cite_spans": [
                {
                    "start": 154,
                    "end": 158,
                    "text": "(57,",
                    "ref_id": null
                },
                {
                    "start": 159,
                    "end": 162,
                    "text": "61)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Generative Autoencoders"
        },
        {
            "text": "We propose Conditional Latent (attribute) Space Sampling, CLaSS, a simple but elegant method to sample from the targeted region of the latent space from an auto-encoder, which was trained in an unsupervised manner (Fig. 2) .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 214,
                    "end": 222,
                    "text": "(Fig. 2)",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "CLaSS -Conditional Latent (Attribute) Space Sampling"
        },
        {
            "text": "We assume a latent variable model (e.g., Autoencoder) that has been trained in an unsupervised manner to meet the evaluation criteria outlined in (26).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "All training data x j are then encoded in latent space: z j,k \u223c q \u03c6 (z|x j ). These z j,k are used to fit an explicit density model Q \u03be (z) to approximate marginal posterior q \u03c6 (z), and a classifier model q \u03be (a i |z) forr attribute a i to approximate the probability p(a i |x). The motivation for fitting a Q \u03be (z) is in order to sample from Q \u03be rather than p(z), since at the end of training the discrepancy between q \u03c6 (z) and p(z) can be significant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "Although any explicit density estimator could be used for Q \u03be (z), here we consider Gaussian mixture density models and evaluate negative log-likelihood on a held-out set to determine the optimal complexity. We find 100 components and untied diagonal covariance matrices to be optimal, giving a held-out log likelihood of 105.1. To fit Q \u03be , we use K=10 random samples from the encoding distribution of the training data,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "Independent simple linear attribute classifiers q \u03be (a i |z) are then fitted per attribute. For each attribute a i , the procedure consists of: (1) collecting dataset with all labeled samples for this attribute (x j , a i ), (2) encoding the labeled data as before, z j,k \u223c q \u03c6 (z|x j ), (3) fitting \u03be, the parameters of logistic regression classifier q \u03be (a i |z) with inverse regularization strength C = 1.0 and 300 lbfgs iterations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "Rejection Sampling for Attribute-Conditioned Generation Recall our aim of sampling novel sequences x \u223c p(x|a), for a desired attribute combination a = [a 1 , . . . , a n ]. We are now able to approach this task through conditional sampling in latent space:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "Wherep \u03be (z|a) will not be approximated explicitly, rather we will use rejection sampling using the models Q \u03be (z) and q \u03be (a i |z) to approximate samples from p(z|a).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "To approach this, we first use Bayes' rule and the conditional independence of the attributes a i conditioned on z, since we assume the latent variable captures all information to model the attributes: a i \u22a5 a j |z (i.e. two attributes a i and a j are independent when conditioned on z)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "This approximation is introduced top \u03be (z|a), using the models Q \u03be and q \u03be above:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "The denominator q \u03be (a) in Eq. (5) could be estimated by approximating the expectation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "However, the denominator is not needed a priori in our rejection sampling scheme, in contrast, q \u03be (a) will naturally appear as the rejection rate of samples from the proposal distribution (see below).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "For rejection sampling distribution with pdf f (z), we need a proposal distribution g(z) and a constant M , such that f (z) \u2264 M g(z) for all z, i.e. M g(z) envelopes f (z). We draw samples from g(z) and accept the sample with probability f (z) M g(z) \u2264 1. In the above, to sample from Eq. (5), we consider a to be constant. We perform rejection sampling through the proposal distribution: g(z) = Q \u03be (z) that can be directly sam-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "The inequality trivially follows from the product of normalized probabilities. The acceptance rate is 1/M = q \u03be (a). Intuitively, the acceptance probability is equal to the product of the classifier's scores, while sampling from explicit density Q \u03be (z). In order to accept any samples, we need a region in z space to exist where Q \u03be (z) > 0 and the classifiers assign a nonzero probability to all desired attributes, i.e. the combination of attributes has to be realizable in z-space. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Density Modeling in Latent Space"
        },
        {
            "text": "Decoder p \u2713 (x|z) Fig. 1) , and (C) Sampling from the z-space using our CLaSS method (Generation in Fig. 1 ). Tables S1 to S7",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 25,
                    "text": "Fig. 1)",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 100,
                    "end": 106,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" d 8 x Q I J e 2 e T q f j r v l P T A r A K 1 G z V Y = \" > A A A E 2 X i c j V N L b x M x E H Z p g B J e L R z h Y J E i F Q l V 2 X C A Y 3 m k 4 t C q S e k j U h y t v N 5 J d l W v v X i d 0 t b y A U 6 I K 7 8 B i S v 8 G / 4 N d p p u 0 r R U W L v S 6 J v v m x n P e K K c p 4 W u 1 / / M X Z u v X L 9 x c + F W 9 f a d u / f u L y 4 9 2 C v k U D H Y Z Z J L 1 Y l o A T w V s K t T z a G T K 6 B Z x G E / O n j r / f u H o I p U i h 1 9 n E M v o w O R 9 l N G t Y P C x c f L 7 Z A c p S s n z z C h e a 7 k E f 4 Y k j z x y H K 4 W K u v 1 k c H X z S C s V F D 4 9 M K l + Z / k F i y Y Q Z C M 0 6 L o h v U c 9 0 z V O m U c b B V M i w g p + y A D q D r T E E z K H p m d A 2 L n z o k x n 2 p 3 C 8 0 H q H T C k O z o j j O I s f M q E 6 K W Z 8 H L / V 5 R E v J i 3 8 p f T 7 v x J v O C z 6 H 5 H C O F G U z x e v + q 5 5 J R T 7 U I N h p 7 f 0 h x 1 p i 3 2 Y c p w q Y 5 s f O o E y l 7 v q Y J V R R p t 0 w q u Q d u P Y o 8 O l e 8 z y h E W h D c n o i G e X W b G 1 + s O Y k 5 5 k 1 7 h O 2 W i V u M o d p D E x m G R W x I Y U j 7 t h u o 2 c I h 7 4 m n I o B B 1 M L 7 H N T a 1 i i 0 k G i i R q h 9 q J c u H 5 3 g 1 K 8 B 0 p j L 8 Z n Q o 8 4 X Q x 9 T F q G + K 5 F k W n Z M 6 x d Y u 0 S W z / F X G V m v Q S b J b F Z Y i 1 m i I Y j P Z q w i f g Q 7 F Q K P E l y j q c g t l N Z J 7 S W u i p c a J S d R N R X R A y d L w F N R 3 Q B n 8 p u b Z c s v H 1 J M / 0 s / n 8 U 0 4 G b T V v 2 D D f P i t w o n 4 L Z s P 5 V M i k K r W g q 9 J j R m T A 6 l z N k Z k h C 3 W 2 3 M h j Q W Y 5 b 7 G B 2 j S 8 a e 4 3 V 4 M V q o 9 2 o r b 0 Z r / g C e o S e o B U U o J d o D b 1 H L b S L G P q C f q J f 6 H e l W / l c + V r 5 d k q 9 N j f W P E T n T u X 7 X 7 u W r Y Y = < / l a t e x i t > (B.i) Encode training data with q(z|x) Sample z j,k~q (z)"
        },
        {
            "text": "References A Supplementary Text",
            "cite_spans": [],
            "ref_spans": [],
            "section": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" M + / d f b u G w T i W v 5 j W I 4 j Y N R V k k z E = \" > A A A E z H i c j V N N b x M x E H X b A C V 8 N I U j F 4 s U q U i o y o Y D H M t H K g 4 t T U s / I s V R 5 H U m y a p e e + W d L W 2 N r / w J r v R H 8 W / w J u k m T U u F t S u N 3 r w 3 M 3 6 2 w 0 R G K d Z q f x Y W l 0 r 3 7 j 9 Y f l h + 9 P j J 0 5 X K 6 r O j V G d G w K H Q U p t W y F O Q k Y J D j F B C K z H A 4 1 D C c X j y K c 8 f n 4 J J I 6 0 O 8 D y B T s w H K u p H g q O H u p W V t a T L c A j I 1 8 9 + X L x e 6 1 a q t Y 3 a a N G b Q T A J q m S y m t 3 V p U v W 0 y K L Q a G Q P E 3 b Q S 3 B j u U G I y H B l V m W Q s L F C R 9"
        },
        {
            "text": "Rational AMP design methods (62, 63) , both in cerebro and in silico, heavily rely upon structureactivity relationship (SAR) studies (42, (64) (65) (66) . However, incomplete understanding of complex sequence-function relationship remains as one major bottleneck for efficient lead generation or optimization, even when restricted to a fixed library of sequences. Recent methods for AMP discovery have therefore relied heavily on statistical learning and optimization, which includes linguistic approaches (51), neural language models (9), evolutionary optimization (12, 52, 53) , or sub-graph matching (54) . These approaches typically learn only from peptide sequences reported with antimicrobial activity, which can limit the exploratory capability of the statistical model. In addition, none of these methods explicitly account for broad-spectrum potency and low toxicity requirements for designing therapeutic AMPs, which is different from the present work.",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 32,
                    "text": "(62,",
                    "ref_id": null
                },
                {
                    "start": 33,
                    "end": 36,
                    "text": "63)",
                    "ref_id": null
                },
                {
                    "start": 133,
                    "end": 137,
                    "text": "(42,",
                    "ref_id": null
                },
                {
                    "start": 138,
                    "end": 142,
                    "text": "(64)",
                    "ref_id": null
                },
                {
                    "start": 143,
                    "end": 147,
                    "text": "(65)",
                    "ref_id": null
                },
                {
                    "start": 148,
                    "end": 152,
                    "text": "(66)",
                    "ref_id": null
                },
                {
                    "start": 566,
                    "end": 570,
                    "text": "(12,",
                    "ref_id": null
                },
                {
                    "start": 571,
                    "end": 574,
                    "text": "52,",
                    "ref_id": null
                },
                {
                    "start": 575,
                    "end": 578,
                    "text": "53)",
                    "ref_id": null
                },
                {
                    "start": 603,
                    "end": 607,
                    "text": "(54)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A.1 Rational AMP Design Methods"
        },
        {
            "text": "Since the peptide sequences are represented as text strings here, we will be limiting our discussion to literature around text generation with constraints. Controlled text sequence generation is non-trivial, as the discrete and non-differentiable nature of text samples does not allow the use of a global discriminator, which is commonly used in image generation tasks to guide generation. To tackle this issue of non-differentiability, policy learning has been suggested, which suffers from high variance during training (13, 67) . Therefore, specialized distributions, such as ",
            "cite_spans": [
                {
                    "start": 522,
                    "end": 526,
                    "text": "(13,",
                    "ref_id": null
                },
                {
                    "start": 527,
                    "end": 530,
                    "text": "67)",
                    "ref_id": "BIBREF66"
                }
            ],
            "ref_spans": [],
            "section": "A.2 Conditional Generation with Autoencoders"
        },
        {
            "text": "Following the work of Gmez-Bombarelli et al. (40) , Bayesian Optimization (BO) in the learned latent space has been employed for molecular optimization for properties such as drug likeliness (QED) or penalized logP. The standard BO routine consists of two key steps: (i) estimating the black-box function from data through a probabilistic surrogate model; usually a Gaussian process (GP), referred to as the response surface; (ii) maximizing an acquisition function that computes a score that trades off exploration and exploitation according to uncertainty and optimality of the response surface. As the dimensionality of the input latent space increases, these two steps become challenging. In most cases, such a method is restricted to local optimization using training data points as starting points, as optimizers are likely to follow gradients into regions of the latent space that the model has not been exposed to during training. Reinforcement learning (RL) based methods provide an alternative approach for molecular optimization (14, (76) (77) (78) , in which RL policies are learned by incorporating the desired attribute as part of the reward. However, a large number of evaluations are typically needed for both BO and RL-based optimizations while trading off exploration and exploitation (79) . Semi-supervised learning has also been used for conditional generation of molecules (15, (80) (81) (82) , which needs labels to be available during the generative model training.",
            "cite_spans": [
                {
                    "start": 45,
                    "end": 49,
                    "text": "(40)",
                    "ref_id": null
                },
                {
                    "start": 1040,
                    "end": 1044,
                    "text": "(14,",
                    "ref_id": null
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "text": "(76)",
                    "ref_id": null
                },
                {
                    "start": 1050,
                    "end": 1054,
                    "text": "(77)",
                    "ref_id": "BIBREF76"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "text": "(78)",
                    "ref_id": null
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "text": "(79)",
                    "ref_id": null
                },
                {
                    "start": 1394,
                    "end": 1398,
                    "text": "(15,",
                    "ref_id": null
                },
                {
                    "start": 1399,
                    "end": 1403,
                    "text": "(80)",
                    "ref_id": null
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "text": "(81)",
                    "ref_id": null
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "text": "(82)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A.3 Conditional Generation for Molecule Design"
        },
        {
            "text": "CLaSS is fundamentally different from these existing approaches, as it does not need expensive optimization over latent space, policy learning, or minimization of complex loss objectives -and therefore does not suffer from cumbersome computational complexity. Furthermore, CLaSS is not limited to local optimization around an initial starting point. Adding a new constraint in CLaSS is relatively simple, as it only requires a simple predictor training; therefore, CLaSS is easily repurposable. CLaSS is embarrassingly parallelizable as well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.3 Conditional Generation for Molecule Design"
        },
        {
            "text": "We compiled a new two-part (unlabeled and labeled) dataset for learning a meaningful representation of the peptide space and conditionally generating safe antimicrobial peptides from that space using the proposed CLaSS method. We consider discriminating for several functional attributes as well as for presence of structure in peptides. Only linear and monomeric sequences with no terminal modifications and length up to 50 amino acids were considered in curating this dataset. As a further pre-processing step, the sequences with non-natural amino acids (B, J, O, U, X, and Z) and the ones with lower case letters were eliminated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "Unlabeled Sequences: The unlabeled data is from Uniprot-SwissProt and Uniprot-Trembl database (27) and contains just over 1.7 M sequences, when considering sequences with length up to 50 amino acid.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 98,
                    "text": "(27)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "Labeled Sequences: Our labeled dataset comprises sequences with different attributes curated from a number of publicly available databases (39, (83) (84) (85) (86) . Below we provide details of the labeled dataset:",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 143,
                    "text": "(39,",
                    "ref_id": null
                },
                {
                    "start": 144,
                    "end": 148,
                    "text": "(83)",
                    "ref_id": null
                },
                {
                    "start": 149,
                    "end": 153,
                    "text": "(84)",
                    "ref_id": null
                },
                {
                    "start": 154,
                    "end": 158,
                    "text": "(85)",
                    "ref_id": null
                },
                {
                    "start": 159,
                    "end": 163,
                    "text": "(86)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "\u2022 Antimicrobial (8683 AMP, 6536 non-AMP);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "\u2022 Toxic (3149 Toxic, 16280 non-Toxic);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "\u2022 Broad-spectrum (1302 Positive, 1238 Negative);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "\u2022 Structured (1170 Positive, 2136 Negative);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "\u2022 Hormone (569 Positive);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "\u2022 Antihypertensive (1659 Positive);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "\u2022 Anticancer (504 Positive).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1 A Dataset for Semi-Supervised Training of AMP Generative Model"
        },
        {
            "text": "Sequences with AMP/non-AMP Annotation. AMP labeled dataset comprises sequences from two major AMP databases: satPDB (83) and DBAASP (84) , as well as a dataset used in an earlier AMP classification study named as AMPEP (86) . Sequences with an antimicrobial function annotation in satPDB and AMPEP or a MIC value against any target species less than 25 \u00b5g/ml in DBAASP were considered as AMP labeled instances. The duplicates between these three datasets were removed to generate a non-redundant AMP dataset. And, the ones with mean activity against all target species > 100 \u00b5g/ml in DBAASP were considered negative instances (non-AMP). Since experimentally verified non-AMP sequences are rare to find, the non-AMP instances in AMPEP were generated from UniProt sequences after discarding sequences that were annotated as AMP, membrane, toxic, secretory, defensive, antibiotic, anticancer, antiviral, and antifungal and were used in this study as well.",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "text": "(83)",
                    "ref_id": null
                },
                {
                    "start": 132,
                    "end": 136,
                    "text": "(84)",
                    "ref_id": null
                },
                {
                    "start": 219,
                    "end": 223,
                    "text": "(86)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.1.1 Details of Labeled Datasets"
        },
        {
            "text": "Sequences with Toxic/nonToxic Annotation: Sequences with toxicity labels are curated from satPDB and DBAASP databases as well as from the ToxinPred dataset (39) . Sequences with \"Major Function\" or \"Sub Function\" annotated as toxic in satPDB and sequences with hemolytic/cytotoxic activities against all reported target species less than 200 \u00b5g/ml in DBAASP were considered as Toxic instances. The toxic-annotated instances from ToxinPred were added to this set after removing duplicates resulting in a total to 3149 Toxic sequences. Sequences with hemolytic/cytotoxic activities > 250 \u00b5g/ml were considered as nonToxic. The nonToxic instances reported in ToxinPred (sequences from SwissProt or TrEMBL that are not found in search using keyword associated with toxins, i.e. keyword (NOT KW800 NOT KW20) or keyword (NOT KW800 AND KW33090), were added to the nonToxic set, totaling to 16280 non-AMP sequences.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 160,
                    "text": "(39)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.1.1 Details of Labeled Datasets"
        },
        {
            "text": "Sequences with Broad-Spectrum Annotation Antimicrobial sequences reported in the sat-PDB or DBAASP database can have both Gram-positive and Gram-negative strains as target groups. We consider such sequences as broad-spectrum. Otherwise, they are treated as narrowspectrum. Through our filtering, we found 1302 broad-spectrum and 1238 narrow-spectrum sequences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1.1 Details of Labeled Datasets"
        },
        {
            "text": "Sequences with Structure/No-Structure Annotation: Secondary Structure assignment was performed for structures from satPDB using the STRIDE algorithm (87) . If more than 60% of the amino acids are helix or beta-strand, we label it as structured (positive). Otherwise, they are labeled as negative. Through this filtering, we found 1170 positive sequences and 2136 negative sequences.",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 153,
                    "text": "(87)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "B.1.1 Details of Labeled Datasets"
        },
        {
            "text": "Peptide Dataset for Baseline Simulations: For the control simulations, three datasets were prepared. The first two were taken from the satpdb dataset, filtering sequences with length smaller than 20 amino acids. The high potency dataset contains the 51 sequences with the lowest average MIC, excluding sequences with cysteine residues. All these sequences have an average MIC of less than 10 \u00b5g / ml. The low potency dataset contains the 41 sequences with the highest MIC, excluding cysteine residues. All these have an average MIC over 300 \u00b5g / ml.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1.1 Details of Labeled Datasets"
        },
        {
            "text": "To create a dataset of inactive sequences, we queried UniProt using the following keywords: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B.1.1 Details of Labeled Datasets"
        },
        {
            "text": "When training the AE model, we sub selected sequences with length \u2264 the hyperparameter max seq length. Furthermore, both AMP-labeled and unlabeled data were split into train, held out, and test set. This reduces the available sequences for training; e.g for unlabeled set the number of available training sequences are 93k for max seq length=25, whereas the number of AMP-labeled sequences was 5000. The sequences with reported activities were considered as confirmed labeled data, and those with confirmed labels were up-sampled at a 1:20 ratio. Such upsampling of peptides with a specific attribute label will allow mitigation of possible domain shift due to unlabeled peptides coming from a different distribution. However, the benefit of transfer learning from unlabeled to labeled data likely outweighs the effects of domain shift.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.2 Autoencoder Training"
        },
        {
            "text": "To obtain the optimal hyperparameter setting for autoencoder training, we adopted an automated hyperparameter optimization. Specifically, we performed a grid search in the hyperparameter space and tracked an L 2 distance between the reconstructions of held-out data and training sequences, which was estimated using a weighted combination of BLEU, PPL, z-classifier accuracy, and amino acid composition-based heuristics. The best hyperparameter configuration obtained using this process was the following: learning rate = 0.001, number of iterations = 200000, minibatch size = 32, word dropout = 0.3. A beam search decoder was used with a beam size of 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.2 Autoencoder Training"
        },
        {
            "text": "Evaluation of generative models is notoriously difficult (88) . In the variational auto-encoder family, two competing objective terms are minimized: reconstruction of the input and a form of regularization in the latent space, which form a fundamental trade-off (89) . Since we want a meaningful and consistent latent space, models that do not compromise the reconstruction quality to achieve lower constraint loss are preferred. We propose an evaluation protocol using four metrics to judge the quality of both heldout reconstructions and prior samples (90) . The metrics are (i) The objective terms, evaluated on heldout data: reconstruction log likelihood \u2212 log p \u03b8 (x|z)",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 61,
                    "text": "(88)",
                    "ref_id": null
                },
                {
                    "start": 262,
                    "end": 266,
                    "text": "(89)",
                    "ref_id": null
                },
                {
                    "start": 554,
                    "end": 558,
                    "text": "(90)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.1.3 Autoencoder Evaluation"
        },
        {
            "text": "and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.3 Autoencoder Evaluation"
        },
        {
            "text": "is the average over heldout encodings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.3 Autoencoder Evaluation"
        },
        {
            "text": "(ii) Encoder variance log(\u03c3 2 j (x i )) averaged over heldout samples, in L 2 over components j. In order to achieve a meaningful latent space, we needed to regularize the encoder variance to not becoming vanishingly small, i.e., for the encoder to become deterministic (61) . Large negative values indicate that the encoder is collapsed to deterministic.",
            "cite_spans": [
                {
                    "start": 270,
                    "end": 274,
                    "text": "(61)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.1.3 Autoencoder Evaluation"
        },
        {
            "text": "(iii) Reconstruction BLEU score on held-out samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.3 Autoencoder Evaluation"
        },
        {
            "text": "(iv) Perplexity (PPL) evaluated by an external language model, for samples from prior p(z) and heldout encoding q \u03c6 (z|x).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.3 Autoencoder Evaluation"
        },
        {
            "text": "Note that (iii) and (iv) involve sampling the decoder (we use beam search with beam 5), which will therefore also take into account any exposure bias (91, 92) . We propose to evaluate peptide generation using the perplexity under an independently trained language model (iv), which is a reasonable heuristic (93) To further validate the performance of our language model, we tested it on sequences with randomly generated synthetic amino acids from the vocabulary of lengths ranging between 10 to 25. As expected, we found it to have a high perplexity of 27.29. Also, when evaluating it for repeated amino acids (sequence consisting of a single token from vocabulary), of length ranging between 10 to 25, we found the perplexity to be very low (3.48) . Upon further investigation, we observed that the training data consists of amino acids with repeated sub-sequences, which the language model by nature, fails to penalize heavily. Due to this behavior, we can conclude that the perplexity of a collapsed peptide model will be closer to 3.48 (as seen in the case of \u03b2-VAE). We have summarized these observation in Table S1 . Table S1 : Performance of various autoencoder schemes against different baselines.",
            "cite_spans": [
                {
                    "start": 150,
                    "end": 154,
                    "text": "(91,",
                    "ref_id": null
                },
                {
                    "start": 155,
                    "end": 158,
                    "text": "92)",
                    "ref_id": "BIBREF91"
                },
                {
                    "start": 744,
                    "end": 750,
                    "text": "(3.48)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 1114,
                    "end": 1122,
                    "text": "Table S1",
                    "ref_id": null
                },
                {
                    "start": 1125,
                    "end": 1133,
                    "text": "Table S1",
                    "ref_id": null
                }
            ],
            "section": "C.1.3 Autoencoder Evaluation"
        },
        {
            "text": "The evaluated metrics on held-out samples for different autoencoder models trained on either labeled or full dataset are also reported in Table S1 . We observed that the reconstruction of WAE is more accurate compared to \u03b2-VAE: we achieve a reconstruction error of 0.2163 and a BLEU score of 0.892 on a held-out set using WAE with \u03c3 of 7 and R(logV ar) of 1e-3 (values are For post-generation screening, we used four sets of monolithic classifiers that are trained directly on peptide sequences. Each of these binary sequence-level classifiers was aimed at capturing one of the following four properties of peptide sequences, namely,",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 138,
                    "end": 146,
                    "text": "Table S1",
                    "ref_id": null
                }
            ],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "\u2022 AMP/Non-AMP : Is the sequence an AMP or not?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "\u2022 Toxicity/Non-Toxic : Is the sequence toxic or not?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "\u2022 Broad/Narrow : Does the sequence show antibacterial activity on both Gram+ and Gramstrains or not?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "\u2022 Structure/No-Structure : Does the sequence have secondary structure or not?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "For each attribute, we trained a bidirectional LSTM-based classifier on the labeled dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "We used a hidden layer size of 100 and a dropout of 0.3. Size of dataset used as well as accuracies are reported in the Table S2 . This initial structure is then passed as in input to martinize.py (96) , which coarsegrains the system. The resulting files are passed into insane.py (97) to create the peptide membrane system. The solvent is a 90:10 ratio of water to antifreeze particles, with the membrane being a 3:1 mixture of POPC to POPG. The system is 15 nm x 15 nm x 30 nm, with the membrane perpendicular to the longest direction. Ions are added to neutralize the system.",
            "cite_spans": [
                {
                    "start": 197,
                    "end": 201,
                    "text": "(96)",
                    "ref_id": null
                },
                {
                    "start": 281,
                    "end": 285,
                    "text": "(97)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 120,
                    "end": 128,
                    "text": "Table S2",
                    "ref_id": null
                }
            ],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "For the CGMD simulations, we used the Martini forcefield (98), as Martini is optimized for predicting the interactions between proteins and membranes while being computationally efficient, it is well suited for the task of a quick but physically-inspired filtering peptide sequences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "After building, the system is minimized for 50,000 steps using Gromacs 2019.1 (99, 100) and the 2.0 version of the Martini forcefield (98) . After minimization, the production run is carried for 1 \u00b5s at a 20 fs timestep. Temperature is kept constant at 310 K using Stochastic Velocity Rescaling (101) applied independently to the protein, lipid, and the solvent groups.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "(99,",
                    "ref_id": null
                },
                {
                    "start": 83,
                    "end": 87,
                    "text": "100)",
                    "ref_id": null
                },
                {
                    "start": 134,
                    "end": 138,
                    "text": "(98)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "The pressure is kept constant at 1 atmosphere using a Parrinello-Rahman barostat (102, 103) applied independently to the same groups as the thermostat.",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 86,
                    "text": "(102,",
                    "ref_id": null
                },
                {
                    "start": 87,
                    "end": 91,
                    "text": "103)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "After 1 \u00b5s of sampling, we estimated the number of peptide-membrane contacts using TCL scripting and in-house Python scripts. The number of contacts between positive residues and the lipid membranes is defined as the number of atoms belonging to a lipid at a distance less than 7.5\u00c5 from a positive residue.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "For the control simulations, three datasets consisting of reported high-potency AMP, lowpotency AMP, and non-AMP sequences were used that are discussed in B.1.1. We performed a set of 130 control simulations. We found that the variance of the number of contacts (cutoff 7.5\u00c5) between positive residues and Martini beads of the membrane lipids is predictive of antimicrobial activity. Specifically, the contact variance distinguishes between high potency and non-antimicrobial sequences with a sensitivity of 88% and specificity of 63%. To screen, we used a cutoff value of 2 beads for the contact variance. We carried out a set of simulations for the 163 amp-positive and 179 amp-negative generated sequences. We further restricted to sequences that bind in less than 500 ns during the 1 \u00b5s long simulation, so that the contact variance is calculated over at least half of the total simulation time. Only sequences that formed at least 5 contacts (averaged over the duration of the simulation) were considered.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.1.4 Autoencoder Variants: Comparison"
        },
        {
            "text": "We used the CHARMM36m (104) forcefield to simulate the binding of four copies of YL12",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3 All-Atom Simulations"
        },
        {
            "text": "and FK13 to a model membrane. Phi and Psi angles in the initial peptide structure were set to what was predicted using a recent deep learning model (105) . A 3:1 DLPC:DLPG bilayer, with shorter tails, was used to speed up the simulation, alongside a smaller water box (98\u00c5) than the Martini simulations, to investigate the short-term effect of peptide-membrane interactions.",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 153,
                    "text": "(105)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.3 All-Atom Simulations"
        },
        {
            "text": "A 160 ns long trajectory was run for the FK13 system. The length of the YI12 simulation was 200 ns. The number of peptide-membrane and peptide-peptide contacts (using a threshold of 7.5\u00c5 and ignoring hydrogen atoms) were found to be converged in less time than the maximum simulation length for both systems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.3 All-Atom Simulations"
        },
        {
            "text": "The bilayer is prepared using CHARMM-GUI (106) , and the peptide sequence is prepared using PeptideBuilder (95) . Solvation and assembly is performed using VMD 1.9.3 (107). The system is simulated using NAMD 2.13 (108) . Temperature is kept constant using a Langevin thermostat and a Nos\u00e9-Hoover Langevin piston barostat The Particle-Mesh Ewald method was used for long-range electrostatics. All simulations used a time step of 2 fs.",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 46,
                    "text": "(106)",
                    "ref_id": null
                },
                {
                    "start": 107,
                    "end": 111,
                    "text": "(95)",
                    "ref_id": null
                },
                {
                    "start": 213,
                    "end": 218,
                    "text": "(108)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.3 All-Atom Simulations"
        },
        {
            "text": "Physicochemical properties like aromaticity, Eisenberg hydrophobicity, charge, charge density, aliphatic index, hydrophobic moment, hydrophobic ratio, isoelectric point, and instability index were estimated using the GlobalAnalysis method in modLAMP (109) . Protparam tool from Expasy (https://web.expasy.org/protparam) was used to estimate the grand average of hydropathicity (GRAVY) score.",
            "cite_spans": [
                {
                    "start": 250,
                    "end": 255,
                    "text": "(109)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.4 Peptide Sequence Analysis"
        },
        {
            "text": "Pairwise evolutionary similarity was estimated using a global alignment method, the PAM30 matrix (36) , a gap open penalty of -9, and a gap extension penalty of -1 using Pairwise2 function of Biopython package (110) . Higher positive values indicate better similarity. To check the correspondence between evolutionary similarity and Euclidean distance in z-space, a random set of sequence encodings were first selected, and then evolutionary similarity and z-distance with their close latent space neighbors were estimated. Sequence similarity with respect to a sequence database was estimated using \"blastp-short\" command from NCBI BLAST sequence similarity search tool (111, 112) was used to query generated short sequences by using a word size of 2, the PAM30 matrix (36) which produce beta-lactamase SHV 18. The broth microdilution method was used to measure MIC values of the AMPs, and the detailed protocol was reported previously (114, 115) .",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 101,
                    "text": "(36)",
                    "ref_id": null
                },
                {
                    "start": 210,
                    "end": 215,
                    "text": "(110)",
                    "ref_id": null
                },
                {
                    "start": 671,
                    "end": 676,
                    "text": "(111,",
                    "ref_id": "BIBREF110"
                },
                {
                    "start": 677,
                    "end": 681,
                    "text": "112)",
                    "ref_id": null
                },
                {
                    "start": 937,
                    "end": 942,
                    "text": "(114,",
                    "ref_id": null
                },
                {
                    "start": 943,
                    "end": 947,
                    "text": "115)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.4 Peptide Sequence Analysis"
        },
        {
            "text": "The selectivity of AMPs towards bacteria over mammalian cells was studied using rat red blood ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.5.2 Hemolytic Activity"
        },
        {
            "text": "The animal study protocols were approved by the Institutional Animal Care and Use Committee of Biological Resource Center, Agency for Science, technology, and Research (A*STAR),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.5.3 Acute in Vivo Toxicity Analysis"
        },
        {
            "text": "Singapore. LD50 values of the AMPs, the dose required to kill 50% mice, were determined using a previously reported protocol (116) . Specifically, female Balb/c mice (8 weeks old, 18-22 g) were employed. AMPs were dissolved in saline and administered to mice by intraperitoneal (i.p.) injection at various doses. Mortality was monitored for 14 days post-AMP administration, and LD50 was estimated using the maximum likelihood method.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 130,
                    "text": "(116)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "C.5.3 Acute in Vivo Toxicity Analysis"
        },
        {
            "text": "The peptides were dissolved at 0.5 mg/mL in either deionized water or deionized water containing 25 mM SDS surfactant. It forms anioic micelles in aqueous solution, which mimic the bacterial membrane. The CD spectra were measured using a CD spectropolarimeter from Jasco",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C.5.4 CD Spectroscopy"
        },
        {
            "text": "Corp. J-810 at room temperature and a quartz cuvette with 1 mm path length. The spectra were acquired by scanning from 190 to 260 nm at 10nm/min after subtraction with the spectrum of the solvent. Figure S2 : Percentage of hemolysis of rat red blood cells as a function of peptide concentration. (B) and (C) show CD Spectra of YI12 and FK13 peptide, respectively, at 0.5 mg/ml concentration in DI water and presence of 20 mM SDS buffer. Both YI12 and FK13 showed a random coil-like structure in the absence of SDS. When SDS was present, both sequences form \u03b1-helical structure (evident from the 208 nm and 222 nm peaks). (D) BLAST search results (alignment score, E-value, percentage of alignment coverage, percentage of identity, percentage of positive matches or similarity, percentage of alignment gap) against full training data for YI12 and FK13.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 197,
                    "end": 206,
                    "text": "Figure S2",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "C.5.4 CD Spectroscopy"
        }
    ],
    "bib_entries": {
        "BIBREF4": {
            "ref_id": "b4",
            "title": "International Conference on Machine Learning",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Jin",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Barzilay",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Jaakkola",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "2323--2332",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "No time to wait: Securing the future from drug-resistant infections",
            "authors": [],
            "year": 2019,
            "venue": "Tech. rep., United Nations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Tackling Drug-Resistant Infections Globally: final report and recommendations",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "O&apos;neill",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The Review on Antimicrobial Resistance",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Antibacterial agents in clinical development",
            "authors": [],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Institutional profile: Community for open antimicrobial drug discovery-crowdsourcing new antibiotics and antifungals",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Desselle",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bowman",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "10--21",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "26. Additional information is available as supplementary materials",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Tolstikhin",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bousquet",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gelly",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "1050",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Proc. of NAACL",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "E"
                    ],
                    "last": "Peters",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Advances in Neural Information Processing Systems",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mccann",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bradbury",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "6297--6308",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Proceedings of the 56th",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Conneau",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Kruszewski",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lample",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Barrault",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Baroni",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [],
            "year": 2018,
            "venue": "",
            "volume": "1",
            "issn": "",
            "pages": "2126--2136",
            "other_ids": {}
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Advances in neural information processing systems",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gretton",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "M"
                    ],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rasch",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "513--520",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Advances in neural information processing systems",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rahimi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Recht",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1177--1184",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Thirty-First AAAI Conference on Artificial Intelligence",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "Advances in Neural Information Processing Systems",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mohamed",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Rezende",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "3581--3589",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "International Conference on Machine Learning",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "P"
                    ],
                    "last": "Xing",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1587--1596",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "Advances in Neural Information Processing Systems",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "You",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Pande",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "6410--6421",
            "other_ids": {}
        },
        "BIBREF91": {
            "ref_id": "b91",
            "title": "Advances in Neural Information Processing Systems",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Vinyals",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Jaitly",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1171--1179",
            "other_ids": {}
        },
        "BIBREF110": {
            "ref_id": "b110",
            "title": "The NCBI Handbook",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Madden",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "National Center for Biotechnology Information (US)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF112": {
            "ref_id": "b112",
            "title": "PATSEQ",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "thetic variant of a 13 amino acid long bactericidal domain (PuroA: FPVTWRWWKWWKG) of Puroindoline-A protein from wheat endosperm. The antimicrobial and hemolysis activities of FK13 are similar to those reported for PuroA (47, 48). Nevertheless, FK13 is significantly different from PuroA; FK13 is K-rich and low in W-content, resulting in lower Grand Average of Hydropathy (GRAVY) score (\u22120.854 vs. \u22120.962), higher aliphatic index (60.0 vs. 22.3), and lower instability index (15.45 vs. 58.30), all together indicative of higher peptide stability. In fact, lower W-content was found beneficial for stabilizing of FK13 during wet-lab experiments,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "For MMD, we used a random features approximation of the Gaussian kernel (60). Details of autoencoder architecture and training, as well as an experimental comparison between different auto-encoder variations tested in this study, can be found in Supplementary Material sections C.1.1, C.1.2 and C.1.4.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Overview and timeline of the proposed AI-driven approach for accelerated antimicrobial design.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "B.ii) Fit explicit density model Q \u21e0 (z) \u21e1 q (z)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "B.iii) Fit classifier for each attribute a i :q \u21e0 (a i |z) C.i) Sample z \u21e0 Q \u21e0 (z) C.ii) Compute q \u21e0 (a i |z) (A)Training a generative Autoencoder (AE) model on peptide sequences (AE Training inFig. 1), (B) Mapping sparse peptide attributes to the model's latent z-space (AE Training in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "(A) Relation between evolutionary similarity and Euclidean distance in latent z-space, when sequences were modeled using WAE (VAE in Inset). Darker points indicate similarity with itself (i.e. the same exact sequence). (B) Class prediction accuracy (%) of attribute classifiers on test data. Classifiers were trained either using WAE z-space encodings (z-) or on sequences (sequence-level). (C) Decoded sequences and their attributes during a linear interpolation between two distant sequences in the WAE latent space. Attributes include (1) physicochemical properties, (2) evolutionary similarity (evo start, evo end) from endpoint sequences, and (3) AMP (z amp) and Toxic (z tox) class probabilities from z-classifiers. Values in orange and blue are in the upper and lower quartile, respectively. Black rectangle indicates sequences with low attribute similarity to endpoint sequences. As an example of further analysis, we show the relation of AMP class probability and instability index for all candidates in the interpolation and a ball-stick rendering of a selected sequence in the path. Comparison of amino acid composition (A), global hydrophobicity (B), hydrophobic moment (C), and charge distribution (D) of CLaSS-generated AMPs with training sequences. Mean and standard deviation were estimated on three different sets, each consisting 3000 randomly chosen samples. Generated AMP: orange; training AMP: blue; training unlabeled: gray. (A) Snapshot from a coarse-grained molecular dynamics simulation of an AMP (in orange) binding with a lipid bilayer (gray). (B) Confusion matrix of the simulation-based classifier that uses peptide-membrane contact variance as feature for detecting AMP sequences.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "(A) MIC values against diverse strains, including one that is multidrug-resistant (MDR), hemolytic activity measured at 50% hemolysis (HC 50 ) using rat red blood cells, and lethal dose toxicity (LD 50 ) values for Balb/C mice for YI12 and FK13, two best CLaSSdesigned AMPs. (B) BLAST search results of YI12 and FK13. (C) Snapshot from all-atom simulation of YI12 and FK13. Selected residues that interact with the membrane are highlighted.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "the Gumbel-softmax(68,69), a concrete distribution(70), or a soft-argmax function(71), have been proposed to approximate the gradient of the model from discrete samples.Alternatively, in a semi-supervised model setting, the minimization of element-wise reconstruction error has been employed(72), which tends to lose the holistic view of a full sentence. Hu et al.(73) proposed a VAE variant that allows both controllable generation and semi-supervised learning. The working principle needs labels to be present during training and encourages latent space to represent them, so the addition of new attributes will require retraining the latent variable model itself. The framework relies on a set of new discrete binary variables in latent space to control the attributes, an ad-hoc wake-sleep procedure. It requires learning the right balance between multiple competing and tightly interacting loss objectives, which is tricky.Engel et al. (74) propose a conditional generation framework without retraining the model, similar in concept to ours, by modeling in latent space post-hoc. Their approach does not need an explicit density model in z-space, rather relies on adversarial training of generator and attribute discriminator and focuses modifying sample reconstructions rather than generating novel samples. Recent Plug and Play Language Model (PPLM) for controllable language generation combines a pre-trained language model (LM) with one or more simple attribute classifiers that guide text generation without any further training of the LM (75).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "NOT keyword:\"Antimicrobial [KW-0929]\" length:[1 TO 20] NOT keyword:\"Toxin [KW-0800]\" NOT keyword:\"Disulfide bond [KW-1015]\" NOT annotation:(type:ptm) NOT keyword:\"Lipidbinding [KW-0446]\" NOT keyword:\"Membrane [KW-0472]\" NOT keyword:\"Cytolysis [KW-0204]\" NOT keyword:\"Cell wall biogenesis/degradation [KW-0961]\" NOT keyword:\"Amphibian defense peptide [KW-0878]\" NOT keyword:\"Secreted [KW-0964]\" NOT keyword:\"Defensin [KW-0211]\" NOT keyword:\"Antiviral protein [KW-0930]\" AND reviewed:yes. From this, we picked 54 random sequences to act as the inactive dataset for simulation. investigate two different types of autoencoding approaches: \u03b2-VAE (24) and WAE (25) in this study. For each of these AEs the default architecture involves bidirectional-GRU encoder and GRU decoder. For the encoder, we used a bi-directional GRU with hidden state size of 80. The latent capacity was set at D = 100.For VAE, we used KL term annealing \u03b2 from 0 to 0.03 by default. We also present an unmodified VAE with KL term annealing \u03b2 from 0 to 1.0. For WAE, we found that the random features approximation of the gaussian kernel with kernel bandwidth \u03c3 = 7 to be performing the best. For comparison sake, we have included variations with \u03c3 values of 3 and 15 too.The inclusion of z-space noise logvar regularization, R(logV ar), helped avoiding collapse to a deterministic encoder. Among different regularization weights used, 1e \u2212 3 had the most desirable behavior on the metrics (see Section C.1.4).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": ", a gap open penalty of -9, a gap extension penalty of -1, threshold of 16, comp based stats set to 0 and window size of 15. Alignment score (Bit score), Expect value (E-value), percentage of alignment coverage, percentage of identity, percentage of positive matches or similarity, percentage of alignment gap were used for analyzing sequence novelty. The E-value is a measure of the probability of the high similarity score occurring by chance when searching a database of a particular size. E-values decrease exponentially as the score of the match increases. For the search against patented sequences, we used the PATSEQ the peptides were amidated at their C-terminus to remove the negative charge of the Cterminal carboxyl group. Antimicrobial activity of the best AMP hits was evaluated against a broad spectrum of bacteria for minimum inhibitory concentration (MIC), which include Grampositive Staphylococcus aureus (ATCC 29737), Gram-negative Escherichia coli (ATCC 25922), Pseudomonas aeruginosa (ATCC 9027) and multi-drug resistant K. pneumoniae (ATCC 700603),",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "cells (rRBCs), which were obtained from the Animal Handling Unit of Biomedical Research Center, Singapore. Negative control: Untreated rRBC suspension in phosphate-buffered saline (PBS); Positive control: rRBC suspension treated with 0.1% Triton X. Percentage of hemolysis of rRBCs was obtained using the following formula: Hemolysis(%) = O.D. 576nm of treated samples \u2212 O.D. 576nm of negative control O.D. 576nm of positive samples \u2212 O.D. 576nm of negative control (6)",
            "latex": null,
            "type": "figure"
        },
        "TABREF2": {
            "text": "if we assume the independent language model captures the distribution p(x) well. External Language Model We trained an external language model on both labeled and unlabeled sequences to determine the perplexity of the generated sequences. Specifically, we used a character-based LSTM language model (LM) with the help of LSTM and QRNN Language Model Toolkit (94) trained on both AMP-labeled and unlabeled data. We trained our language model with a total of 92624 sequences, with a maximum sequence length of 25. Our best model achieves a test perplexity of 13.26.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "WAE, \u03c3 = 7, R(logV ar) = 1e \u2212 2 15.16 0.665 0.685 -0.3962 WAE, \u03c3 = 7, R(logV ar) = 1e \u2212 3 12.87 0.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "1.079 and 0.493 for \u03b2-VAE with \u03b2 set to 0.03). The advantage of using abundant unlabeled data compared to only the labeled ones for representation learning is evident, as the language model perplexity (PPL, captures sequence diversity) for the WAE model trained on a full dataset is closer to that of the test perplexity (13.26), and the BLEU score is also higher when compared to the WAE model trained only on AMP-labeled sequences. For reference, PPL of random peptide sequences is > 25, and for repeated sequences is 3.48. The z-classifier trained on the latent space of the best WAE model achieved a test accuracy of87.4, 68.9, 77.4, 98.3, 76.3%    for detecting peptides with AMP/non-AMP, toxic/non-toxic, anticancer, antihypertensive, and hormone annotation.",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Table S2: Performance of classifiers based on different attributes.Based on the distribution of the scores (classification probabilities/logits), we determined the threshold by considering the 50 th percentile (median) of the scores ( reported in the last column ofTable S2). Similarly, we selected a PPL threshold of 16.04 that is the 25 th percentile of the PPL distribution of samples generated from the prior distribution of the best WAE model and also closer to the perplexity of our trained language model on test data.C.2.2 CGMD Simulations -Contact Variance as a Metric for Classifying Membrane BindingGiven a peptide sequence as an input, PeptideBuilder (95) is used to prepare a PDB file of the all-atom representation of the peptide. This is prepared either as an alpha helix (with dihedral angles \u03c6 = \u221257, \u03c8 = \u221247) or as a random coil, with \u03c6 and \u03c8 dihedral angles taking random values between \u221250 \u2022 and 50 \u2022 .",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "E-value <= 0.001 <= 0.01 <= 0.1 <= 1 <= 10 > 10 Percentage of CLaSS-generated AMP sequences in different categories of Expect value (E-value). E-value for the match with the highest score was considered, as obtained by performing BLAST similarity search against AMP-labeled training sequences (top row) and unlabeled training sequences (bottom row).SequenceScore E-Value % Coverage % Identity % Positive % Gap",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "We acknowledge Youssef Mroueh and Kush Varshney for insightful discussions. We also thank Oscar Chang, Elham Khabiri, and Matt Riemer for help with the initial phase of the work.We would like to acknowledge David Cox, Yuhai Tu, Pablo Meyer Rojas, and Mattia Rigotti for providing valuable feedback on the manuscript. F.C. thanks Patrick Simcock for sharing knowledge.The authors declare that they have no competing financial interests. Correspondence and requests for materials should be addressed to Payel Das (email: daspa@us.ibm.com).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments"
        },
        {
            "text": ">1000 1000 LRPAFKVSK-CONH2 >1000 >1000 ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}