{
    "paper_id": "7ccf3ce62744d7453c5777407022a09aefc9e7f7",
    "metadata": {
        "title": "Optimally (Distributional-)Robust Kalman Filtering",
        "authors": [
            {
                "first": "Peter",
                "middle": [],
                "last": "Ruckdeschel",
                "suffix": "",
                "affiliation": {},
                "email": "peter.ruckdeschel@itwm.fraunhofer.de"
            }
        ]
    },
    "abstract": [
        {
            "text": "We present optimality results for robust Kalman filtering where robustness is understood in a distributional sense, i.e.; we enlarge the distribution assumptions made in the ideal model by suitable neighborhoods. This allows for outliers which in our context may be system-endogenous or -exogenous, which induces the somewhat conflicting goals of tracking and attenuation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The corresponding minimax MSE-problems are solved for both types of outliers separately, resulting in closed-form saddle-points which consist of an optimally-robust procedure and a corresponding least favorable outlier situation. The results are valid in a surprisingly general setup of state space models, which is not limited to a Euclidean or time-discrete framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The solution however involves computation of conditional means in the ideal model, which may pose computational problems. In the particular situation that the ideal conditional mean is linear in the observation innovation, we come up with a straight-forward Huberization, the rLS filter, which is very easy to compute. For this linearity we obtain an again surprising characterization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Robustness issues in Kalman filtering have long been a research topic, with first (non-verified) hits on a quick search for \"robust Kalman filter\" on scholar. google.com as early as 1962 and 1967, i.e.; the former even before the seminal Huber (1964) paper, often referred to as birthday of Robust Statistics.",
            "cite_spans": [
                {
                    "start": 238,
                    "end": 250,
                    "text": "Huber (1964)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In the meantime there is an ever growing amount of literature on this topic - Kassam and Poor (1985) have already compiled as many as 209 references to that subject in 1985. Excellent surveys are given in, e.g. Kassam and Poor (1985) , Stockinger and Dutter (1987) , Schick and Mitter (1994) , K\u00fcnsch (2001) .",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 100,
                    "text": "Kassam and Poor (1985)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 211,
                    "end": 233,
                    "text": "Kassam and Poor (1985)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 236,
                    "end": 264,
                    "text": "Stockinger and Dutter (1987)",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 267,
                    "end": 291,
                    "text": "Schick and Mitter (1994)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 294,
                    "end": 307,
                    "text": "K\u00fcnsch (2001)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In these references you find many different notions of robustness, all somewhat related to stability but measuring this stability w.r.t. deviations of various \"input parameters\"; in this paper we are concerned with (distributional) minimax robustness; i.e.; we work with suitable distributional neighborhoods about an ideal model, already used by Birmiwal and Shen (1993) and Birmiwal and Papantoni-Kazakos (1994) , and then solve the problem to find the procedure minimizing the maximal predictive inaccuracy on these neighborhoodsmeasured in terms of mean squared error (MSE)-in quite generality, compare Theorems 3.2, 3.10, 4.1. In the particular situation that the ideal conditional mean is linear in the observation innovation (for a definition see subsection 2.3.2), the minimax filter is a straight-forward Huberization, the rLS filter, which is extremely easy to compute. For this linearity we obtain a surprising characterization in Propositions 3.4 and 3.6. This motivates a corresponding optimal test for linearity, Proposition 3.8. Even in situations where no or only partial knowledge of the size of the contamination is available we can distinguish an optimal procedure, compare Lemma 3.1.",
            "cite_spans": [
                {
                    "start": 347,
                    "end": 371,
                    "text": "Birmiwal and Shen (1993)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 376,
                    "end": 413,
                    "text": "Birmiwal and Papantoni-Kazakos (1994)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we start with some definitions and assumptions. We are working in the context of state space models (SSM's) as to be found in many textbooks, cf. e.g. Anderson and Moore (1979) , Harvey (1991) , and Durbin and Koopman (2001) .",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 193,
                    "text": "Anderson and Moore (1979)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 196,
                    "end": 209,
                    "text": "Harvey (1991)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 216,
                    "end": 241,
                    "text": "Durbin and Koopman (2001)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Ideal model"
        },
        {
            "text": "The most prominent setting in this context is the linear, time-discrete, Euclidean setup, which will serve as reference setting in this paper: An unobservable p-dimensional state X t evolves according to a possibly time-inhomogeneous vector autoregressive model of order 1 (VAR(1)) with innovations v t and transition matrices F t , i.e., X t = F t X t\u22121 + v t (2.1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Discrete, linear Euclidean Setup"
        },
        {
            "text": "The statistician observes a q-dimensional linear transformation Y t of X t and in this makes an additive observation error \u03b5 t ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Discrete, linear Euclidean Setup"
        },
        {
            "text": "In the ideal model we work in a Gaussian context, that is we assume",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Discrete, linear Euclidean Setup"
        },
        {
            "text": "3) X 0 , v s , \u03b5 t , s, t \u2208 N stochastically independent (2.4)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Discrete, linear Euclidean Setup"
        },
        {
            "text": "As usual, normality assumptions may be relaxed to working only with specified first and second moments, if we restrict ourselves to linear unbiased procedures as in the Gauss-Markov setting. For this paper, we assume the hyper-parameters F t , Z t , Q t , V t , a 0 to be known.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time Discrete, linear Euclidean Setup"
        },
        {
            "text": "Parts of our results (more specifically, all of sections 3.2, 3.4) also cover much more general SSMs; in this paragraph we sketch some of these. To begin with, as long as MSE makes sense for the range of the states, these results cover general Hidden Markov Models for arbitrary observation space as given by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalizations covered by the present approach"
        },
        {
            "text": "In this setting, we assume known (and existing) [regular conditional] densities p X0 0 , p \u00b7 | \u00b7 t , q \u00b7 | \u00b7 t w.r.t. known measures \u03bd t , \u00b5 t on B q and B p , respectively. Dynamic (generalized) linear models as discussed in West et al. (1985) and West and Harrison (1989) are covered as well -under corresponding assumptions as to (conditional) densities and range of the states. In applications of Mathematical Finance we also need to cover continuous time settings, i.e.; there is an unobservable state evolving according to an SDE",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 244,
                    "text": "West et al. (1985)",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 249,
                    "end": 273,
                    "text": "West and Harrison (1989)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Generalizations covered by the present approach"
        },
        {
            "text": "where for X 0 we assume (2.5), while W t , is a Wiener process, and f and q are suitably measurable, known functions, and observations Y t are either formulated as a time-continuous observation process (as in Tang (1998) ) or-more oftenat discrete, but not necessarily equally spaced times, compare, e.g. Nielsen et al. (2000) and Singer (2002) . In this context, but also for corresponding nonlinear time-discrete SSMs, a straightforward approach linearizes the corresponding transition and observation functions to give the (continuous-discrete) Extended Kalman Filter (EKF) After this linearization we are again in the context of a (time-inhomogeneous) linear SSM, hence the methodology we develop in the sequel applies to this setting as well. So far we do not cover approaches to improve on this simple linearization, notably the second order nonlinear filter (SNF) introduced in Jazwinski (1970), also cf. Singer (2002, sec. 4.3.1) . the unscented Kalman filter (UKF) (Julier et al., 2000) and Hermite expansions as in A\u00eft-Sahalia (2002) , see also Singer (2002, sec. 4.3) .",
            "cite_spans": [
                {
                    "start": 209,
                    "end": 220,
                    "text": "Tang (1998)",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 305,
                    "end": 326,
                    "text": "Nielsen et al. (2000)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 331,
                    "end": 344,
                    "text": "Singer (2002)",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 912,
                    "end": 937,
                    "text": "Singer (2002, sec. 4.3.1)",
                    "ref_id": null
                },
                {
                    "start": 974,
                    "end": 995,
                    "text": "(Julier et al., 2000)",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1025,
                    "end": 1043,
                    "text": "A\u00eft-Sahalia (2002)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1055,
                    "end": 1078,
                    "text": "Singer (2002, sec. 4.3)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Generalizations covered by the present approach"
        },
        {
            "text": "Going one more step ahead, to cover applications such as portfolio optimization, we may allow for controls U t to be set or determined by the statistician, and which are fed back in the state equations. In the context of the continuous time model, this is also known as SDEX, cf. Nielsen et al. (2000) , and for the application of stochastic control to portfolio optimization, cf. Korn (1997) . In this setting, controls U t are usually assumed measurable w.r.t. \u03c3(Y t\u2212 ); to integrate them into our setting, we simply have to integrate them in the corresponding condition vectors.",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 301,
                    "text": "Nielsen et al. (2000)",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 381,
                    "end": 392,
                    "text": "Korn (1997)",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Generalizations covered by the present approach"
        },
        {
            "text": "Finally, the question of specifying the order of conditioning left aside, we do not make use of the linearity of time, so our minimax results also cover suitable formulations of indirectly observed random fields.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalizations covered by the present approach"
        },
        {
            "text": "As usual with Robust Statistics, the ideal model assumptions we have specified so far are extended by allowing (small) deviations, most prominently generated by outliers. In our notation, suffix \"id\" indicates the ideal setting, \"di\" the distorting (contaminating) situation, \"re\" the realistic, contaminated situation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deviations from the ideal model"
        },
        {
            "text": "In SSM context (and contrary to the independent setting), outliers may or may not propagate. Following the terminology of Fox (1972) , we distinguish innovation outliers (or IO's) and additive outliers (or AO's). Historically, AO's denote gross errors affecting the observation errors, i.e., AO ::",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 132,
                    "text": "Fox (1972)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "AO's and IO's"
        },
        {
            "text": "where L(\u03b5 di t ) is arbitrary, unknown and uncontrollable (a.u.u.) and 0 \u2264 r AO \u2264 1 is the AO-contamination radius, i.e.; the probability for an AO. IO's on the other hand are usually defined as outliers which affect the innovations,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "AO's and IO's"
        },
        {
            "text": "where again L(v di t ) is a.u.u. and 0 \u2264 r IO \u2264 1 is the corresponding radius. We stick to this distinction for consistency with literature, although we rather use these terms in a wider sense, unless explicitly otherwise stated: IO's denote endogenous outliers affecting the state equation in general, hence distortion propagates into subsequent states. This also covers level shifts or linear trends; which if |F t | < 1 are not included in (2.10), as IO's would then decay geometrically in t. We also extend the meaning of AO's to denote general exogenous outliers which enter the observation equation only and thus do not propagate, like substitutive outliers or SO's defined as SO ::",
            "cite_spans": [],
            "ref_spans": [],
            "section": "AO's and IO's"
        },
        {
            "text": "where again L(Y di t ) is a.u.u. and 0 \u2264 r SO \u2264 1 is the corresponding radius. Apparently, the SO-ball of radius r consisting of all L(Y re t ) according to (2.11) contains the corresponding AO-ball of the same radius when Y re t = Z t X t + \u03b5 re t . However, for technical reasons, we make the additional assumption that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "AO's and IO's"
        },
        {
            "text": "and then this relation no longer holds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "AO's and IO's"
        },
        {
            "text": "In the presence of AO's we would like to attenuate their effect, while when there are IO's, the usual goal in online applications would be tracking, i.e.; detect structural changes as fast as possible and/or react on the changed situation. A situation where both AO's and IO's may occur poses an identification problem: Immediately after a suspicious observation we cannot tell IO type from AO type. Hence a simultaneous treatment of both types will only be possible with a certain delay-see Ruckdeschel (2010) .",
            "cite_spans": [
                {
                    "start": 492,
                    "end": 510,
                    "text": "Ruckdeschel (2010)",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Different and competing goals induced by endogenous and exogenous outliers"
        },
        {
            "text": "The most important problem in SSM formulation is to reconstruct the unobservable states X t based on the observations Y t . For abbreviation let us denote",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Filter Problem"
        },
        {
            "text": "Then using MSE risk, the optimal reconstruction is distinguished as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Filter Problem"
        },
        {
            "text": "Depending on s this is a prediction (s < t), a filtering (s = t) and a smoothing problem (s > t). In the sequel we will confine ourselves to the filtering problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Filter Problem"
        },
        {
            "text": "It is well-known that the general solution to (2.14) is the corresponding conditional expectation E[X t |Y 1:s ]. Except for the Gaussian case, this exact conditional expectation may be computational too expensive. Hence similar to the Gauss-Markov setting, it is common to restrict oneself to linear filters. In this context, the seminal work of Kalman (1960) (discrete-time setting) and Kalman and Bucy (1961) (continuous-time setting) introduced effective schemes to compute this optimal linear filter X t|t . In discrete time, we reproduce it here for later reference:",
            "cite_spans": [
                {
                    "start": 389,
                    "end": 411,
                    "text": "Kalman and Bucy (1961)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Kalman-Filter"
        },
        {
            "text": "Init.:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kalman-Filter"
        },
        {
            "text": "Corr.:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kalman-Filter"
        },
        {
            "text": "and where \u2206X t is the prediction error, \u2206Y t the observation innovation, and \u03a3 t|t = Cov(\u2206X t ), \u03a3 t|t\u22121 = Cov(X t \u2212 X t|t\u22121 ), \u2206 t = Cov(\u2206Y t ); M 0 t is the socalled Kalman gain, and \u2206 \u2212 t stands for the Moore-Penrose inverse of \u2206 t .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Kalman-Filter"
        },
        {
            "text": "Realizing that M 0 t \u2206Y t is an orthogonal projection, it is not hard to see that the (classical) Kalman filter solves problem (2.14) (for s = t) among all linear filters. Using orthogonality of {\u2206Y t } t once again, we may setup similar recursions for the corresponding best linear smoother; see, e.g. Anderson and Moore (1979) , Durbin and Koopman (2001) . Under normality, i.e.; assuming (2.3), we even have X t|t[\u22121] = E[X t |Y 1:t[\u22121] ], i.e.; the Kalman filter is optimal among all Y 1:t[\u22121]measurable filters. It also is the posterior mode of L(X t |Y 1:t ) and X t|t can also be seen to be the ML estimator for a regression model with random parameter; for the last property, compare Duncan and Horn (1972) .",
            "cite_spans": [
                {
                    "start": 303,
                    "end": 328,
                    "text": "Anderson and Moore (1979)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 331,
                    "end": 356,
                    "text": "Durbin and Koopman (2001)",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 692,
                    "end": 714,
                    "text": "Duncan and Horn (1972)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Optimality of the Kalman-Filter"
        },
        {
            "text": "The Kalman filter stands out for its clear and understandable structure: it comes in three steps, all of which are linear, hence cheap to evaluate and easy to interpret. Due to the Markovian structure of the state equation, all information from the past useful for the future may be captured in the value of X t|t\u22121 , so only very limited memory is needed. From a (distributional) Robustness point of view, this linearity at the same time is a weakness of this filter-y enters unbounded into the correction step which hence is prone to outliers. A good robustification of this approach would try to retain as much as possible from these positive properties of the Kalman filter while revising the unboundedness in the correction step.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Features of the Kalman-Filter"
        },
        {
            "text": "In a first step we limit ourselves to AO's. Notationally, where clear from the context, we suppress the time index t. As no (new) observations enter the initialization and prediction steps, these steps may be left unchanged. In the correction step, we will have to modify the orthogonal projection present in (2.17). Suggested by H. Rieder and worked out in Ruckdeschel (2001, ch. 2), the following robustification of the correction step is straightforward: Instead of M 0 \u2206Y , we use a Huberization of this correction",
            "cite_spans": [
                {
                    "start": 358,
                    "end": 380,
                    "text": "Ruckdeschel (2001, ch.",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "robustifying recursive Least Squares: rLS"
        },
        {
            "text": "for some suitably chosen clipping height b. Apparently, this proposal removes the unboundedness problem of the classical Kalman filter while still remaining reasonably simple, in particular this modification is non-iterative, hence especially useful for online-purposes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "robustifying recursive Least Squares: rLS"
        },
        {
            "text": "For the choice of the clipping height b, we have two proposals. Both are based on the simplifying assumption that E id [\u2206X|\u2206Y ] is linear, which will turn out to only be approximately right. The first one, an Anscombe criterion,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of the clipping height b"
        },
        {
            "text": "\u03b4 may be interpreted as \"insurance premium\" to be paid in terms of loss of efficiency in the ideal model compared to the optimal procedure in this (ideal) setting, i.e.; the classical Kalman filter. The second criterion for a given radius r",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of the clipping height b"
        },
        {
            "text": "Assuming linear ideal conditional expectations, this will produce the minimax-MSE procedure for U SO (r) according to Theorem 3.2 below. One might object that (3.3) assumes r to be known, which in practice hardly ever is true. If r is unknown however, we translate an idea worked out in Rieder et al. (2008) : Assume we have limited knowledge about r, say r \u2208 [r l , r u ], 0 \u2264 r l < r u \u2264 1. Then we distinguish a least favorable radius r 0 defined in the following expressions",
            "cite_spans": [
                {
                    "start": 287,
                    "end": 307,
                    "text": "Rieder et al. (2008)",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Choice of the clipping height b"
        },
        {
            "text": "and use the corresponding b(r 0 ). Procedure rLS(b(r 0 )) then minimizes the maximal inefficiency \u03c1 0 (s) among all procedures rLS(b(r)), i.e.; each rLS for some clipping height b(r) = b(r 0 ) has an inefficiency no smaller than \u03c1 0 (r 0 ) for some r \u2208 [r l , r u ]. Radius r 0 can be computed quite effectively by a bisection method: Let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of the clipping height b"
        },
        {
            "text": "Then the following analogue to Kohl (2005, Lemma 2.2.3) holds:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of the clipping height b"
        },
        {
            "text": "and there exists somer 0 \u2208 [r l , r u ] such that Ar 0 /A r l = Br 0 /B ru . Thisr 0 is least favorable, i.e., min r\u2208[r l ,ru] \u03c1 0 (r) = \u03c1 0 (r 0 ). Moreover, if r u = 1, r 0 = r u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of the clipping height b"
        },
        {
            "text": "In particular, the last equality shows that one should restrict r u to be strictly smaller than 1 to get a sensible procedure.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Choice of the clipping height b"
        },
        {
            "text": "The (so-far) ad-hoc robustification proposed in the rLS filter has some remarkable optimality properties: Let us first forget about the time structure and instead consider the following simplified, but general \"Bayesian\" model:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "We have an unobservable but interesting signal X \u223c P X (dx), where for technical reasons we assume that in the ideal model E |X| 2 < \u221e. Instead of X we rather observe a random variable Y taking values in an arbitrary space of which we know the ideal transition probabilities; more specifically, we assume that these ideal transition probabilities for almost all x have densities w.r.t. some measure \u00b5,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "Our approach uses MSE as accuracy criterion for the reconstruction, so is limited to ranges of X where this makes sense. On the other hand it is this reduction to the \"Bayesian\" model which makes the generalizations sketched in section 2.1 possible. As (wide-sense) AO model, we consider an SO outlier model, i.e.;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "for U independent of (X, Y id ) and (X, Y di ) and some distorting random variable Y di for which, in a slight variation of condition (2.12) we assume Y di , X stochastically independent (3.11) and the law of which is arbitrary, unknown and uncontrollable. As a first step consider the set \u2202U SO (r) defined as \u2202U SO (r) = L(X, Y re ) | Y re acc. to (3.10) and (3.11) (3.12)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "Because of condition (3.11), in the sequel we refer to the random variables Y re and Y di instead of their respective (marginal) distributions only, while in the common gross error model as present in (2.9) or (2.10), reference to the respective distributions would suffice. Condition (3.11) also entails that in general, contrary to the usual setting, L(X, Y id ) is not element of \u2202U SO (r), i.e.; not representable itself as some L(X, Y re ) in this neighborhood. As corresponding (convex) neighborhood we define",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "In the sequel where clear from the context we drop the superscript SO and the argument r.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "With this setting we may formulate two typical robust optimization problems:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "Minimax-SO problem Minimize the maximal MSE on an SO-neighborhood, i.e.; find a measurable reconstruction f 0 for X s.t.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "Lemma5-SO problem As an analogue to Hampel (1968, Lemma 5) , minimize the MSE in the ideal model but subject to bound on the bias to be fulfilled on the whole neighborhood, i.e.; find a measurable reconstruction f 0 for X s.t.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 58,
                    "text": "Hampel (1968, Lemma 5)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "The solution to both problems can be summarized as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "The value of the minimax risk of Problem (3.14) is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "(2) f 0 from (3.16) also is the solution to Problem (3.15) for b = \u03c1/r.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "or in SSM formulation: M 0 is just the classical Kalman gain and f 0 the (one-step) rLS.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(One-Step)-Optimality of the rLS"
        },
        {
            "text": "Identifying X in model (3.9) with \u2206X t and \u03c0(y, x) \u00b5(dy) with L(Z t \u2206X t + \u03b5 t )(dy), our \"Bayesian\" Model (3.9) covers the SSM context. Hence, if \u2206X t is normal, (3) applies and rLS is SO-minimax.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Identifications for the SSM context"
        },
        {
            "text": "To illustrate the result of Theorem 3.2, we have plotted the ideal density of P Y id , the (least favorable) contaminated density of P Y re 0 , and the (least favorable) contaminating density of P Y di 0 in Figure 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 207,
                    "end": 215,
                    "text": "Figure 1",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Example for SO-least favorable densities"
        },
        {
            "text": "for P X = P \u03b5 = N (0, 1), r = 0.1; note the \"thin\" tails.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Example for SO-least favorable densities"
        },
        {
            "text": "Remark 3.3. (a) Without using this name, SO neighborhoods have already been used by Birmiwal and Shen (1993) and Birmiwal and Papantoni-Kazakos (1994) , although only in a one-dim. model. (b) Explicit solutions to robust optimization problems in a finite sample setting are rare, which is why one usually appeals to asymptotics instead. Important exceptions are Huber (1968) , Huber and Strassen (1973) , and even there, in the former case one is limited to a special loss function and to one dimension. Our results however are valid in a finite sample context and in whole generality. (c) Although the structure of our model resembles a location model-interpreting X as a random location parameter-our saddle-point differs from the one obtained in Huber (1964) . To see this, let us look at the tails of the least favorable P Y re 0 assuming a Gaussian model for simplicity: while in Huber's setting the tails decay as ce \u2212k|x| for some c, k > 0, in our setting they decay as c |x|e \u2212x 2 /2 so appear even \"less harmful\" than in the location case. (d) Attempts to solve corresponding optimization problems in a (narrow-sense) AO neighborhood are much more difficult and only partial results in this context have been obtained in Donoho (1978) , Bickel (1981) , and Bickel and Collins (1983) ; in particular one knows, that in the setup of our example the least favorableP \u03b5 = P \u03b5 di 0 must be discrete with only possible accumulation points \u00b1\u221e. In addition, existence of a saddlepoint follows from abstract compactness and continuity arguments, but in order to obtain specific solutions one has to recur to numeric approximation techniques as e.g. worked out in Ruckdeschel (2001, sec. 8.3) ; in particular, one obtains redescending optimal filters. (e) Redescenders are also used in the ACM filter by Masreliez and Martin (1977) which formally translates the Huber (1964) minimax variance result to this dynamic setting (formally, because of the randomness of the \"location parameter\" \u2206X). It should be noted though that the least-favorable SO-situation for the ACM then is not in the tails but rather where the corresponding \u03c8 function takes its maximum in absolute value. An SO outlier could easily place contaminating mass on this maximum, while this is much harder if not impossible to achieve in a (narrow-sense) AO situation. Hence in simulations where we produce \"large\" outliers, the ACM filter tends to outperform the rLS filter, as these \"large\" outliers are least favorable for the rLS but not for the ACM. The \"inliers\" producing the least favorable situation for the ACM on the other hand will be much harder to detect on na\u00efve data inspection than \"large\" outliers, in particular in higher dimensions.",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 108,
                    "text": "Birmiwal and Shen (1993)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 113,
                    "end": 150,
                    "text": "Birmiwal and Papantoni-Kazakos (1994)",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 362,
                    "end": 374,
                    "text": "Huber (1968)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 377,
                    "end": 402,
                    "text": "Huber and Strassen (1973)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 749,
                    "end": 761,
                    "text": "Huber (1964)",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1230,
                    "end": 1243,
                    "text": "Donoho (1978)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1246,
                    "end": 1259,
                    "text": "Bickel (1981)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1266,
                    "end": 1291,
                    "text": "Bickel and Collins (1983)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1663,
                    "end": 1691,
                    "text": "Ruckdeschel (2001, sec. 8.3)",
                    "ref_id": null
                },
                {
                    "start": 1803,
                    "end": 1830,
                    "text": "Masreliez and Martin (1977)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1861,
                    "end": 1873,
                    "text": "Huber (1964)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Example for SO-least favorable densities"
        },
        {
            "text": "So far, in this section, we have ignored the fact that our X in model (3.9) resp. \u2206X t in the SSM context will stem from a past which has already used our robustified version of the Kalman filter. In particular, the law of \u2206X t (even in the ideal model) is not straightforward and hence (ideal) conditional expectation appearing in the optimal solution f 0 in Theorem 3.2 in practice are not so easily computable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Back in the \u2206X Model for t > 1"
        },
        {
            "text": "The issue to assess the law \u2206X t from a non-linear filter past is common to other robustifications, and hence there already exist a couple of approaches to deal with it: Masreliez and Martin (1977) and Martin (1979) assume L(\u2206X t ) normal and propose using robust location estimators (with redescending \u03c8-function) as alternatives to the linear correction step. Contradicting this assumption in the rLS case, we have the following proposition Proposition 3.4. Whenever in one correction step in the \u2206X t past one has used the rLS-filter, then {\u2206X t } (as a process) cannot be normally distributed; this assertion cannot even hold asymptotically, as long as",
            "cite_spans": [
                {
                    "start": 170,
                    "end": 197,
                    "text": "Masreliez and Martin (1977)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 202,
                    "end": 215,
                    "text": "Martin (1979)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Approaches to go back"
        },
        {
            "text": "Similar assertions can also be proven for particular \u03c8-functions used in the ACM filter of Masreliez and Martin (1977) and Martin (1979) . Schick (1989) and Schick and Mitter (1994) use Taylor-expansions for nonnormal L(\u2206X t ); doing so they end up with stochastic error terms but do not give an indication as to uniform integrability. Hence it is not clear whether the approximation stays valid after integration. More importantly, at time instance t, they come up with a bank of (at least t) Kalman-filters which is not operational. Birmiwal and Shen (1993) work with the exact L(\u2206X t ) and hence have to split up the integration according to the the history of outlier occurrences which yields 2 t different terms-which is not operational either.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 118,
                    "text": "Masreliez and Martin (1977)",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 123,
                    "end": 136,
                    "text": "Martin (1979)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 139,
                    "end": 152,
                    "text": "Schick (1989)",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 157,
                    "end": 181,
                    "text": "Schick and Mitter (1994)",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 535,
                    "end": 559,
                    "text": "Birmiwal and Shen (1993)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Approaches to go back"
        },
        {
            "text": "Remark 3.5. One of the features of the ideal Gaussian model is that Eid[\u2206Xt|Y1:t] is Markovian in the sense that Eid[\u2206Xt|Y1:t] = Eid[\u2206Xt|\u2206Yt] hence only depends on the one value of \u2206Yt. When using bounded correction steps, however, this property gets lost, hence the restriction to strictly recursive procedures as is the rLS filter is a real restriction.",
            "cite_spans": [
                {
                    "start": 68,
                    "end": 81,
                    "text": "Eid[\u2206Xt|Y1:t]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Approaches to go back"
        },
        {
            "text": "Theorem 3.2 does not make any normality assumptions, but in assertion (3), we have seen that the rLS would result optimal once we can show that E id [\u2206X t |\u2206Y t ] for \u2206X stemming from an rLS past is linear. This leads to the question: When is E id [\u2206X|\u2206Y ] linear? Omitting time indices t, the answer is Proposition 3.6. Assume rk(I p \u2212 M Z) = p, p = q and rk Z = p, and that where \u03a0 is the projector onto ker Z and\u03a0 = Ip \u2212 \u03a0. In fact we prove Proposition 3.6 in this more general case. Assumption (3.25) is needed, as \u03a0\u2206X is invisible for \u2206Y . (c) Equivalence (3.23) together with Proposition 3.4 shows that, stemming from an rLS-past, rLS can only be SO-optimal in the very first time step.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approaches to go back"
        },
        {
            "text": "(d) Simulations however show that rLS gives very reasonable results. So in fact we could/should be close to an ideal linear conditional expectation. \"Closeness\" to linearity could be quantified by the second derivative \u2202 2 /\u2202y 2 Eid[\u2206X|\u2206Y = y], which in fact leads us to expression (3.24). (e) Equivalence (3.24), i.e.; conditional unskewedness of \u2206X, is somewhat surprising, as it seems much weaker than normality of the prediction error. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approaches to go back"
        },
        {
            "text": "In particle filter context where you simulate many stochastically independent filter realizations in parallel, Proposition 3.6 suggests the following test for linearity/normality: Proposition 3.8. Let \u2206X i , i = 1, . . . , n be an i.i.d. sample from L(\u2206X t ), the law of the prediction errors of some filter at time t; let \u03a3 = Cov(\u2206X t ), \u03c3 2 its maximal eigenvalue and e a corresponding eigenvector (of norm 1); let \u03a3 n ,\u03c3 2 n , and\u00ea n the corresponding empirical counter parts (all assumed consistent). Define the test statistic T n = 1 n n i=1 (\u00ea \u03c4 n \u2206X i ) 3 . Then under normality of L(\u2206X t ), \u221a n T n \u2212\u2192 w N (0, 15\u03c3 6 ) (3.26) and the test I(|T n | > 15/n\u03c3 3 n u \u03b1/2 ) (3.27)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A test for linearity"
        },
        {
            "text": "for u \u03b1 the upper \u03b1-quantile of N (0, 1) is asymptotically most powerful among all unbiased level-\u03b1-tests for testing",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A test for linearity"
        },
        {
            "text": "One explanation for the good empirical findings for the rLS is given by a further extension of the original SO-neighborhoods-the extended SO or eSO-model :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "In this model, we also allow for model deviations in X, i.e.; we assume a realistic (X re , Y re ) according to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "U and (X id , Y id ) independent as well as (mutually) U, X di , Y di (3.30) and the joint law P X id ,Y id and the radius r = r eSO are known, while P X di , P Y di are arbitrary, unknown and uncontrollable; however, we assume that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "for some known 0 < E id |X id | 2 \u2264 G < \u221e, and accordingly define Remark 3.9. At first glance, moment condition (3.31) seems to violate (distributional) robustness; however, this condition has not been introduced to induce a higher degree of robustness, but rather to extend the applicability of Theorem 3.2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "Theorem 3.10 (minimax-eSO). The pair (f 0 , P Y di 0 ), optimal in the Minimax-SO-problem to radius r SO = r from Theorem 3.2, extended to f 0 , P Y di 0 \u2297 P X di 0 for any P X di 0 such that E di |X di | 2 = G, remains a saddle-point in the corresponding Minimax-Problem on the eSO-neighborhood U eSO to the same radius r-no matter what bound G in equation (3.31) holds. The value of the minimax risk is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "As an application of Theorem 3.10, we now invoke a coupling idea: In the Gaussian setup, i.e.; we assume (2.3), we no longer regard the (SO-) saddle-point solution to an U(r)-neighborhood around L(\u2206X) stemming from an rLS-past, but use Theorem 3.10 as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "Proposition 3.11. Assume that for each time t there is a (fictive) random variable \u2206X N \u223c N p (0, \u03a3) such that \u2206X rLS t stemming from an rLS-past can be considered an X di in the corresponding eSO-neighborhood around \u2206X N with radius r. Then, rLS is exactly minimax for each time t.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "Remark 3.12. (a) Existence of \u2206X N \u223c Np(0, \u03a3) in a general setting is not yet proved. To this end one has to show moment condition (3.31) and that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "where p \u2206X N t , p \u2206X t are the corresponding Lebesgue densities and sup \u03bb is the corresponding essential supremum w.r.t. Lebesgue measure in the respective dimension. Clearly condition (3.34) is the difficulty, while condition (3.31) is not hard to fulfillwe only need to check that Eid \u2206Xt = 0, which for the rLS follows from symmetry of the distributions in the ideal model, and that the second moment is bounded-which also clearly holds. (b) As to the choice of covariance \u03a3 for \u2206X N t , we have two candidates: \u03a3 = Cov \u2206X rLS t and \u03a3 = \u03a3 t|t\u22121 from the classical Kalman filter. While the former takes up the actual error covariances, the latter is much easier to compute. In our numerical examples in Ruckdeschel (2001), we could not find any significant advantages for the former in terms of precision and hence propose the latter for computational reasons.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "(c) For p = 1, (3.34) could be checked numerically in a number of models, cf. Ruckdeschel (2001 , Table 8 .1). For p > 1, particle filter techniques should be helpful.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 96,
                    "end": 105,
                    "text": ", Table 8",
                    "ref_id": null
                }
            ],
            "section": "Way out: eSO-Neighborhoods"
        },
        {
            "text": "In this section, we translate the preceding optimality results to the IO situation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IO-optimality"
        },
        {
            "text": "We have already noted that in this case, instead of attenuating (the influence of) a dubious observation we would rather want to follow an IO outlier as fast as possible. It is well-known that the Kalman filter tends to be too inert for this task and faster tracking filters are needed. To do so, let us go back to our \"Bayesian\" model (3.9) but now we specify the transition densities \u03c0(y, x) to come from an observation Y which is built up additively as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IO-optimality"
        },
        {
            "text": "Equation (4.1) reveals a remarkable symmetry of X and \u03b5 which we are going to exploit now: Apparently",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IO-optimality"
        },
        {
            "text": "This is helpful if we are now assuming that \u03b5 will be ideally distributed, and instead the states X get corrupted. To this end, we retain the SO-model from the preceding sections, i.e., Y id will be replaced from time to time by Y di . Contrary to the AO formulation however, we now assume that this replacement by Y di reflects a corresponding change in X, as we now want to track the distorted signal. As a consequence this gives the following IO-version of the minimax problem (where the only visible difference is the superscript \"re\" for X).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IO-optimality"
        },
        {
            "text": "But, using X re = Y re \u2212 \u03b5, and settingf (y) = y \u2212 f (y) we obtain the equivalent formulation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IO-optimality"
        },
        {
            "text": "and we are back in the situation of subsection (3.2) with the respective r\u00f4les of X and \u03b5 interchanged. That is; the corresponding theorems translate word by word. Skipping the Lemma 5 solution we obtain (1)' In this situation, there is a saddle-point Note that contrary to Theorem 3.2 where E X need not be 0, here E \u03b5 = 0, which simplifies the definition ofD in (4.7). Details on how to use this for a corresponding IO-robust variant of rLS are given in Ruckdeschel (2010) .",
            "cite_spans": [
                {
                    "start": 456,
                    "end": 474,
                    "text": "Ruckdeschel (2010)",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "IO-optimality"
        },
        {
            "text": "In the extremely flexible class of dynamic models consisting in SSMs we were able to obtain optimality results for filtering. In this generality this is a novelty. We stress the fact that our filters are non-iterative, recursive, hence fast, and valid for higher dimensions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and Outlook"
        },
        {
            "text": "So far, we have not said much about the implementation of these filters. rLS.AO was originally implemented to XploRe, compare Ruckdeschel (2000) . In an ongoing project with Bernhard Spangl, BOKU, Vienna, and Irina Ursachi (ITWM), we are about to implement the rLS filter to R, (R Development Core Team (2010)), more specifically to an R-package robKalman, the development of which is done under r-forge project https://r-forge.r-project. org/projects/robkalman/, (R-Forge Administration and Development Team (2008)). Under this address you will also find a preliminary version available for download.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 144,
                    "text": "Ruckdeschel (2000)",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion and Outlook"
        },
        {
            "text": "In an extra paper, which for the moment is available as technical report, Ruckdeschel (2010) , we also check the properties of our filters at simulations and discuss the extension of these optimally-robust filters to a filter that combines the two types (for system-endogenous and -exogenous outlier situation). This hybrid filter is capable to treat (wide-sense) IO's and AO's simultaneouslyalbeit with minor delay.",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 92,
                    "text": "Ruckdeschel (2010)",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion and Outlook"
        },
        {
            "text": "Proof to Lemma 3.1 We use the fact that for 0 \u2264 a, b, c, d,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Hence A r is increasing in r, and B r decreasing, B r from \u221e to 0. By dominated convergence b(r), and hence A r and B r are continuous in r. Thus existence of r 0 follows. For r u = 1, one argues letting r n \u2208 [0, 1) tend to 1. To show equality in (6.1), we parallel Kohl (2005, Lemma 2.2.3), and first show that for r \u2265 s, s fixed, \u03c1(r, s) is increasing and correspondingly, for r \u2264 s, s fixed, decreasing, which entails (3.8): Let 0 \u2264 s < r 1 < r 2 \u2264 1. Then by monotony of A r , B r , (A s B \u22121 s + r 1 ) \u22121 \u2265 (A r1 B \u22121 r1 + r 1 ) \u22121 ; multiplying this inequality with (r 2 \u2212 r 1 ), we get (r 2 \u2212 r 1 )B s (A s + r 1 B s ) \u22121 \u2265 (r 2 \u2212 r 1 )B r1 (A r1 + r 1 B r1 ) \u22121 . Now, due to optimality of A r + rB r for radius r,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Multiplying with (A s +r 1 B s )/(A r2 +r 2 B r2 ), we obtain indeed \u03c1(r 2 , s) \u2265 \u03c1(r 1 , s), and similarly for 0 \u2265 s > r 1 > r 2 \u2265 1. Next, forr 0 least favorable, we show that for r fixed, and s \u2265 r, \u03c1(r, s) is increasing and correspondingly, for s \u2264 r, decreasing: Let 0 \u2264 r < r 1 < r 2 \u2264 1. Then, due to optimality of A r1 \u2212 r 1 B r1 ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "and similarly for 0 \u2265 r > r 1 > r 2 \u2265 1. For the last assertion, note that by (3.3), b(1) = 0, hence B 1 = 0. Hence max A s /A r l , B s /B 1 = \u221e for s < 1, while for s = 1, we get \u03c1 0 (1) = max{A 1 /A r l , 1} = 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Proof to Theorem 3.2 (1) Let us solve max \u2202U min f [. . .] first, which amounts to min \u2202U E re [ E re [X|Y re ] 2 ]. For fixed element P Y di assume w.l.o.g. that \u00b5 P Y di for \u00b5 from (3.9)-otherwise we replace \u00b5 by \u00b5 + P Y di ; this gives us a \u00b5-density q(y) of P Y di . Determining the joint (real) law P X,Y re (dx, dy) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "we deduce that \u00b5(dy)-a.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "a 1 q(y)+a 2 (y) a 3 q(y)+a 4 (y) (6.3)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Hence we have to minimize F (q) := |a 1 q(y) + a 2 (y)| 2 a 3 q(y) + a 4 (y) \u00b5(dy)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "To this end, we note that F is convex on the non-void, convex cone M = {q \u2208 L 1 (\u00b5) | q \u2265 0} so, for some\u03c1 \u2265 0, we may consider the Lagrangian L\u03c1(q) := F (q) +\u03c1 q d\u00b5 . So by continuity, there is some \u03c1 \u2208 (0, \u221e) with H(\u03c1) = 1. On M 0 , q d\u00b5 = 1, butq \u03c1 = q s=\u03c1 \u2208 M 0 and is optimal on M \u2283 M 0 hence it also minimizes F on M 0 . In particular, we get representation (3.17) and note that, independently from the choice of \u00b5, the least favorable P Y di 0 is dominated according to P Y di 0 P Y id , i.e.; nondominated P Y di are even easier to deal with.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "As next step we show that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "To this end we first verify (3.16) determining f 0 (y) as f 0 (y) = E re;P [X|Y re = y]. Writing a sub/superscript \"re; P \" for evaluation under the situation generated by P = P Y di andP for P Y di 0 , we obtain the the risk for general P as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "This is maximal for any P that is concentrated on the set |D(Y di;,q )| > \u03c1 , which is true forP . Hence (6.5) follows, as for any contaminating P",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Finally, we pass over from \u2202U to U: Let f r ,P r denote the components of the saddle-point for \u2202U(r), as well as \u03c1(r) the corresponding Lagrange multiplier and w r the corresponding weight, i.e., w r = w r (y) = min(1, \u03c1(r) / |D(y)|). Let R(f, P, r) be the MSE of procedure f at the SO model \u2202U(r) with contaminating P Y di = P . As can be seen from (3.17), \u03c1(r) is antitone in r; in particular, asP r is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Note that R(f s , P, 0) = R(f s , Q, 0) for all P, Q-hence passage toR(f s , P, r) = R(f s , P, r) \u2212 R(f s , P, 0) is helpful-and that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Hence the saddle-point extends to U(r); in particular the maximal risk is never attained in the interior U(r) \\ \u2202U(r). (3.19) follows by plugging in the results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "(2) Letf (Y ) = f (Y ) \u2212 E X, and X 0 = X \u2212 E X; then (3.15) becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "The assertion follows upon noting that sup U | E ref | = sup |f | (to be shown just as in Rieder (1994, chap. 5) ) and writing",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 112,
                    "text": "Rieder (1994, chap. 5)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "is linear in Y , the corresponding optimal matrix M 0 is just the respective Fourier coefficient, i.e.; Cov(X, Y ) Var Y \u2212 . We have already recalled that the classical Kalman filter is optimal among all linear filters; hence the corresponding Kalman gain M 0 is then the optimal linear transformation in the SSM context. Remark 6.1. (a) Birmiwal and Shen (1993) proceed similarly for their result. However, they invoke a minimax result by Ferguson (1967) which in our infinite dimensional setting is not applicable. Also their setting is restricted to one dimension, and they assume Lebesgue densities right away-also in the contaminated situation. In particular, they do not realize the connection to the exact conditional mean present in equation (3.18).",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 362,
                    "text": "Birmiwal and Shen (1993)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 440,
                    "end": 455,
                    "text": "Ferguson (1967)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "(b) For an alternative proof, see Ruckdeschel (2001, pp.156-163) : It uses Rieder (1994, App. B) , showing existence of Lagrange multipliers in (1) by abstract compactness and continuity arguments. (c) The fact that the solutions to Problems (3.14) and (3.15) coincide parallels the situation in the estimation problem for a one-dimensional location parameter.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 64,
                    "text": "Ruckdeschel (2001, pp.156-163)",
                    "ref_id": null
                },
                {
                    "start": 75,
                    "end": 96,
                    "text": "Rieder (1994, App. B)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Proof to Proposition 3.4 Recall that by the Cram\u00e9r-L\u00e9vy Theorem (cf. Feller (1971, Thm. 1, p. 525) ) the sum of two independent random variables has Gaussian distribution iff each summand is Gaussian. This can easily be translated into a corresponding asymptotic statement, cf. Ruckdeschel (2001, Prop. A.2.4) , i.e.; the sum of two independent random variables converges weakly to a Gaussian distribution iff each summand converges weakly to a Gaussian distribution. We first consider (for fixed t, omitted from notation where clear) the filter error,",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 98,
                    "text": "Feller (1971, Thm. 1, p. 525)",
                    "ref_id": null
                },
                {
                    "start": 278,
                    "end": 309,
                    "text": "Ruckdeschel (2001, Prop. A.2.4)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "where we assume \u2206X, \u03b5, and v normal. Then for the conditional law of \u2206X given \u2206Y is N p (g, (I p \u2212 M 0 Z)\u03a3) for \u03a3 = Cov \u2206X and g :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "which by Cram\u00e9r-L\u00e9vy cannot be normal, as g is obviously not normal. Consequently \u2206X t+1 = F t+1 \u2206X t + v t+1 cannot be normal either. Hence starting with normal \u2206X t and \u03b5 t , \u2206X t+1 cannot be normal. The same assertion clearly holds if v t is not normal. As by (3.21), g t does neither converge to 0 nor to M 0 \u2206Y , the asymptotic version of Cram\u00e9r-L\u00e9vy also excludes asymptotic normality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Remark 6.2. A similar assertion for the case that vt is normal but not both \u2206Xt and \u03b5t are, seems plausible and we conjecture that this is true; it may also be proven in particular cases, but in general, it is hard to obtain due to the lack of independence of \u2206X \u2212 g and \u2206Y .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Proof to Proposition 3.6 For the second equivalence in Proposition 3.6 we use the following lemma and a corollary of it:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Lemma 6.3. Let \u03b5 \u223c N q (0, V ), X \u223c P X and for some measurable function h : range(X) \u2192 R q let Y = h(x) + \u03b5. Let g \u2208 L l 1 (P X ), i.e., g : range(X) \u2192 R l measurable and E P X |g(X)| < \u221e. Then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Proof. For simplicity, we only consider rk V = q; otherwise we may pass to \u03b5 = A\u03b5 for some\u03b5 \u223c Nq(0,\u1e7c ) with rk\u1e7c =q and use the generalized inverse V \u2212 instead of V \u22121 everywhere in the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Let p \u03b5 be the Lebesgue density of \u03b5 and denote \u039b \u03b5 (\u03b5) := \u2202 \u2202\u03b5 log p \u03b5 (\u03b5). Then, no matter whether \u03b5 is Gaussian, it holds that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "As \u03b5 is normal, we may interchange differentiation and integration and obtain that \u2202 \u2202y",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "But as \u03b5 \u223c N q (0, V ), it holds that \u039b \u03b5 (\u03b5) = \u2212V \u22121 \u03b5, which entails (6.9) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Corollary 6.4. In our linear time discrete, Euclidean SSM, ommiting indices t, assume that rk V = q and let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Proof. During the proof we will omit \u2206 in notation. Equation (6.11) is just plugging in Lemma 6.3. We note that equivalently to (6.9) we could have written",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Hence applying Lemma 6.3 for g(X) = X i U j and g(X) = U j to the last two terms we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Proof to Proposition 3.6 Equivalence (3.23):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "If L(\u2206X) is normal, the uncorrelated random variables \u03a0\u2206X and\u03a0\u2206X are independent and again normal, while the random variables \u2206X, \u2206Y are jointly normal, hence linearity of conditional expectation is a well-known fact.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "If E id [\u2206X|\u2206Y ] is linear, after subtracting E M Z\u2206X from both sides, the defining equation for the conditional expectation P Y (dy)-a.e. reads M (y \u2212 Zx)p \u03b5 (y \u2212 Zx) P X (dx) = (I p \u2212 M Z) xp \u03b5 (y \u2212 Zx) P X (dx) (6.13) Let us introduce q \u03b5 (y) = yp \u03b5 (y) and the signed measure Q X (dx) = x P (dx); if we denote the mapping h : R q \u2192 R, y \u2192 h(y) = f (y \u2212 Zx) G(dx) by f * Z G, (6.13) becomes M q \u03b5 * Z P X = (I p \u2212 M Z)p \u03b5 * Z Q X (6.14)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "We pass over to the Fourier transforms (denoted with\u00b7 ) for s \u2208 R p , t \u2208 R q q X (s) = e is \u03c4 x Q X (dx),p X (s) = e is \u03c4 x P X (dx), q \u03b5 (t) = e it \u03c4 x q \u03b5 (y) dy,p \u03b5 (t) = e it \u03c4 x p \u03b5 (y) dy,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "As usual, convolution translates into products in Fourier space, in our case",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "and hence (6.14) in Fourier space is Mq \u03b5pX (Z \u03c4 \u00b7 ) = (I p \u2212 M Z)p \u03b5qX (Z \u03c4 \u00b7 ). For the derivatives (p X ) (s), (p \u03b5 ) (t) for s \u2208 R p and t \u2208 R q , we obtain (p X ) (s) = iq X (s), (p \u03b5 ) (t) = iq \u03b5 (t) (6.15) By assumption, I p \u2212 M Z is invertible and \u03b5 \u223c N q (0, V ), hencep \u03b5 (t) = exp(\u2212t \u03c4 V t/2) > 0 and together with (6.15), this gives the linear differential equation This is the characteristic function of a normal distribution, so Z\u2206X, hence als\u014d \u03a0\u2206X are normal, and together with (3.25) the assertion follows. On the other hand, Cov Z\u2206X = Z\u03a3Z \u03c4 , so we have also shown that Z(I p \u2212 M Z) \u22121 M V = Z\u03a3Z \u03c4 , which otherwise is tricky unless assuming \u03a3 and \u2206 invertible.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "If E id [\u2206X|\u2206Y ] is linear, by equivalence (3.23) \u2206X and \u2206Y are jointly normal with expectation 0, so the conditional law of \u2206X given \u2206Y is again normal with expectation 0, hence in particular symmetric so the assertion follows. Now assume E e \u03c4 (\u2206X \u2212 E[\u2206X|\u2206Y ]) 3 \u2206Y = 0 \u2200 e \u2208 R p (6.17)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "Apparently, E id [\u2206X|\u2206Y ] is linear iff \u2202 2 /\u2202y\u2202y \u03c4 E id [\u2206X|\u2206Y ] = 0. But Corollary 6.4 gives (in the notation of (6.10)) \u2202 2 \u2202y j \u2202y k E[\u2206X i |\u2206Y = y] = E(\u2206X 0 i U 0 j U 0 k |\u2206Y = y) (6.18) By complete polarization (compare Weyl (1997, Chap. I.1)), (6.17) also entails that the symmetric multilinear form given by E[\u2206X 0 i \u2206X 0 j \u2206X 0 k |Y = y] i,j,k\u2208{1,...,p} is identically 0. So the assertion follows, as withZ = ZV \u22121 , the RHS of (6.18) is just p h,l=1Z j,hZk,l E(\u2206X 0 i \u2206X 0 h \u2206X 0 l |\u2206Y = y)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "Proof to Theorem 3.10 We proceed as in Theorem 3.2, but note that in the eSO context (6.2) becomes P (X \u2208 A, Y re \u2208 B) = (1 \u2212 r) I A (x) I B (y)\u03c0(y, x) P X id (dx) \u00b5(dy)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "+r I A (x) I B (y)q(y) P X di (dx) \u00b5(dy) and hence (6.3) becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "But by (3.31), the RHS of (6.19) is exactly F (q) from (6.3). Thus, we may jump to the proof of Theorem 3.2 from this point on, replacing tr Cov X b\u1ef9 G := tr Cov P X di 0 X di = G \u2212 | E id X id | 2 in equation (6.6). For passing from \u2202U eSO to U eSO , let f r ,P r \u2297Q r be the components of the saddle-point at \u2202U eSO (r) and R(f, P \u2297 Q, r) be the MSE of procedure f at \u2202U eSO (r) with contaminating P Y di \u2297 P X di = P \u2297 Q. Instead of equation (6.7), we use \u2206G :=G \u2212 tr Cov id X id = G \u2212 E id |X id | 2 \u2265 0 and abbreviating R(f, P \u2297 Q, r) \u2212 R(f, P \u2297 Q, 0) byR(f, P \u2297 Q, r) we obtai\u00f1 R(f s , P \u2297 Q, r) = r tr Cov Q X di \u2212 Cov id X id + E P [min(|D(Y di )|, \u03c1(s)) 2 ] \u2264 \u2264 r \u2206G + E id |D(Y id )| 2w s (Y id ) + \u03c1(s) 2 = =R(f s ,P r \u2297Q r , r) <R(f s ,P r \u2297Q r , s) =R(f s ,P s \u2297Q s , s)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "Hence the saddle-point extends to U eSO (r). (3.33) follows by plugging in the results. \u223c N (0, \u03c3 2 ). Thus by the Lindeberg-L\u00e9vy CLT,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "But the sixth moment of N (0, \u03c3 2 ) is just 15\u03c3 6 . Hence by the assumed consistency of\u00ea n for e, Slutsky's Lemma yields (3.26). Asymptotically, the testing problem is a test for a normal mean \u00b5 to be 0 or not, which yields the corresponding optimality for the Gauss test given in (3.27).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "Proof to Proposition 3.11 Let us identify X \u2206X N , Y \u2206Y N := Z\u2206X N + \u03b5, and set P \u03b5 = N q (0, V ), P X = N p (0, \u03a3), and let p \u03b5 the corresponding Lebesgue density, then \u03c0(y, x) = p \u03b5 (y \u2212 Zx). Assertions (1') and (3') of Theorem 3.10 show that the eSO-optimal f 0 in our \"Bayesian\" model of subsection 3.2 is just f 0 (y) = M 0 (y) min{1, \u03c1/ M 0 y } with \u03c1 according to (3.17) such that dP Y di 0 = 1 and M 0 = \u03a3Z \u03c4 (Z\u03a3Z \u03c4 + V ) \u22121 . By assumption, \u2206X rLS lies in the corresponding eSO-neighborhood U(r) about \u2206X N so the value of the saddle-point from equation (3.19) is also a bound for the MSE of X rLS t|t on U(r). Remark 6.5. One should mention, however, that due to assumption (2.12) resp.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        },
        {
            "text": "(3.11), members of an SO-neighborhood U (r ) about L(\u2206X rLS , \u2206Y rLS ) need not lie in an eSO neighborhood U(r + r ) about L(\u2206X N , \u2206Y N ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Equivalence (3.24):"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Maximum likelihood estimation of discretely sampled diffusions: A closed-form approximation approach",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "A\u00eft-Sahalia",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Econometrica",
            "volume": "70",
            "issn": "",
            "pages": "223--262",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Optimal filtering. Information and System Sciences Series",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "D O"
                    ],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "B"
                    ],
                    "last": "Moore",
                    "suffix": ""
                }
            ],
            "year": 1979,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Minimax estimation of the mean of a normal distribution when the parameter space is restricted",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Bickel",
                    "suffix": ""
                }
            ],
            "year": 1981,
            "venue": "Ann. Stat",
            "volume": "9",
            "issn": "",
            "pages": "1301--1309",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Minimizing Fisher information over mixtures of distributions",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Bickel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Collins",
                    "suffix": ""
                }
            ],
            "year": 1983,
            "venue": "Sankhya, Ser. A",
            "volume": "45",
            "issn": "",
            "pages": "1--19",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Optimal robust filtering",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Birmiwal",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Stat. Decis",
            "volume": "11",
            "issn": "2",
            "pages": "101--119",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Outlier resistant prediction for stationary processes",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Birmiwal",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Papantoni-Kazakos",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Stat. Decis",
            "volume": "12",
            "issn": "4",
            "pages": "395--427",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The asymptotic variance formula and large-sample criteria for the design of robust estimators",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "L"
                    ],
                    "last": "Donoho",
                    "suffix": ""
                }
            ],
            "year": 1978,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Linear dynamic recursive estimation from the viewpoint of regression analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Duncan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "D"
                    ],
                    "last": "Horn",
                    "suffix": ""
                }
            ],
            "year": 1972,
            "venue": "J. Am. Stat. Assoc",
            "volume": "67",
            "issn": "",
            "pages": "815--821",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Time Series Analysis by State Space Methods",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Durbin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Koopman",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "An introduction to probability theory and its applications II",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Feller",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Mathematical statistics. A decision theoretic approach",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "S"
                    ],
                    "last": "Ferguson",
                    "suffix": ""
                }
            ],
            "year": 1967,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Outliers in time series",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Fox",
                    "suffix": ""
                }
            ],
            "year": 1972,
            "venue": "J. R. Stat. Soc., Ser. B",
            "volume": "34",
            "issn": "",
            "pages": "350--363",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Contributions to the theory of robust estimation. Dissertation, University of California",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "R"
                    ],
                    "last": "Hampel",
                    "suffix": ""
                }
            ],
            "year": 1968,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Forecasting",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Harvey",
                    "suffix": ""
                }
            ],
            "year": 1991,
            "venue": "Structural Time Series Models and the Kalman Filter . Reprint",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Robust estimation of a location parameter",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Huber",
                    "suffix": ""
                }
            ],
            "year": 1964,
            "venue": "Z. Wahrscheinlichkeitstheor. Verw. Geb",
            "volume": "35",
            "issn": "",
            "pages": "269--278",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Minimax tests and the Neyman-Pearson lemma for capacities",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Huber",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Strassen",
                    "suffix": ""
                }
            ],
            "year": 1973,
            "venue": "Ann. of Statist",
            "volume": "11",
            "issn": "",
            "pages": "251--263",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Stochastic processes and filtering theory",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Jazwinski",
                    "suffix": ""
                }
            ],
            "year": 1970,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A new method for the nonlinear transformation of means and covariances in filters and estimators",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Julier",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uhlmann",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "F"
                    ],
                    "last": "Durrant-White",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "IEEE Trans. Autom. Control",
            "volume": "45",
            "issn": "",
            "pages": "477--482",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A new approach to linear filtering and prediction problems",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Kalman",
                    "suffix": ""
                }
            ],
            "year": 1960,
            "venue": "Journal of Basic Engineering-Transactions of the ASME",
            "volume": "82",
            "issn": "",
            "pages": "35--45",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "New results in filtering and prediction theory",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "E"
                    ],
                    "last": "Kalman",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bucy",
                    "suffix": ""
                }
            ],
            "year": 1961,
            "venue": "Journal of Basic Engineering-Transactions of the ASME",
            "volume": "83",
            "issn": "",
            "pages": "95--108",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Robust techniques for signal processing: A survey",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Kassam",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "V"
                    ],
                    "last": "Poor",
                    "suffix": ""
                }
            ],
            "year": 1985,
            "venue": "Proc. IEEE",
            "volume": "73",
            "issn": "3",
            "pages": "433--481",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Numerical contributions to the asymptotic theory of robustness. Dissertation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kohl",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Optimal Portfolios. Stochastic Models for Optimal Investment and Risk Management in Continuous Time",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Korn",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "State space models and Hidden Markov Models",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "R"
                    ],
                    "last": "K\u00fcnsch",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "109--173",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Approximate conditional-mean type smoothers and interpolators",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "D"
                    ],
                    "last": "Martin",
                    "suffix": ""
                }
            ],
            "year": 1979,
            "venue": "Smoothing techniques for curve estimation. Proc. Workshop Heidelberg",
            "volume": "757",
            "issn": "",
            "pages": "117--143",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Robust Bayesian estimation for the linear model and robustifying the Kalman filter",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Masreliez",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Martin",
                    "suffix": ""
                }
            ],
            "year": 1977,
            "venue": "IEEE Trans. Autom. Control, AC",
            "volume": "22",
            "issn": "",
            "pages": "361--371",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "R: A language and environment for statistical computing. R Foundation for Statistical Computing",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "N"
                    ],
                    "last": "Nielsen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Madsen",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Melgaard",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Partially Observed Stochastic Differential Equations. Report, Informatics and Mathematic Modelling",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "The cost of not knowing the radius",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rieder",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kohl",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ruckdeschel",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Stat. Meth. & Appl",
            "volume": "17",
            "issn": "",
            "pages": "13--40",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Optimally Robust Kalman Filtering at Work: AO-, IO-, and simultaneously IO-and AO-robust filters",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ruckdeschel",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "XploRe. Application Guide",
            "volume": "64",
            "issn": "",
            "pages": "483--516",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Robust recursive estimation of a discrete-time stochastic linear dynamic system in the presence of heavy-tailed observation noise. Dissertation",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "C"
                    ],
                    "last": "Schick",
                    "suffix": ""
                }
            ],
            "year": 1989,
            "venue": "Massachusetts Institute of Technology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Robust recursive estimation in the presence of heavytailed observation noise",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "C"
                    ],
                    "last": "Schick",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Mitter",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Ann. Stat",
            "volume": "22",
            "issn": "2",
            "pages": "1045--1080",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Parameter Estimation of Nonlinear Stochastic Differential Equations: Simulated Maximum Likelihood vs",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "J. Comput. Graph. Statist",
            "volume": "11",
            "issn": "4",
            "pages": "972--995",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Robust time series analysis: A survey. Kybernetika, 23",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Stockinger",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dutter",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "The maximum principle for partially observed optimal control of stochastic differential equations",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "SIAM J. Control Optim",
            "volume": "36",
            "issn": "5",
            "pages": "1596--1617",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Dynamic generalized linear models and Bayesian forecasting",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "West",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Harrison",
                    "suffix": ""
                }
            ],
            "year": 1985,
            "venue": "J. Am. Stat. Assoc",
            "volume": "80",
            "issn": "",
            "pages": "73--97",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "The Classical Groups. Their Invariants and Representations. 15th Reprint of 2nd",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Weyl",
                    "suffix": ""
                }
            ],
            "year": 1953,
            "venue": "Princeton Landmarks in Mathematics and Physics series",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "(a) Assumption rk(Ip \u2212 M Z) = p is fulfilled in most situations; otherwise there is a one-dimensional projection of the filter error that is 0 almost sure. (b) For Z non-invertible, in particular for p = q, equivalence (3.23) still holds, if we require Lid(\u03a0\u2206X) = Np(0, \u03a0\u03a3\u03a0), \u03a0\u2206X independent of\u03a0\u2206X (3.25)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "(f) Condition (3.22) could be relaxed to \u03b5 \u223c P , P some infinitely divisible distribution, and the normality assumption in (3.25) be dropped. Equivalence (3.23) would then become: For each M \u2208 R p\u00d7q there can be at most one distribution Q = Q(M, P ) on B p , such that E[\u2206X|\u2206Y ] = M \u2206Y for L(\u03a0\u2206X) = Q; for p = q = 1 and Z = 0, there always is such a Q; seeRuckdeschel (2001, Thm. 1.3.1).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "eSO (r) := 0\u2264s\u2264r \u2202U eSO (s), \u2202U eSO (r) := { L(X re , Y re ) acc. to (3.29)-(3.31) } (3.32)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Minimax-IO).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "dy) = 1 and D(y) = y \u2212 E id [X|Y = y] (4.7) (3)' If E id [X|Y ] is linear in Y , i.e.; E id [X|Y ] = M Y for some matrix M , then necessarily M = M 0 = Cov(X, Y ) Var Y \u2212 (4.8) -or in the SSM formulation: M 0 is just the classical Kalman gain and f 1 the (one-step) rLS.IO defined below.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "positive Lagrange multiplier\u03c1. Pointwise minimization in y of L\u03c1(q) gives q s (y) = 1\u2212r r ( D(y) s \u2212 1) + p Y (y) for some constant s = s(\u03c1) = ( | E X| 2 +\u03c1/r) 1/2 , Pointwise in y,q s is antitone and continuous in s \u2265 0 and lim s\u21920[\u221e] q s (y) = \u221e[0], hence by monotone convergence, H(s) = q s (y) \u00b5(dy) too, is antitone and continuous and lim s\u21920[\u221e] H(s) = \u221e[0]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "(p X ) (Z \u03c4 t) = \u2212(I p \u2212 M Z) \u22121 M V tp X (Z \u03c4 t) (6.16) Fixing any direction t 0 such that Z \u03c4 t 0 = 0, this becomes an ODE g (s) = \u2212t \u03c4 0 Z(I p \u2212 M Z) \u22121 M V t 0 sg(s), g(0) = 1 which has a unique solution given by g(s) = exp(\u2212t \u03c4 0 Z(I p \u2212 M Z) \u22121 M V t 0 s 2 /2)",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Proof to Proposition 3.8 Under H 0 , due to Proposition 3.6, \u2206X i i.i.d. \u223c N p (0, \u03a3). Hence e \u03c4 \u2206X i i.i.d.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The author would like to acknowledge and thank for the stimulating discussion he had with Gerald Kroisandt at ITWM which led to the definition of rLS.IO. He also wants to thank Helmut Rieder for several suggestions as to notation and formulations which have much improved clarity and readability of this paper. Many thanks go to Nataliya Horbenko for proof-reading this paper. Of course, the opinions expressed in this paper as well as any errors are solely the responsibility of the author.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ]
}