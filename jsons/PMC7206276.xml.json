{
    "paper_id": "PMC7206276",
    "metadata": {
        "title": "Level Set Estimation with Search Space Warping",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Manisha",
                "middle": [],
                "last": "Senadeera",
                "suffix": "",
                "email": "manisha.senadeera@deakin.edu.au",
                "affiliation": {}
            },
            {
                "first": "Santu",
                "middle": [],
                "last": "Rana",
                "suffix": "",
                "email": "santu.rana@deakin.edu.au",
                "affiliation": {}
            },
            {
                "first": "Sunil",
                "middle": [],
                "last": "Gupta",
                "suffix": "",
                "email": "sunil.gupta@deakin.edu.au",
                "affiliation": {}
            },
            {
                "first": "Svetha",
                "middle": [],
                "last": "Venkatesh",
                "suffix": "",
                "email": "svetha.venkatesh@deakin.edu.au",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Previous work into level-set estimation problems have been performed by the LSE algorithm [8] and the Truncated Variance Reduction (TruVar) algorithm [2]. In LSE, the ambiguity acquisition function is adapted from the Straddle heuristic [4], providing a balance between exploration and exploitation. This algorithm is an online method that utilises confidence intervals to classify points as being either above or below the level. Similar in nature to LSE in its classification, the TruVar algorithm provides functionality for both Level set estimation and BO applications, utilising the common GP based approach to unite the methods. For Level set estimation applications, the algorithm uses lookahead to select the next sample point as one which provides the greatest reduction in the sum of truncated variances within a set of unclassified points. This method further incorporates point-wise costs and heteroscedastic noise into its selection. In the application of this method in both [8] and [2], the authors have utilised a monotonically decreasing set for unclassified points, based on the bounds of the GP. This method of classification however is suited when sampling is done on a discrete domain.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 92,
                    "mention": "8",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 151,
                    "end": 152,
                    "mention": "2",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 238,
                    "end": 239,
                    "mention": "4",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 990,
                    "end": 991,
                    "mention": "8",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 998,
                    "end": 999,
                    "mention": "2",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Related Work ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "For super-level set estimation, [16] proposes Maximum Improvement for Level-Set Estimation (MILE), a one-step lookahead algorithm, to locate points that exceed a threshold with a specified high probability. Aiming to find the largest region that exists above a certain level, it operates by sampling points which provide the greatest expected improvement in the set of points classified as being above the threshold. Convergence guarantees were provided even for misspecified prior distribution, however, only for discrete domains.",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 35,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Related Work ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "Level set estimation lends itself to estimating system probability of failure. Work by [1] develops a Bayesian framework for such tasks. They propose a one step-look ahead sequential sampling strategy called stepwise uncertainty reduction (SUR). In [5] this method is adapted from sequential to batch sampling, allowing for parallel sampling of the function. Convergence analysis was not provided.",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 89,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 250,
                    "end": 251,
                    "mention": "5",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Related Work ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "We assume a function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f:\\mathbf {x}\\rightarrow y$$\\end{document}, where\n\nis a compact subset from a D-dimensional real vector space and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y\\in \\mathbb {R}$$\\end{document} is from the real line. We wish to find the level set of a function i.e.1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} D_{h} = \\{\\mathbf {x}:f(\\mathbf {x})=h\\} \\end{aligned}$$\\end{document}where h is the desired level. A small tolerance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\eta $$\\end{document} is permitted. For some problems it may be useful to find a super-level set i.e. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H = \\{\\mathbf {x}:f(\\mathbf {x})> h\\}$$\\end{document} or a sub-level set i.e. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L = \\{\\mathbf {x}:f(\\mathbf {x}) < h\\}$$\\end{document}. We assume that the function f(.) is expensive, and only noisy evaluations i.e. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y=f(\\mathbf {x})+\\epsilon $$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon \\sim \\mathcal {N}(0,\\sigma _n^2)$$\\end{document} are available. Hence, we need to find the level set in an sample-efficient way.",
            "cite_spans": [],
            "section": "Problem Definition ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "Bayesian optimisation has been adapted for level set estimation of an expensive function, because of its high sample efficiency. Usually a Gaussian process is used to serve the probabilistic model of the function, which is then utilised to select the next sample point using a surrogate function. In the following we outline the Gaussian process and the acquisition function specific to level set estimation.",
            "cite_spans": [],
            "section": "Background ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "Gaussian Processes. Gaussian process is a commonly used prior over the space of smooth functions [13]. It is fully defined by a mean and covariance function. Without loss of generality we can assume the mean to be a zero function, then a GP is fully defined by the co-variance function alone, i.e. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f\\sim \\mathcal {GP}(0,k(\\mathbf {x},\\mathbf {x}'))$$\\end{document}. Given a set of observations (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{\\mathbf {x}_i,y_i\\}_{i=1}^t$$\\end{document}), the posterior is also a GP whose predictive mean and covariance can be computed as, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu _{t}(\\mathbf{x} ) = \\mathbf{k} _{t}(\\mathbf{x} )^T(K_{t} + \\sigma _{n}^2\\mathbf {I})^{-1}{} \\mathbf{y} _{t}$$\\end{document}, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k_{t}(\\mathbf{x} ,\\mathbf{x} ') = k(\\mathbf{x} ,\\mathbf{x} ') - \\mathbf{k} _t(\\mathbf{x} )^T(K_t + \\sigma _{n}^2\\mathbf {I})^{-1}{} \\mathbf{k} _t(\\mathbf{x} ')$$\\end{document}, with variance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _{t}^2(\\mathbf{x} ) = k_{t}(\\mathbf{x} ,\\mathbf{x} )$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{k} _t(\\mathbf{x} ) = [k(\\mathbf{x} _i,\\mathbf{x} )]_{i=1}^t$$\\end{document} and, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K_t = [k(\\mathbf{x} _t,\\mathbf{x} _{t^{'}})]_{t,t^{'}}$$\\end{document} is the kernel Gram matrix.",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 100,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Background ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "A popular kernel used for the covariance function is the squared exponential of the form (assuming stationarity):2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} k(\\mathbf{x} _i,\\mathbf{x} _j) = \\sigma _{f}^2 \\text {exp}(-\\frac{1}{2} \\sum _{d=1}^{D} \\frac{(x_{d,i} - x_{d,j})^2}{l_{d}^2}) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _{f}^2$$\\end{document} is the signal variance, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_{d}$$\\end{document} is a constant length scale for the d-th dimension.",
            "cite_spans": [],
            "section": "Background ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "Level Set Estimation Algorithm. We use the algorithm proposed by [8]. Based on the GP model, they used the ambiguity acquisition function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_t(\\mathbf{x} )$$\\end{document} to sample the next point. The authors [8] worked on the task of classifying discrete points into super-level and sub-level sets, and the name \u2018ambiguity\u2019 reflects the uncertainty during the classification process. This acquisition function, described in Eq. (3), aims to minimises the distance between the mean and desired threshold h (exploitation) whilst maximising the uncertainty (exploration).3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} a_t(\\mathbf{x} ) = -\\mid \\mu _{t-1}(\\mathbf{x} ) - h \\mid + \\sqrt{\\beta _{t}} \\sigma _{t-1}(\\mathbf{x} ) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\beta _{t}}$$\\end{document} trades between exploitation and exploration. The next point is determined as:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathbf{x} _t = \\mathop {\\mathrm {argmax}}\\limits _{x} a_t(\\mathbf{x} ) \\end{aligned}$$\\end{document}The sequence of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta _t$$\\end{document} can be set in a specific way [15] to achieve an efficient sub-linear convergence rate for cumulative regret (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\triangleq \\sum _{i=1}^t |f(\\mathbf {x_i})-h|$$\\end{document}).",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 67,
                    "mention": "8",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 479,
                    "end": 480,
                    "mention": "8",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 2366,
                    "end": 2368,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Background ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "The algorithm can be run either until the iteration budget expires (continuous case) or all the points have been classified between level-set and the rest (discrete case).",
            "cite_spans": [],
            "section": "Background ::: Introduction",
            "ref_spans": []
        },
        {
            "text": "Snoek et al. [14] warped the input space of non-stationary functions to convert them to stationary functions. We utilise this concept to construct a complex covariance function via a non-homogenous length scale. Because the complex covariance function is unknown, adapting the length scale instead alleviates the need to pre-define the covariance function. For this, the following form of the kernel [11] is used:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} k(\\mathbf{x} _i,\\mathbf{x} _j) = \\sigma _{f}^2 \\mid \\varSigma _i\\mid ^{\\frac{1}{4}} \\mid \\varSigma _j \\mid ^{\\frac{1}{4}} \\mid \\frac{\\varSigma _i + \\varSigma _j}{2} \\mid ^{-\\frac{1}{2}} g(\\mathbf{x} _i,\\mathbf{x} _j) \\end{aligned}$$\\end{document}where\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} g(\\mathbf{x} _i,\\mathbf{x} _j) = \\text {exp} \\bigg [-(\\mathbf{x} _i - \\mathbf{x} _j)^{T} \\Big (\\frac{\\varSigma _i + \\varSigma _j}{2}\\Big )^{-1} (\\mathbf{x} _i - \\mathbf{x} _j)\\bigg ] \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varSigma _i$$\\end{document}, known as the kernel matrix, is the covariance matrix of the Gaussian kernel at \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{x} _{i}$$\\end{document} [11]. In the isotropic case, this matrix has the form \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_i^2 \\times I_D$$\\end{document} [12]. This can be extended to an anisotropic case of the form \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varSigma _i =$$\\end{document} diag\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$({{\\varvec{l}}}(\\mathbf{x} _{i})^2)$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}(\\mathbf{x} _{i})$$\\end{document} is a vector of length scales for each dimension at \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{x} _{i}$$\\end{document}, ensuring \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k(\\mathbf{x} _i,\\mathbf{x} _j)$$\\end{document} remains positive semi-definite.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 16,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 401,
                    "end": 403,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 2133,
                    "end": 2135,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 2489,
                    "end": 2491,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "With the balancing act of the acquisition function being to encourage selection of points which minimise the distance between h and the mean whilst maximising uncertainty, this same objective was incorporated into the length scale warping metric. For problems involving level set estimation an argument can be made for there to be a stronger emphasis on exploitation compared to a BO problem. The reason for this is that unlike BO, once a single point at the desired level is found, it can be safely assumed, for a continuous function, that points close by will also be at that level. Shown in (6) is the metric by which, for a given point, a new length scale value is determined.6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {{\\varvec{l}}}(\\mathbf{x} ) = {{\\varvec{l}}}_{0} \\text {log}\\bigg (1 + \\Big (\\frac{\\mid \\mu _{t-1}(\\mathbf{x} ) - h \\mid +\\, \\epsilon }{\\sqrt{\\beta _{t}} \\sigma _{t-1}(\\mathbf{x} ) + \\epsilon }\\Big )^2\\bigg )+ {{\\varvec{l}}}_{1} \\end{aligned}$$\\end{document}The length scale for a point through (6) can be added into the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varSigma $$\\end{document} matrix in (5).",
            "cite_spans": [],
            "section": "Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "By (6), areas with small length scales encourage sampling, as the standard deviation and mean return to prior values of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _{f}$$\\end{document} and 0 faster, behaving like an expanded space. Areas with larger length scales discourage sampling as the mean and standard deviation will remain similar to neighbouring points, mimicking regions with a contraction in size.",
            "cite_spans": [],
            "section": "Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "A small term, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon $$\\end{document}, in the denominator acts to prevent undefined values, should the uncertainty term reach 0 (as the case for a sampled point without noise). \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon $$\\end{document} in the numerator allows uncertainty to still influence warping for points where the mean is equal to the threshold h. Additionally, a 1 is added within the log term to ensure l(x) remains a positive function, and both \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_{0}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_{1}$$\\end{document} are to be positive.",
            "cite_spans": [],
            "section": "Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "It is necessary that the form of the length scale warping metric be different to that of the acquisition function, whilst still valuing a similar exploration-exploitation balance. This avoids both metrics always preferring the same point (avoiding a doubling up).",
            "cite_spans": [],
            "section": "Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "It is possible to apply multiple warpings where after the original GP is warped, the resulting GP is again warped multiple times. After this, the acquisition function is applied to the final warped GP to select the next best point. Warping causes the acquisition function to behave more exploitatively, as shown in Fig. 1. Initially the point selected by the acquisition function (indicated by the red square) is more explorative but, after multiple warpings, the point is more exploitative. More exploitation is not necessarily good as samples from the level set would start to look very similar. The balance lies somewhere in the middle where exploitation is high enough to use the contiguous nature of the level set, but low enough to give variability in the samples. In our experience one level of warping tends to give sufficient exploitation behaviour.\n",
            "cite_spans": [],
            "section": "Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": [
                {
                    "start": 320,
                    "end": 321,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Figure 2 illustrates the exploitative behaviour of warping compared to the unwarped kernel approach. Without warping, both x and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x'$$\\end{document} have the same acquisition value and are equally likely to be selected. With warping, these points are differentiated as x will have a lower length scale than \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x'$$\\end{document}, giving it a higher chance of being selected.\n",
            "cite_spans": [],
            "section": "Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "The LSE Algorithm with input warping is described in Algorithm 1. Though the acquisition function itself is unchanged, the search space for selecting the next sample point is warped. As such the form of the acquisition function used is shown in (7).7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} a_t(\\mathbf{x} ) = -\\mid \\mu _{w_{t-1}}(\\mathbf{x} ) - h \\mid + \\sqrt{\\beta _{t}} \\sigma _{w_{t-1}}(\\mathbf{x} ) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu _{w}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _{w}$$\\end{document} are the mean and variance from a warped GP with the complex covariance function using length scale given by (6). The GP used to classify the test points remains un-warped as it is based on a true model selection approach making it the most accurate model of the function. Classification of points into the super-level H, sub-level L and unclassified U sets follow the same approach as [8], however these sets are no longer monotonic in size and allow for re-classification. Furthermore in [8] sampling for the LSE algorithm was limited to points from the unclassified U set. In Algorithm 1, to allow extension to continuous domain, this constraint was removed.\n",
            "cite_spans": [
                {
                    "start": 1664,
                    "end": 1665,
                    "mention": "8",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1768,
                    "end": 1769,
                    "mention": "8",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "LSE with Input Warping ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "For any \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h \\in \\mathbb {R}$$\\end{document}, let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta \\in (0,1)$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta _t = 2\\parallel f\\parallel _k^2 + 300\\gamma _{t}$$\\end{document} log\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(t/ \\delta )^3$$\\end{document}, then with probability \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge 1 - \\delta $$\\end{document}, the length scale will be bounded between \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_{1} \\le {{\\varvec{l}}}(\\mathbf{x} ) \\le {{\\varvec{l}}}_{0}$$\\end{document} log\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\bigg (1 + \\Big (1 + \\frac{\\varDelta _{fmax}}{\\epsilon }\\Big )^2\\bigg )+ {{\\varvec{l}}}_{1}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta _{fmax} = $$\\end{document}max\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\mid f(\\mathbf{x} )-h \\mid $$\\end{document}.",
            "cite_spans": [],
            "section": "Theorem 1 ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Proof of Theorem 1 is provided in supplementary materials.\n",
            "cite_spans": [],
            "section": "Proof ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Gotovos et al. [8] provided theoretical convergence bounds for the acquisition function in discrete domain problems by bounding the number of samples required for a specified confidence. Theorem 2 provides a cumulative regret bound for the acquisition function in a continuous domain, where regret is the same as that defined in [15].",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 17,
                    "mention": "8",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 330,
                    "end": 332,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Proof ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta \\in (0,1)$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta _t = 2\\parallel f\\parallel _k^2 + 300\\gamma ^w_{t} ln^3(t/ \\delta ), \\gamma ^w_{t}$$\\end{document} be maximum information gain for the warped squared exponential kernel after t iterations, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma ^2$$\\end{document} be variance of the measurement noise and h be the desired threshold. Then with probability of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge 1 - 2\\delta $$\\end{document}, the cumulative regret of the ambiguity acquisition function of (3) follows the sublinear rate \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_{T} \\le \\sqrt{\\frac{8T \\beta _{T} \\gamma _{t}}{log(1+\\sigma ^{-2})}} + T \\mid f(\\mathbf{x} ^*)- h \\mid $$\\end{document}.",
            "cite_spans": [],
            "section": "Theorem 2 ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Proof of Theorem 2 is provided in supplementary material.",
            "cite_spans": [],
            "section": "Proof ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "The regret bound assures that the algorithm converges to the desired level, with cumulative regret reflecting the rate of convergence. Theorem 2 demonstrates the average cumulative regret vanishes when \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f(\\mathbf{x} ^{*}) = h$$\\end{document}, or reaches \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mid f(\\mathbf{x} ^*)- h \\mid $$\\end{document} when the specified level does not exist for the function, with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{x} ^*$$\\end{document} being the set of points resulting in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f(\\mathbf{x} ^*)$$\\end{document} being the closest to h. In Theorem 2, cumulative regret is bounded as a function of the maximum information gain \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma _t$$\\end{document}. The existing results in Theorem 5 of [15] provide an upper bound for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma _{t}$$\\end{document} for a squared exponential kernel with homogeneous length scale.",
            "cite_spans": [
                {
                    "start": 1986,
                    "end": 1988,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Proof ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "In Theorem 3 we provide a bound on the maximum information gain in the presence of a heterogeneous length scale as bounded by Theorem 1. Even with a heterogeneous length scale for the GP, it can be shown that maximum information gain remains bounded by the same order as that of a homogeneous length scale. This is described in Theorem 3 and provides guarantees for Theorem 2 under a non-stationary GP.",
            "cite_spans": [],
            "section": "Proof ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D \\subset \\mathbb {R}^d$$\\end{document} be compact and convex, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d \\in \\mathbb {N}$$\\end{document}. Assume the kernel function satisfies \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k(\\mathbf{x} , \\mathbf{x} ') \\le 1$$\\end{document}. Then for our proposed covariance function with varying length scale as described in (6), the maximum information gain at iteration T is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {O}((\\log T)^{d+1})$$\\end{document}.",
            "cite_spans": [],
            "section": "Theorem 3 ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Proof of Theorem 3 is provided in supplementary material.",
            "cite_spans": [],
            "section": "Proof ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Note in Theorem 3, regret bounds are of order \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {O}(T)$$\\end{document} only when the function does not have a level at h i.e. either \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h>f_{max}$$\\end{document} or \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h<f_{min}$$\\end{document}. When \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_{min}\\le h \\le f_{max}$$\\end{document}, then this term will go to 0, leaving an order of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {O}(\\sqrt{T(log T)^{d+1}})$$\\end{document}.",
            "cite_spans": [],
            "section": "Proof ::: Theoretical Guarantees ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "Before warping, the hyper-parameters \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_{0}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_{1}$$\\end{document} must be estimated. This can be done by separating a set of observations into training and test set. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_{0}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_{1}$$\\end{document} are optimised for values which, when Algorithm 1 is applied using the training set, classification produces highest F1 score for the test set, after a pre-defined number of iterations.",
            "cite_spans": [],
            "section": "Tuning Warping Hyper-parameters ::: Problem Setup and Proposed Algorithm",
            "ref_spans": []
        },
        {
            "text": "In this experiment, we intend to find a super-level set of the Mishra\u2019s Bird benchmark function of the form \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f(x_1, x_2) = sin(x_1)e^{(1-cos(x_2))^2}+cos(x_2)e^{(1-sin(x_1))^2}+(x_1-x_2)^2$$\\end{document} at \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h=10$$\\end{document}. At this level there are multiple disconnected regions. For the search, the length scale was warped with values for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_0$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{l}}}_1$$\\end{document} being [0.02, 0.38] and [1.32, 0.002] respectively. The value of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon $$\\end{document} was 0.1 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sqrt{\\beta _{t}}$$\\end{document} = \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sqrt{\\nu \\tau _{t}}$$\\end{document} as defined in [3] where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tau _{t} = 2log(t^{d/2 + 2}\\pi ^2/3\\delta )$$\\end{document} with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta = 0.01$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\nu = 1$$\\end{document}.\n",
            "cite_spans": [
                {
                    "start": 2487,
                    "end": 2488,
                    "mention": "3",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Mishra\u2019s Bird Function ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "Figure 3a) illustrates experimentally the impact of warping the length scale on the acquisition function. Positive difference indicates that values in the warped scenario are higher than in the no-warping scenario. Results show for smaller length scales, the acquisition function is increased, while at larger length scales, the value is decreased. Changing the acquisition function via warping increases (or decreases) the chance of a point being selected as the next sample point.",
            "cite_spans": [],
            "section": "Mishra\u2019s Bird Function ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Figure 3b) shows the comparative performance over 50 randomly initialised trials with and without warping of the search space. The classification accuracy and rate are improved notably with input warping accompanying the acquisition function.",
            "cite_spans": [],
            "section": "Mishra\u2019s Bird Function ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "LS-DYNA, is a finite element modelling program which simulates complex scenarios in the physical world [9]. Using a simplified car crash simulation, we demonstrate an important application of level set estimation for the design of safe vehicles. The problem focuses on a vehicle moving at a constant velocity and crashing into a pole, resulting in the front of the car deforming. A car must be designed to maintain the safety of passengers. In both experiments below, the input parameters represent the mass of various car components. Altering these inputs alters the rigidity of the car. If too rigid, the passengers will experience injury from the forces of impact (eg. whiplash). If not rigid enough, the front of the car may crush and intrude into the passenger space. The objective of such a problem is to maximise the \u201ccrashworthiness\u201d of the vehicle.",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 105,
                    "mention": "9",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Car Crashworthiness Design ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "Experiment 1. In the first crash experiment there are two design parameters: mass of the front bumper bar tbumper and, mass of the front, hood and underside of the bonnet thood. Both range between 1 and 5, representing the thickness of the component. To construct the dataset, each of the two input parameters were sampled within the entire range in steps of 0.1, resulting in 1681 combinations. The output of the simulation is a Head injury criterion (HIC) with the objective being to maintain \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$HIC < 250$$\\end{document}.",
            "cite_spans": [],
            "section": "Car Crashworthiness Design ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "Experiment 2. In this scenario, the number of inputs is 6, representing thickness of hood \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textit{thood}$$\\end{document}, grill \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textit{tgrill}$$\\end{document}, roof \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textit{troof}$$\\end{document}, bumper \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textit{tbumper}$$\\end{document}, front of rails \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textit{trailf}$$\\end{document} and back of rails \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textit{trailb}$$\\end{document}. Inputs were sampled over a grid of 15,625 points. The output is frequency of car torsional vibration. The objective is to maintain torsional mode frequency \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$< 1.9$$\\end{document} Hz.",
            "cite_spans": [],
            "section": "Car Crashworthiness Design ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "Figure 4 shows the comparative results.\n",
            "cite_spans": [],
            "section": "Car Crashworthiness Design ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "Design of high entropy alloys (HEA) with exceptional physical properties is an active research area in the material science community. To assist in the design of such alloys, many practitioners use the High Entropy Alloy Database (TCHEA) on the Thermo-Calc software. Thermo-Calc is a powerful tool in Computational Thermodynamics and is popular for thermochemical calculations of heterogeneous phase equilibria and multicomponent phase analysis [10].",
            "cite_spans": [
                {
                    "start": 446,
                    "end": 448,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Ductile Alloy Design ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "In this experiment we utilise the TCHEA database in Thermo-Calc for the design of 4 element alloy systems consisting of Iron (Fe), Nickle (Ni), Cobalt (Co) and Chromium (Cr). The objective is to determine the set of alloy compositions that, when cast at room temperature (27\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^\\circ $$\\end{document}), resulted in an Face-Centered Cubic (FCC) proportion of at least 80%. The input space was constrained such that the four element\u2019s mass percentage could range between 0\u201350%, and the sum of the elements must equal 100%. Figure 5a) shows the target region.",
            "cite_spans": [],
            "section": "Ductile Alloy Design ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 794,
                    "end": 795,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "Due to dependent nature of input variables (by constraint that sum is 100%), only 3 elements were used. Figure 5b) shows the comparative results for 50 trials.",
            "cite_spans": [],
            "section": "Ductile Alloy Design ::: Experimental Results",
            "ref_spans": [
                {
                    "start": 111,
                    "end": 112,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "For most experiments classification from warping is faster in early iterations before converging, demonstrating the exploitative behaviour of the acquisition function from warping. This justifies the use of warping for level set estimation, particularly when function evaluations are expensive and budget limits the number of samples.\n",
            "cite_spans": [],
            "section": "Ductile Alloy Design ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "Computational impact of warping comes from constructing the complex covariance matrix in the non-stationary GP. The computational efficiency of being able to vectorise the covariance matrix construction with a constant length scale is not possible in the changing length scale scenario and for loops are needed. For example, run time for 50 iterations of Mishra\u2019s Bird function without warping is on average 12 s, whilst with the warping, time is around 5 min. However, it is assumed that function evaluation time for the real world cases are well above the optimiser\u2019s run time.",
            "cite_spans": [],
            "section": "Computational Time ::: Experimental Results",
            "ref_spans": []
        },
        {
            "text": "This paper presented a novel means in which a complex covariance function can be constructed by distorting the length scale of the GP from which the acquisition function samples from. By doing so, areas with a high potential for being at the level are expanded, thereby increasing the chance of sampling in these regions. Conversely, areas with lower potential are contracted. The warping metric valued the same characteristics as the acquisition function, allowing the two to operate together. The warping metric however results in the acquisition function behaving more exploitatively, which is beneficial in level set estimation problems. Guarantees of convergence were presented as well as bounds on the length scale range and maximum information gain.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "FIGREF0": {
            "text": "Fig. 1.: Impact to selection of next point by acquisition function due to multiple length scale warpings. Increased warping layers result in the acquisition function behaving more exploitatively. (Color figure online)",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: (a) Ambiguity acquisition function value equal for both x and x\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{\\prime }$$\\end{document} - equally likely to be selected as next best point. (b) Differentiation of acquisition value for x and x\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{\\prime }$$\\end{document} following warping",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: a) Change in acquisition function vs length scale. Positive value indicates increase in acquisition function value. Increased acquisition function values at shorter lengths vs longer length scales. b) F1 vs iteration results for classification of Bird Function at threshold \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h = 10$$\\end{document}",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 4.: (a) F1 score vs iteration for HIC \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$< 250$$\\end{document}. LSE with warping outperforms standard LSE in initial iterations. LSE slightly outperforms LSE warped in final stage. This is considered negligible. (b) F1 score vs iteration for torsional mode frequency \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$< 1.9$$\\end{document} Hz. LSE with warping outperforming LSE considerably in early stages.",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig. 5.: a) FCC vs elemental compositions. Regions where FCC \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} 80% indicated in blue. b) Results for classification of alloy with threshold at 80% FCC. LSE with warping outperforms standard LSE before converging to same rate in later iterations. (Color figure online)",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Sequential design of computer experiments for the estimation of a probability of failure",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bect",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ginsbourger",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Picheny",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "V\u00e1zquez",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Stat. Comput.",
            "volume": "22",
            "issn": "3",
            "pages": "773-793",
            "other_ids": {
                "DOI": [
                    "10.1007/s11222-011-9241-4"
                ]
            }
        },
        "BIBREF1": {
            "title": "Thermo-Calc and DICTRA, computational tools for materials science",
            "authors": [
                {
                    "first": "J-O",
                    "middle": [],
                    "last": "Andersson",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Helander",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "H\u00f6glund",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sundman",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Calphad",
            "volume": "26",
            "issn": "",
            "pages": "273-312",
            "other_ids": {
                "DOI": [
                    "10.1016/S0364-5916(02)00037-8"
                ]
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Nonstationary Gaussian process regression using point estimates of local smoothness",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Plagemann",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kersting",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Burgard",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Machine Learning and Knowledge Discovery in Databases",
            "volume": "",
            "issn": "",
            "pages": "204-219",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Rasmussen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Gaussian Processes for Machine Learning, Adaptive Computation and Machine Learning",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Robust super-level set estimation using Gaussian processes",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zanette",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "MJ",
                    "middle": [],
                    "last": "Kochenderfer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Machine Learning and Knowledge Discovery in Databases",
            "volume": "",
            "issn": "",
            "pages": "276-291",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Fast parallel kriging based stepwise uncertainty reduction with application to the identification of an excursion set",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Chevalier",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bect",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ginsbourger",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Vazquez",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Picheny",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Richet",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Technometrics",
            "volume": "56",
            "issn": "4",
            "pages": "455-465",
            "other_ids": {
                "DOI": [
                    "10.1080/00401706.2013.860918"
                ]
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}