{"paper_id": "2615a358ae189a9a390c553b3f1f8e8e03401405", "metadata": {"title": "News Timeline Generation: Accounting for Structural Aspects and Temporal Nature of News Stream", "authors": [{"first": "Mikhail", "middle": [], "last": "Tikhomirov", "suffix": "", "affiliation": {"laboratory": "", "institution": "Lomonosov Moscow State University", "location": {"settlement": "Moscow", "country": "Russia"}}, "email": "tikhomirov.mm@gmail.com"}, {"first": "Boris", "middle": [], "last": "Dobrov", "suffix": "", "affiliation": {"laboratory": "", "institution": "Lomonosov Moscow State University", "location": {"settlement": "Moscow", "country": "Russia"}}, "email": "dobrov_bv@srcc.msu.ru"}]}, "abstract": [{"text": "The number of news articles that are published daily is larger than any person can afford to study. Correct summarization of the information allows for an easy search for the event of interest. This research was designed to address the issue of constructing annotations of news story. Standard multidocument summarization approaches are not able to extract all information relevant to the event. This is due to the fact that such approaches do not take into account the variability of the event context in time. We have implemented a system that automatically builds timeline summary. We investigated impact of three factors: query extension, accounting for temporal nature and structure of news article in form of inverted pyramid. The annotations that we generate are composed of sentences sorted in chronological order, which together contain the main details of the news story. The paper shows that taking into account the described factors positively affects the quality of the annotations created.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "Due to the explosive growth of the amount of content on the Internet, the problems of extraction and automatically summarizing useful information in the incoming data stream arises. One of such problems is the summarization of news articles on an event. The news story -is a set of news reports from various sources dedicated to describing an event. Such problems are often investigated and solved by news aggregators, for example, Google.News 1 [17] or Yandex.News. 2 This is due to the fact that to work with such problems the researcher needs a huge and diverse collection of news articles.", "cite_spans": [{"start": 446, "end": 450, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 467, "end": 468, "text": "2", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Introduction"}, {"text": "The typical \"lifetime\" of the news story (the time of active discussion of the event) is usually a day or two, but not all events are so short. Some news stories have a \"history\" in the form of a set of previous events that occurred at different moments and are more or less related to each other. Existing multi-document summarization approaches do not take into account the fact that the context, actors, geography and other event properties can vary over time.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The fact that journalists are returning to the same events, for example, with the appearance of new data, indicates that such events are important for the society. The need for a brief summary of the event raises the problem of forming a \"timeline summary\". Timeline summary is a type of multi-document summary, containing the essential details of the subject matter under discussion. The construction of such annotations is a complex task, performed by journalists or analysts manually. This implies that the automation of such a process is a urgent problem.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "In this paper we consider challenges and solutions for the automatic generation of temporal summaries. We consider this problem as a multi-document summarization on a query over a representative collection of news documents. The query in this case is the text of the news message. The situation corresponds to the scenario when a user would like to receive a timeline summary after reading the news document. The result should be a time-ordered list of descriptions of the key sub-events related to main event. The result consists of parts of existing sentences, since our solution refers to extractive summarization approaches.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "A system was developed to automate the timeline summarization process. Experiments were conducted over a collection of 2 million Russian news for the first half of 2015. Three new factors were investigated to improve the results of constructing a timeline summary: query extension using pseudo-relevance feedback, accounting for the timing characteristics of news stories and the structure of the inverted pyramid. This is a follow-up study of timeline summarization problem reported in previous paper [25] . In this study, we expanded the collection of standard annotations three-fold. The evaluation process was improved by dividing the collection into a training and test parts. An optimization module was added for fitting the configurations. As a result, substantial progress was achieved. Taking into account the structure of the inverted pyramid showed a significant increase in the values of metrics, which was not achieved in the previous article.", "cite_spans": [{"start": 502, "end": 506, "text": "[25]", "ref_id": "BIBREF24"}], "ref_spans": [], "section": "Introduction"}, {"text": "Currently, there are quite a number of methods for automatic text summarization [3] . Some methods that use large linguistic ontologies [12, 15] , that may be automatically supplemented during the analysis. Other methods are based on the statistical properties of texts [16] or machine learning [13] .", "cite_spans": [{"start": 80, "end": 83, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 136, "end": 140, "text": "[12,", "ref_id": "BIBREF11"}, {"start": 141, "end": 144, "text": "15]", "ref_id": "BIBREF14"}, {"start": 270, "end": 274, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 295, "end": 299, "text": "[13]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Automatic Text Summarization Problem"}, {"text": "During the generation of the annotations, the following problems occur [3, 7, 11] :", "cite_spans": [{"start": 71, "end": 74, "text": "[3,", "ref_id": "BIBREF2"}, {"start": 75, "end": 77, "text": "7,", "ref_id": "BIBREF6"}, {"start": 78, "end": 81, "text": "11]", "ref_id": "BIBREF10"}], "ref_spans": [], "section": "Automatic Text Summarization Problem"}, {"text": "\u2022 Ensuring the completeness of the presentation of information, including the most up-to-date information. \u2022 Decreasing of redundancy in the information provided.", "cite_spans": [], "ref_spans": [], "section": "Automatic Text Summarization Problem"}, {"text": "\u2022 Ensuring the coherence and understandability of the information provided.", "cite_spans": [], "ref_spans": [], "section": "Automatic Text Summarization Problem"}, {"text": "To ensure the completeness of the resulting annotation, it is often necessary to find links between sentences or documents [20] .", "cite_spans": [{"start": 123, "end": 127, "text": "[20]", "ref_id": "BIBREF19"}], "ref_spans": [], "section": "Automatic Text Summarization Problem"}, {"text": "To determine the redundancy in the generated annotations, various measures of similarity between sentences are used. One of the most common approaches is clustering -the selection of content groups of sentences [6] . Another approach to reduce redundancy is to compare candidate sentence with sentences that have already been included in the summary and to evaluate novel information. Example of such approach is the Maximal Marginal Relevance (MMR) [2] .", "cite_spans": [{"start": 211, "end": 214, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 450, "end": 453, "text": "[2]", "ref_id": "BIBREF1"}], "ref_spans": [], "section": "Automatic Text Summarization Problem"}, {"text": "The problem of ensuring the coherence of information in the summary arises both in the methods of generating the annotation [18, 19] , and in the methods of evaluation, because in order to assess the connectivity and linguistic qualities of the annotation, it is necessary to perform a manual evaluation.", "cite_spans": [{"start": 124, "end": 128, "text": "[18,", "ref_id": "BIBREF17"}, {"start": 129, "end": 132, "text": "19]", "ref_id": "BIBREF18"}], "ref_spans": [], "section": "Automatic Text Summarization Problem"}, {"text": "The problem of timeline summary construction has a number of differences from the standard summarization problem. For example, the temporal nature of events must be taken into account [9] . Also, to ensure completeness of the information provided, it is required to find documents from all sub-events of the topic under consideration.", "cite_spans": [{"start": 184, "end": 187, "text": "[9]", "ref_id": "BIBREF8"}], "ref_spans": [], "section": "Timeline Summary"}, {"text": "When constructing timeline summary, data processing is mainly carried out over huge collections. In such collections, most of the information is not relevant to the user's request. This problem can be solved by using clustering methods [10, 14] . But the clustering methods have some issues. First, such a task should be solved many times over huge collections of documents, which affects the response time of the system. Secondly, the degree of closeness can be significantly smaller with standard measures of similarity for documents that describe far-in-time but related events. And, of course, it is required to identify the most characteristic objects [1, 9] , for example, taking into account the structural features of the flow of documents [5, 8] .", "cite_spans": [{"start": 236, "end": 240, "text": "[10,", "ref_id": "BIBREF9"}, {"start": 241, "end": 244, "text": "14]", "ref_id": "BIBREF13"}, {"start": 657, "end": 660, "text": "[1,", "ref_id": "BIBREF0"}, {"start": 661, "end": 663, "text": "9]", "ref_id": "BIBREF8"}, {"start": 748, "end": 751, "text": "[5,", "ref_id": "BIBREF4"}, {"start": 752, "end": 754, "text": "8]", "ref_id": "BIBREF7"}], "ref_spans": [], "section": "Timeline Summary"}, {"text": "The problem of constructing a timeline summary is a query-oriented. In the most general case, the user has a news document as a query. So further this problem will be considered as a problem of automatic creation of a summary on a query in the form of a text document. The output of the system is an annotation of n sentences. The connectivity between the sentences in this paper is not required. Figure 1 provides an example of a possible summary about the conflict on cemetery taken from the Interfax website. 3 The aim of the work is to study the influence of various factors on the quality of the annotation.", "cite_spans": [{"start": 512, "end": 513, "text": "3", "ref_id": "BIBREF2"}], "ref_spans": [{"start": 397, "end": 405, "text": "Figure 1", "ref_id": "FIGREF0"}], "section": "General Description"}, {"text": "The problem described above can be formalized in the following way. Let Q \u00bc q 1 ; q 2 ; . . .; q m f gbe a set of queries and an associated set of reference annotations", "cite_spans": [], "ref_spans": [], "section": "Mathematical Statement of the Problem"}, {"text": "be an associated set of reference annotations. The system generates a set of summary", "cite_spans": [], "ref_spans": [], "section": "Mathematical Statement of the Problem"}, {"text": "in response to queries Q by algorithm A. Then the problem is reduced to maximizing the following functional:", "cite_spans": [], "ref_spans": [], "section": "Mathematical Statement of the Problem"}, {"text": "where M is the proximity function between the annotations. Optimization is carried out for all parameters of the algorithm.", "cite_spans": [], "ref_spans": [], "section": "Mathematical Statement of the Problem"}, {"text": "As mentioned earlier, the input collection contains 2 million news articles. It is not possible to work directly with such amount of information, therefore, it was decided to interact with the collection through a search engine. Search engine allows:", "cite_spans": [], "ref_spans": [], "section": "Collection Processing"}, {"text": "\u2022 Get a list of documents by text request.", "cite_spans": [], "ref_spans": [], "section": "Collection Processing"}, {"text": "\u2022 For a given document from collection, get the basic information: text, index, metainformation. ", "cite_spans": [], "ref_spans": [], "section": "Collection Processing"}, {"text": "In this paper the following factors were investigated:", "cite_spans": [], "ref_spans": [], "section": "Studied Features"}, {"text": "\u2022 Query extension strategy.", "cite_spans": [], "ref_spans": [], "section": "Studied Features"}, {"text": "\u2022 Accounting for the temporal nature of news stories.", "cite_spans": [], "ref_spans": [], "section": "Studied Features"}, {"text": "\u2022 Accounting for the structure of a news article in the form of an inverted pyramid.", "cite_spans": [], "ref_spans": [], "section": "Studied Features"}, {"text": "Information that can be obtained from a query document is basically not enough to effectively build this type of annotation. This fact is a consequence of the fact that most news articles are not a general description of the event, but a discussion of some particular incident or fact.", "cite_spans": [], "ref_spans": [], "section": "Query Extension Strategy"}, {"text": "To avoid this problem, it is necessary to use the query extension techniques. The developed algorithm uses the idea of pseudo-relevance feedback, which is widely used in information retrieval problems [21] . For the query-document, the algorithm includes the following steps:", "cite_spans": [{"start": 201, "end": 205, "text": "[21]", "ref_id": "BIBREF20"}], "ref_spans": [], "section": "Query Extension Strategy"}, {"text": "1. The most significant K-terms are chosen on the basis of tf-idf weights forming thus the first-level query. 2. Further on the basis of the first-level query documents are retrieved. 3. The extracted cluster of documents is analyzed to find the most important terms forming thus the second-level query: a. For each document, the most significant T terms are considered. b. For each term, it is calculated how often it was in the top T terms on all cluster. c. The list of terms is sorted by frequency, the best M terms are selected. 4. Steps 2-3 are repeated (A double query extension process that forms a third-level query). 5. Output of the algorithm is a vector of N terms representing to some extent the semantics of the input document.", "cite_spans": [], "ref_spans": [], "section": "Query Extension Strategy"}, {"text": "Note that K; T; M; N are parameters of the algorithm and they must be configured. As an example of the work of the query extension module, consider the algorithm steps on a news article about the terrorist attack in Paris ( Table 1 ). The table shows that a higher-level query has more significant terms for this event. ", "cite_spans": [], "ref_spans": [{"start": 224, "end": 231, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Query Extension Strategy"}, {"text": "Since any event depends on time, the content of publications and their number also depend on the time. As an example, Fig. 2 shows a graph of the time dependence of publications on the \"Earthquake in Nepal\" event.", "cite_spans": [], "ref_spans": [{"start": 118, "end": 124, "text": "Fig. 2", "ref_id": "FIGREF1"}], "section": "Temporal Nature of News Stories"}, {"text": "To take into account this factor, for the set D of extracted documents the following procedure is undertaken:", "cite_spans": [], "ref_spans": [], "section": "Temporal Nature of News Stories"}, {"text": "1. The entire timeline of the event is divided into days with labels T \u00bc t 1 ; t 2 ; . . .; t n f g . 2. Each document receives a label from T based on the publication date D t i . 3. Documents published on days with a number of publications less then the NDoc tr threshold (2) are discarded.", "cite_spans": [], "ref_spans": [], "section": "Temporal Nature of News Stories"}, {"text": "4. The output is a sorted list of collections C \u00bc C t 1 ; C t 2 ; . . .; C t n f g , where each collection C t i contains only documents with the label t i .", "cite_spans": [], "ref_spans": [], "section": "Temporal Nature of News Stories"}, {"text": "The strategy of writing a high-quality news article often relies on the structure of the \"inverted pyramid\" (Fig. 3) . The greatest interest is the upper and lower parts of the pyramid:", "cite_spans": [], "ref_spans": [{"start": 108, "end": 116, "text": "(Fig. 3)", "ref_id": "FIGREF2"}], "section": "Inverted Pyramid"}, {"text": "\u2022 The upper part contains the most concentrated information about the event under discussion. \u2022 The lower part may contain references to important related events in the past. Inter-Document Feature. This feature is taken into account in the following way:", "cite_spans": [], "ref_spans": [], "section": "Inverted Pyramid"}, {"text": "1. For a set of documents D, a similarity matrix between the upper and lower parts of the documents is constructed. If the specified similarity threshold is exceeded, it is considered that there is a link between the documents D i and D j . 2. The importance of documents is calculated by using the LexRank algorithm over the constructed graph [4] . 3. For documents whose weight is greater than a certain threshold, the previously described procedure for expanding the query is performed.", "cite_spans": [{"start": 344, "end": 347, "text": "[4]", "ref_id": "BIBREF3"}], "ref_spans": [], "section": "Inverted Pyramid"}, {"text": "As a result, the output is a ranked list of documents D and a set of Q D new queries, which further, together with accounting for the temporal nature of the news story, will help in sentence ranking algorithm. Among other things, document weights will also be taken into account in the ranking functions.", "cite_spans": [], "ref_spans": [], "section": "Inverted Pyramid"}, {"text": "Intra-Document Feature. To this feature into account for the following procedure is undertaken: during the ranking of sentences, the weight of the sentence is multiplied by a coefficient that lowers the weight of sentences in the middle of the document. Also, after described inter-document procedure, all constructed extended queries Q D are mapped to t i labels from T (Fig. 4) . ", "cite_spans": [], "ref_spans": [{"start": 371, "end": 379, "text": "(Fig. 4)", "ref_id": "FIGREF3"}], "section": "Inverted Pyramid"}, {"text": "At various stages of the algorithm, there are a number of points where the measure of closeness between sentences is calculated. For these purpose a cosine measure of similarity (3) is used in all cases.", "cite_spans": [], "ref_spans": [], "section": "Similarity of Sentences"}, {"text": "The choice of representation of a sentence plays an important role for calculating similarity. In this article we used the standard tf-idf representation. But to calculate the similarity between sentences when searching for links between documents, word2vec [24] representation was used. To achieve this, the resulting sentence vector is represented as a weighted mean of word2vec word representations. Weighing was carried out by tf-idf.", "cite_spans": [{"start": 258, "end": 262, "text": "[24]", "ref_id": "BIBREF23"}], "ref_spans": [], "section": "Similarity of Sentences"}, {"text": "Word2vec model was trained on the entire collection of 2 million news articles. During preprocessing removal of stop words and lemmatization were applied. The width of the window was chosen to be 5, and the length of the vector was 100.", "cite_spans": [], "ref_spans": [], "section": "Similarity of Sentences"}, {"text": "This module deals with the ranking of sentences. The ranking is a modified version of the MMR algorithm -MMRT (4) taking into account all the factors described in Sect. 4.2:", "cite_spans": [], "ref_spans": [], "section": "Sentence Ranking Module"}, {"text": "where INC s t iis a term describing the positive part of the formula, which depends on the similarity of the sentence to the query, the weight of the document from which the sentence is taken, and the sentence number in the document. The parameters a and k are configurable parameters of the algorithm, I iis document weight D s , which includes a sentence under the index i, S t iis estimated sentence under the index i with label t, Q tquery for this time label, cmultiplier, which reduce the weight of sentences from the middle of the document. DEC s t i is the penalty term. It depends on the similarity to the already extracted sentences:", "cite_spans": [], "ref_spans": [], "section": "Sentence Ranking Module"}, {"text": "where S j is one of the extracted sentences, S is the set of all already extracted sentences.", "cite_spans": [], "ref_spans": [], "section": "Sentence Ranking Module"}, {"text": "Processing of sentences occurs in chronological order with a restriction on the maximum number of sentences per day.", "cite_spans": [], "ref_spans": [], "section": "Sentence Ranking Module"}, {"text": "The features described in Subsect. 4.2 are realized at various stages of the system. The general scheme of the algorithm is shown on Fig. 5 . ", "cite_spans": [], "ref_spans": [{"start": 133, "end": 139, "text": "Fig. 5", "ref_id": "FIGREF4"}], "section": "System Diagram"}, {"text": "The system was evaluated using several metrics: ROUGE-1, ROUGE-2, and Sentence Recall R sent :", "cite_spans": [], "ref_spans": [], "section": "Metrics for Evaluation"}, {"text": "where N A is the set of n-grams for the constructed annotations, N g is the set of n-grams for the reference (gold) annotations.", "cite_spans": [], "ref_spans": [], "section": "Metrics for Evaluation"}, {"text": "where S A is the set of sentences from the constructed annotations, S g is the set of sentences from the reference annotations. Operator denotes the following: the result of the S A S g is a subset of S A such that their semantic equivalent is present in S g .", "cite_spans": [], "ref_spans": [], "section": "Metrics for Evaluation"}, {"text": "Since a test set of annotations is required for evaluating procedure, in the course of the research, timeline summaries were manually prepared. The procedure for the formation of such a collection was as follows:", "cite_spans": [], "ref_spans": [], "section": "Data Preparation"}, {"text": "1. At the first stage with the help of Wikipedia there high-profile events were selected, which were actively covered in the press for the beginning of 2015. 2. Further, for most of the events on the site \"Interfax\", the search for the corresponding story was carried out. On the basis of documents corresponding to the story, a timeline summary was created. 3. If there is no corresponding story on the \"Interfax\", the materials were studied on the topic and a timeline summary was created on the basis of the documents read.", "cite_spans": [], "ref_spans": [], "section": "Data Preparation"}, {"text": "As a result, 45 annotations on 15 news stories were created (Table 2) .", "cite_spans": [], "ref_spans": [{"start": 60, "end": 69, "text": "(Table 2)", "ref_id": "TABREF1"}], "section": "Data Preparation"}, {"text": "Since the system contains a large number of parameters (total 23 parameters), some of which are presented in Table 3 , there was a need to optimize the choice of the values of these parameters. To achieve this, the entire collection of the reference annotations was divided into train and test parts with the ratio 2 to 1. Further, the functional (1) was implemented in Python using an open hyperopt [22] package based on machine learning. This package uses the technique of Sequential model-based optimization (SMBO) [23] for the parameters selection. The parameters were trained on the training part. After that, the final evaluation of the configurations took place on the test part.", "cite_spans": [{"start": 400, "end": 404, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 518, "end": 522, "text": "[23]", "ref_id": "BIBREF22"}], "ref_spans": [{"start": 109, "end": 116, "text": "Table 3", "ref_id": "TABREF2"}], "section": "Optimization of Algorithm Parameters"}, {"text": "In order to evaluate the contribution of the considered features, a fitting and evaluation of the following 6 configurations was made:", "cite_spans": [], "ref_spans": [], "section": "Results"}, {"text": "1. baselinea simple approach to summarization, without taking into account the factors considered, using MMR as ranking algorithm. . importancetemporal + accounting for the structure of a news article in the form of an inverted pyramid, when tf-idf representation is used (Sect. 4.5). 6. w2v-impimportance, but using w2v for computing sentence similarity when accounting for the structure of a news article (Sect. 4.6).", "cite_spans": [], "ref_spans": [], "section": "Results"}, {"text": "The result of evaluation of the configurations can be seen in Table 4 . This table shows that each of the features considered gives a positive contribution to the quality of generation of timeline summary. As an example of the final annotation, one can consider a fragment of the annotation on the previously mentioned incident of the crash in Taiwan in Table 5 . The Transasia plane crashed on February 4 in Taiwan, because the pilot accidentally turned off the running engine when the second engine stalled", "cite_spans": [], "ref_spans": [{"start": 62, "end": 69, "text": "Table 4", "ref_id": null}, {"start": 354, "end": 361, "text": "Table 5", "ref_id": null}], "section": "Results"}, {"text": "In this article we presented an approach for building a timeline summary. The conducted research shows that the problem of constructing the timeline summary differs from the standard MDS problem. The effectiveness of using the following features was shown:", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}, {"text": "\u2022 Query extension strategy.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}, {"text": "\u2022 Accounting for the temporal nature of news stories.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}, {"text": "\u2022 Accounting for the structure of a news article in the form of an inverted pyramid.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}, {"text": "Extending the query, as expected, has a positive effect on the event representation discussed in the document. But the interesting fact is that re-extension the query (double query extension) has a much greater effect. This is because the documents that are retrieved on the first-level query are not sufficient for a good presentation of the event.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}, {"text": "The fact that accounting for the temporal nature of news stories improves the quality of the annotation is an obvious consequence of the fact that news stories and events have temporal characteristics.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}, {"text": "Taking into account the structure of the inverted pyramid gives an improvement. Increase the values of metrics on the w2v-imp configuration means that the correctness of the recognized links between the documents plays a significant role. This fact raises challenges for future research.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}, {"text": "Using structural features of news articles make it possible to obtain information, the use of which can significantly improve the quality of generated annotations.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and Future Work"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Predicting relevant news events for timeline summaries", "authors": [{"first": "T", "middle": ["G"], "last": "Binh", "suffix": ""}, {"first": "M", "middle": [], "last": "Alrifai", "suffix": ""}, {"first": "D", "middle": [], "last": "Quoc Nguyen", "suffix": ""}], "year": 2013, "venue": "Proceedings of the 22nd International Conference on World Wide Web", "volume": "", "issn": "", "pages": "91--92", "other_ids": {}}, "BIBREF1": {"ref_id": "b1", "title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "authors": [{"first": "J", "middle": [], "last": "Carbonell", "suffix": ""}, {"first": "J", "middle": [], "last": "Goldstein", "suffix": ""}], "year": 1998, "venue": "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "volume": "", "issn": "", "pages": "335--336", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Overview of DUC", "authors": [{"first": "H", "middle": ["T"], "last": "Dang", "suffix": ""}], "year": 2006, "venue": "Proceedings of the Document Understanding Workshop, Presented at HLT-NAACL 2006", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "Lexrank: graph-based lexical centrality as salience in text summarization", "authors": [{"first": "G", "middle": [], "last": "Erkan", "suffix": ""}, {"first": "D", "middle": ["R"], "last": "Radev", "suffix": ""}], "year": 2004, "venue": "J. Artif. Intell. Res", "volume": "22", "issn": "", "pages": "457--479", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Exploring the interactions of storylines from informative news events", "authors": [{"first": "P", "middle": [], "last": "Hu", "suffix": ""}, {"first": "M", "middle": ["L"], "last": "Huang", "suffix": ""}, {"first": "X", "middle": ["Y"], "last": "Zhu", "suffix": ""}], "year": 2014, "venue": "J. Comput. Sci. Technol", "volume": "29", "issn": "3", "pages": "502--518", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Centroid-based summarization of multiple docuemtns: sentence extraction, utility-based evaluation, and user studies", "authors": [{"first": "D", "middle": [], "last": "Radev", "suffix": ""}, {"first": "H", "middle": [], "last": "Jing", "suffix": ""}, {"first": "M", "middle": [], "last": "Budzikowska", "suffix": ""}], "year": 2000, "venue": "Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization", "volume": "", "issn": "", "pages": "21--30", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Introduction to the special issue on summarization", "authors": [{"first": "D", "middle": [], "last": "Radev", "suffix": ""}, {"first": "K", "middle": [], "last": "Mckeown", "suffix": ""}, {"first": "E", "middle": [], "last": "Hovy", "suffix": ""}], "year": 2002, "venue": "Comput. Linguist", "volume": "28", "issn": "4", "pages": "399--408", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Connecting two (or less) dots: discovering structure in news articles", "authors": [{"first": "D", "middle": [], "last": "Shahaf", "suffix": ""}, {"first": "C", "middle": [], "last": "Guestrin", "suffix": ""}], "year": 2012, "venue": "ACM Trans. Knowl. Discov. Data (TKDD)", "volume": "5", "issn": "4", "pages": "24--54", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Timeline summarization from relevant headlines", "authors": [{"first": "G", "middle": [], "last": "Tran", "suffix": ""}, {"first": "M", "middle": [], "last": "Alrifai", "suffix": ""}, {"first": "E", "middle": [], "last": "Herder", "suffix": ""}], "year": 2015, "venue": "ECIR 2015", "volume": "9022", "issn": "", "pages": "245--256", "other_ids": {"DOI": ["10.1007/978-3-319-16354-3_26"]}}, "BIBREF9": {"ref_id": "b9", "title": "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution", "authors": [{"first": "R", "middle": [], "last": "Yan", "suffix": ""}], "year": 2011, "venue": "Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval", "volume": "", "issn": "", "pages": "745--754", "other_ids": {"DOI": ["10.1145/2009916.2010016"]}}, "BIBREF10": {"ref_id": "b10", "title": "A topic modeling based approach to novel document automatic summarization", "authors": [{"first": "Z", "middle": [], "last": "Wu", "suffix": ""}, {"first": "L", "middle": [], "last": "Lei", "suffix": ""}, {"first": "G", "middle": [], "last": "Li", "suffix": ""}, {"first": "H", "middle": [], "last": "Huang", "suffix": ""}, {"first": "C", "middle": [], "last": "Zheng", "suffix": ""}, {"first": "E", "middle": [], "last": "Chen", "suffix": ""}, {"first": "G", "middle": [], "last": "Xu", "suffix": ""}], "year": 2017, "venue": "Expert Syst. Appl", "volume": "84", "issn": "", "pages": "12--23", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "An ontology-based approach to text summarization", "authors": [{"first": "L", "middle": [], "last": "Hennig", "suffix": ""}, {"first": "W", "middle": [], "last": "Umbrath", "suffix": ""}, {"first": "R", "middle": [], "last": "Wetzker", "suffix": ""}], "year": 2008, "venue": "Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology", "volume": "3", "issn": "", "pages": "291--294", "other_ids": {}}, "BIBREF12": {"ref_id": "b12", "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "authors": [{"first": "R", "middle": [], "last": "Nallapati", "suffix": ""}, {"first": "B", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "C", "middle": [], "last": "Gulcehre", "suffix": ""}, {"first": "B", "middle": [], "last": "Xiang", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1602.06023"]}}, "BIBREF13": {"ref_id": "b13", "title": "A semantic approach for text clustering using WordNet and lexical chains", "authors": [{"first": "T", "middle": [], "last": "Wei", "suffix": ""}, {"first": "Y", "middle": [], "last": "Lu", "suffix": ""}, {"first": "H", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Q", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "X", "middle": [], "last": "Bao", "suffix": ""}], "year": 2015, "venue": "Expert Syst. Appl", "volume": "42", "issn": "4", "pages": "2264--2275", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Text summarization techniques: a brief survey", "authors": [{"first": "M", "middle": [], "last": "Allahyari", "suffix": ""}, {"first": "S", "middle": [], "last": "Pouriyeh", "suffix": ""}, {"first": "M", "middle": [], "last": "Assefi", "suffix": ""}, {"first": "S", "middle": [], "last": "Safaei", "suffix": ""}, {"first": "E", "middle": ["D"], "last": "Trippe", "suffix": ""}, {"first": "J", "middle": ["B"], "last": "Gutierrez", "suffix": ""}, {"first": "K", "middle": [], "last": "Kochut", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1707.02268"]}}, "BIBREF15": {"ref_id": "b15", "title": "A neural attention model for abstractive sentence summarization", "authors": [{"first": "A", "middle": ["M"], "last": "Rush", "suffix": ""}, {"first": "S", "middle": [], "last": "Chopra", "suffix": ""}, {"first": "J", "middle": [], "last": "Weston", "suffix": ""}], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1509.00685"]}}, "BIBREF16": {"ref_id": "b16", "title": "Introducing Google News Timeline", "authors": [{"first": "A", "middle": [], "last": "Hertzfeld", "suffix": ""}], "year": 2018, "venue": "Accessed", "volume": "10", "issn": "", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Towards Coherent Multi-Document Summarization", "authors": [{"first": "J", "middle": [], "last": "Christensen", "suffix": ""}, {"first": "S", "middle": ["S"], "last": "Mausam", "suffix": ""}, {"first": "S", "middle": [], "last": "Soderland", "suffix": ""}, {"first": "O", "middle": [], "last": "Etzioni", "suffix": ""}], "year": 2013, "venue": "HLT-NAACL", "volume": "", "issn": "", "pages": "1163--1173", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Learning to generate coherent summary with discriminative hidden semi-markov model", "authors": [{"first": "H", "middle": [], "last": "Nishikawa", "suffix": ""}, {"first": "K", "middle": [], "last": "Arita", "suffix": ""}, {"first": "K", "middle": [], "last": "Tanaka", "suffix": ""}, {"first": "T", "middle": [], "last": "Hirao", "suffix": ""}, {"first": "T", "middle": [], "last": "Makino", "suffix": ""}, {"first": "Y", "middle": [], "last": "Matsuo", "suffix": ""}], "year": 2014, "venue": "COLING", "volume": "", "issn": "", "pages": "1648--1659", "other_ids": {}}, "BIBREF19": {"ref_id": "b19", "title": "Using lexical chains for text summarization", "authors": [{"first": "R", "middle": [], "last": "Barzilay", "suffix": ""}, {"first": "M", "middle": [], "last": "Elhadad", "suffix": ""}], "year": 1999, "venue": "Advances in Automatic Text Summarization", "volume": "", "issn": "", "pages": "111--121", "other_ids": {}}, "BIBREF20": {"ref_id": "b20", "title": "Zero-example event search using multimodal pseudo relevance feedback", "authors": [{"first": "L", "middle": [], "last": "Jiang", "suffix": ""}, {"first": "T", "middle": [], "last": "Mitamura", "suffix": ""}, {"first": "S", "middle": ["I"], "last": "Yu", "suffix": ""}, {"first": "A", "middle": ["G"], "last": "Hauptmann", "suffix": ""}], "year": 2014, "venue": "Proceedings of International Conference on Multimedia Retrieval", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Hyperopt: a python library for model selection and hyperparameter optimization", "authors": [{"first": "J", "middle": [], "last": "Bergstra", "suffix": ""}, {"first": "B", "middle": [], "last": "Komer", "suffix": ""}, {"first": "C", "middle": [], "last": "Eliasmith", "suffix": ""}, {"first": "D", "middle": [], "last": "Yamins", "suffix": ""}, {"first": "D", "middle": ["D"], "last": "Cox", "suffix": ""}], "year": 2015, "venue": "Comput. Sci. Discov", "volume": "8", "issn": "1", "pages": "", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "Sequential model-based optimization for general algorithm configuration", "authors": [{"first": "F", "middle": [], "last": "Hutter", "suffix": ""}, {"first": "", "middle": [], "last": "Hoos", "suffix": ""}, {"first": "H", "middle": [], "last": "Holger", "suffix": ""}, {"first": "K", "middle": [], "last": "Leyton-Brown", "suffix": ""}], "year": 2011, "venue": "LION 2011", "volume": "6683", "issn": "", "pages": "507--523", "other_ids": {"DOI": ["10.1007/978-3-642-25566-3_40"]}}, "BIBREF23": {"ref_id": "b23", "title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method", "authors": [{"first": "Y", "middle": [], "last": "Goldberg", "suffix": ""}, {"first": "O", "middle": [], "last": "Levy", "suffix": ""}], "year": 2014, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1402.3722"]}}, "BIBREF24": {"ref_id": "b24", "title": "Using news corpora for temporal summary formation (in Russian)", "authors": [{"first": "M", "middle": ["M"], "last": "Tikhomirov", "suffix": ""}, {"first": "B", "middle": ["V"], "last": "Dobrov", "suffix": ""}], "year": 2017, "venue": "Selected Papers of the XIX International Conference on Data Analytics and Management in Data Intensive Domains", "volume": "2022", "issn": "", "pages": "165--171", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Timeline summary part about conflict at cemetery.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "Dependence of the number of publications per day for an event This structure is taken into account in two ways:1. Inter-document feature based on the graph approach. 2. Intra-document feature, which increases the weight of sentences located in the upper and lower parts of the inverted pyramid.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "Inverted pyramid on the example of an article. (https://themoscowtimes.com/articles/ moscow-museum-takes-you-inside-north-korea-60240)", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Query mapping.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Working scheme.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Query extension algorithm stages example. Journal, Muhammad, Satirical, Attack, Prophet, Terrorist act, Paris, Caricature, Hollande, Herbo, Charlie", "latex": null, "type": "table"}, "TABREF1": {"text": "News stories on which the reference annotations are made.", "latex": null, "type": "table"}, "TABREF2": {"text": "Some system parameters. The number of the most significant lemmas extracted in the work of the algorithm for constructing an extended query 18 MinLinkScore The minimum value of similarity between the top and bottom of the documents to identify the reference The maximum number of sentences per day 15 Doc Boundary Threshold of the importance of documents for building new queries 0.61 2. querry-exadding a query extension strategy feature to baseline (Sect. 4.3), but without double query extension. 3. double-exquerry-ex + double query extension (Sect. 4.3). 4. temporaldouble-ex + accounting for the temporal nature of news stories (Sect. 4.4). 5", "latex": null, "type": "table"}, "TABREF3": {"text": "Evaluation results. The generated timeline summary fragment about the plane crash in Taiwan. Date Sentence 11.02.2015 Transasia Airways will pay relatives of victims of a plane crash in Taiwan for 470 thousand 11.02.2015 The tragedy in Taiwan, one-fifth of the pilots of Taiwan's Transasia airline have not passed the proficiency test 12.02.2015 Rescuers completed the search operation for victims of the crash of an airline to Transasia Airways, which crashed on February 4 in Taiwan 01.07.2015 Crew crashed in Taiwan aircraft Transasia Airways shut off the engines after a loss of power 02.07.2015", "latex": null, "type": "table"}}, "back_matter": []}