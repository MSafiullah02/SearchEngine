{
    "paper_id": "PMC7245509",
    "metadata": {
        "title": "Trust in and Ethical Design of Carebots: The Case for Ethics of Care",
        "authors": [
            {
                "first": "Gary",
                "middle": [
                    "Chan",
                    "Kok"
                ],
                "last": "Yew",
                "suffix": "",
                "email": "garychan@smu.edu.sg",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Care robots are utilised in hospitals and homes to provide care and support for vulnerable persons such as the elderly, children and those suffering from physical and mental disabilities. They monitor the health conditions of patients, give medication, manually lift and aid the movements of disabled patients, and provide social companionship.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "There are tangible advantages in using carebots to relieve human caregivers (whether healthcare professionals, social workers in nursing homes or family members and loved ones) from the significant assistive manual work and medication reminders to patients and care recipients. Care-O-bot\u2014a home companion for the elderly\u2014open doors and fetch household items, remind the elderly of his or her daily routine and seek help in the event of a fall or accident.1 Robear acts as a nursing care robot to lift patients from the bed to a wheelchair. The US-based robotics corporation, Acrotek, advertise a robot for sale in an aged-care role, claiming on its website that the Actron MentorBotTM will keep track of the elderly persons or remind them to take pills. It is touted to have the ability to call the authorities and report a problem or make a call to loved ones when the patient has gone astray.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Apart from the obvious benefits in relieving the human caregivers of the manual work and routine administration of medication, the carebots can serve as social companions. Sony\u2019s AIBO robotic dog and NeCoRo (OMRON), a robotic cat covered in synthetic fur, come to mind. PARO\u2014an interactive robotic seal developed by AIST in Japan to comfort dementia sufferers\u2014makes high-pitched sounds, and responds to touch by moving its head and tail. It has been reported in studies that patients may prefer interacting with robots than human companions.2 In addition, robots can help in facilitating therapeutic exercises and rehabilitation for patients suffering from both physical and mental disabilities. Since 1998, project Aurora (AUtonomous RObotic platform as a Remedial tool for children with Autism) has encouraged children with autism to become more engaged in human interactions.3 In a pilot study, the autonomous humanoid KASPAR robot taught children with autism to engage in collaborative play through a video game. It found that the children were more engaged by the robot than the human counterpart (though the children showed more examples of collaborative play and cooperation when playing with the human adult).4 Robots can also teach young children with illness or disabilities to deal with them.5 The report on robot Matilda6 that was trialed in Australian residential aged care facilities indicated a statistically significant improvement in the emotional and behavioural engagement of the older people suffering from dementia with the social robot [39]. To deal with physical disabilities, leg rehabilitation robots allow for physiotherapy7 and gait assessment robots can capture gait parameters to customise rehabilitation interventions and exercises to cater to the patient\u2019s needs.8",
            "cite_spans": [
                {
                    "start": 1559,
                    "end": 1561,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Notwithstanding the substantial benefits (actual and potential) that carebots can bring to the vulnerable segments of the population, it is first and foremost, observed that there are unique challenges arising from the use of carebots. These challenges relate to issues of both ethical design and the public or users\u2019 trust in carebots. The second part notes that the existing ethical theories (of utilitarianism, deontology and virtue ethics) as well as ethical guidelines, standards and regulations on robots do not provide an overall ethical framework which developers can take into account in the design of robots or sufficiently concrete principles for application to the caring contexts. The third part will then argue for the Ethics of Care as the most appropriate ethical framework that can apply to the caring contexts. This central framework, together with the tenets of Principlism (comprising autonomy, non-maleficence, beneficence and justice) and other ethical theories (utilitarianism, deontology and virtue ethics) can deal with the current challenges presented by the use of carebots.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Robots may be defined as artificial beings possessing the four characteristics of mobility, interactivity, communication, and autonomy [UNESCO 76, p. 4]. These robots may be powered by artificial intelligence that includes the ability to reason, communicate, and learn from its perception of the surrounding environment, past experiences and even errors. It is artificial intelligence that enables them to respond in socially interactive ways with humans. Advancements in machine learning technology including deep learning and natural language processing have significant potential to enhance the functionality of social robots such as the care robots used in the healthcare sector. The specific roles that carebots are expected to play in healthcare however give rise to unique challenges.Extent of robot care",
            "cite_spans": [
                {
                    "start": 143,
                    "end": 145,
                    "mention": "76",
                    "ref_id": "BIBREF73"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Sparrow and Sparrow9 have argued that robots are incapable of meeting the social and emotional needs of older persons under their care. Robots do not understand human frailties and therefore cannot show care for vulnerable people. Moreover, the use of carebots may result in a decrease in the amount of human contact experienced by the older persons, which would be detrimental to their well-being. It is unethical in their view to attempt to substitute robot simulacra for genuine social interaction. The inability of robots to care for humans and their increased role of robots in social interactions give rise to potential problems of deception and over-reliance on robots by users (which we will discuss below).",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "On this issue of the carebots\u2019 inability to care, we should take note of both internal and external aspects. The internal aspect is that carebots lack human consciousness and do not possess any human emotions.10 As such, they lack human understanding as to how it feels to care for a human. It might be argued, somewhat ironically, that the robot\u2019s inability to feel pain and suffering or experience vulnerability as humans do accounts largely for their inability to show care to humans.11",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Technological developments suggest that robots have the capacity to \u201cperceive\u201d human emotions and facial expressions.12 For example, Huggable was designed for therapeutic applications in nursing homes. The interactive social robot possesses the neural network to recognise nine different classes of affective touch and can react accordingly. Another example is the Model of User\u2019s Emotions which has been developed for the healthcare industry to assess patients\u2019 emotions from various sources (including heart rate, breathing pattern, temperature, vocal characteristics and facial expressions). As Wallach and Allen described, technology has enabled the translation of sensory data such that they can be received as \u201ccognitive representations of emotions\u201d.13 This does not mean, however, that robots are capable of perceiving real human emotions or capable of having emotions as humans do.",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Despite this internal incapacity to care, carebots are capable of exhibiting the external or \u2018outward\u2019 aspect of care. Robots can be designed to respond to humans and human behaviours in a manner that may be perceived as \u2018caring\u2019 by the human care recipients. The external aspect of care may be demonstrated through words importing kindness and encouragement, a gentle pat on the back of the care recipient or a programmed smile on a humanoid carebot without the carebot possessing any corresponding (internal) emotions. The philosopher John Searle [62] has demonstrated in his famous Chinese room illustration that AI systems lack true understanding of the Chinese language terms being processed by them. The computer receives inputs on Chinese language terms and produces Chinese characters as outputs. Despite not understanding these Chinese terms, the computer would be able to pass the Turing test [75] (that is, to convince a Chinese speaker that it possesses knowledge of the Chinese language) by \u201csimulating\u201d an understanding of the Chinese language. Thus, the computer does not possess a \u201cmind\u201d or \u201cthink\u201d the way humans do. Indeed, Turing [75] had remarked that the question \u201ccould a machine think?\u201d was in itself meaningless.",
            "cite_spans": [
                {
                    "start": 550,
                    "end": 552,
                    "mention": "62",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 904,
                    "end": 906,
                    "mention": "75",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 1150,
                    "end": 1152,
                    "mention": "75",
                    "ref_id": "BIBREF72"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Drawing an analogy from human\u2013human interactions, the fact that a human cannot read the (internal) minds and emotions of other humans does not prevent him from showing the external aspect of care to another human being. In truth, the \u201cappearance\u201d of emotions are not real human emotions. Nonetheless, the absence of (internal) human emotions in robots can be compensated for by the robots\u2019 external appearances and behaviour. Coeckelbergh [15] conjectured that advanced human\u2013robot communications can be analogised to normal human\u2013human interactions in that both can interpret \u201cthe other\u2019s appearance and behaviour as an emotion\u201d.14 This approach requires a shift in mindset \u201cfrom the \u2018inside\u2019 (what is \u2018in the mind\u2019 of robots) to the \u2018outside\u2019 (what robots do to us)\u201d in personal, social and emotional contexts.152.Deception",
            "cite_spans": [
                {
                    "start": 440,
                    "end": 442,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Indeed, it is this ability of carebots to exhibit \u201cexternal\u201d care that gives rise to the problem of deception. Carebots that aim to imitate a human companion or caregiver raise the possibility that the user (especially the vulnerable) will be unable to judge whether they are communicating with a real person or with technology. This could be experienced as a form of deception or fraud.16 Can the deception be regarded as benevolent or benign? Does that make the deception morally permissible?",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "The form that the carebot takes may be significant. Coeckelbergh et al. [17] indicated that zoomorphic robots may be less problematic than robots that look too much like humans. This does not imply that anthropomorphic designs should be prohibited. In fact, anthropomorphic designs of robots are arguably desirable if they advance the function of the technology17 such as to encourage the use of or interactions with a robot for the patient\u2019s well-being.",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 75,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Grodzinsky et al. [33] regarded deception by appearances as a question of trust.18 A robot is deceptive where it misleads the human user into believing that or behaving as if the robot is a human or animal. They argue that where the software developer has used deception in the design of the robot in order to help the user, the deception is considered benign and does not involve a breach of trust. For example, manipulations or deceptions of a robot for the purpose of calming a patient with dementia which might otherwise cause injury to the hospital staff or others would be benign without involving any breach of trust and are thereby morally permissible.19 The underlying basis for Grodzinsky et al. [33] appears to be the positive consequences generated or the avoidance of negative consequences or harms. This is analogous to the justification for therapeutic privilege exercised by doctors to withhold relevant information from the patient about their health condition if the disclosure of information is likely to cause serious physical or mental harm to the patient. But if the developer manipulates the user and does not act in the user\u2019s best interests, there is a breach of trust which cannot be condoned.20",
            "cite_spans": [
                {
                    "start": 19,
                    "end": 21,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 707,
                    "end": 709,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "The default position should be that deception is morally wrong due to deontological grounds (e.g., respect for humanity) and because they offend the virtues of honesty and trustworthiness. They argued that an exception should apply where the intention of the developer is ethical and the consequences are good.21 Grodzinsky et al. [33] also added that the deceptive feature should be an essential functionality of the robot Isaac and Bridewell22 argued that deception-capable robots can be ethical even when telling outright lies and that the ethicality of human or robot communication must be assessed with regard to its underlying motives (such as the achievement of pro-social goals and functions). However, Isaac and Bridewell were concerned with social interactions and conversations between robots and people generally as opposed to the relationship between carebots and the very young and the elderly who are vulnerable and require assistance.",
            "cite_spans": [
                {
                    "start": 332,
                    "end": 334,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Taking Grodzinsky et al. [33] as laying the basic requirements, I would argue that deception by carebots with regard to vulnerable persons may be allowed only if (a) the intention of the developer is ethical with respect to the care recipients, (b) the consequences are indeed positive for the care recipient, (c) there are no viable alternatives to the use of deception; and (d) the extent of infringement of the care recipient\u2019s autonomy is not more than that via other means. The additional two requirements (c) and (d) would further protect the vulnerable children and elderly from deceptive robots. Consider the analogy to lying to a dementia patient using Simulated Presence. This is a device developed for Alzheimer\u2019s patients comprising an audiotape that records the telephone conversation of the family member concerning his or her memories of the patient. Even if the tape is played back repeatedly via a device that looks like a telephone, the Alzheimer\u2019s patient may not realise and continue to regard it as a fresh conversation.23 This is a form of deception; however, the underlying motive is arguably to improve the patients\u2019 emotional well-being. It must be shown that the device would in fact enhance the patients\u2019 emotional well-being. It must also be asked whether there are alternatives to Simulated Presence which do not deceive and whether it might be feasible to instead invest time to carry out real conversations with patients.24 Where there are no viable alternatives to the use of the device, and the extent of infringement of personal autonomy is not more than via other means, such deception may be justified as benign vis-\u00e0-vis the care recipient and thus morally permissible.",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 28,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "The question of trust can also be assessed from the users\u2019 perspective which may in turn depend on the level of communications between the robot developers and users.25 It is possible that though the patients know that the robot is not real, they have chosen to invest emotionally in the robots nonetheless. Humans can trust artificial agents on an emotional level instead of merely being dependent on them for practical functionality even if the humans know for a fact that they are only machines.26 Mentally healthy humans who are aware that they are interacting with a robot are not deceived.27 In such a case, there is arguably no breach of trust.",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Hoorn and Winter [35] found in a study that, insofar as delivering bad health news to patients is concerned, the participants preferred the robot doctor and the robot\u2019s message to the human counterpart. Moreover, the robot garnered more compliance to the medical treatment. They noted that robots may outperform humans on emotional tasks and this can relieve physicians from the demanding duty of disclosing unfavourable information to a patient.3.Over-reliance on and over-attachment to carebots.",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 20,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Over-reliance can adversely affect both care recipients and caregivers. Caregivers may be over-reliant on robots to do the caring work, and technology (for example, to aid patients with motor impairment) may at times impede the improvement in health conditions of care recipients (where the care recipient refuses to make the attempt to walk without technological assistance). Vulnerable patients may also suffer from over-attachment to carebots. It is commonly found that children suffer from distress and grief when separated from their robot companions28 and the reliance on carebots without adult guidance may impede the development of interactive abilities of babies and infants in the long term.29 As mentioned above, carebots do not possess human attributes such as compassion.30 Clinical practice often involves complex judgments and abilities that AI technology is currently unable to replicate, such as contextual knowledge and the ability to read social cues.31 Concerns have been raised about a loss of human contact and increased social isolation if AI technologies are used to replace therapists or family time with patients.32",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "In addition, if caregivers rely on carebots to take over the caring tasks, less time will be spent on human\u2013human interactions. Vallor argued that caregiving teaches us reciprocity and empathy.33 Thus, carebots should not \u201cliberate\u201d us from care but instead provide support that draw us into care-giving practices.34 To mitigate the problem of over-reliance (and over-trust) by care recipients, human designers may consider suitable measures such as providing warning indicators or built-in tasks that requires the attention of the user from time to time.35",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Given the challenges of using carebots, it is proposed that carebots should not replace human caregivers in interacting directly with the patient but merely assist the human caregiver in supporting or taking care of the patient.36 Where the carebots are used as social companions and for social interactions together with human caregivers to care for those who are vulnerable, the risk of over-reliance or over-attachment to the robots is mitigated. Coeckelbergh et al.37 suggested that we should develop and use robots based on the notion of \u201csupervised autonomy\u201d. This would likely create more trust among stakeholders and improve the quality of the therapy. Vallor [77]38 has also noted that the use of carebots can deprive potential caregivers of goods (such as the virtues of reciprocity and empathy or, applying the capabilities approach,39 the preservation and enhancement of their human capacities for affiliation, practical reason and emotion) that she considered to be central to care practices.4.Informed consent to use of carebots and patient privacy",
            "cite_spans": [
                {
                    "start": 669,
                    "end": 671,
                    "mention": "77",
                    "ref_id": "BIBREF74"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "As the use of carebots may bring risks for the patients and users, the procedures of obtaining informed consent should be customised to ensure each patient understands the purpose and risks from using carebots. The patient\u2019s consent to the use of carebots has to be voluntarily and unequivocal. Greater caution should be exercised in the communication of the relevant information to children and elderly patients prior to obtaining consent. In particular, the patient\u2019s knowledge or lack thereof about the potential restraints on the patient\u2019s desire for independent living, and the deprivations of liberty or privacy that he or she may be subject to with the use of carebots should be taken into account; and such restraints or deprivations should be balanced against the potential benefits of safety and security that may be afforded by the carebots\u2019 role.40 For persons who are not mentally capable of giving consent such as the very young children or elderly patients with advanced dementia, there may be a need to consider proxy decision-making with respect to the use of carebots.41",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Carebots capture, store and process personal and sensitive data about the care recipient\u2019s health conditions and movements. They are networked devices that collect, store or process the data from various localities and in the cloud. The typical care recipient especially the young children and the elderly may not be aware of the significant data-processing capacity of carebots. Significant amounts of personal data and confidential information such as the health conditions and emotional responses of the care recipient during social interactions may be disclosed to and stored in the carebot42 to which the robotics companies have access. There is a risk that the collection and disclosure of the data by carebots during such interactions including intimate situations would infringe privacy rights and thereby cause embarrassment to users. There is an additional risk that the database containing information on the patients\u2019 health conditions might be hacked or retrieved by unauthorised third parties. It is indeed timely to pay more attention to security and privacy issues with respect to data used for communication between people and robots and artificial intelligence [23].43",
            "cite_spans": [
                {
                    "start": 1180,
                    "end": 1182,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "As a summary, it is pertinent to highlight that none of the abovementioned challenges (carebots\u2019 limited extent of care, the possibility of deception on vulnerable humans, (over)reliance and (over)attachment to carebots, and the lack of informed consent and potential infringement of users\u2019 privacy), serious as they are, would suggest that the use of all forms and types of care robots should be banned. Instead, we should pay close attention to the possibility of abuse and advocate the appropriate design of carebots according to ethical principles with a view to engender and maintain trust. We will now examine these concepts of trust and ethical design and their inter-relationship.",
            "cite_spans": [],
            "section": "Challenges in the Use of Carebots",
            "ref_spans": []
        },
        {
            "text": "Human beings seek to adapt to their environment by reducing complexity and uncertainty. Trust is one mechanism that allows humans to cope with this complexity and uncertainty [45]. On one level, human trust in another person or thing is based on belief. Gambetta [29] stated that trust depends on the \u201csubjective probability with which an agent assesses that another agent or group of agents will perform a particular function\u201d.44",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 178,
                    "mention": "45",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 264,
                    "end": 266,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "Taddeo [70] has laid further groundwork on the concept of trust (and e-trust). E-trust occurs in environments which there is no direct or physical contact and, in this regard, may not be entirely applicable to human interactions with social or care robots. However, certain concepts of trust remain relevant for our purposes. First, she noted that trust is a relation between A (the trustor) and B (the trustee). The trustee can be a human or artificial entity. Second, trust is a \u201cdecision\u201d by A to delegate to B some aspect of importance to A in achieving a goal in which A\u2019s decisions are \u201cdesigned and implemented with the assumption that there is a high probability that B will behave as expected\u201d. Third, Taddeo observed that trust involves risk and that \u201cthe less information the trustor A has about the trustee B, the higher the risk and the more trust is required\u201d. Fourthly, the A has the expectation of gain by trusting B. Fifthly, positive outcomes that are generated when A trusts B encourage A to continue trusting B.45 Taddeo\u2019s model essentially relied on the \u2018rational agent\u201d which is capable of making the \u201cbest option for itself, given a specific scenario and a goal to achieve\u201d.46 In Taddeo [71], she referred to the quantification of risks underlying trust based on the ratio of successful actions to the total number of actions necessary to achieve the goal.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 10,
                    "mention": "70",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 1211,
                    "end": 1213,
                    "mention": "71",
                    "ref_id": "BIBREF68"
                }
            ],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "Ferrario et al. [25] noted that apart from pragmatic reasons for trusting an AI system dependent on the expectation of gain from trusting, there may also be epistemic reasons for trusting which are based on the trustor\u2019s belief in the trustee\u2019s trustworthiness. The AI system may be viewed as trustworthy in (a) a relative sense in the context of the trust relationship from a person\u2019s perspective but not another or (b) an absolute sense where there are objective reasons that make the AI system trustworthy for everyone whatever the contexts. The model on trust discussed thus far can be further extended in three ways: first, trust is not based on purely cognitive or rational beliefs but there is the other aspect of \u201cattitude\u201d; second, by incorporating normative features and thirdly, by extending the range of stakeholders involved.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 19,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "First, human trust in care robots cannot be assessed based on rational grounds alone given the issues we have raised in the section above on subjective human perceptions of the roles, functions and appearances of care robots and potential deception by such robots. Jones [38]47 regarded trust as an \u201caffective attitude\u201d with the implication that we are justified to trust even when we are not justified in predicting a favourable outcome from the person being trusted. Gompei and Umemuro [31] have studied both concepts of cognitive and affective trust of humans as applied to their interactions with social robots.",
            "cite_spans": [
                {
                    "start": 272,
                    "end": 274,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 489,
                    "end": 491,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "On the second point relating to the normative features of trust, Buechner and Tavani [11] have described trust as \u201ca (moral or nonmoral) normative relationship affecting two agents\u201d\u2014A and B\u2014in which \u201cA has the disposition to normatively expect that B will do such and such responsibly.\u201d48 Tuomela and Hofmann [74] distinguished rational social normative trust (based on the trustor\u2019s social right to expect the trustee\u2019s intentional good-willed performance) from predictive trust and reliance which are reason-based beliefs about the trustee\u2019s intentional good-willed performance. Grounds for the abovementioned \u2018social right\u201d to expect the trustee\u2019s intentional good-willed performance include friendship, sincere agreement, and a relationship governed by mutually respected social norms.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 88,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 310,
                    "end": 312,
                    "mention": "74",
                    "ref_id": "BIBREF71"
                }
            ],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "Trust should not be defined narrowly as the \u201cdecision\u201d taken by the trustor with respect to the trustee or the delegated task. Rather, it can also explain the basis that underlies the decision (for example, as in the statement \u201cI decide to delegate to B because I trust B to achieve the goal\u201d). This will naturally lead to the question \u201cwhy should I trust B\u201d which in turns triggers the examination of the normative basis for trust.",
            "cite_spans": [],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "The third point concerns the range of stakeholder interests49 (that is, who do you trust?). Trustees in question may include not merely the artificial entity or carebot but also the manufacturer of the carebot or the software developers of the artificial intelligence system to be used for the carebot. The trustors cover consumers, users and members of the public. We should also consider the social norms applicable to the range of stakeholders whether it is between the consumers and users of care robots and/or the human designers of the care robots.",
            "cite_spans": [],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "In sum, trust is a relational and normative concept. The idea of trust implies some uncertainty or risks that delegation of task to artificial agent may not proceed as planned. Whether trust exists and the extent thereof depends on the trustor as a rational agent weighing the potential benefits and losses from reliance on the artificial systems. The reasons may be pragmatic or epistemic. Apart from the rational aspects, human affective attitudes towards care robots should be taken into account. Moreover, the applicable social norms and stakeholders\u2019 interests are also part of the trust concept.",
            "cite_spans": [],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "Trust is also an instrumental concept in that trust can lead to certain positive ends. With respect to care robots, engendering human trust in such robots will potentially facilitate its widespread use in the healthcare sector. Conversely, distrust arising from a mishap in the use of care robots can easily hamper the applications of care robots in the healthcare sector. At the same time, we should also guard against \u201covertrust\u201d of robots that can arise from automation bias where the human underestimates the \u201closs associated with a trust violation\u201d and/or \u201cthe chance the robot will make such a mistake\u201d.50",
            "cite_spans": [],
            "section": "Trust",
            "ref_spans": []
        },
        {
            "text": "Ethical Design refers to the process by which ethical values or principles are taken into account or embedded in the design process of a product or device or technology. The ethical values and principles may be taken into account in two ways via a \u201ctop-down\u201d approach (by feeding in advance a set of ethical principles embedded in the robot algorithm) or a \u201cbottom up\u201d approach (in which machine learning adapts and learns about an external set of values and principles based on the robots\u2019 observations of humans, human behaviours and the operative environment) or a combination of both approaches.51 The first two methods represent ideal types. The top-down approach selects in advance a theory to apply and analyses the requirements for the design of algorithms and subsystems in order to implement the theory.52 The task of determining the universal ethical values or principles in advance for carebots under the \u201ctop-down\u201d approach is at best unwise (and at worst, impossible). We need to pay attention to the particular contexts in which these values or principles are meant to apply.",
            "cite_spans": [],
            "section": "Ethical Design",
            "ref_spans": []
        },
        {
            "text": "On the other hand, the \u201cbottom-up\u201d approach assesses a task based on a performance measure and the outcomes are analysed after fulfilment of the performance measure in order to yield a theory.53 The expectation or demand that the machine learning platform in a carebot generate appropriate ethical values and principles from the carebot\u2019s perceptions of humans and human behavior would be impracticable. We would need to least supply the carebot a preliminary ethical framework or a set of ethical reference points.",
            "cite_spans": [],
            "section": "Ethical Design",
            "ref_spans": []
        },
        {
            "text": "In practice, ethical design is inevitably the product of both the \u201ctop-down\u201d and \u201cbottom-up\u201d approaches.54 In this regard, a basic ethical framework should be in place to guide the actions and decisions of the carebot in the initial stages; and the carebot should also be enabled to \u201clearn\u201d from the environment and contexts it encounters and be permitted to make adjustments to the ethical framework or even develop new moral norms within the parameters of the general ethical framework. As Wallach and Allen put it, it is the \u201cdynamic interplay\u201d between the analysis of project structure and testing of system designed to reach goals.55",
            "cite_spans": [],
            "section": "Ethical Design",
            "ref_spans": []
        },
        {
            "text": "The design process may cover aspects such as the physical form the robot takes, the way it communicates, the contents of the communication, the actions or practices it is capable of carrying out and the contexts in which they operate. To properly design carebots, the designer will have to be aware of the contexts in which the carebots will be utilised in the healthcare sector, the challenges arising from the use of robots in caregiving, and also their potential impact (in physical, mental and social terms) on humans (both care recipients and caregivers) in those specific contexts.",
            "cite_spans": [],
            "section": "Ethical Design",
            "ref_spans": []
        },
        {
            "text": "Similar to the concept of trust, there is also a need to take account of the range of stakeholders\u2019 interests\u2014of the patient and their loved ones, the healthcare professionals, caregivers, manufacturers and software developers\u2014in ethical design. Polls may, for example, be taken of potential users of carebots such as the aged56 in order to build \u201cdemocratic spaces\u201d for the voices of stakeholders to be heard.57 There is also a need to consider the cognitive dimension of the user and his needs in the design process so as to develop robot care technology or devices that he can better use or interact with.58",
            "cite_spans": [],
            "section": "Ethical Design",
            "ref_spans": []
        },
        {
            "text": "Public trust in technology is dependent on whether the technology generates benefits for humans, improves their well-being and ensures safety. Should there be an accident or mishap in its use, distrust can easily creep in. Whether trust can be regained will depend on the efficiency and reliability of investigations and the follow-up actions by the developer and authorities. Another aspect is the consistency of functionality and performance of the technology such as carebots. In this regard, artificial agents are generally more predictable than humans and hence more trustworthy.\n59 Whether the technology performs its role and functions for which it is designed (such as the roles of caring for its users\u2019 health and the enhancement of human\u2013robot interactions for users\u2019 mental well-being through, for example, displaying \u201csocially acceptable\u201d emotions)60 will be important for engendering trust in the technology. Finally, the ethical design of carebots can and should take account of potential risks arising from humans\u2019 automation bias that result in overtrust in robots.",
            "cite_spans": [],
            "section": "Ethical Design",
            "ref_spans": []
        },
        {
            "text": "Thus, in terms of concept and purpose, both trust and ethical design are well aligned insofar as the use of carebots is concerned. I will also argue that there are appropriate criteria we can consider for the promotion of trustworthy and ethically designed carebots in the healthcare context.",
            "cite_spans": [],
            "section": "Ethical Design",
            "ref_spans": []
        },
        {
            "text": "In addition to the ethical theories (utilitarianism, deontology, virtue ethics, and principlism) discussed above, there are several emerging ethical guidelines, standards and regulations pertaining specifically to the design of robots and other artificial intelligent systems.74 Regulation is itself a multi-faceted exercise comprising the different modalities of law, market, social norms, and technology. Most if not all of these standards are connected to and resonate with the existing ethical theories. They are also consistent with the emphasis on ethical design in this paper.",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "The European Parliament [22],75 as mentioned above, whilst acknowledging the potential capacity of robots to enhance the mobility and integration of people with disabilities and elderly people, stressed that \u201chumans will still be needed in caregiving and will continue to provide an important source of social interaction that is not fully replaceable\u201d. Subsequently, the European Parliament [23] noted the enabling capacity of AI and robotics to allow doctors and nurses to spend more time in high value activities including patient interaction,76 and at the same time, recognised the impact of the increased use of sensors in robotics to enable patients to have \u201cmore personalised treatment and services and receive care remotely from their own homes, while also generating more meaningful data\u201d.77",
            "cite_spans": [
                {
                    "start": 25,
                    "end": 27,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 393,
                    "end": 395,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "The IEEE Global Initiative on \u201cEthics of Autonomous and Intelligent Systems. Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems\u201d78 focus on the following general principles:Human Rights Ensure they do not infringe on internationally recognized human rights.Well-being Prioritize metrics of well-being in their design and use.Accountability Ensure that their designers and operators are responsible and accountable.Transparency Ensure they operate in a transparent manner.Awareness of misuse Minimize the risks of their misuse.",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "On human rights, according to the UN Convention on the Rights of Persons with Disabilities, the care recipient\u2019s desire for independent living should also be respected.79 A report by the Rathenau Instituut has recommended an individual\u2019s right to choose to have meaningful human contact rather than with a robot.80",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "The Foundation for Responsible Robotics81 highlights that \u201c[r]esponsible robotics starts before the robot has been constructed. Ethical decisionmaking begins in the R&D phase.\u201d The British Standards Institute Ethical Design of Robots (BS 8611) identifies potential ethical harms arising from robots and autonomous systems and provides guidelines on risk management associated with the ethical hazards. The standard covers safe design, protective measures and information for the design and application of different types of robots, including those used for industrial, personal care and medical purposes. According to the Engineering and Physical Sciences Research Council (EPSRC) Principles of Robotics 2011,82 in addition to ethical design to ensure safety and security, \u201c[r]obots should be designed; operated as far as is practicable to comply with existing laws & fundamental rights & freedoms, including privacy.\u201d Special reference was made to robots used in the healthcare sector and the importance of data privacy and protection:",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "a robot used in the care of a vulnerable individual may well be usefully designed to collect information about that person 24/7 and transmit it to hospitals for medical purposes. But the benefit of this must be balanced against that person\u2019s right to privacy and to control their own life e.g. refusing treatment. Data collected should only be kept for a limited time; again the law puts certain safeguards in place. Robot designers have to think about how laws like these can be respected during the design process (e.g. by providing off-switches).Further, \u201c[robots]\u2026 should not be designed in a deceptive way to exploit vulnerable users; instead their machine nature should be transparent.\u201d Two of Murphy and Wood\u2019s Three Laws of Responsible Robotics state that:A human may not deploy a robot without the human\u2013robot work system meeting the highest legal and professional standards of safety and ethics.A robot must respond to humans as appropriate for their roles.",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "In the Future of Life Institute Asilomar Principles for Beneficial AI (Jan 2017), the two principles with a focus on design are:Value Alignment Highly autonomous AI systems should be designed so that their goals and behaviors can be assured to align with human values throughout their operation.Human Values AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity.",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "In addition, ISO 1348283 specifies guidelines for safe design and protective measures in respect of personal care robots (mobile servant robot, physical assistant robot and person carrier robot).",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Drawing from the above discussion on ethical guidelines, standards and regulations, the ethical design of carebots should focus on a few crucial aspects: safety and security, respect for human rights such as the right to independent living, meaningful contact with humans, dignity and privacy, transparency, freedom from deception and the accountability of human designers of AI.84 Following Murphy and Wood, it is also important to consider the proper \u201crole\u201d morality of carebots in response to humans especially care recipients. Notwithstanding that the above ethical guidelines, standards and regulations are consistent with the main challenges with respect to carebots, they are fairly general in nature and do not provide sufficiently concrete guidance as to how the ethical design of carebots should be approached. I propose below the Ethics of Care as a central ethical framework with sub-level principles applicable to the caring contexts in the healthcare sector.",
            "cite_spans": [],
            "section": "Ethical Guidelines, Standards and Regulations on Robots ::: Finding the Right Ethical Framework for the Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "The story of Ethics of Care began with Gilligan\u2019s In a Different Voice in reaction to Kohlberg\u2019s scale of moral development from children to ethics grounded in universality and rationality in adulthood. Gilligan [30] argued that women tended to focus more on empathy, emotions and compassion when discussing moral dilemmas at the expense of arguments grounded in universality and rationality. One is not superior to the other; they are merely different approaches to morality. Several commentators have since built upon Gilligan\u2019s ideas and arguably established Ethics of Care either as an ethical approach that supplements existing ethical theories or as a standalone ethical theory.85",
            "cite_spans": [
                {
                    "start": 213,
                    "end": 215,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "Noddings [53] viewed ethics of care as imposing an obligation on us to act with an attitude or motive of caring towards others. Instead of abstract general ethical principles, the Ethics of Care is focused on the individual\u2019s emotional sensitivity to particular others. In addition, a person who cares for another is \u201cengrossed\u201d in that other person; the former does not impose his viewpoint on the other but rather gives due attention to the latter\u2019s perspective concerning the world and his or her relationship to the world.86 Thus, both Gilligan and Noddings take the position that \u201ccare ethics aims to meet the concrete needs of individuals in context-specific and responsive ways\u201d.87",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 12,
                    "mention": "53",
                    "ref_id": "BIBREF48"
                }
            ],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "Care Ethics, according to Virginia Held [34], is premised on a \u201ccaring relation\u201d and care is a \u201cpractice\u201d of responding to needs.88 For her, care is both a practice and a value. Care Ethics focuses on the agent\u2019s ability to engage in the practice of care and the exercise of that ability and not merely the agent\u2019s motives of caring.89 Persons are relational and interdependent not individualistic autonomous rational agents.90 Care Ethics also values emotions [34, p. 10].",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 43,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 462,
                    "end": 464,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "In contrast to Held, Slote [65] advocated an agent-based virtue ethics of caring. He viewed the caring person as a benevolent person who may prefer those who are near and dear. In Ethics of Care and Empathy, Slote [66] affirmed that ethics of care is rooted in \u201cmoral sentimentalism\u201d and that empathy is the \u201cprimary mechanism of caring, benevolence, compassion, etc.\u201d.91",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 30,
                    "mention": "65",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 215,
                    "end": 217,
                    "mention": "66",
                    "ref_id": "BIBREF62"
                }
            ],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "Tronto [72] arguably provides the most important set of concrete principles applicable to the caring contexts. First, she defined good care as comprising both a caring attitude and a caring activity. To her, care practices\u2014as opposed to mere tasks to be carried out\u2014involve both \u201cthought and action\u201d which are directed towards some end.92 Second, caring entails four phases: caring about, taking care of, care-giving and care-receiving.93 Tronto [73]94 argued for institutionalised caring based on the purpose of care, a recognition of power relations and the need for pluralistic and particular tailoring of care to meet individual needs. In essence, it is about \u201cpurpose, power and particularity\u201d. Tronto [73] also made the point that potential care recipients should be able to \u201cstate what their needs are\u201d and to make choices as to how their needs can be met.95 For the caregivers, consideration should be given to the negative aspect of the heavy burdens of caregiving to the extent that \u201cthere was no space in our lives outside of the circles of care\u201d.96 Tronto laid down four ethical elements of care:Attentiveness\u2014care requires a recognition of others\u2019 needs in order to respond to them.Responsibility\u2014that we take upon ourselves to care for others.Competence\u2014skills in which care is given.Responsiveness\u2014the \u201cresponsiveness of the care receiver to the care\u201d in order to guide the caregiver.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 10,
                    "mention": "72",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 447,
                    "end": 449,
                    "mention": "73",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 708,
                    "end": 710,
                    "mention": "73",
                    "ref_id": "BIBREF69"
                }
            ],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "Tronto\u2019s four elements connect well with the challenges encountered in the use of carebots as well as the tenets of Principlism and the basic ethical theories of Utilitarianism, Kantianism and Virtue ethics. The first element of attentiveness to patient needs constitutes one important step towards advancing respect for patient autonomy. This element recognises the need for the care recipient\u2019s privacy (both physical and informational privacy) to be respected. This would mean that the informed consent of care recipients to the use of carebots should be obtained. Yet the promotion of patient autonomy can at times conflict with the aim to meet their other needs (for example, when patients in exercising their autonomy refuse medical treatment or medication to cure their illness). Furthermore, some vulnerable patients may be mentally incapable of exercising autonomy in which case the decision to use carebots to cater to their needs must be made in their best interests upon taking into consideration their important physical and emotional needs Care ethics remains particularly relevant for such vulnerable and dependent persons such as patients, elderly, young children and disabled. In Love\u2019s Labor Revisited [41], Kittay argued that dependency is not something exceptional but is in fact \u201cintegral\u201d to human life. At the same time, care recipients may not wish to be overly dependent on human caregivers and in this regard, the use of carebots could \u201cliberate\u201d the care recipient from this feeling of dependence [8, p. 282].",
            "cite_spans": [
                {
                    "start": 1221,
                    "end": 1223,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 1525,
                    "end": 1526,
                    "mention": "8",
                    "ref_id": "BIBREF77"
                }
            ],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "The concept of \u201cbeneficence\u201d in Beauchamp and Childress\u2019 Principlism is related to the second element of \u201cresponsibility\u201d. Though we cannot always be certain that human caregivers are exhibiting genuine care for their care recipients, they must act professionally according to their roles to benefit the care recipients in catering to their physical and emotional needs. Vanlaere and Gastmans97 have sought to justify care ethics by reference to the notion of \u201cour own personhood and the personhood of the vulnerable other\u201d. It is ultimately about human \u201cdignity\u201d in the relationship between the caregiver as well as the care recipient.",
            "cite_spans": [],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "Competence is linked to the skills and efficiency of the caregiver in providing care. The achievement of excellence in one\u2019s craft or work (in this case, in providing care) is an Aristotelean virtue.",
            "cite_spans": [],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "Reciprocity is a common value in various ethical theories namely Utilitarianism, Kantianism and Virtue theory. Such a notion is captured in Tronto\u2019s fourth moral element of \u201cresponsiveness\u201d of the care recipient to the caregiver. The relationship is reciprocal in order to ensure that the caregiver\u2019s objectives to provide care to the care recipient are met. Held [34, p. 34, 35] opined that \u201c[c]aring is a relation in which carer and cared-for share an interest in their mutual well-being\u201d. Hence, Tronto\u2019s theory aims to benefit both caregiver and care recipient.",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 367,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "Justice at the societal level may at first blush be perceived as alien to the Ethics of Care with a focus on personal relationships. Noddings commented that caring is \u201cengrossment\u201d in the specific needs and desires of another person whilst justice is a commitment to abstract principles and rules. But Ethics of Care and justice are not diametrical opposites. Engster\u2019s theory of justice, for instance, is based on caring practices.98 In fact, he goes further to advocate that the government ought to establish institutional frameworks for supporting and accommodating caring practices.99",
            "cite_spans": [],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "The partiality towards or prioritisation of close relationships in the Ethics of Care does not present a problem for carebots as long as the needs of the patients under their assigned scope of duty are attended to. After all, similar to the role of the doctors, nurses and healthcare professionals, the carebot\u2019s role should be to act in the best interests of specific patients under its care. However, basic justice concerns should be adhered to. For example, patients in an institutional setting with similar needs and placed in similar circumstances should receive similar level of care from the assigned carebots within the institution.",
            "cite_spans": [],
            "section": "The Ethics of Care",
            "ref_spans": []
        },
        {
            "text": "van Wynsberghe [80]103 advocated the use of care values as the foundational values to be integrated into carebots (which she refers to as \u201ccare centered value-sensitive design\u201d). The value-based framework is meant to evaluate care robots in an ethical manner both retrospectively and prospectively. The retrospective stance allows the designers to consider the impact of their design on care practices. According to the prospective angle, the designers seek to integrate the ethical framework in the design process. Van Wynsberghe\u2019s analysis is premised on Tronto\u2019s care ethics, which are in turn based on the moral elements of attentiveness, responsibility, competence and reciprocity. In addition to providing a normative account of the values in care, she took the view that the ethical design process can \u201cfoster trust between the public and the resulting robots\u201d.",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 18,
                    "mention": "80",
                    "ref_id": "BIBREF78"
                }
            ],
            "section": "Care Ethics Offers Fairly Concrete Sub-level Principles for Application to (Health)care Context ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "According to van Wynsberghe [80], the ethical framework should cover the following components: context (hospital, nursing home or home), practice (such as lifting and feeding), the actors involved (nurse, patient and robot), the types of robots (enabling, assistance versus replacement) and the manifestation of moral elements (attentiveness, responsibility, competence and responsiveness).104",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 31,
                    "mention": "80",
                    "ref_id": "BIBREF78"
                }
            ],
            "section": "Care Ethics Offers Fairly Concrete Sub-level Principles for Application to (Health)care Context ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Attentiveness to care recipients\u2019 needs cover not only their physical needs but also their emotional needs for social interaction with human caregivers and carebots. Care recipients should be sufficiently informed of the scope of use of carebots and potential use of confidential information. The care recipient\u2019s right to independent living and meaningful contact with humans rather than robots if he so desires should not be ignored. Caregivers and carebots ought to respect the care recipients\u2019 physical privacy (such as by not eavesdropping on human conversations, and helping to mitigate the patient\u2019s potential embarrassment or distress in having to depend on their family members or relatives to take care of their toileting or bathing). Other examples include deactivating the video monitors during intimate procedures,105 and programming the carebots to announce their presence in the midst of a human being especially in private spaces.",
            "cite_spans": [],
            "section": "Care Ethics Offers Fairly Concrete Sub-level Principles for Application to (Health)care Context ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Although carebots cannot exhibit genuine human care from the \u201cinternalist\u201d perspective, they can nonetheless discharge their responsibility towards care recipients by showing outwardly caring behaviour towards them using AI sensors and technology. Meacham and Studley [46] note that a caring relation is based on a care environment formed by gestures, movements and articulations that express attentiveness and responsiveness to vulnerabilities within the relevant context. This partially endorses Tronto\u2019s elements of attentiveness and responsiveness. Carebots should not be used to deceive vulnerable care recipients into thinking they are human caregivers unless the underlying motive or purpose is to improve the well-being of the care recipient, the well-being of the care recipient is indeed enhanced, there are no viable alternatives to achieve that purpose and the infringement is not more serious than other means.",
            "cite_spans": [
                {
                    "start": 269,
                    "end": 271,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                }
            ],
            "section": "Care Ethics Offers Fairly Concrete Sub-level Principles for Application to (Health)care Context ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Carebots should be designed to have the competence to carry out their tasks to fetch items for care recipients, aid their transfer from bed to wheelchair and be equipped with adequate machine learning to interact effectively with the care recipients. This may also include the development of acceptable AI models of emotions displayed by the robots in their interactions with the users [55].",
            "cite_spans": [
                {
                    "start": 387,
                    "end": 389,
                    "mention": "55",
                    "ref_id": "BIBREF50"
                }
            ],
            "section": "Care Ethics Offers Fairly Concrete Sub-level Principles for Application to (Health)care Context ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "The responsiveness of care recipients to guide caregivers in their caring roles suggest a notion of limited reciprocity which is likely to depend in practice on the mental and physical condition of the care recipients to respond to the caregiver. In line with Vanlaere and Gastmans\u2019 \u201cconcept of \u201cpersonhood\u201d above, Khosla et al. [39] also referred to the need for social robots to be designed to enable personalisation of its services and contents to suit the preferences and health conditions of aged people with dementia. They opined that designing carebots with the social context in mind can \u201cfacilitate a long-term meaningful reciprocal relationship between social robots and people with dementia\u201d.106",
            "cite_spans": [
                {
                    "start": 330,
                    "end": 332,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Care Ethics Offers Fairly Concrete Sub-level Principles for Application to (Health)care Context ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "We have already encountered the material differences amongst the ethical theories. Nevertheless, Care Ethics complemented by Principlism cohere with certain aspects of the ethical theories discussed above. For example, Utilitarianism\u2014to the extent that it recognises the importance of satisfaction of needs as part of the principle of maximising the sum of happiness to the greatest number in the weighing of inter-subjective preferences\u2014is relevant to Tronto\u2019s Ethics of Care that incorporates the moral element relating to Attentiveness to Needs. Utilitarianism also resonates with certain aspects of Principlism in that the former considers the avoidance of harms (non-maleficence) and the generation of benefits (Beneficence) in the overall utilitarian calculus.",
            "cite_spans": [],
            "section": "Coherence with Other Ethical Theories and Principlism ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Deontology is not merely about rational and universal principles. The root of deontology also arises from the sentimental and emotional aspects of human nature.107 The Kantian categorical imperative to respect humanity as an end in itself is a case in point. Humanity can only be properly appreciated when we consider the rational, intellectual as well as emotional aspects of being human. In this regard, we need to respect the autonomy of the mentally capable patients and elderly to make decisions for their own health, an important ingredient in Principlism. The responsibility to take care of others in the Ethics of Care is a duty or obligation of beneficence that is derived from Kantian categorical imperatives and is again in line with Principlism.",
            "cite_spans": [],
            "section": "Coherence with Other Ethical Theories and Principlism ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Bearing in mind the critique of virtue ethics that it depends entirely on what a virtuous agent would choose as a right action without any epistemological grounding, Swanton [69]108 proposed a target-centred virtue ethics according to context. The aim of virtues of practice\u2014which according to Swanton includes virtues of focus on a problem, creative virtues and \u201cDewey\u2019s imaginative deliberation\u201d and virtues of dialogue109\u2014is to arrive at right solutions by acting in a virtuous way to solve problems. Every problem or dilemma comes with constraints on its solutions (such as time and costs, conflicts amongst virtues and their targets). Two virtues may seem to conflict thus preventing the agent from making a decision without sacrificing a virtue. The virtuous agent, when he or she encounters an ethical problem requiring a decision, will seek to \u201cintegrate constraints on solutions\u201d to the problems. This may sometimes involve modifying the initial constraints to solutions in a way that increases the possibilities for resolution. Swanton [69] gave the example where the initial constraint of \u201ckeeping promise to children\u201d may be modified to \u201cBe sincere. Show respect to the children and consult with them. Maintain trust, enjoyment\u201d.110 This modification is made pursuant to the exercise of virtues of practice. Resolving open-ended problems requires practical wisdom, experience and expertise.111",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 177,
                    "mention": "69",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 1047,
                    "end": 1049,
                    "mention": "69",
                    "ref_id": "BIBREF65"
                }
            ],
            "section": "Coherence with Other Ethical Theories and Principlism ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "This target-centred approach to virtue ethics does not require the agent to act from inner motives. This approach would prima facie suit robots which actions are not actuated by inner motives. However, to apply this target-centred virtue ethics, the algorithm would have to be capable of modifying the initial constraint from a strict principle (or prohibition) into one that is more general and contextual based on the virtues of practice. The added complication is that the processes for exercising these latter virtues are themselves open-ended or ill-structured rendering the task of designing algorithms to capture the nuances difficult indeed. At the same time, this complication reflects the unpredictable workings of human consciousness and behaviour. The human mind, according to Swanton [69, p. 280], is a \u201cpattern completer\u201d rather than a \u201clogic machine\u201d applying strict rules.",
            "cite_spans": [
                {
                    "start": 798,
                    "end": 800,
                    "mention": "69",
                    "ref_id": "BIBREF65"
                }
            ],
            "section": "Coherence with Other Ethical Theories and Principlism ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Principlism (with the norms of non-maleficence, beneficence and autonomy) can be integrated into the care practices. The example of robots administering medication to patients in a hospital or nursing home is apt. The design of the carebot must strike a judicious balance between giving the patient autonomy to decide and ensuring that potential harms to the patient\u2019s physical and mental health are avoided and that his or her health needs are met. Though carebots are not capable of exhibiting genuine human care towards others, they must be designed to exhibit caring behaviour towards the care recipients and promote their well-being (beneficence) at least from the \u201cexternal\u201d angle. Moreover, through empathic caring for others which also involves an obligation to respect them, the \u201crelational\u201d character of autonomy is emphasised in Care Ethics [66, p. 56].",
            "cite_spans": [
                {
                    "start": 853,
                    "end": 855,
                    "mention": "66",
                    "ref_id": "BIBREF62"
                }
            ],
            "section": "Coherence with Other Ethical Theories and Principlism ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "For carebots, the idea of justice in the macro-societal context may not be necessary. The carebot is typically assigned to an individual patient in a home setting or a group of patients within a hospital ward. Within the institutional care context, for example, carebots can exhibit a limited form of justice in the fair allocation of care to the care recipients. It may be based on a few factors: the proper assignment of a carebot to an approximately equal number of patients with similar needs, a first-come-first-served rule to serve new patients, and the possibility of re-allocation of assigned tasks in emergency situations.",
            "cite_spans": [],
            "section": "Coherence with Other Ethical Theories and Principlism ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "We have already discussed the close relationships between trust and ethical design. Baier [5] specially underscored trust, a basic relation between particular persons, as the fundamental concept of morality.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 92,
                    "mention": "5",
                    "ref_id": "BIBREF44"
                }
            ],
            "section": "Close Associations Between Care Ethics and Trust ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "It is argued that adherence to Care Ethics promotes trust. Held [34, p. 92] stated that \u201ccare is not the same thing as trust, but caring relations should be characterized by trust, and caring and trust sustain each other\u201d.112 For her, trust is important for care but it is not enough. There is still the need to do the work of care. Tronto\u2019s care ethics emphasised the relationship between the caregiver and care recipient and the latter\u2019s needs. This focus on the relationship between the parties advances mutual bonding and trust. Kittay [41, p. 248] noted that when a person has to depend on another, he or she also learns to trust.",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 67,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 541,
                    "end": 543,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                }
            ],
            "section": "Close Associations Between Care Ethics and Trust ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Nissenbaum [52] related trust to security in the online context; however, it is not a straightforward and linear relationship. For care robots, the safety aspects of the care robots and the protection of the privacy of the patients and users will enhance their physical and psychological security and build trust. Nevertheless, it is merely one of the factors. Other factors include the competence of carebots, the promotion of wellbeing (beneficence) and the responsiveness of care recipients towards the caregivers and the care provided.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 14,
                    "mention": "52",
                    "ref_id": "BIBREF47"
                }
            ],
            "section": "Close Associations Between Care Ethics and Trust ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "In sum, trust as a normative concept is promoted when care ethics is based on the competence of the carebots in an assistive role to benefit care recipients, the proper allocation of tasks and shared roles between the carebots, human caregivers, designers and even the care recipients themselves as well as the expectations engendered by care practices which should be conducted in a responsible manner towards the care recipients.",
            "cite_spans": [],
            "section": "Close Associations Between Care Ethics and Trust ::: Care Ethics for Trust in and Ethical Design of Carebots",
            "ref_spans": []
        },
        {
            "text": "Carebots serve as a source of companionship and reminders for patients, the elderly and children and are able to alleviate the manual burdens and work of caregivers in taking care of such patients and vulnerable persons. The use of carebots encounters certain challenges as to the extent of care they are capable of giving, the problem of deception arising from the anthropomorphic form and appearance of robots, the concern with overreliance and attachment to the carebots, and the need to obtain informed consent and to show respect for the privacy interests of patients and vulnerable persons.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "Trust is needed in order to enable the adoption of carebots in the healthcare context so that the benefits from the use of carebots can be optimised and risks minimised. Trust and ethical design are intertwined in the context of carebots in the healthcare for vulnerable patients, children and the elderly.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "Care Ethics as the central ethical framework is capable of dealing with the abovementioned challenges. It has the potential to form the bedrock of an appropriate ethical framework for the design of carebots in the healthcare context. Ethical design and trust, though separate concepts, are mutually reinforcing insofar as carebots are concerned. Care Ethics is particularly relevant for vulnerable and dependent persons such as patients, elderly, young children and disabled. Tronto\u2019s version of Care Ethics offers concrete sub-principles based on the moral elements of attentiveness to needs, responsibility, competence and responsiveness that are applicable to carebots in the various caring contexts, and are not inconsistent with certain aspects of the main ethical theories (Utilitarianism, Deontology and Virtue Ethics). The three facets of Principlism (namely non-maleficence, beneficence and autonomy) and to a lesser extent, justice concerns, can also be integrated into the care practices.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        },
        {
            "text": "It is an on-going process to ensure proper fit between Care Ethics in the design of carebots and public and user trust according to the different contexts of use in the healthcare sector. To promote the feasibility of the ethical design, the interests and needs of multiple stakeholders namely the needs and viewpoints of care recipients and their family members and the impact on human caregivers and healthcare professionals should also be taken into consideration at the design stage.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {},
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Clinical application of a humanoid robot in pediatric cancer interventions",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Alemi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ghanbarzadeh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Meghdari",
                    "suffix": ""
                },
                {
                    "first": "LJ",
                    "middle": [],
                    "last": "Moghadam",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Int J Soc Robot",
            "volume": "8",
            "issn": "",
            "pages": "743-759",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Robot emotion: a functional perspective",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Breazeal",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Brooks",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Who needs emotions? The brain meets the robot",
            "volume": "",
            "issn": "",
            "pages": "271-310",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Trust and multi-agent systems: applying the \u2018diffuse, default model\u2019 of trust to experiments involving artificial agents",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Buechner",
                    "suffix": ""
                },
                {
                    "first": "HT",
                    "middle": [],
                    "last": "Tavani",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Ethics Inf Technol",
            "volume": "13",
            "issn": "1",
            "pages": "39-51",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "How the machine \u2018thinks\u2019: Understanding opacity in machine learning algorithms",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Burrell",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Big Data Soc",
            "volume": "3",
            "issn": "1",
            "pages": "1-12",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "A principle-based approach",
            "authors": [
                {
                    "first": "JF",
                    "middle": [],
                    "last": "Childress",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "A companion to bioethics",
            "volume": "",
            "issn": "",
            "pages": "67-76",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Personal robots, appearance, and human good: a methodological reflection on roboethics",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Coeckelbergh",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Int J Soc Robot",
            "volume": "1",
            "issn": "",
            "pages": "217-221",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Healthcare, capabilities and AI assistive technologies",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Coeckelbergh",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Ethic Theory Moral Pract",
            "volume": "13",
            "issn": "",
            "pages": "181-190",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Care robots, virtual virtue, and the best possible life",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Coeckelbergh",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "The good life in a technological age",
            "volume": "",
            "issn": "",
            "pages": "281-292",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "A survey of expectations about the role of robots in robot-assisted therapy for children with ASD: ethical acceptability, trust, sociability, appearance, and attachment",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Coeckelbergh",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Pop",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Simut",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Peca",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pintea",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Vanderborght",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Sci Eng Ethics",
            "volume": "22",
            "issn": "1",
            "pages": "47-65",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "Who\u2019s Johnny? An anthropomorphic framing in human\u2013robot interaction, integration, and policy",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Darling",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robot ethics 2.0: from autonomous cars to artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "173-188",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "Towards interactive robots in autism therapy: background, motivation and challenges",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Dautenhahn",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Werry",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Pragmat Cogn",
            "volume": "12",
            "issn": "1",
            "pages": "1-35",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Engster",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "The heart of justice",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Transparency you can trust: transparency requirements for artificial intelligence between legal norms and contextual concerns",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Felzmann",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fosch-Villaronga",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lutz",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tamo-Larrieux",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Big Data Soc January to June",
            "volume": "6",
            "issn": "1",
            "pages": "1-14",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "In AI we trust incrementally: a multi-layer model of trust to analyze human-artificial intelligence interactions",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ferrario",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Loi",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Vigan\u00f2",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Philos Technol",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1007/s13347-019-00378-3"
                ]
            }
        },
        "BIBREF18": {
            "title": "\u00cf\u2019ll take care of you\u201d, said the robot",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fosch-Villaronga",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Albo-Canals",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Paladyn J Behav Robot",
            "volume": "10",
            "issn": "",
            "pages": "77-93",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "\u201cRegulation, I presume?\u201d Said the robot\u2014towards an iterative regulatory process for robot governance",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fosch-Villaronga",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Heldeweg",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Comput Law Secur Rev",
            "volume": "34",
            "issn": "6",
            "pages": "1258-1277",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "The progressive intertwinement between design, human needs and the regulation of care technology: the case of lower-limb exoskeletons",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fosch-Villaronga",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "\u00d6zcan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Int J Soc Robot",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1007/s12369-019-00537-8"
                ]
            }
        },
        "BIBREF21": {
            "title": "Can we trust trust?",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Gambetta",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Trust: making and breaking cooperative relations",
            "volume": "",
            "issn": "",
            "pages": "213-238",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Robot be Good: a call for ethical autonomous machines",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                },
                {
                    "first": "SL",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Sci Am",
            "volume": "303",
            "issn": "4",
            "pages": "72-77",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gilligan",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "In a different voice: psychological theory and women\u2019s development",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Factors and development of cognitive and affective trust on social robots",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Gompei",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Umemuro",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Lecture notes in computer science",
            "volume": "",
            "issn": "",
            "pages": "45-54",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "Developing artificial agents worthy of trust: \u201cWould you buy a used car from this artificial agent?\u201d",
            "authors": [
                {
                    "first": "FS",
                    "middle": [],
                    "last": "Grodzinsky",
                    "suffix": ""
                },
                {
                    "first": "KW",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "MJ",
                    "middle": [],
                    "last": "Martin",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Ethics Inform Tech",
            "volume": "13",
            "issn": "1",
            "pages": "17-27",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "Developing automated deceptions and the impact on trust",
            "authors": [
                {
                    "first": "FS",
                    "middle": [],
                    "last": "Grodzinsky",
                    "suffix": ""
                },
                {
                    "first": "KW",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "MJ",
                    "middle": [],
                    "last": "Wolf",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Philos Technol",
            "volume": "28",
            "issn": "",
            "pages": "91-105",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Held",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "The ethics of care",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "Here comes the bad news: doctor robot taking over",
            "authors": [
                {
                    "first": "JF",
                    "middle": [],
                    "last": "Hoorn",
                    "suffix": ""
                },
                {
                    "first": "SD",
                    "middle": [],
                    "last": "Winter",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int J Soc Robot",
            "volume": "10",
            "issn": "4",
            "pages": "519-535",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "Social and assistive robotics in dementia care: ethical recommendations for research and practice",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ienca",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Jotterand",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Vica",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Elger",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Int J Soc Robot",
            "volume": "8",
            "issn": "",
            "pages": "565-573",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "White lies on silver tongues: why robots need to deceive (and how)",
            "authors": [
                {
                    "first": "AMC",
                    "middle": [],
                    "last": "Isaac",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Bridewell",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robot ethics 2.0: from autonomous cars to artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "157-172",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "Trust as an affective attitude",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Ethics Inf Technol",
            "volume": "107",
            "issn": "1",
            "pages": "4-25",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "Human robot engagement and acceptability in residential aged care",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Khosla",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "M-T",
                    "middle": [],
                    "last": "Chu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int J Hum Comput Interact",
            "volume": "33",
            "issn": "6",
            "pages": "510-522",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "What should we want from a robot ethic?",
            "authors": [
                {
                    "first": "PM",
                    "middle": [],
                    "last": "Asaro",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Int Rev Inf Ethics",
            "volume": "6",
            "issn": "12",
            "pages": "9-16",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "Trust and human\u2013robot interactions",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kirkpatrick",
                    "suffix": ""
                },
                {
                    "first": "EN",
                    "middle": [],
                    "last": "Hahn",
                    "suffix": ""
                },
                {
                    "first": "AJ",
                    "middle": [],
                    "last": "Haufler",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robot ethics 2.0: from autonomous cars to artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "142-156",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "Love\u2019s labor revisited",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kittay",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Hypatia",
            "volume": "17",
            "issn": "3",
            "pages": "237-250",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "Regulatory challenges of robotics: some guidelines for addressing legal and ethical issues",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Leenes",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Palmerini",
                    "suffix": ""
                },
                {
                    "first": "B-J",
                    "middle": [],
                    "last": "Koops",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bertolini",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Salvini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Lucivero",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Law Innov Technol",
            "volume": "9",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "When robots care: public deliberations on how technology and humans may support independent living for older adults",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lehoux",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Grimard",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Soc Sci Med",
            "volume": "211",
            "issn": "",
            "pages": "330-337",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Loder",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Nicholas",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Confronting Dr Robot: creating a people-powered future for AI in health",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Luhmann",
                    "suffix": ""
                }
            ],
            "year": 1979,
            "venue": "Trust and power",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "Could a robot care? It\u2019s all in the movement",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Meacham",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Studley",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robot ethics 2.0: from autonomous cars to artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "97-112",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "Arash: a social robot buddy to support children with cancer in a hospital environment",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Meghdari",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shariati",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alemi",
                    "suffix": ""
                },
                {
                    "first": "GR",
                    "middle": [],
                    "last": "Vossoughi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Eydi",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc Inst Mech Eng",
            "volume": "232",
            "issn": "6",
            "pages": "605-618",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "Design performance characteristics of a social robot companion \u201cArash\u201d for pediatric hospitals",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Meghdari",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shariati",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alemi",
                    "suffix": ""
                },
                {
                    "first": "AA",
                    "middle": [],
                    "last": "Nobaveh",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int J Humanoid Rob",
            "volume": "15",
            "issn": "5",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF43": {
            "title": "Design and realization of a sign language educational humanoid robot",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Meghdari",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alemi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zakipour",
                    "suffix": ""
                },
                {
                    "first": "SA",
                    "middle": [],
                    "last": "Kashanian",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "J Intell Rob Syst",
            "volume": "95",
            "issn": "",
            "pages": "3-17",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "Hume: the woman\u2019s moral theorist?",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Baier",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "Women and moral theory",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "Two principles for robot ethics",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Metzinger",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Robotik und Gesetzbung",
            "volume": "",
            "issn": "",
            "pages": "263-302",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "The doctor will not see you now",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mittelstadt",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "3TH1CS: a reinvention of ethics in the digital age?",
            "volume": "",
            "issn": "",
            "pages": "68-77",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "Securing trust online: wisdom or oxymoron",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Nissenbaum",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Boston Univ Law Revew",
            "volume": "81",
            "issn": "3",
            "pages": "635-664",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Noddings",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Caring: a relational approach to ethics and moral education",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF49": {
            "title": "",
            "authors": [
                {
                    "first": "MC",
                    "middle": [],
                    "last": "Nussbaum",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Frontiers of justice: disability, nationality, species membership",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF50": {
            "title": "The essence of ethical reasoning in robot\u2013emotion processing",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ojha",
                    "suffix": ""
                },
                {
                    "first": "M-A",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Johnston",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int J Socl Robot",
            "volume": "10",
            "issn": "",
            "pages": "211-223",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF51": {
            "title": "Lifting the burden of women\u2019s care work: should robots replace the \u201chuman touch\u201d?",
            "authors": [
                {
                    "first": "JA",
                    "middle": [],
                    "last": "Parks",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Hypatia",
            "volume": "25",
            "issn": "",
            "pages": "100-120",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF52": {
            "title": "Human\u2013robot facial expression reciprocal interaction platform: case studies on children with autism",
            "authors": [
                {
                    "first": "AG",
                    "middle": [],
                    "last": "Pour",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Taheri",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alemi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Meghdari",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int J Soc Robot",
            "volume": "10",
            "issn": "",
            "pages": "179-198",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF53": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF54": {
            "title": "",
            "authors": [
                {
                    "first": "WD",
                    "middle": [],
                    "last": "Ross",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "The right and the good",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF55": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF56": {
            "title": "On ethical, legal and social issues of care robots",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Salvini",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Intelligent assistive robots: recent advances in assistive robots for everyday activities",
            "volume": "",
            "issn": "",
            "pages": "431-445",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF57": {
            "title": "Telling the truth: the ethics of deception and white lies in dementia care",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schermer",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "The law and ethics of dementia",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF58": {
            "title": "Minds, brains and programs",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Searle",
                    "suffix": ""
                }
            ],
            "year": 1980,
            "venue": "Behav Brain Sci",
            "volume": "3",
            "issn": "3",
            "pages": "417-457",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF59": {
            "title": "Living with robots: ethical tradeoffs in eldercare",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sharkey",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sharkey",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Close engagements with artificial companions: key social, psychological, ethical and design issues",
            "volume": "",
            "issn": "",
            "pages": "245-256",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF60": {
            "title": "Children, the elderly and interactive robots",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sharkey",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sharkey",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Robot Autom Mag",
            "volume": "18",
            "issn": "1",
            "pages": "32-38",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF61": {
            "title": "",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Slote",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Morals from motives",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF62": {
            "title": "",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Slote",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "The ethics of care and empathy",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF63": {
            "title": "Robot carers, ethics, and older people",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Sorell",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Draper",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Ethics Inf Technol",
            "volume": "16",
            "issn": "",
            "pages": "183-195",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF64": {
            "title": "In the hands of machines? The future of aged care",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Sparrow",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sparrow",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Mind Mach",
            "volume": "16",
            "issn": "2",
            "pages": "141-161",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF65": {
            "title": "",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Swanton",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Virtue ethics: a pluralistic view",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF66": {
            "title": "",
            "authors": [
                {
                    "first": "TL",
                    "middle": [],
                    "last": "Beauchamp",
                    "suffix": ""
                },
                {
                    "first": "JF",
                    "middle": [],
                    "last": "Childress",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Principles of biomedical ethics",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF67": {
            "title": "Defining trust and E-trust: from old theories to new problems",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Taddeo",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Int J Technol Hum Int",
            "volume": "5",
            "issn": "2",
            "pages": "23-35",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF68": {
            "title": "Modeling trust in artificial agents: a first step toward the analysis of e-trust",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Taddeo",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Mind Mach",
            "volume": "29",
            "issn": "2",
            "pages": "243-257",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF69": {
            "title": "",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tronto",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Moral boundaries: a political argument for an ethic of care",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF70": {
            "title": "Creating caring institutions: politics, plurality, and purpose",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tronto",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Ethics Soc Welf",
            "volume": "4",
            "issn": "2",
            "pages": "158-171",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF71": {
            "title": "Simulating rational social normative trust, predictive trust, and predictive reliance between agents",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tuomela",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hofmann",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Ethics Inf Technol",
            "volume": "5",
            "issn": "",
            "pages": "163-176",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF72": {
            "title": "Computing Machinery and Intelligence",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Turing",
                    "suffix": ""
                }
            ],
            "year": 1950,
            "venue": "Mind",
            "volume": "LIX",
            "issn": "236",
            "pages": "433-460",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF73": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF74": {
            "title": "Carebots and caregivers: sustaining the ethical ideal of care in the twenty-first century",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vallor",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Philos Technol",
            "volume": "24",
            "issn": "",
            "pages": "251-268",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF75": {
            "title": "",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vallor",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Technology and the virtues",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF76": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF77": {
            "title": "Robot caregivers: harbingers of expanded freedom for all?",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Borenstein",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Pearson",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Ethics Inf Technol",
            "volume": "12",
            "issn": "3",
            "pages": "277-288",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF78": {
            "title": "Designing robots for care: care centered value-sensitive design",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "van Wynsberghe",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Sci Eng Ethics",
            "volume": "19",
            "issn": "",
            "pages": "407-433",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF79": {
            "title": "The use of care robots in aged care: a systematic review of argument-based ethics literature",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Vandemeulebroucke",
                    "suffix": ""
                },
                {
                    "first": "BD",
                    "middle": [],
                    "last": "de Casterle",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gastmans",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Arch Gerontol Geriatr",
            "volume": "74",
            "issn": "",
            "pages": "15-25",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF80": {
            "title": "A personalist approach to care ethics",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Vanlaere",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gastmans",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Nurs Ethics",
            "volume": "18",
            "issn": "2",
            "pages": "161-173",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF81": {
            "title": "A vision of responsible innovation",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Von Schomberg",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Responsible innovation",
            "volume": "",
            "issn": "",
            "pages": "51-74",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF82": {
            "title": "Overtrust in the robotic age: the ethical challenge",
            "authors": [
                {
                    "first": "AR",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Borenstein",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Commun ACM",
            "volume": "61",
            "issn": "99",
            "pages": "22-24",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF83": {
            "title": "A pilot study with a novel setup for collaborative play of the humanoid robot KASPAR with children with Autism",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wainer",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Dautenhahn",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Robins",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Amirabdollahian",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Int J Soc Robot",
            "volume": "6",
            "issn": "1",
            "pages": "45-65",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF84": {
            "title": "",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wallach",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Allen",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Moral machines: teaching robots rights from wrong",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF85": {
            "title": "Ethical governance is essential to building trust in robotics and artificial intelligence systems",
            "authors": [
                {
                    "first": "AFT",
                    "middle": [],
                    "last": "Winfield",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jirotka",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Philos Trans R Soc A",
            "volume": "376",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF86": {
            "title": "Pediatric robots and ethics: the robot is ready to see you now, but should it be trusted?",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Borenstein",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "AR",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Robot ethics 2.0: from autonomous cars to artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "127-141",
            "other_ids": {
                "DOI": []
            }
        }
    }
}