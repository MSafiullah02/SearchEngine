{
    "paper_id": "PMC7148059",
    "metadata": {
        "title": "SlideImages: A Dataset for Educational Image Classification",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "David",
                "middle": [],
                "last": "Morris",
                "suffix": "",
                "email": "David.Morris@tib.eu",
                "affiliation": {}
            },
            {
                "first": "Eric",
                "middle": [],
                "last": "M\u00fcller-Budack",
                "suffix": "",
                "email": "Eric.Mueller@tib.eu",
                "affiliation": {}
            },
            {
                "first": "Ralph",
                "middle": [],
                "last": "Ewerth",
                "suffix": "",
                "email": "Ralph.Ewerth@tib.eu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Convolutional neural networks (CNNs) are making great strides in computer vision, driven by large datasets of annotated photos, such as ImageNet [1]. Many images relevant for information retrieval, such as charts, tables, and diagrams, are created with software rather than through photography or scanning.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 147,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "There are several applications in information retrieval for a robust classifier of educational illustrations. Search tools might directly expose filters by predicted label, natural language systems could choose images by type based on what information a user is seeking. Further analysis systems could be used to extract more information from an image to be indexed based on its class. In this case, we have classes such as pie charts and x-y graphs that indicate what type of information is in the image (e.g., proportions, or the relationship of two numbers) and how it is symbolized (e.g., angular size, position along axes).",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Most educational images are created with software and are qualitatively different from photos and scans. Neural networks designed and trained to make sense of the noise and spatial relationships in photos are sometimes suboptimal for born-digital images and educational images in general.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Educational images and illustrations are under-served in training datasets and challenges. Competitions such as the Contest on Robust Reading for Multi-Type Web Images [2] and ICDAR DeTEXT [3] have shown that these tasks are difficult and unsolved. Research on text extraction such as Morris et al. [4] and Nayef and Ogier [5] has shown that even noiseless born-digital images are sometimes better analyzed with neural nets than with handcrafted features and heuristics. Born-digital and educational images need further benchmarks on challenging information retrieval tasks in order to test generalization.",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 170,
                    "mention": "2",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 190,
                    "end": 191,
                    "mention": "3",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 300,
                    "end": 301,
                    "mention": "4",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 324,
                    "end": 325,
                    "mention": "5",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we introduce SlideImages, a dataset which targets images from educational presentations. Most of these educational illustrations are created with diverse software, so the same symbols are drawn in different ways in different parts of the image. As a result, we expect that effective synthetic datasets will be hard to create, and methods effective on SlideImages will generalize well to other tasks with similar symbols. SlideImages contains eight classes of image types (e.g. bar charts and x-y plots) and a class for photos. The labels we have created were made with information extraction for image summarization in mind.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In the rest of this paper, we discuss related work in Sect. 2, details about our dataset and baseline method in Sect. 3, results of our baseline method in Sect. 4, and conclude with a discussion of potential future developments in Sect. 5.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Prior information retrieval publications used or could use document figure classification. Charbonnier et al. [6] built a search engine with image type filters. Aletras and Mittal [7] automatically label topics in photos. Kembhavi et al.\u2019s [8] diagram analysis assumes the input figure is a diagram. Hiippala and Orekhova extended that dataset by annotating it in terms of Relational Structure Theory, which implies that the same visual features communicate the same semantic relationships. De Herrera et al. [9] seek to classify image types to filter their search for medical professionals.",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 112,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 181,
                    "end": 182,
                    "mention": "7",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 241,
                    "end": 242,
                    "mention": "8",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 510,
                    "end": 511,
                    "mention": "9",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "We intend to use document figure classification as a first step in automatic educational image summarization applications. A similar idea is followed by Morash et al. [10], who built one template for each type of image, then manually classified images and filled out the templates, and suggested automating the steps of that process. Moraes et al. [11] mentioned the same idea for their SIGHT (Summarizing Information GrapHics Textually) system.",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 170,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 349,
                    "end": 351,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "A number of publications on document image classification such as Afzal et al. [12] and Harley et al. [13] use the RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing) dataset, which covers scanned documents. While document scans and born-digital educational illustrations have materially different appearance, these papers show that the utility of deep neural networks is not limited to scene image tasks (Fig. 1).",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 82,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 103,
                    "end": 105,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Related Work",
            "ref_spans": [
                {
                    "start": 429,
                    "end": 430,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "A classification dataset of scientific illustrations was created for the NOA project [14]. However, their dataset is not publicly available, and does not draw as many distinctions between types of educational illustrations. Jobin et al.\u2019s DocFigure [15] consists of 28 different categories of illustrations extracted from scientific publications totaling 33,000 images.\n",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 88,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 250,
                    "end": 252,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "When building our taxonomy, we have chosen classes such that one class would have the same types of salient features, and appropriate summaries would also be similar in structure. Our classes are also all common in educational materials. Beyond the requirements of our taxonomy, our datasets needed to be representative of common educational illustrations in order to fit real-world applications, and legally shareable to promote research on educational image classification. Educational illustrations are created by a variety of communities with varying expertise, techniques, and tools, so choosing a dataset from one source may eliminate certain variables in educational illustration. To identify these variables, we kept our training and test data sources separate.",
            "cite_spans": [],
            "section": "SlideImages Dataset ::: Dataset and Baseline System",
            "ref_spans": []
        },
        {
            "text": "We assembled training and validation datasets from various sources of open access illustrations. Bar charts, x-y plots, maps, photos, pie charts, slide images, table images, and technical drawings were manually selected by a student assistant (supported by the main author) using the Wikimedia Commons image search for related terms. We manually selected graph diagrams, which we also call node-edge diagrams or \u201cstructured diagrams,\u201d from the Kembhavi et al. [8] AllenAI Diagram Understanding (AI2D) dataset; not all AI2D images contain graph edges [8]. The training dataset of SlideImages consists of 2,938 images and is intended for fine-tuning CNNs, not training from scratch. The SlideImages test set is derived from a snapshot of SlideWiki open educational resource platform (https://slidewiki.org/) datastore obtained in 2018. From that snapshot, two annotators manually selected and labeled 691 images. Our data are available at our code repository: https://github.com/david-morris/SlideImages/.",
            "cite_spans": [
                {
                    "start": 461,
                    "end": 462,
                    "mention": "8",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 551,
                    "end": 552,
                    "mention": "8",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "SlideImages Dataset ::: Dataset and Baseline System",
            "ref_spans": []
        },
        {
            "text": "The SlideImages training dataset is small compared to datasets like ImageNet [1], with over 14 million images, RVL-CDIP [13] with 400,000 images, or even DocFigure [15] with 33,000 images. Much of our methodology is shaped by needing to confront the challenges of a small dataset. In particular, we aim to avoid overfitting: the tendency of a classifier to identify individual images and patterns specific to the training set rather than the desired semantic concepts.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 79,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 121,
                    "end": 123,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 165,
                    "end": 167,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Baseline Approach ::: Dataset and Baseline System",
            "ref_spans": []
        },
        {
            "text": "For our pre-training dataset, a large, diverse dataset is required that contains a large proportion of educational and scholarly images. We pre-trained on a dataset of almost 60,000 images labeled by Sohmen et al. [6] (NOA dataset), provided by the authors on request. The images are categorized as composite images, diagrams, medical imaging, photos, or visualizations/models.",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 216,
                    "mention": "6",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Baseline Approach ::: Dataset and Baseline System",
            "ref_spans": []
        },
        {
            "text": "To mitigate overfitting, we used data augmentation: distorting an image while keeping relevant traits. We used image stretching, brightness scaling, zooming, and color channel shifting as shown in our source code. We also added dropout with a rate of 0.1 on the extracted features before the fully connected and output layers. We used similar image augmentation for pre-training and training.",
            "cite_spans": [],
            "section": "Baseline Approach ::: Dataset and Baseline System",
            "ref_spans": []
        },
        {
            "text": "We use MobileNetV2 [16] as our network architecture. We chose MobileNetV2 as a compromise between a small number of parameters and performance on ImageNet. Intuitively, a smaller parameter space implies a model with more bias and lower variance, which is better for smaller datasets. We initialized our weights from an ImageNet model and pre-trained for a further 40 epochs with early stopping on the NOA dataset using the Adam (adaptive moment estimation) [17] optimizer. This additional pre-training was intended to cause the lower levels of the network to extract more features specific to born-digital images. We then trained for 40 epochs with Adam and a learning rate schedule. Our schedule drops the learning rate by a factor of 10 at the 15th and 30th epoch. Our implementation is available at https://github.com/david-morris/SlideImages/.\n",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 22,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 458,
                    "end": 460,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Baseline Approach ::: Dataset and Baseline System",
            "ref_spans": []
        },
        {
            "text": "We set a baseline for our dataset with the classifier described in Sect. 3.2. The confusion matrix in Fig. 2 shows that misclassifications do tend towards a few types of errors, but none of the classes have collapsed. While certain classes are likely to be misclassified as another specific class (such as structured diagrams as slides), those relationships do not happen in reverse, and a correct classification is more likely. Figure 2 shows that our baseline leaves room for improvement, and our test set helps to identify challenges in this task. Viewing individual classification errors highlighted a few problems with our training data. Our training data do not include sufficient structured diagrams with illustrated arrows, or edges which travel only at 90\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^\\circ $$\\end{document} increments, such as organigrams or some Unified Modeling Language diagrams. Our photos do not include examples with the background removed, but these are common in educational images. These problems should be remedied in future training datasets for this and similar problems.\n",
            "cite_spans": [],
            "section": "Baseline ::: Preliminary Results",
            "ref_spans": [
                {
                    "start": 107,
                    "end": 108,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 436,
                    "end": 437,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "The related DocFigure dataset covers similar images and has much more data than SlideImages. To justify SlideImages, we have created a head-to-head comparison of classifiers trained in the same way (as described in Sect. 3.2) on the SlideImages and DocFigure datasets. All the SlideImages classes except slides have an equivalent in DocFigure. We have shown the reduction in the data used, and the relative sizes of the datasets, in Table 1. The Head-to-head datasets contain only the matching classes, and in the case of the DocFigure dataset, the original test set has been split into validation and test sets.",
            "cite_spans": [],
            "section": "Head-to-Head Comparison ::: Preliminary Results",
            "ref_spans": [
                {
                    "start": 439,
                    "end": 440,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "After obtaining the two trained networks, we have tested each network on both the matching test set, and the other test set. Although we were unable to reproduce the VGG-V baseline used by Jobin et al., we used a linear SVM with VGG-16 features and achieved comparable results on the full DocFigure dataset (90% macro average compared to their 88.96% with a fully neural feature extractor). The results (Table 2) show that SlideImages is a more challenging and potentially more general task. The net trained on SlideImages did even better on the DocFigure test set than on the SlideImages test set. Despite having a different source and approximately a fifth of the size of the DocFigure dataset, the net trained on SlideImages training set was better on our test set.",
            "cite_spans": [],
            "section": "Head-to-Head Comparison ::: Preliminary Results",
            "ref_spans": [
                {
                    "start": 410,
                    "end": 411,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "In this paper, we have presented the task of classifying educational illustrations and images in slides and introduced a novel dataset SlideImages. The classification remains an open problem despite our baseline and represents a useful task for information retrieval. We have provided a test set derived from actual educational illustrations, and a training set compiled from open access images. Finally, we have established a baseline system for the classification task. Other potential avenues for future research include experimenting with the DocFigure dataset in the pre-training and training phases, and experimenting with text extraction for multimodal classification.",
            "cite_spans": [],
            "section": "Conclusions and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Comparison of different datasets including the number of classes and images.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Head-to-head comparison of accuracy (weighted averages).\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Train set class examples clockwise from top left: bar charts, photos, pie charts, slide images, tables, structured diagrams, technical drawings, x-y plots, and maps.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Left: examples of bar charts from the DocFigure [15] train set and our own test set. Right: confusion matrix of our baseline system on SlideImages. Entries show percent of true members of the class on the left margin labeled as on the bottom margin. Weighted accuracy average is 80% over all 691 images.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Guiding novice web workers in making image descriptions using templates",
            "authors": [
                {
                    "first": "VS",
                    "middle": [],
                    "last": "Morash",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Siu",
                    "suffix": ""
                },
                {
                    "first": "JA",
                    "middle": [],
                    "last": "Miele",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hasty",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Landau",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "TACCESS",
            "volume": "7",
            "issn": "4",
            "pages": "12:1-12:21",
            "other_ids": {
                "DOI": [
                    "10.1145/2764916"
                ]
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Figures in scientific open access publications",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sohmen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Charbonnier",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Bl\u00fcmel",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wartena",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Heller",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Digital Libraries for Open Knowledge",
            "volume": "",
            "issn": "",
            "pages": "220-226",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "NOA: a search engine for reusable scientific images beyond the life sciences",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Charbonnier",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sohmen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Rothman",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Rohden",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wartena",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "797-800",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Labeling Topics with Images Using a Neural Network",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Aletras",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "500-505",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "A diagram is worth a dozen images",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kembhavi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Salvato",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kolve",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Seo",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hajishirzi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Farhadi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision \u2013 ECCV 2016",
            "volume": "",
            "issn": "",
            "pages": "235-251",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Semi\u2013supervised learning for image modality\u00a0classification",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garc\u00eda Seco de Herrera",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Markonis",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Joyseeree",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schaer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Foncubierta-Rodr\u00edguez",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Multimodal Retrieval in the Medical Domain",
            "volume": "",
            "issn": "",
            "pages": "85-98",
            "other_ids": {
                "DOI": []
            }
        }
    }
}