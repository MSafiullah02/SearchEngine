{
    "paper_id": "a7cd0e506e1c9d4d89025807924bd0b1b2aa9950",
    "metadata": {
        "title": "CheckThat! at CLEF 2020: Enabling the Automatic Identification and Verification of Claims in Social Media",
        "authors": [
            {
                "first": "Alberto",
                "middle": [],
                "last": "Barr\u00f3n-Cede\u00f1o",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universit\u00e0 di Bologna",
                    "location": {
                        "settlement": "Forl\u00ec",
                        "country": "Italy"
                    }
                },
                "email": ""
            },
            {
                "first": "(",
                "middle": [
                    "B"
                ],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Preslav",
                "middle": [],
                "last": "Nakov",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Qatar University",
                    "location": {
                        "settlement": "Doha",
                        "country": "Qatar"
                    }
                },
                "email": "pnakov@hbku.edu.qa"
            },
            {
                "first": "Giovanni",
                "middle": [],
                "last": "Da",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "San",
                "middle": [],
                "last": "Martino",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "HBKU",
                    "location": {
                        "settlement": "Doha",
                        "country": "Qatar"
                    }
                },
                "email": "gmartino@hbku.edu.qa"
            },
            {
                "first": "Maram",
                "middle": [],
                "last": "Hasanain",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Qatar University",
                    "location": {
                        "settlement": "Doha",
                        "country": "Qatar"
                    }
                },
                "email": "maram.hasanain@qu.edu.qa"
            },
            {
                "first": "Reem",
                "middle": [],
                "last": "Suwaileh",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Qatar University",
                    "location": {
                        "settlement": "Doha",
                        "country": "Qatar"
                    }
                },
                "email": ""
            },
            {
                "first": "Fatima",
                "middle": [],
                "last": "Haouari",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Qatar University",
                    "location": {
                        "settlement": "Doha",
                        "country": "Qatar"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We describe the third edition of the CheckThat! Lab, which is part of the 2020 Cross-Language Evaluation Forum (CLEF). CheckThat! proposes four complementary tasks and a related task from previous lab editions, offered in English, Arabic, and Spanish. Task 1 asks to predict which tweets in a Twitter stream are worth fact-checking. Task 2 asks to determine whether a claim posted in a tweet can be verified using a set of previously fact-checked claims. Task 3 asks to retrieve text snippets from a given set of Web pages that would be useful for verifying a target tweet's claim. Task 4 asks to predict the veracity of a target tweet's claim using a set of potentially-relevant Web pages. Finally, the lab offers a fifth task that asks to predict the check-worthiness of the claims made in English political debates and speeches. CheckThat! features a full evaluation framework. The evaluation is carried out using mean average precision or precision at rank k for ranking tasks, and F1 for classification tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The mission of the CheckThat! lab is to foster the development of technology that would enable the automatic verification of claims. Automated systems for claim identification and verification can be very useful as supportive technology for investigative journalism, as they could provide help and guidance, thus saving time [14, 22, 24, 33] . A system could automatically identify check-worthy claims, make sure they have not been fact-checked already by a reputable fact-checking organization, and then present them to a journalist for further analysis in a ranked list. Additionally, the system could identify documents that are potentially useful for humans to perform manual fact-checking of a claim, and it could also estimate a veracity score supported by evidence to increase the journalist's understanding and the trust in the system's decision. CheckThat! at CLEF 2020 is the third edition of the lab. 1 The 2018 edition [29] of CheckThat! focused on the identification and verification of claims in political debates. 2 Whereas the 2019 edition [9, 10] also focused on political debates, isolated claims were considered as well, in conjunction with a closed set of Web documents to retrieve evidence from. 3 In 2020, CheckThat! turns its attention to social media-in particular to Twitter -as information posted on that platform is not checked by an authoritative entity before publication and such information tends to disseminate very quickly. Moreover, social media posts lack context due to their short length and conversational nature; thus, identifying a claim's context is sometimes key for enabling effective fact-checking [7] .",
            "cite_spans": [
                {
                    "start": 325,
                    "end": 329,
                    "text": "[14,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 330,
                    "end": 333,
                    "text": "22,",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 334,
                    "end": 337,
                    "text": "24,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 338,
                    "end": 341,
                    "text": "33]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 912,
                    "end": 913,
                    "text": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 931,
                    "end": 935,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1029,
                    "end": 1030,
                    "text": "2",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1217,
                    "end": 1218,
                    "text": "3",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1642,
                    "end": 1645,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The lab is mainly organized around four tasks, which correspond to the four main blocks in the verification pipeline, as illustrated in Fig. 1 . Tasks 1, 3, and 4 can be seen as reformulations of corresponding tasks in 2019, which enables re-use of training data and systems from previous editions of the lab (cf. Sect. 3). Task 2 runs for the first time. While Tasks 1-4 are focused on Twitter, Task 5 (not in Fig. 1 ) focuses on political debates as in the previous two editions of the lab. All tasks are run in English. Additionally, Tasks 1, 3, and 4 are also offered in Arabic and/or Spanish.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 136,
                    "end": 142,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 411,
                    "end": 417,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Description of the Tasks"
        },
        {
            "text": "Task 1 is formulated as follows: Given a topic and a stream of potentially-related tweets, rank the tweets according to their check-worthiness for the topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 1: Check-Worthiness on Tweets"
        },
        {
            "text": "Previous work on check-worthiness focused primarily on political debates and speeches, but here we focus on tweets instead.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 1: Check-Worthiness on Tweets"
        },
        {
            "text": "We include \"topics\" this year, as we want to have a scenario that is close to that from 2019; a topic gives a context just like a debate did. We construct the dataset by tracking a set of manually-created topics in Twitter. A sample of tweets from the tracked stream (per topic) is shared with the participating systems as input for Task 1. The systems are asked to submit a ranked list of the tweets for each topic. Finally, using pooling, a set of tweets is selected and then judged by in-house annotators.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "Evaluation. We treat Task 1 as a ranking problem. Systems are evaluated using ranking evaluation measures, namely Mean Average Precision (MAP) and precision at rank k (P@k). The official measure is P@30.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "Given an input claim c and a set V c = {v i } of verified claims, we consider each pair (c, v i ) as Relevant if v i would save the process of verifying c from scratch, and as Irrelevant otherwise. Note that there might be more than one relevant verified claim per input claim, e.g., because the input claim might be composed of multiple claims. The task is similar to paraphrasing and textual similarity tasks, as well as to textual entailment [8, 12, 30] .",
            "cite_spans": [
                {
                    "start": 445,
                    "end": 448,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 449,
                    "end": 452,
                    "text": "12,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 453,
                    "end": 456,
                    "text": "30]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Task 2 is defined as follows: Given a check-worthy claim and a dataset of verified claims, rank the verified claims, so that those that verify the input claim (or a sub-claim in it) are ranked on top."
        },
        {
            "text": "Dataset. Verified claims are retrieved from fact-checking websites such as Snopes and PolitiFact.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 2 is defined as follows: Given a check-worthy claim and a dataset of verified claims, rank the verified claims, so that those that verify the input claim (or a sub-claim in it) are ranked on top."
        },
        {
            "text": "Evaluation. Mean Average Precision on the first 5 retrieved claims (MAP@5) is used to assess the quality of the rankings submitted by the participants. A perfect ranking will have on top all v i such that (c, v i ) is Relevant, in any order, followed by all Irrelevant claims. In addition to MAP@5, we also report MRR, MAP@k (k = 3, 10, 20, all) and Recall@k for k = 3, 5, 10, 20 in order to provide participants with more information about their systems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 2 is defined as follows: Given a check-worthy claim and a dataset of verified claims, rank the verified claims, so that those that verify the input claim (or a sub-claim in it) are ranked on top."
        },
        {
            "text": "Task 3 is defined as follows: Given a check-worthy claim on a specific topic and a set of text snippets extracted from potentially-relevant webpages, return a ranked list of all evidence snippets for the claim. Evidence snippets are those snippets that are useful in verifying the given claim.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 3: Evidence Retrieval"
        },
        {
            "text": "Dataset. While tracking on-topic tweets, we search the Web to retrieve top-m Web pages using topic-related queries. This would ensure the freshness of the retrieved pages and enable reusability of the dataset for real-time verification tasks. Once we acquire annotations for Task 1, we share with participants the Web pages and text snippets from them solely for the check-worthy claims, which would enable the start of the evaluation cycle for Task 3. In-house annotators will label each snippet as evidence or not for a target claim.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 3: Evidence Retrieval"
        },
        {
            "text": "Evaluation. Tasks 3 is a ranking problem. We evaluate the ranked list per topic using MAP and P@k. The official measure is P@10.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 3: Evidence Retrieval"
        },
        {
            "text": "Task 4 is defined as follows: Given a check-worthy claim on a specific topic and a set of potentially-relevant Web pages, predict the veracity of the claim. This task closes the verification pipeline.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 4: Claim Verification"
        },
        {
            "text": "Dataset. The dataset for this task is the same as for Task 3. The only difference is that the in-house annotators judge each claim as true or false.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 4: Claim Verification"
        },
        {
            "text": "Evaluation. Task 4 is a binary classification problem. Therefore, it is evaluated using standard classification evaluation measures: Precision, Recall, F 1 , and Accuracy. The official measure is macro-averaged F 1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 4: Claim Verification"
        },
        {
            "text": "Task 5 is defined as follows: Given a debate segmented into sentences, together with speaker information, prioritize sentences for fact-checking. This is a ranking task and each sentence should be associated with a score.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 5: Check-Worthiness on Debates"
        },
        {
            "text": "Dataset. This is the third iteration of this task. We believe it is important to keep it alive as we have a large body of annotated data already and new material arrives with the coming 2020 US Presidential elections.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 5: Check-Worthiness on Debates"
        },
        {
            "text": "Evaluation. Task 5 is yet another ranking problem. We use MAP as the official evaluation measure. We further report P@k for k \u2208 {5, 10, 20, 50}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Task 5: Check-Worthiness on Debates"
        },
        {
            "text": "Two editions of CheckThat! have been held so far. While the datasets come from different genres, some of the tasks in the 2020 edition are reformulated. Hence, considering some of the most successful approaches applied in the past represents a good starting point to address the current challenges.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Previously on CheckThat!"
        },
        {
            "text": "The 2019 edition featured two tasks [10] : Task 1 2019 . Given a political debate, interview, or speech, transcribed and segmented into sentences, rank the sentences by the priority with which they should be fact-checked.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "CheckThat! 2019"
        },
        {
            "text": "The most successful approaches used neural networks for the individual classification of the instances. For example, Hansen et al. [19] learned domain-specific word embeddings and syntactic dependencies and applied an LSTM classifier.",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 135,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "CheckThat! 2019"
        },
        {
            "text": "Using some external knowledge paid off-they pre-trained the network with previous Trump and Clinton debates, supervised weakly with the ClaimBuster system. Some efforts were carried out in order to consider context. Favano et al. [11] trained a feed-forward neural network, including the two previous sentences as context. Whereas many approaches opted for embedding representations, feature engineering was also popular [13] . The systems for evidence passage identification followed two approaches. BERT was trained and used to predict whether an input passage is useful to fact-check a claim [11] . Other participating systems used classifiers (e.g., SVM) with a variety of features including similarity between the claim and a passage, bag of words, and named entities [20] . As for predicting claim veracity, the most effective approach used a textual entailment model. The input was represented using word embeddings and external data was also used in training [15] .",
            "cite_spans": [
                {
                    "start": 230,
                    "end": 234,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 421,
                    "end": 425,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 595,
                    "end": 599,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 773,
                    "end": 777,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 967,
                    "end": 971,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "CheckThat! 2019"
        },
        {
            "text": "In the 2020 edition, Task 1 2019 becomes Task 5, and Task 1 is a reformulation based on tweets (cf. Sect. 2.1). See [2] for further details. Task 2 2019 becomes Tasks 3 and 4 (cf. Sects. 2.3 and 2.4). See [21] for further details.",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 119,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 205,
                    "end": 209,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "CheckThat! 2019"
        },
        {
            "text": "The 2018 edition featured two tasks [29] :",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "CheckThat! 2018"
        },
        {
            "text": "Task 1 2018 was identical to Task 1 2019 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CheckThat! 2018"
        },
        {
            "text": "The most successful approaches used either a multilayer perceptron or an SVM. Zuo et al. [36] enriched the dataset by producing pseudo-speeches as a concatenation of all interventions by a debater. They used averaged word embeddings and bag-of-words as representations. Hansen et al. [18] represented the entries with embeddings, part of speech tags, and syntactic dependencies. They used a GRU neural network with attention. See [1] for further details.",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 93,
                    "text": "[36]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 284,
                    "end": 288,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 430,
                    "end": 433,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "CheckThat! 2018"
        },
        {
            "text": "Task 2 2018 . Given a check-worthy claim in the form of a (transcribed) sentence, determine whether the claim is likely to be true, half-true, or false.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CheckThat! 2018"
        },
        {
            "text": "The best way to address this task was to retrieve relevant information from the Web, followed by a comparison to the claim in order to assess its factuality. 4 After retrieving such evidence, it is fed into the supervised model, together with the claim in order to assess its veracity. In the case of [18] , they fed the claim and the most similar Web-retrieved text to convolutional neural networks and SVMs. Meanwhile, Ghanem et al. [16] computed features, such as the similarity between the claim and the Web text, and the Alexa rank for the website. See [4] for further details.",
            "cite_spans": [
                {
                    "start": 158,
                    "end": 159,
                    "text": "4",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 301,
                    "end": 305,
                    "text": "[18]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 435,
                    "end": 439,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 558,
                    "end": 561,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "CheckThat! 2018"
        },
        {
            "text": "There has been work on checking the factuality/credibility of a claim, of a news article, or of an information source [3, 25, 26, 28, 31, 35] . Claims can come from different sources, but special attention has been given to those from social media [17, 27, 32, 34] . Check worthiness estimation is still a fairly-new problem especially in the context of social media [14, [22] [23] [24] .",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 121,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 122,
                    "end": 125,
                    "text": "25,",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 126,
                    "end": 129,
                    "text": "26,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 130,
                    "end": 133,
                    "text": "28,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 134,
                    "end": 137,
                    "text": "31,",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 138,
                    "end": 141,
                    "text": "35]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 248,
                    "end": 252,
                    "text": "[17,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 253,
                    "end": 256,
                    "text": "27,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 257,
                    "end": 260,
                    "text": "32,",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 261,
                    "end": 264,
                    "text": "34]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 367,
                    "end": 371,
                    "text": "[14,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 372,
                    "end": 376,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 377,
                    "end": 381,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 382,
                    "end": 386,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "CheckThat! further shares some aspects with other initiatives that have been run with high success in the past, e.g., stance detection (Fake News 5 ), semantic textual similarity (STS at SemEval 6 ), and community question answering (cQA at SemEval 7 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "We have presented the 2020 edition of the CheckThat! Lab, which features tasks that span the full verification pipeline: from spotting check-worthy claims to checking whether they have been fact-checked elsewhere already, to retrieving useful passages within relevant pages, to finally making a prediction about the factuality of a claim. To the best of our knowledge, this is the first shared task that addresses all steps of the fact-checking process. Moreover, unlike previous editions of the CheckThat! Lab, our main focus here is on social media, which are the center of \"fake news\" and disinformation. We further feature a more realistic information retrieval scenario with pooling for evaluation, as done at IR venues such as TREC. Last but not least, in-line with the general mission of CLEF, we promote multi-linguality by offering our tasks in different languages.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We hope that these tasks and the associated datasets will serve the mission of the CheckThat! initiative, which is to foster the development of datasets, tools and technology that would enable the automatic verification of claims and will support human fact-checkers in their fight against \"fake news\" and disinformation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Overview of the CLEF-2018 CheckThat! Lab on automatic identification and verification of political claims. Task 1: check-worthiness",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Cappellato",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Overview of the CLEF-2019 CheckThat! Lab on automatic identification and verification of claims. Task 1: check-worthiness",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Karadzhov",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mohtarami",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Da San Martino",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "VERA: a platform for veracity estimation over web data",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "L"
                    ],
                    "last": "Ba",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Berti-Equille",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Hammady",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 25th International Conference Companion on World Wide Web",
            "volume": "",
            "issn": "",
            "pages": "159--162",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Overview of the CLEF-2018 CheckThat! Lab on automatic identification and verification of political claims. Task 2: factuality",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Cappellato",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Working Notes of CLEF 2019 Conference and Labs of the Evaluation Forum. CEUR Workshop Proceedings. CEUR-WS.org",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cappellato",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ferro",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Losada",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Working Notes of CLEF 2018-Conference and Labs of the Evaluation Forum. CEUR Workshop Proceedings. CEUR-WS.org",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cappellato",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Ferro",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "Y"
                    ],
                    "last": "Nie",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A content management perspective on fact-checking",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Cazalens",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lamarre",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leblay",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Manolescu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tannier",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of The Web Conference",
            "volume": "",
            "issn": "",
            "pages": "565--574",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "SemEval-2017 Task 1: semantic textual similarity multilingual and crosslingual focused evaluation",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cer",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Diab",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Agirre",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Lopez-Gazpio",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Specia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation",
            "volume": "",
            "issn": "",
            "pages": "1--14",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "CheckThat! at CLEF 2019: automatic identification and verification of claims",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ECIR 2019",
            "volume": "11438",
            "issn": "",
            "pages": "309--315",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-15719-7_41"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Overview of the CLEF-2019 CheckThat! Lab: automatic identification and verification of claims",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CLEF 2019",
            "volume": "11696",
            "issn": "",
            "pages": "301--321",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-28577-7_25"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "TheEarthIsFlat's submission to CLEF'19",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Favano",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Carman",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lanzi",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Structural representations for learning relations between pairs of texts",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Filice",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Da San Martino",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Moschitti",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1003--1013",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "The IPIPAN team participation in the check-worthiness task of the CLEF2019 CheckThat! Lab",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gasior",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Przyby La",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A contextaware approach for detecting worth-checking claims in political debates",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Gencheva",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Koychev",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "267--276",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "UPV-UMA at CheckThat! Lab: verifying Arabic claims using cross lingual approach",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ghanem",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Glava\u0161",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Giachanou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ponzetto",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Rosso",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Rangel",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "UPV-INAOE-Autoritas -Check That: preliminary approach for checking worthiness of claims",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ghanem",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Montes-Y G\u00f3mez",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Rangel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Rosso",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "TweetCred: real-time credibility assessment of content on Twitter",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kumaraguru",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Castillo",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Meier",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "SocInfo 2014",
            "volume": "8851",
            "issn": "",
            "pages": "228--243",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-13734-6_16"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "The Copenhagen team participation in the check-worthiness task of the competition of automatic identification and verification of claims in political debates of the CLEF-2018 fact checking lab",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hansen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hansen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Simonsen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lioma",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Neural weakly supervised fact check-worthiness detection with contrastive sampling-based ranking loss",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hansen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hansen",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Simonsen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lioma",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Cappellato",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "bigIR at CLEF 2019: automatic verification of Arabic claims over the web",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Haouari",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ali",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Overview of the CLEF-2019 CheckThat! Lab on automatic identification and verification of claims. Task 2: evidence and factuality",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hasanain",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Suwaileh",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Elsayed",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Detecting check-worthy factual claims in presidential debates",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hassan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tremayne",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM 2015",
            "volume": "",
            "issn": "",
            "pages": "1835--1838",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Comparing automated factual claim detection against judgments of journalism organizations",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hassan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tremayne",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Arslan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computa-tion+Journalism Symposium",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "ClaimBuster: the first-ever end-to-end fact-checking system",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Hassan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. VLDB Endow",
            "volume": "10",
            "issn": "",
            "pages": "1945--1948",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Fully automated fact checking using external sources",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Karadzhov",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Koychev",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "344--353",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Detecting rumors from microblogs with recurrent neural networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI 2016",
            "volume": "",
            "issn": "",
            "pages": "3818--3824",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "CREDBANK: a large-scale social media corpus with associated credibility annotations",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gilbert",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the Ninth International AAAI Conference on Web and Social Media, ICWSM 2015",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Leveraging joint interactions for credibility analysis in news communities",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mukherjee",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Weikum",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM 2015",
            "volume": "",
            "issn": "",
            "pages": "353--362",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Overview of the CLEF-2018 lab on automatic identification and verification of claims in political debates",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "SemEval-2016 Task 3: community question answering",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation",
            "volume": "",
            "issn": "",
            "pages": "525--545",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Credibility assessment of textual claims on the web",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Popat",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Mukherjee",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Str\u00f6tgen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Weikum",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016",
            "volume": "",
            "issn": "",
            "pages": "2173--2178",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Fake news detection on social media: a data mining perspective",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Shu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sliva",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "SIGKDD Explor. Newsl",
            "volume": "19",
            "issn": "1",
            "pages": "22--36",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "It takes nine to smell a rat: neural multi-task learning for check-worthiness prediction",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vasileva",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Atanasova",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "M\u00e0rquez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Barr\u00f3n-Cede\u00f1o",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Nakov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Processing",
            "volume": "2019",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Enquiring minds: early detection of rumors in social media from enquiry posts",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Resnick",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Mei",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th International Conference on World Wide Web",
            "volume": "",
            "issn": "",
            "pages": "1395--1405",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Analysing how people orient to and spread rumours in social media by looking at conversational threads",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zubiaga",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Liakata",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Procter",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "W S"
                    ],
                    "last": "Hoi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Tolmie",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "PLoS ONE",
            "volume": "11",
            "issn": "3",
            "pages": "1--29",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "A hybrid recognition system for check-worthy claims using heuristics and supervised learning",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zuo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Karakas",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Information verification pipeline. Our tasks cover all four steps. (Box 1 maps to task 1 whereas boxes 3-4 map to task 2 of the 2018 and 2019 editions[10,29].)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Given a claim and a set of Web pages potentially relevant with respect to the claim, identify which of the pages (and passages thereof ) are useful for assisting a human in fact-checking the claim. Finally, determine the factuality of the claim.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgments. The work of Tamer Elsayed and Maram Hasanain was made possible by NPRP grant# NPRP 11S-1204-170060 from the Qatar National Research Fund (a member of Qatar Foundation). The work of Reem Suwaileh was supported by GSRA grant# GSRA5-1-0527-18082 from the Qatar National Research Fund and the work of Fatima Haouari was supported by GSRA grant# GSRA6-1-0611-19074 from the Qatar National Research Fund. The statements made herein are solely the responsibility of the authors. This research is also part of the Tanbih project, developed by the Qatar Computing Research Institute, HBKU and MIT-CSAIL, which aims to limit the effect of \"fake news\", propaganda, and media bias.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}