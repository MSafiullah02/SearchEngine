{
    "paper_id": "75570407f08bd39d157fdffb8fad4d51fc36e926",
    "metadata": {
        "title": "SPSTracker: Sub-Peak Suppression of Response Map for Robust Object Tracking",
        "authors": [
            {
                "first": "Qintao",
                "middle": [],
                "last": "Hu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Chengdu",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Lijun",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Chengdu",
                        "country": "China"
                    }
                },
                "email": "zhoulijun16@mails.ucas.edu.cn"
            },
            {
                "first": "Xiaoxiao",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of California",
                    "location": {
                        "settlement": "Davis",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Yao",
                "middle": [],
                "last": "Mao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Chengdu",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Jianlin",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Chengdu",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Qixiang",
                "middle": [],
                "last": "Ye",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "qxye@ucas.ac.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "Modern visual trackers usually construct online learning models under the assumption that the feature response has a Gaussian distribution with target-centered peak response. Nevertheless, such an assumption is implausible when there is progressive interference from other targets and/or background noise, which produce sub-peaks on the tracking response map and cause model drift. In this paper, we propose a rectified online learning approach for sub-peak response suppression and peak response enforcement and target at handling progressive interference in a systematic way. Our approach, referred to as SPSTracker, applies simple-yetefficient Peak Response Pooling (PRP) to aggregate and align discriminative features, as well as leveraging a Boundary Response Truncation (BRT) to reduce the variance of feature response. By fusing with multi-scale features, SPSTracker aggregates the response distribution of multiple sub-peaks to a single maximum peak, which enforces the discriminative capability of features for robust object tracking. Experiments on the OTB, NFS and VOT2018 benchmarks demonstrate that SPSTrack outperforms the state-of-the-art real-time trackers with significant margins 1 * Both authors contributed equally.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In the past few years, deep convolutional neural networks (CNNs) have significantly improved the performance of visual object tracking, by providing frameworks for end-toend representation learning (Ma et al. 2016; Danelljan et al. 2016a) , online correlation filter learning (Henriques et al. 2015; Danelljan et al. 2016b) , and discriminative classifier learning (Danelljan et al. 2017; Qi et al. 2016; Nam and Han 2015; Nam et al. 2016; Han, Sim, and Adam 2017) . However, CNN-based trackers suffer the performance degradation caused by the multi-target occlusion, appearance variance and/or background noise. Especially during the tracking procedure, the neighboring targets and background noise could introduce progressive interference and result in the vital model drift, as shown in Fig. 1(up) , particularly when objects have scale variations and complex motions. (Danelljan et al. 2019) . For the interference from multiple targets, ATOM (first row) produces response maps of multiple sub-peaks (frame 153 to frame 165), which cause model drift. In contrast, our approach (second row) aggregates the response distribution of multiple sub-peaks to a single maximum peak, which leads to robust target tracking. (Best viewed in color and with zoom in)",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 214,
                    "text": "(Ma et al. 2016;",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 215,
                    "end": 238,
                    "text": "Danelljan et al. 2016a)",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 276,
                    "end": 299,
                    "text": "(Henriques et al. 2015;",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 300,
                    "end": 323,
                    "text": "Danelljan et al. 2016b)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 365,
                    "end": 388,
                    "text": "(Danelljan et al. 2017;",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 389,
                    "end": 404,
                    "text": "Qi et al. 2016;",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 405,
                    "end": 422,
                    "text": "Nam and Han 2015;",
                    "ref_id": null
                },
                {
                    "start": 423,
                    "end": 439,
                    "text": "Nam et al. 2016;",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 440,
                    "end": 464,
                    "text": "Han, Sim, and Adam 2017)",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 872,
                    "end": 895,
                    "text": "(Danelljan et al. 2019)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 790,
                    "end": 800,
                    "text": "Fig. 1(up)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "To mitigate the interference, the Siamese network structure was introduced to improve the discriminative capacity of trackers by extensively training the network (Tao, Gavves, and Smeulders 2016; Xu et al. 2017) . EArly-Stopping Tracker incorporates object representation decision-making policies with the reinforcement learning method . Nevertheless, these approaches usually rely on using additional data for offline training and lack the capability to adapt trackers adaptability in complex conditions.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 195,
                    "text": "(Tao, Gavves, and Smeulders 2016;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 196,
                    "end": 211,
                    "text": "Xu et al. 2017)",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To conquer the issue, the dynamic Siamese network (Guo et al. 2017 ) uses a fast transformation learning method to model target appearance variation and handles background suppression from previous frames. The ATOM tracker (Danelljan et al. 2019 ) combines offline pre-training with online learning in a multi-task learning framework by incorporating the objectives of target localization and targetbackground classification. While incorporating high-level knowledge into the target estimation through extensive offline learning, these methods remain overlooking the progressive interference from context area in a systematic manner. How to directly model the interference and regularize the tracking response distribution is still a open problem.",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 66,
                    "text": "(Guo et al. 2017",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 223,
                    "end": 245,
                    "text": "(Danelljan et al. 2019",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we propose a simple-yet-effective approach, referred to as SPSTracker for robust object tracking. Our motivation is based on the observation that most failure tracking is caused by the interference around the target. Such interference produces multi-peak tracking response, and the sub-peak may progressively \"grow\" and eventually cause model drift. Therefore ,we propose suppressing the subpeaks to aggregating a single-peak response, with the aim of preventing model drift from the perspective of tracking response regularization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Specifically, we introduce a Peak Response Pooling (PRP) module, which concentrates the maximum values of tracking response into the geometric centers of targets, as shown in Fig. 2 . The pooling procedure is implemented by an efficient maximization and substitution operation on the tracking response maps. During the network forward procedure, PRP aggregates multiple sub-peaks into a single centered peak for target tracking. During the backward propagation procedure, the response map with a single peak guides the online learning (fine-tuning) to explore discriminative features.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 175,
                    "end": 181,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "Based on PRP, we further propose the Boundary Response Truncation (BRT) operation to clip the response map by simply setting the values of the pixels far away from the peak response to be zero. The operation reduces the variance of feature response map meanwhile further aggregates the singlepeak response. If the response map is approximated as a Gaussian distribution, PRP targets at aggregating the mean values while BRT reducing the variance. RPR together with BRT facilitates the model to learn an enforced response map for robust object tracking.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "SPSTracker is built upon the CNN framework with a target classification branch and a target localization branch atop the convolutional layers. The classification network, equipped with the PRP and BRT modules, identifies the coarse locations (bounding boxes). These coarse locations are further fed to the target localization branch to estimate the precise target location.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main contributions of this work can be summarized as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 An Sub-Peak Suppression tracker (SPSTracker) is presented to reduce the risk of model drift by online suppressing the potential sub-peaks while aggregating the maximum peak response. \u2022 A simple-yet-efficient Peak Response Pooling (PRP) module is proposed to aggregate and align discriminative features, and a Boundary Response Truncation (BRT) module is designed to reduce the variance of feature response. \u2022 Our proposed tracker achieves leading performance on six benchmarks, including OTB2013, OTB2015, OTB50, VOT2016, VOT2018 and NFS. In particular, we improve the state-of-the-arts on the VOT2016 and VOT2018 benchmarks with significant margins.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The research about visual object tracking has a long history. Modern object trackers were usually constructed on three kinds of methods, including correlation filtering, online classification, and metric learning. With the rise of deep neural networks, these methods have been integrated with feature learning in an end-to-end manner.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Correlation Filters. The filtering procedure refers to matching templates with the Gaussian distribution to track targets of various appearance variation. The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample. By introducing CNNs, the representative capacity of correlation filters has been greatly improved. DeepSRDCF (Danelljan et al. 2015) fed the features from the pre-trained CNN to a correlation filter and introduced spatial regularization on the basis of KCF (Henriques et al. 2014) , mitigating the boundary effect. CCOT (Danelljan et al. 2016b ) and ECO (Danelljan et al. 2017) proposed the implicit interpolation model to pose the learning problem in the continuous spatial domain, leading to efficient integration of multiresolution deep features.",
            "cite_spans": [
                {
                    "start": 407,
                    "end": 430,
                    "text": "(Danelljan et al. 2015)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 555,
                    "end": 578,
                    "text": "(Henriques et al. 2014)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 618,
                    "end": 641,
                    "text": "(Danelljan et al. 2016b",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 652,
                    "end": 675,
                    "text": "(Danelljan et al. 2017)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Online Classification. Tracking can also be formulated as an online classification problem. DeepTrack (Li, Li, and Porikli 2015) leveraged a sample selection mechanism and a lazy updating scheme to learn online classifiers. FCNT (Wang et al. 2016 ) utilized hierarchical convolutional features to construct a network which handles various interference. CNN-SVM (Hong et al. 2015) used the pre-trained deep convolutional network to extract features of the target, and then used SVM to perform online target-background classification. These methods fully utilized the representation capabilities of deep learning features and the discriminative capacity of online classifiers. However, they often overlook the problem of accurate target state estimation.",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 128,
                    "text": "(Li, Li, and Porikli 2015)",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 229,
                    "end": 246,
                    "text": "(Wang et al. 2016",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 361,
                    "end": 379,
                    "text": "(Hong et al. 2015)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Metric Learning. To facilitate state estimation, the tracking problem was formulated in the metric (similarity) learning framework. Classification and state estimation were integrated into a Siamese network (Bertinetto et al. 2016b) that measures the similarity between the target and the candidates for tracking. Semantic branches and appearance branches were constructed in the dual Siamese network (He et al. 2018) , and saliency mechanisms were introduced in the attention-based Siamese network . SiamRPN combined the Siamese network with the region proposal network (RPN) to allow trackers estimating target extent when positioned accurately. SiamMask ) involved a unified framework for visual target tracking (VOT) and video object segmentation (VOS). To put it simply, a tracker is trained offline, which relies on the position specified by the first frame for semi-supervised learning to achieve target tracking and mask estimation. Despite of the efficiency, Siamese trackers are less robust to the interference from background due to ignoring offline training. ATOM (Danelljan et al. 2019 ) solved this issue by using a large number of samples for offline training. Nevertheless, with multiple sampled features, the target response map could have multiple sub-peaks, which aggregates the risk of model drift, particularly when there is interference from target appearance variation and/or background noise. ",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 232,
                    "text": "(Bertinetto et al. 2016b)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 401,
                    "end": 417,
                    "text": "(He et al. 2018)",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1076,
                    "end": 1098,
                    "text": "(Danelljan et al. 2019",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "We propose the Sub-Peak Response Suppression tracker (SPSTracker), with Peak Response Pooling (PRP) and Boundary Response Truncation (BRT) modules, to aggregate the multiple sub-peaks on a tracking response map into a single enforced peak, as shown in Fig. 2 . SPSTracker is built upon the ATOM tracker (Danelljan et al. 2019) , with a target classification branch and a target localization branch. The classification branch converts the feature map into a response map and provides the coarse locations of the target. The localization branch uses the bounding-box regression to localize targets. Upon the classification branch, the PRP and BRT modules are applied in a plug-and-play manner, as shown in Fig. 3 .",
            "cite_spans": [
                {
                    "start": 303,
                    "end": 326,
                    "text": "(Danelljan et al. 2019)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 252,
                    "end": 258,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 704,
                    "end": 710,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Methodology"
        },
        {
            "text": "The classification branch is a CNN online structure, which learns from minimizing the tracking response and Gaussian priori y j . Denote the feature map of a current video frame (the test image) from CNN as x. The classification branch is a 2-layer fully convolutional network parameterized with w, which predicts the tracking response map f (x; w), as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tracking Response Prediction"
        },
        {
            "text": "where w 1 and w 2 denote parameters for first and second convolutional layers, respectively, the symbol of * denotes standard multi-channel convolution, and \u03c6 1 and \u03c6 2 are the activation functions. During object tracking, the parameters of the classification branch are updated by minimizing the following objective function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tracking Response Prediction"
        },
        {
            "text": "where j denotes the index of training samples, x j denotes the features from the j th sample, y j set to a sampled Gaussian prior at the target location (Danelljan et al. 2019) ,as shown in Fig 2(Gaussian prior) . \u03b3 j denotes the weight of the corresponding training sample, and \u03bb k is a parameter to trade-off the contributions of the two terms. By optimizing Eq.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 176,
                    "text": "(Danelljan et al. 2019)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 190,
                    "end": 211,
                    "text": "Fig 2(Gaussian prior)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Tracking Response Prediction"
        },
        {
            "text": "(2) with a conjugate gradient descent method, the model predicts the target response map, as shown in Fig. 2 . Due to the response map is a weighted sum of response maps from multi-scale samples and thereby appears a multi-peak distribution. This makes the maximum response not consistent with the target geometric centers, thus increasing the classification error and the risk of model drift.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 102,
                    "end": 108,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Tracking Response Prediction"
        },
        {
            "text": "To conquer the issue that Sub-Peak Response causes the model drift, we propose Sub-Peak Response Suppression method, which can prevent the sub-peak from \"growing\" into the main-peak. Specifically, we directly operate the target response map predicted by f (x j ; w) and reformulate Eq. (2) as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "where P denotes the Peak Response Pooling applied on each sampled response map, g(x j ) denotes the feature after Boundary Response Truncation (BRT) operation which decreases the variance of target response and reduces the boundary effect, and \u03b2 j denotes the weight for the j th sample. By using PRP and BRT, we can apply feature fusion to aggregate the response maps from multiple samples into the target response map, as well as guaranteeing the the response map has a single peak centered at the target.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "By minimizing the objective function of Eq.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "(2), we can force the response map f (x j ; w) to approximate the Gaussian priori y j . However, for the targets of partial occlusion or background noises, f (x j ; w) could be acenteric and unlikely to be a Gaussian distribution. The operations of PRP and BRT in Eq. (4) make f (x j ; w) close to the Gaussian prior y j via Eq. (3), and eventually facilitates the online learning procedure. Figure 3 : The flowchart of the proposed SPSTracker. It has a target classification branch and a target localization branch. The classification branch converts the feature map into a response map and provides the coarse locations of the target. The localization branch uses bounding-box regression to localize targets. Upon the classification branch, the PRP and BRT modules are applied in a plug-and-play manner.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 392,
                    "end": 400,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "Peak Response Pooling (PRP) We propose a Peak Response Pooling (PRP) module, which concentrates the maximum values on the tracking response map to the target geometric center. On the target response map output by the classification branch, horizontal PRP is first performed to concentrate the response map into a horizontal pooling map. This procedure is done by finding the maximum response in each row of the response map and assigns all pixels of the line the maximum response value. In a similar way, vertical PRP is performed in each column on the response map to obtain the vertical pooling map. As a result, the element value of the response map after the PRP operation can be calculated as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "where x pq denotes the original response value at the location of the p th row and the q th column. The horizontal and vertical pooling maps are summed to obtain the rectified response map, which tends to aggregate large response values to the target geometric center. After multiple learning iterations, the target response is concentrated to approximate a 2D Gaussian distribution, which fits the Gaussian priori distribution for robust object tracking, as shown in Fig. 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 468,
                    "end": 474,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "The Peak Response Pooling (PRP) is inspired by the center/corner pooling (Duan et al. 2019; Law and Jia 2018) developed for object detection. However, PRP is different from the center/corner pooling from the following two aspects: 1) PRP targets at aggregating the response map to a single-peak distribution so that the Gaussian prior distribution can be well fitted. In contrast, the center/corner pooling aligns features to handle the appearance variance of objects; 2) PRP leverages more efficient row-and column-wise maximization operations to aggregate the large response to target centers, while the center/corner pooling uses comparison and substitution operations.",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 91,
                    "text": "(Duan et al. 2019;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 92,
                    "end": 109,
                    "text": "Law and Jia 2018)",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "Boundary Response Truncation (BRT) When recognizing objects and determining an objective boundary, the human visual system does not align objects with some fixed data points but uses Fovea in eyeballs that concentrate peak response to central regions for object localization (Kong et al. 2019 ). This concentration procedure inspires us to develop the BRT module for object tracking.",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 292,
                    "text": "(Kong et al. 2019",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "During tracking, for the pixel on the extent of the target but far away target centers could have ambiguous features (either background or foreground). The PRP module can concentrate the target response to the target centers but does not consider the variance of the target response. In complex scenes, the response maps could have large variance for the significant response from the target boundary, which is called the boundary effect. Considering that a single-peak response map with small variance could alleviate the boundary effect and improve the tracking robustness, we further introduce the Boundary Response Truncation (BRT) operation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "As shown in Fig. 2 , BRT is a simple clip operation, which sets the pixels far away from the peak response to be zero. This operation discards the response at the target boundary and reduces the variance of the response map. With BRT, we may miss some informative target response. However, it is experimentally validated that clipping the response map by 10%; we lose 4% foreground information and 12% background information, i.e., BRT reduces more ambiguous response while enhancing the classification ability of the tracker.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Sub-Peak Response Suppression"
        },
        {
            "text": "SPSTracker is built upon the state-of-the-art ATOM tracker (Danelljan et al. 2019) , with a target classification branch and a target localization branch. The classification branch produces coarse region proposals by evaluating the target response map. The target localization branch finetunes the network parameters to fit the reference target box with multiple region proposals (Danelljan et al. 2019) . Upon the classification branch, the PRP and BRT modules are applied in a plug-and-play manner, as shown in Fig. 3 .",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 82,
                    "text": "(Danelljan et al. 2019)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 380,
                    "end": 403,
                    "text": "(Danelljan et al. 2019)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 513,
                    "end": 519,
                    "text": "Fig. 3",
                    "ref_id": null
                }
            ],
            "section": "Object Tracking"
        },
        {
            "text": "Note that the ATOM tracker uses only the last convolutional layer of ResNet-18 (Block4) as feature representation. Shallow convolutional features are more important for ex- tracting some low-level information such as color and edge, while deep convolutional features are rich in high-level semantics. The fusion of multi-scale (shallow and deep) features enforces the representation capability, but it produces sub-peaks on response maps and deteriorates the tracking performance. By introducing the PRP and BRT modules, the multi-scale features can be well integrated for target representation and tracking. As shown in Fig. 4 , the multiple sub-peaks produced by multi-scale features can be concentrated into a maximum peak, which bridges the gap between f (x j ; w) and y j and facilitates robust tracking.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 621,
                    "end": 627,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Object Tracking"
        },
        {
            "text": "In this section, we first describe the implementation details of SPSTracker. We then present the ablation study to validate the PRP and BRT modules proposed in this paper. At last, we evaluate the SPSTracker on commonly used benchmarks and compare it with state-of-the-art trackers. All the experiments are carried out with Pytorch on a Intel i5-8600k 3.4GHz CPU and a single Nvidia GTX 1080ti GPU with 24GB memory.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment"
        },
        {
            "text": "SPSTracker is implemented upon the ATOM architecture (Danelljan et al. 2019) , by using ResNet-18 (He et al. 2016) pre-trained on ImageNet as the backbone network. The Block3 and Block4 features extracted from the test image are first passed through two Conv layers. Regions defined by the input bounding boxes are then pooled to a fixed size using pooling layers. The pooled features are modulated by channel-wise multiplication with the coefficient vector returned by the reference branch. The features are then passed through fully-connected layers to predict the Intersection over Union (IoU). All Conv and FC layers are followed by BatchNorm and ReLU. The target response map is obtained by fusing the response obtained by ResNet's block3 and block4.",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 76,
                    "text": "(Danelljan et al. 2019)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 98,
                    "end": 114,
                    "text": "(He et al. 2016)",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Implementation details"
        },
        {
            "text": "For the proposed PRP and BRT modules, we perform ablation analysis to investigate their impact on the tracking per-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ablation Study"
        },
        {
            "text": "SPSTracker ATOM formance. We also analyze the impact of the multi-scale features in SPSTracker. All the ablation studies are carried out on the VOT2018 (Kristan et al. 2018) benchmark. Peak Response Pooling (PRP). From the results in Table 1, we can see that the introduction of PRP module to the classification branch significantly aggregates the tracking performance. Specifically, it improves the expected average overlap (EAO) value by 0.19 (0.401 to 0.420), which is a significant margin, considering the strong baseline ATOM. It also improves the tracking accuracy and robustness as indicated by the last two rows of Table 1 .",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 173,
                    "text": "(Kristan et al. 2018)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 623,
                    "end": 630,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "GroundTruth"
        },
        {
            "text": "Boundary Response Truncation (BRT). The BRT module improves the EAO value by 0.13 (0.401 to 0.414), as reported in Table 1 , which is also a significant margin. This validates that the truncation operation is able to eliminate response variance and benefit online classifier learning by filtering out ambiguous samples.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 115,
                    "end": 122,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "GroundTruth"
        },
        {
            "text": "We test the truncation size and validate that the best performance is obtained when clipping 10% \u223c 12.5% width/height of the response map. For all the experiments, we clip a 10% height/width of the response map.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "GroundTruth"
        },
        {
            "text": "Multi-scale Feature Fusion. By using the multi-scale feature fusion, we improve the EAO value by 0.1. Combining feature fusion with PRP and BRT modules, we can improve EAO by 0.33 (0.434 vs. 0.401), as reported in Table 1. The significant performance gain demonstrates that the proposed PRP and BRT modules facilitate the fusion of multi-scale features, and reduce the negative effect brought by the multiple sub-peaks and the feature fusion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "GroundTruth"
        },
        {
            "text": "Sub-peak suppression. In Fig. 4 , we compare the target response maps of the ATOM tracker and SPSTracker. It can be seen that SPSTracker suppresses multiple sub-peaks while producing the response map of a single peak centered at the target. The peak response can well fit a Gaussian distribution prior y j .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 31,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "GroundTruth"
        },
        {
            "text": "Tracking speed. With a single GPU, the proposed SP-STracker achieves a tracking speed at 35 fps. Compared with the speed (40 fps) of the baseline ATOM, SPSTracker achieves significant performance gains with a negligible Figure 6 : EAO ranking of the tested trackers on VOT2016. computational cost overhead.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 220,
                    "end": 228,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "GroundTruth"
        },
        {
            "text": "OTB. The object tracking benchmarks (OTB) (Wu, Lim, and Yang 2013; consists of the three datasets, namely OTB-2013 (Wu, Lim, and Yang 2013) , OTB-50 and OTB-100 which consist of 51, 50 and 100 fully annotated videos, respectively. OTB100 includes OTB2013 and OTB50. All sequences belong to 11 typical tracking interference properties. Two evaluation metrics, success rate and precision, are used on OTB. The precision plot shows the percentage of frames whose tracking results are within a certain distance, which is determined by a given threshold. The success plot shows the ratio of successful frames when the threshold changes from 0 to 1, where a successful frame indicates that its overlap is greater than the given threshold. The area under the curve (AUC) of each success plot is used to rank the tracking methods.",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 66,
                    "text": "(Wu, Lim, and Yang 2013;",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 115,
                    "end": 139,
                    "text": "(Wu, Lim, and Yang 2013)",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "OTB2015 Precision plots of OPE"
        },
        {
            "text": "By using the success rate and precision plot in the onepass evaluation (OPE) as the evaluation metric, we compare the SPSTracker with state-of-the-art trackers including ATOM (Danelljan et al. 2019) , DaSiamRPN (Zhu et al. 2018) , ECO-HC (Danelljan et al. 2017) , SiamRPN , CF2 (Valmadre et al. 2017) , CNN-SVM (Hong et al. 2015) , SRDCF (Danelljan et al. 2015) and Staple (Bertinetto et al. 2016a) . As shown in Fig. 5 2.1%, 4.4% and 3.0%, respectively. VOT2016 and VOT2018. From the visual object tracking (VOT) benchmark, we select VOT2016 (Kristan et al. 2016 ) and VOT2018 (Kristan et al. 2018 ) to evaluate the trackers. VOT2016 contains 60 challenging videos, while VOT2018 includes 10 more challenging sequences. Whenever the tracking bounding box drifts way from the ground truth, the tracker re-initializes after five frames. The trackers are evaluated by the EAO metric, which is the inner product of empirically estimated average overlap and typical sequence length distribution. In addition, accuracy (average overlap) and failures/robustness (average number of failures) are used for evaluation as well.",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 198,
                    "text": "(Danelljan et al. 2019)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 211,
                    "end": 228,
                    "text": "(Zhu et al. 2018)",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 238,
                    "end": 261,
                    "text": "(Danelljan et al. 2017)",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 278,
                    "end": 300,
                    "text": "(Valmadre et al. 2017)",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 311,
                    "end": 329,
                    "text": "(Hong et al. 2015)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 338,
                    "end": 361,
                    "text": "(Danelljan et al. 2015)",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 373,
                    "end": 398,
                    "text": "(Bertinetto et al. 2016a)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 543,
                    "end": 563,
                    "text": "(Kristan et al. 2016",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 578,
                    "end": 598,
                    "text": "(Kristan et al. 2018",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 413,
                    "end": 419,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "OTB2015 Precision plots of OPE"
        },
        {
            "text": "SPSTracker is compared with 10 state-of-the-art trackers on VOT-2016, as shown in Fig. 6 . SPSTrack achieves the leading performance and significantly outperforms other trackers. Table 2 (Zhu, Porikli, and Li 2016) and SRBT (Kristan et al. 2016) . The EAO score of the proposed SPSTracker is 0.459, which is significantly higher than the peer trackers.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 214,
                    "text": "(Zhu, Porikli, and Li 2016)",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 224,
                    "end": 245,
                    "text": "(Kristan et al. 2016)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 82,
                    "end": 88,
                    "text": "Fig. 6",
                    "ref_id": null
                },
                {
                    "start": 179,
                    "end": 186,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "OTB2015 Precision plots of OPE"
        },
        {
            "text": "SPSTracker is also compared with the 10 state-of-the-art trackers on VOT-2018. As shown in Fig. 7 , SPSTracker also obtains the best performance. Table 3 shows the details of the comparison.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 91,
                    "end": 97,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 146,
                    "end": 153,
                    "text": "Table 3",
                    "ref_id": "TABREF5"
                }
            ],
            "section": "OTB2015 Precision plots of OPE"
        },
        {
            "text": "SPSTracker achieves an EAO score of 0.434, which is sig-nificantly better than that of SiamRPN++ , ATOM and other state-of-the-art trackers. Particularly, it outperforms the state-of-the-art SiamRPN++ by 2 , ATOM by 3.3 and SiamMask by 5.4, which are significant margins for object tracking on the challenging benchmark. NFS. The Need for Speed (NFS) (Galoogahi et al. 2017 ) dataset consists of 100 videos (380K frames). All frames are annotated with axis-aligned bounding boxes, and all sequences are manually labeled with nine visual attributes, including occlusion, fast motion, background clutter. We evaluate the trackers on the 35 FPS version of the NFS dataset. Table 4 reports the AUC scores of the compared trackers. SPSTracker slightly outperforms the baseline ATOM tracker, while significantly outperforms other state-of-theart tracking methods. Fig. 8 shows tracking examples on the OTB benchmark, from which we can see that SPSTracker correctly localizes the targets under serious interference from foreground and backgrounds. In contrast, other trackers have failure cases.",
            "cite_spans": [
                {
                    "start": 351,
                    "end": 373,
                    "text": "(Galoogahi et al. 2017",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 670,
                    "end": 677,
                    "text": "Table 4",
                    "ref_id": "TABREF7"
                },
                {
                    "start": 858,
                    "end": 864,
                    "text": "Fig. 8",
                    "ref_id": null
                }
            ],
            "section": "OTB2015 Precision plots of OPE"
        },
        {
            "text": "Visual tracking has been extensively investigated in the past few years. Nevertheless, the problem about how to model interference from multiple targets, appearance variance and/or background noise remains unsolved. In this paper, we proposed modeling the interference from the perspective of peak distribution and designed a rectified online learning approach for sub-peak response suppression and peak response",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Staple: Complementary learners for real-time tracking",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertinetto",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Valmadre",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Golodetz",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Miksik",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "H S"
                    ],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Fully-convolutional siamese networks for object tracking",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertinetto",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Valmadre",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Henriques",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "H S"
                    ],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Learning spatially regularized correlation filters for visual tracking",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Danelljan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hger",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "S"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Felsberg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Convolutional features for correlation filter based visual tracking",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Danelljan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hager",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "S"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Felsberg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ICCVw",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Beyond correlation filters: Learning continuous convolution operators for visual tracking",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Danelljan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Robinson",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "S"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Felsberg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ECCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Eco: Efficient convolution operators for tracking",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Danelljan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Bhat",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "S"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Felsberg",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Atom: Accurate tracking by overlap maximization",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Danelljan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Bhat",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "S"
                    ],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Felsberg",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Centernet: Keypoint triplets for object detection",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Need for speed: A benchmark for higher frame rate object tracking",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "K"
                    ],
                    "last": "Galoogahi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fagg",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lucey",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Learning dynamic siamese network for visual object tracking",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Rui",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Branchout: Regularization for online ensemble tracking with convolutional neural networks",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sim",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A twofold siamese network for real-time object tracking",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Chong",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "High-speed tracking with kernelized correlation filters",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Henriques",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Caseiro",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Martins",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Batista",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "37",
            "issn": "3",
            "pages": "583--596",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "High-speed tracking with kernelized correlation filters",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Henriques",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Caseiro",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Martins",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Batista",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "37",
            "issn": "3",
            "pages": "583--596",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Online tracking by learning discriminative saliency map with convolutional neural network",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "You",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kwak",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ICML",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Learning policies for adaptive tracking with deep feature cascades",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lucey",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ramanan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Foveabox: Beyond anchor-based object detector",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "The visual object tracking vot2016 challenge results",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kristan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Leonardis",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Matas",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Felsberg",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pflugfelder",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cehovin",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Vojir",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hager",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lukezic",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Fernandez",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "8926",
            "issn": "",
            "pages": "191--217",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "The sixth visual object tracking vot2018 challenge results",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kristan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Leonardis",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Matas",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Felsberg",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Pflugfelder",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "C"
                    ],
                    "last": "Zajc",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Vojir",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Bhat",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lukeic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Eldesokey",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ECCV",
            "volume": "",
            "issn": "",
            "pages": "3--53",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Cornernet: Detecting objects as paired keypoints",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Law",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jia",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Journal of Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "1--15",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "High performance visual tracking with siamese region proposal network",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Siamrpn++: Evolution of siamese visual tracking with very deep networks",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "Yan",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Deeptrack: Learning discriminative feature representations online for robust visual tracking",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Porikli",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Transactions on Image Processing",
            "volume": "25",
            "issn": "4",
            "pages": "1834--1848",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Learning multi-domain convolutional neural networks for visual tracking",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "J.-B",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M.-H. ; H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "ICCV. Nam",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Modeling and propagating cnns in a tree structure for visual tracking",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Nam",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Baek",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Hedged deep tracking",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                },
                {
                    "first": "M.-H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Siamese instance search for tracking",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gavves",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "W M"
                    ],
                    "last": "Smeulders",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "End-to-end representation learning for correlation filter based tracking",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Valmadre",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertinetto",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "F"
                    ],
                    "last": "Henriques",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vedaldi",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "H S"
                    ],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Visual tracking with fully convolutional networks",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ouyang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ICCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Learning attentions: Residual attentional siamese network for high performance online visual tracking",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Teng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Maybank",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Fast online object tracking and segmentation: A unifying approach",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bertinetto",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "H S"
                    ],
                    "last": "Torr",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Online object tracking: A benchmark",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Object tracking benchmark",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lim",
                    "suffix": ""
                },
                {
                    "first": "M.-H",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "volume": "37",
            "issn": "9",
            "pages": "1834--1848",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "End-to-end learning of driving models from large-scale video datasets",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhipeng",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Houwen",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Qiang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "CVPR. Zhu",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Beyond local search: Tracking objects everywhere with instance-specific proposals",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Porikli",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "CVPR",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Comparison of our approach with the state-ofthe-art ATOM tracker",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Illustration of Boundary Response Truncation (BRT) and Peak Response Pooling (PRP) modules. First, with BRT, we clips the feature response map meanwhile aggregates the single-peak response. Then ,with PRP, we sum the horizontal and vertical pooling maps to aggregate multiple sub-peaks(the surrounding small dots in pooling maps) into a single centered peak(large dot) for target tracking. (Best viewed in color)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Comparison of the target response maps of the ATOM tracker (up) and SPSTracker (down).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The precision plots and success plots on OTB-2015, OTB-2013 and OTB-50 benchmarks.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "EAO ranking of the tested trackers on VOT2018.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "reports the details of the comparison with SiamMask (Wang et al. 2019),DWSiam (Zhipeng, Houwen, and Qiang 2019), CCOT (Danelljan et al. 2016b), TCNN (Nam et al. 2016), SSAT (Kristan et al. 2016),MLDF (Kristan et al. 2016), Staple (Bertinetto et al. 2016a), DDC (Kristan et al. 2016), EBT",
            "latex": null,
            "type": "figure"
        },
        "TABREF5": {
            "text": "Performance comparison on VOT-2018.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": ", the proposed SPSTracker achieves the best performance on the three benchmarks, by obtaining 0.692, 0.703 and 0.658 AUC scores on OTB-2015 and OTB-2013, OTB-50, respectively. Compared with ATOM (Danelljan et al. 2019), SPSTracker improves by Ours ATOM UPDT CCOT ECO MDNet HDT DaSiamRPN FCNT SRDCF BACF",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "Performance comparison on the NFS dataset.Figure 8: Qualitative results of state-of-the-art trackers on sequences Box, Matrix, ClifBar, Ironman, Deer and CarScale. SP-STracker can localize objects with interference from either foreground or backgrounds. In contrast, other compared methods have failure cases. (Best viewed in color with zoom in)",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "enforcement. We proposed plug-and-play Peak Response Pooling (PRP) to aggregate and align discriminative fea-tures, and designed Boundary Response Truncation (BRT) to reduce the variance of feature response. Based on PRP and BRT, we integrate multi-scale features in SPSTracker to learn the discriminative features for robust object tracking. SPSTracker achieved the new state-of-the-art performance on six widely-used benchmarks, which verifies the effectiveness of the proposed peak response modeling approach.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}