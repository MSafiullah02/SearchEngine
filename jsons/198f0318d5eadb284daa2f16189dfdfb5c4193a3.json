{
    "paper_id": "198f0318d5eadb284daa2f16189dfdfb5c4193a3",
    "metadata": {
        "title": "FRONT MATTER Title Arranging Test Tubes in Racks Using Combined Task and Motion Planning",
        "authors": [
            {
                "first": "W",
                "middle": [],
                "last": "Wan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Osaka University",
                    "location": {
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "T",
                "middle": [],
                "last": "Kotaka",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Miraca Research Institute G. K",
                    "location": {
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "K",
                "middle": [],
                "last": "Harada",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Osaka University",
                    "location": {
                        "country": "Japan"
                    }
                },
                "email": ""
            },
            {
                "first": "Affiliations",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "A robot manipulation system that separates and arranges test tubes in racks with the help of 3D vision and artificial intelligence (AI) reasoning/planning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The large amount of infections by COVID-19 drives people to perform thousands of polymerase chain reaction tests, antibody tests, etc. These tests require to handle a huge amount of test tubes, which are not only labor-intensive but also pressing. Employing a simple-to-use robot to do the job and consequently replace human labor is highly expected.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In Japan, we are collaborating with Miraca Research Institute G.K. (MRI) to develop a robotic manipulation system that helps to handle the test tubes for clinical examinations. MRI and its group company not only receive COVID-19, but also accept thousands of samples for various clinical tests every day. Employing human to handle their test tubes is a nightmare. Under this background, we started considering automatic tube manipulation. Our expectation is that a person puts a rack with mixed and non-arranged tubes in front of a robot. The robot performs recognition, reasoning, planning, manipulation, etc., and returns a rack with separated and arranged tubes. The system should be simple-to-use and there shall be no requests for expert knowledge in robotics.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We develop such a system using a robotic manipulator (Yumi, ABB) and a 3D vision sensor (Phoxi M, Photoneo). We realize a combined task and motion planner to autonomously generate the robot's logical sequences, grasping poses and motion trajectories. Compared with previous research (1, 2, 3) , our contribution is the combined planning with backward updates between vision, planning, and execution, which makes the system robust and simple-to-use. Our system workflow is shown in Figure 1 . A complete planning-execution-re-exploration demo is available in Movie S1.",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 286,
                    "text": "(1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 287,
                    "end": 289,
                    "text": "2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 290,
                    "end": 292,
                    "text": "3)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 481,
                    "end": 489,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": ""
        },
        {
            "text": "We have performed experiments to examine the developed system. In the first experiment, a person drops a rack in the workspace, and the robot arranges the tubes inside the rack. Results show that a person may ask the system to arrange tubes into different sections of the rack by simply specifying new goal patterns (Movie S2). The second experiment is more pragmatic. A robot is requested to move the tubes of a different type to an empty rack. The task is easier since an empty rack is less obstructed. The search gets faster and the planner finds more solutions (Movie S3). ",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Vision: The first step is recognizing racks and tubes. Since tubes are small and mostly transparent, conventional global search and model-matching based methods (4) are not applicable. Thus, instead of directly detecting tubes, we detect a rack first (5) . As tubes are always in the holes of a rack, we can then concentrate on the point clouds inside the rack holes and recognize the tubes locally.",
            "cite_spans": [
                {
                    "start": 251,
                    "end": 254,
                    "text": "(5)",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "System workflow"
        },
        {
            "text": "We train a neural network to perform the recognition. A registration process, which is shown inside the \"Preparation\" box of Figure 1 , is needed to collect the training data. During the registration, the tubes are manually arranged following a ground-truth pattern and are dropped under our vision sensor for perception. Since we know the ground truth, we know the correspondence between the perceived point clouds and the types of the tubes, and thus get the data and their labels for training. The trained neural network is later used in the \"Recognition\" box of Figure 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 125,
                    "end": 133,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 566,
                    "end": 574,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "System workflow"
        },
        {
            "text": "The combined task and motion planner: The combined planner has a hierarchical structure. At the highest level, the planner employs adapted heuristic search (6) to find a logical sequence for arrangement. This is illustrated by the \"Task planning\" box of Figure 1 . Here, a goal pattern needs to be prepared in advance to tell the planner where to rest different types of tubes. The goal pattern can be changed following the user's needs. The adapted heuristic search finds a logical sequence of moving the tubes according to the goal pattern. The obtained sequence at this level is logical rather than feasible since there is no consideration of the robotic mechanisms.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 254,
                    "end": 262,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "System workflow"
        },
        {
            "text": "In the middle level, the planner employs geometric reasoning (7) to determine if there are available grasps to grab the tubes identified by the logical sequence. Beforehand, we annotate or plan grasps considering the size of the tubes. Then, for a tube at its initial and goal poses in a logical step, we compute their collision-free and IK-feasible grasps. When there is an identical grasping pose for the tube at both its initial and goal poses, we determine the tube can be grasped by the robot. The geometric reasoning process is illustrated in the \"Grasp reasoning\" box of the figure. It solves geometric conflictions at static poses but there is no consideration about motion.",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 64,
                    "text": "(7)",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "System workflow"
        },
        {
            "text": "At the lowest level, the planner employs constrained probabilistic motion planning (8) to find motion trajectories for the robot. Constraints like tube directions are considered during planning to avoid splitting. The trajectories found by the planner in this level are directly executable by a robot.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System workflow"
        },
        {
            "text": "The lower levels update higher levels and trigger re-explorations recursively to ensure the completeness of the planner. The red arrows in the Figure indicate the flow of this update. A failed grasp reasoning will invalidate the current pick-and-place pair and trigger a new task planning. A failed motion planning will invalidate the current candidate configuration pair and iterate to the next one. It will act in the same way as a failed grasp reasoning if all configuration pairs are invalid.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System workflow"
        },
        {
            "text": "Error Recovery: The planned results are not necessarily successful during execution. The reasons could be wrong visual detection, slippery in the finger pads, changes in rack positions, etc. (9) The system will recover by triggering a new visual detection and restart the combined planner from the \"Recognition\" box.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System workflow"
        },
        {
            "text": "We have shown that the developed system can arrange test tubes in racks without much human intervention. This owes to the help of the 3D visual detection and the combined planner that plans sequences and trajectories at different levels. We expect such a system to play an important role in helping managing public health (10) . In the future, we hope such a system could play more hard-core roles in clinical manipulation like handling mixers and pipettes.",
            "cite_spans": [
                {
                    "start": 322,
                    "end": 326,
                    "text": "(10)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Text S1. More explanations about the system input. There are three inputs to the system: 1. A tube rack with random tubes; 2. The goal pattern; 3. A tube rack with known ground truth for machine learning. Item 1 is needed each time a new task is to be performed. Item 2 is set once when people hope the robot arranges the tubes into a new pattern. Item 3 is required when unforeseen tubes need to be registered. The frequency of these three inputs are 1 >> 2 >> 3. In Figure 1 , items 1 and 3 are illustrated by the \"Unarranged racks\" box and the \"Registration\" box respectively. They must be the real rack and tubes dropped under the vision sensor since we need to collect point cloud data for them. Especially for item 3, the tubes must be prepared following the ground truth. Item 2 is shown by the \"Goal pattern\" box. It could either be a real rack dropped by users under the vision sensor or directly inputted into the system by using a keyboard. In the former case, the system detects the goal sections for different tubes using the trained neural network.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 468,
                    "end": 476,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "SUPPLEMENTARY MATERIALS"
        },
        {
            "text": "Text S2. More explanations about the detection. The rack can be found using Iterative Closest Point (ICP)-based template matching since it is made of solid materials and has good geometric features. The workflow is shown in Figure S1 . First, we crop the potential point area using segmentation (i.e., DBSCAN, Density-Based Spatial Clustering of Applications with Noise). Then, Principal Component Analysis (PCA) is applied to roughly align the main axes of the cropped point cloud and the template. ICP is used after the alignment for fine adjustment. Compared to the rack, the tubes are difficult to be detected as (1) there are reflective noises that are difficult to tell, (2) they are transparent and the captured point cloud has holes. For these reasons, we train a neural network to automatically learn how to classify them. Text S3. Details of the adapted heuristic search. In the adapted heuristic search, each state of the tube rack is treated as a node. The cost of expanding into a new node is computed as the total cost of previous nodes plus a heuristic (the displacements of the tubes in the new node from the goal pattern). Whether we can expand to a new node is determined by using simple filters to check whether a tube in one rack hole can be moved to another. The simple filters play the role of a rough collision detector and examine if holes are empty so that gripper fingers or tubes can be potentially inserted. Figure S2(a) shows six exemplary simple filters. These filters roughly assume that a tube can be picked up when its vertical, horizontal, or diagonal adjacent rack holes are empty. Also, it assumes an empty hole could be used to rest tubes under the same condition. The simple filters are a rough estimation of collisions at the logical layer. Exact examinations are done later in geometric reasoning.",
            "cite_spans": [
                {
                    "start": 617,
                    "end": 620,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 224,
                    "end": 233,
                    "text": "Figure S1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1436,
                    "end": 1448,
                    "text": "Figure S2(a)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "SUPPLEMENTARY MATERIALS"
        },
        {
            "text": "Using the filters, the adapted heuristic search expands to new nodes like Figure S2 (c). There are several possible children to expand to. The adapted heuristic search determines the next one by selecting the node that has smallest g(x)+h(x), where g(x) is the number of steps to move from the very initial node to the next node (the total cost), h(x) is the number of tubes that are not in the expected sections of the goal pattern (the heuristic). The computation is illustrated in Figure S2 (b). When two children have the same g(x)+h(x), the algorithm will select the one that has a smaller h(x). Text S4. The backward updates. The backward updates evolve the adapted heuristic search by using a weight map to trace the failures. The weight map is essentially a list of records remembering the failures at the rack holes. When failure happens, the planner adds a record to the list so that the heuristic search could avoid expanding to recorded nodes. The \"Weight map\" dash box in Figure S2 (a) and the red arrows pointing into the \"Task planning\" box in Figure 1 show where the weight map functions.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 74,
                    "end": 83,
                    "text": "Figure S2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 484,
                    "end": 493,
                    "text": "Figure S2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 985,
                    "end": 994,
                    "text": "Figure S2",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1059,
                    "end": 1067,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "SUPPLEMENTARY MATERIALS"
        },
        {
            "text": "Text S5. Investigating re-explorations and backward updates of the example in the main article. Figure S3 shows the complete re-explorations and backward updates during planning the task in the main article. The abbreviations TP, GR, and MP represent Task Planning, Grasp Reasoning, and Motion Planning respectively. The numbers after them are the sequential ID. The blue arrows indicate success in both grasp reasoning and motion planning. The orange arrows indicate a successful in grasp reasoning but failed motion planning. The red arrows indicate a failed grasp reasoning (In this case, there are no candidate configuration pairs and the workflow cannot move forward to motion planning.). When there is a failure, either it would be an MP failure or GR failure, the system will update the weight map and trigger a re-exploration. The blue dash arrows in Figure S3 indicate the re-explorations. The example in the figure triggered 7 re-explorations before it finds a solution. The successful logical sequence is highlighted with blue shadows for the reader's convenience. It has 8 logical steps.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 96,
                    "end": 105,
                    "text": "Figure S3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 859,
                    "end": 868,
                    "text": "Figure S3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "SUPPLEMENTARY MATERIALS"
        },
        {
            "text": "Detailed visualization of the failures is illustrated in Figure S4 . Here, the bouquets after the logical steps illustrate the grasp reasoning. Each model in the bouquet is a candidate grasping pose. The ones colored by green indicate feasible grasps. The pink ones indicate collided grasps. The orange ones indicate IK-infeasible grasps. Grasp reasoning fails when there are no common green grasps between two nodes. In the same figure, the simulated robots after the bouquets illustrate the motion planning. A red robot arm indicates motion planning failed at that configuration. The time-lapse arms illustrate the successfully planned motion between the green configurations (The green configurations are the IK solutions of the green grasping poses). The abbreviations and their sequential IDs are identical to those in Figure S3 . The successful grasp reasoning, motion planning, and their correspondent logical sequence are also highlighted with blue shadows for the reader's convenience. Note that Figure S4 is not complete. Some parts of TP6 and TP7 are omitted to avoid a too large figure size. The final motion found by the planner is shown in Figure  S5 . Figure S3 . The IDs and colors are identical to those in Figure S3 . The bouquets after the logical steps illustrate the grasp reasoning. Each model in the bouquet is a candidate grasping pose. Green models indicate collision-free and IK-feasible grasps. The simulated robots after the bouquets illustrate the motion planning. A red robot arm indicates motion planning failed at that configuration. The time-lapse arms illustrate the successfully planned motion. The figure is not complete. Some parts of TP6 and TP7 are omitted to avoid a too large figure size. Figure S5 . The final motion trajectories found by the planner. The motion piece IDs on the upper right corners are identical to those shown in Figure S3 and S4.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 57,
                    "end": 66,
                    "text": "Figure S4",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 824,
                    "end": 833,
                    "text": "Figure S3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1005,
                    "end": 1014,
                    "text": "Figure S4",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1154,
                    "end": 1164,
                    "text": "Figure  S5",
                    "ref_id": null
                },
                {
                    "start": 1167,
                    "end": 1176,
                    "text": "Figure S3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1224,
                    "end": 1233,
                    "text": "Figure S3",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1730,
                    "end": 1739,
                    "text": "Figure S5",
                    "ref_id": null
                },
                {
                    "start": 1874,
                    "end": 1883,
                    "text": "Figure S3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "SUPPLEMENTARY MATERIALS"
        },
        {
            "text": "Text S6. Execution and error recovery. The real-world execution of the planned motion in Figure S5 is shown in Figure S6 . The motion piece IDs in the sub-titles are identical to those in Figure S5 . During the execution, the robot encounters a motor supervision error (overload) at (f) MP6.2. The reason is the squeezing force of the robot fingers caused a large reaction force from the table. At the failure, the system restarts by triggering a new visual recognition and combined planning. The robot gets a new sequence shown in TP9 and successfully re-plans and executes the motion in (i)-(l). A video showing the details of the plan-execution flow is in Movie S1. Some other examples could be found in Movie S2. Movie S1. A complete planning-execution-re-exploration demo.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 89,
                    "end": 98,
                    "text": "Figure S5",
                    "ref_id": null
                },
                {
                    "start": 111,
                    "end": 120,
                    "text": "Figure S6",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 188,
                    "end": 197,
                    "text": "Figure S5",
                    "ref_id": null
                }
            ],
            "section": "SUPPLEMENTARY MATERIALS"
        },
        {
            "text": "The robot is asked to arrange the tubes following different goal patterns.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movie S2. Two other examples."
        },
        {
            "text": "Text S7. Separating tubes into a new rack. When the goal is to separate tubes of a different type to a new rack, we do the following modification: 1. The node in the adapted heuristic search is changed to incorporate both states of the original rack and the new rack. 2. The simple filter additionally requires a tube must be picked up from the original rack and placed down into a hole in the new rack. Movie S3 shows two examples of separating into a new rack.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movie S2. Two other examples."
        },
        {
            "text": "Movie S3. Using the same framework to plan separating tubes into a new rack. The robot in the right video raises lower. Thus, the motion planning bears more workload.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movie S2. Two other examples."
        },
        {
            "text": "Text S8. The workload of motion planning. The pick-up and place-down motion are treated as a linear movement along the depth direction of the rack holes. Motion planning plays the role between the end of picking up and the start of placing down. The length of the linear movement is crucial for motion planning. A low length increases the workload of motion planning. On the other hand, a long length reduces the success rate to find a feasible motion. But the workload of motion planning is lighter and the planned trajectory is simpler and preferable. Figure S7 shows the comparison. Details can also be found in Movie S3. .1-4) The tube is raised higher. The planned motion is a straight line. The motion is preferable but the robot is sometimes not able to raise the tube that high. (b.1-4) The tube is raised lower. Motion planning bears a larger workload.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 554,
                    "end": 563,
                    "text": "Figure S7",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 625,
                    "end": 630,
                    "text": ".1-4)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Movie S2. Two other examples."
        },
        {
            "text": "Author contributions: W. Wan designed and developed the whole system, as well as led the writing of the paper. T. Kotaka contributed to the conception and financial supports. K. Harada provided suggestions to essential algorithms and contributed to the management. Competing interests: The authors declare that they do not have competing interests.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movie S2. Two other examples."
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Hierarchical task and motion planning in the now",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "P"
                    ],
                    "last": "Kaelbling",
                    "suffix": ""
                },
                {
                    "first": "L.-P",
                    "middle": [],
                    "last": "Tomas",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of 2011 IEEE International Conference on Robotics and Automation",
            "volume": "",
            "issn": "",
            "pages": "1470--1477",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Combined task and motion planning through an extensible planner-independent interface layer",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Riano",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chitnis",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Russell",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of 2014 IEEE International Conference on Robotics and Automation",
            "volume": "",
            "issn": "",
            "pages": "639--646",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Can robots assemble an IKEA chair?",
            "authors": [
                {
                    "first": "S.-R",
                    "middle": [],
                    "last": "Francisco",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Q.-C",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Science Robotics",
            "volume": "3",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Iterative continuous convolution for 3D template matching and global localization",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Guizilini",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ramos",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of 32nd AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "6493--6500",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A survey and technology trends of 3D features for object recognition",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hashimoto",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Akizuki",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Takei",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEJ Transactions on Electronics, Information and Systems",
            "volume": "136",
            "issn": "",
            "pages": "1038--1046",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Object search by manipulation",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "R"
                    ],
                    "last": "Dogar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "Koval",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tallavajhula",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Srinivasa",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Autonomous Robots",
            "volume": "36",
            "issn": "",
            "pages": "153--167",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Geometric backtracking for combined task and motion planning in robotic systems",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bidot",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Karlsson",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Lagriffoul",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Saffiotti",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Artificial Intelligence",
            "volume": "247",
            "issn": "",
            "pages": "229--265",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Sampling-based methods for motion planning with constraints",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Kingston",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Moll",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "E"
                    ],
                    "last": "Kavraki",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Robotics, and Autonomous Systems",
            "volume": "1",
            "issn": "",
            "pages": "159--185",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Technique of recovery process and application of AI in error recovery using task stratification and error classification",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nakamura",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nagata",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Harada",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Yamanobe",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Journal of Robotics, Networking and Artificial Life",
            "volume": "5",
            "issn": "",
            "pages": "56--62",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Combating COVID-19 -The role of robotics in managing public health and infectious diseases",
            "authors": [
                {
                    "first": "G.-Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Science Robotics",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The workflow of our tube-handling system. The blue arrows are the main forward stream. The red arrows are the failure stream. The gray arrows denote the data that should be prepared in advance.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Detecting the rack and extract the tube point cloud clusters. (a) Sensor configuration. (b) The workflow: Segmentation is done using DBSCAN. Rough pose estimation is done using PCA. Fine pose estimation is done using ICP.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Filtering and expansion in the adapted heuristic search. (a) Using simple filters and a weight map to determine expansion. (b) The heuristic functions. (c) An exemplary expansion.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Five of the motion pieces (MP1.0, MP6.2, MP7.0, MP7.1, MP7.2) move the red tubes. The remaining (MP3.0, MP6.0, and MP6.1) moves the blue tubes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The complete re-explorations and backward updates during planning the task in the main text. TP, GR, and MP represent Task Planning, Grasp Reasoning, and Motion Planning respectively. A blue arrow indicates success in both grasp reasoning and motion planning. An orange arrow indicates a successful grasp reasoning but failed motion planning. A red arrow indicates a failed grasp reasoning. A blue dash arrow indicates a re-exploration.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Detailed visualization of the failures and re-explorations in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Executing the planned trajectories. The robot gets a motor supervision error (overload) at (f) MP6.2, performs a re-planning in (h), and successfully executes the new motion in (i)-(l).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Comparing the workload of motion planning. (a",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}