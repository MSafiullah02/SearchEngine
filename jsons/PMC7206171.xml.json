{
    "paper_id": "PMC7206171",
    "metadata": {
        "title": "Multi-level Memory Network with CRFs for Keyphrase Extraction",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Tao",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "email": "zhou.tao1@outlook.com",
                "affiliation": {}
            },
            {
                "first": "Yuxiang",
                "middle": [],
                "last": "Zhang",
                "suffix": "",
                "email": "yxzhang@cauc.edu.cn",
                "affiliation": {}
            },
            {
                "first": "Haoxiang",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "email": "zhu.hx@outlook.com",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Automatic keyphrase extraction is to recommend a set of representative phrases that are related to the main topics discussed in a document. Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing tasks such as information extraction, text summarization and question answering [21]. However, the performance of existing methods is still far from being satisfactory [10]. The main reason is that it is very challenging to determine whether a phrase or sets of phrases can accurately capture main topics that are presented in the document.",
            "cite_spans": [
                {
                    "start": 364,
                    "end": 366,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 452,
                    "end": 454,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Existing methods for keyphrase extraction can be broadly divided into unsupervised and supervised methods. Specifically, unsupervised approaches directly treat keyphrase extraction as a ranking problem, scoring each word using various measures such as TF-IDF (term frequency-inverse document frequency) and graph-based ranking scores (e.g., degree centrality or PageRank score) [7, 8, 22, 27]. Supervised methods usually treat the keyphrase extraction as a binary classification task, in which a classifier is trained on the features of labeled keyphrases to determine whether a candidate phrase is a keyphrase [9, 10, 19]. Compared with unsupervised methods, supervised approaches can yield good results given sufficient training samples.",
            "cite_spans": [
                {
                    "start": 379,
                    "end": 380,
                    "mention": "7",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 382,
                    "end": 383,
                    "mention": "8",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 385,
                    "end": 387,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 389,
                    "end": 391,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 612,
                    "end": 613,
                    "mention": "9",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 615,
                    "end": 617,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 619,
                    "end": 621,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Recently, EK-CRF [9] employed the CRFs to extract keyphrases from scientific research articles, which was trained on token-based features incorporating linguistic, document structure information, and expert knowledge. This work achieved better performance on keyphrase extraction task and was shown to be state-of-the-art in previous traditional supervised methods. However, if we can not consider the features used in EK-CRF, CRFs only capture local structural dependencies. In addition, EK-CRF mainly relies on manual feature engineering, which may require considerable effort and domain-specific knowledge.",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 19,
                    "mention": "9",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this work, we aim to capture the long-range contextual information hidden in the text sequence and remove the need of manual feature engineering to extract keyphrases form scientific papers. Specifically, we formulate the keyphrase extraction as a sequence labeling task. We first use the memory network [23] to capture the long-range contextual information hidden in text data. Note that although plain recurrent neural networks (RNNs) can encode the sequential text and their variants such as long short-term memory (LSTM) models can further capture non-local patterns, they still exhibit a significant local bias in practice [14]. In order to make full use of the effective information hidden in text sequence, we extend the input memory of the memory network with two different levels: sentence level and document level. Secondly, we use the CRF model to capture the dependencies between adjacent words in text sequence and determine whether a candidate phrase is a keyphrase. Finally, we conduct comprehensive experiments over two publicly available datasets (KDD and WWW) in Computer Science area. Experimental results show that the proposed approach outperforms several state-of-the-art supervised methods.",
            "cite_spans": [
                {
                    "start": 308,
                    "end": 310,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 632,
                    "end": 634,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The remainder of this paper is organized as follows. We firstly summarize related works on keyphrase extraction and memory networks in Sect. 2. Secondly, the proposed model for keyphrase extraction is described in Sect. 3. Then, the datasets, experimental results and discussions are illustrated in Sect. 4. Finally, we conclude this paper in Sect. 5.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "As mentioned in Sect. 1, existing approaches for keyphrase extraction can be broadly divided into unsupervised and supervised methods. This work is mainly related to supervised methods which have been proven to be effective in the keyphrase extraction task. Research on supervised methods has focused on two issues: classifier selection and feature design. Current state-of-the-art classifiers typically include Na\u00efve Bayes [3, 24], decision trees [19], CRFs [9], deep recurrent neural networks (RNN) [16], etc. The features used to represent an instance can be broadly divided into three categories: statistical features, structural features and syntactic features [10].",
            "cite_spans": [
                {
                    "start": 425,
                    "end": 426,
                    "mention": "3",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 428,
                    "end": 430,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 449,
                    "end": 451,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 460,
                    "end": 461,
                    "mention": "9",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 502,
                    "end": 504,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 667,
                    "end": 669,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Keyphrase Extraction ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Zhang et al. [26] is the first to use the CRFs extracting keyphrases, which provides a way to explore the local contextual information in text sequence and traditional features, to identify each candidate word by sequence labeling. Bhaskar et al. [2] employed CRFs trained mainly on linguistic features such as part-of-speech (POS), chunking and named-entity tags for keyphrase extraction. Gollapalli et al. [9] also utilized CRFs to extract keyphrases from research papers, which was trained on token-based features incorporating linguistic, document-structure information and expert knowledge. CopyRNN [16] is the first to employ the sequence-to-sequence (Seq2Seq) deep learning model to predict keyphrases for documents. Following CopyRNN, a few extensions have been proposed to help better generate keyphrases [4, 28]. In addition, Alzaidy et al. [1] integrated the CRF with the bidirectional long short term memory networks (LSTMs) to extract keyphrases from research papers. However, this method didn\u2019t capture the long-range contextual dependencies between words in text. We extend the memory network with stronger storage capacity to jointly maintain local structural information provided by RNNs with long-range dependencies in the long text.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 16,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 248,
                    "end": 249,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 409,
                    "end": 410,
                    "mention": "9",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 605,
                    "end": 607,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 815,
                    "end": 816,
                    "mention": "4",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 818,
                    "end": 820,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 852,
                    "end": 853,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Keyphrase Extraction ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Despite of the success of the RNNs on various text modeling tasks, simple RNNs still exhibit a significant local bias in practice [25]. Memory network [23] enhances the long-term memory capability of deep network by augmenting the internal memory with a series of extra memory components, and provides a general approach for modeling long-range dependencies and making multi-hop reasoning, which has advanced many NLP tasks such as question answering [20] and reading documents [17]. Sukhbaatar et al. [20] proposed the end-to-end memory network, which can be trained end-to-end without any intervention. Kumar et al. [12] proposed the dynamic memory network, which uses a sentence-level attention mechanism to update its internal memory during multi-hop inference. Miller et al. [17] encoded prior knowledge by introducing a key memory structure which stores facts to address to the relevant memory value. Taking inspiration from these works, we design the multi-level memory network with CRFs to extract keyphrases from research papers.",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 133,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 152,
                    "end": 154,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 452,
                    "end": 454,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 479,
                    "end": 481,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 503,
                    "end": 505,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 619,
                    "end": 621,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 781,
                    "end": 783,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Memory Network ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Keyphrase extraction is formulated as a task of sequence labeling, predicting a label for each word in the input text sequence. More specifically, we denote the input source document as a sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{x} = \\{x_{1}, x_{2}, ..., x_{l}\\}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_{t}$$\\end{document} represents t-th input word and l is the length of the sequence. The goal of the model is to predict a sequence of labels \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{y} = \\{y_{1}, y_{2}, ..., y_{l}\\}$$\\end{document}, where each label \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_t$$\\end{document} corresponding to the input word \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_t$$\\end{document} represents whether \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_t$$\\end{document} is a keyphrase word or not keyphrase word.",
            "cite_spans": [],
            "section": "Problem Definition ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Figure 1 illustrates the overview of the Multi-Level Memory network with CRFs (MLM-CRF) for keyphrase extraction. This model includes two main parts: the memory layer, capturing long-range dependencies using the deep memory network; and the CRF layer, capturing local dependencies and labeling each word of input text sequence with five different labels (detailed in Subsect. 4.3).",
            "cite_spans": [],
            "section": "Model Overview ::: Methodology",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The memory layer can be divided into two parallel modules at different levels: sentence level memory and document level memory. Each module further includes three components similar to the works [15, 20]: (1) the input memory vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{m}}}_{t}$$\\end{document}, which captures the information from the word embedding layer of the text sequence; (2) the current input embedding \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{u}}}_t$$\\end{document}, which is the representation of the current word; and (3) the output vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{c}}}_{t}$$\\end{document}, which is similar to the input memory \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{m}}_{t}$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 196,
                    "end": 198,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 200,
                    "end": 202,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Model Overview ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "In addition, the output memory representation of the memory layer \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{o}}$$\\end{document}, summarizing the long-range semantic and structure information from the input text sequence without distance limitation, is calculated by a weighted sum over the output representations, in which the attention weights are determined by measuring the similarity between the input memory vector and the current input embedding. Finally, the output of the memory layer \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{o}}$$\\end{document} is fed into the CRF layer to predict keyphrases for documents using sequence labelling model CRF. In the remainder of this section, we will present the MLM-CRF in detail.\n",
            "cite_spans": [],
            "section": "Model Overview ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Input Memory Representation. At first, the word embedding look-up table, trained by GloVe [18], is applied to map each word \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_t$$\\end{document} in the text sequence into an embedding vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{x}}_{t}$$\\end{document}. Although we can directly use this embedding vector as the input memory representation in the context of the memory network, in order to tackle the drawback of insensitivity to temporal information between memory cells [15], we obtain the input memory representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{m}}_t$$\\end{document} by adopting the bidirectional gate recurrent unit (GRU) [5] to encode the word embedding vectors \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{x}}_{t}$$\\end{document}:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\overrightarrow{{\\varvec{m}}}_t = \\overrightarrow{\\mathbf{GRU }}({\\varvec{x}}_t, \\overrightarrow{{\\varvec{m}}}_{t-1}) \\end{aligned}$$\\end{document}\n2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\overleftarrow{{\\varvec{m}}}_t = \\overleftarrow{\\mathbf{GRU }}({\\varvec{x}}_t, \\overleftarrow{{\\varvec{m}}}_{t+1}) \\end{aligned}$$\\end{document}\n3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{m}}_t = \\mathtt {tanh}(\\overrightarrow{{\\varvec{W}}}_{m}\\overrightarrow{{\\varvec{m}}}_{t} + \\overleftarrow{{\\varvec{W}}}_{m}\\overleftarrow{{\\varvec{m}}}_{t} + {\\varvec{b}}_{m}) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{{\\varvec{W}}}_{m}$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overleftarrow{{\\varvec{W}}}_{m}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{b}}_{m}$$\\end{document} are three trainable parameters to adjust the input memory representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{m}}_t$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 93,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 981,
                    "end": 983,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1385,
                    "end": 1386,
                    "mention": "5",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Current Input Representation. In order to calculate the attention weight of each element in the input memory, we enforce the current input to be in the same space as the input memory. More specifically, we use the obtained \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{m}}_{t}$$\\end{document} to represent the current input representation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}_{t}$$\\end{document}, i.e., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}_{t}={\\varvec{m}}_{t}$$\\end{document}. Note that as illustrated in Fig. 1, the current input \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}_{t}$$\\end{document} in the sentence level and the document level is set to the same. We will detail it in the subsection Extensions.",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": [
                {
                    "start": 1233,
                    "end": 1234,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Attention Weight Calculation. We compute the attention weight of each element in the input memory by measuring the relevance between the current input \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}_{t}$$\\end{document} and each element of the input memory \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{m}}_{i}$$\\end{document} with a softmax function:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} p_{t,i} = \\mathtt {softmax}({\\varvec{u}}^{T}_{t}{\\varvec{m}}_{i}) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\mathtt softmax(x _{i}) = \\frac{e^{x _{i}}}{\\sum _{j}e^{x _{j}}}$$\\end{document}.",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Output Memory Representation. Similar to the input memory vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{m}}_{t}$$\\end{document}, the output vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{c}}_{i}$$\\end{document} is also the contextual representation vector which is used to capture the contextual semantic information of the text sequence. The output vector is also encoded by bidirectional GRU, but with different parameters in the GRUs function and tanh layers of Eq. (1), (2) and (3).",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "The output vector is used to generate the final output memory of the memory layer \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{o}}_{t}$$\\end{document}, which is the weighted sum over the attention weight and the output vector, as:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{o}}_{t} = \\sum _{i}p_{t,i} {\\varvec{c}}_{i}. \\end{aligned}$$\\end{document}The output memory allows the model to have unrestricted access to elements in previous steps as opposed to a single hidden state in RNNs, which will helps the CRF fully utilize the long-range dependencies in the text sequence to better predict keyphrases for documents.",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Extensions. Many existing works [6, 17] discussed the influence of the different lengths covered by the attention mechanism in the memory network. Inspired by these works, we explore the different length of the input memory at two different levels: the sentence level and document level.",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 34,
                    "mention": "6",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 36,
                    "end": 38,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "In the sentence level, the attention mechanism covers just the sentence containing the current word, and the corresponding attention weight is calculated as:6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} p_{t,i}^{s} = \\mathtt {softmax}({\\varvec{u}}^{T}_{t}{\\varvec{m}}_{i}^{s}) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i \\in [1,n]$$\\end{document} and n is the length of the sentence.",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "In the document level, the whole document is covered by the attention mechanism and the attention weight as computed as:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} p_{t,i}^{d} = \\mathtt {softmax}({\\varvec{u}}^{T}_{t}{\\varvec{m}}_{i}^{d}) \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i \\in [1,l]$$\\end{document} and l is the length of the document. The current input \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}_{t}$$\\end{document} used in both the sentence level and the document level is set to the same.",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Then we calculate the final output memory of the memory layer as:8\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{o}}_t = \\lambda \\sum _{i}p_{t,i}^{d}{\\varvec{c}}_{i}^{d}+(1-\\lambda )\\sum _{i}p_{t,i}^{s}{\\varvec{c}}_{i}^{s} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda $$\\end{document} is a hyper-parameter, which is set to adjust weight from the different levels. Thus we replace Eq. (4) and (5) with Eq. (6), (7) and (8). The output memory of the memory layer can capture the information from not only the local but also the long-distance context by using the two-level output vectors.",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "We can further extend the model by stacking multiple memory hops for capturing multiple fact from the memory, which stacks hops between the current input \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}_{t}$$\\end{document} and the k-th hop \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{o}}^{k}_{t}$$\\end{document} to be the input to the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(k + 1)$$\\end{document}-th hop:9\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{u}}^{k+1}_{t} = {\\varvec{u}}^{k}_{t} +{\\varvec{o}}^{k}_{t} \\end{aligned}$$\\end{document}In our model, we simply limit the hop to only 1.",
            "cite_spans": [],
            "section": "Memory Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "In this section, we feed the output of the memory layer into the CRF layer for extracting keyphrases. CRF [13] has been proven to be effective for sequence labeling tasks. We use CRF jointly with the memory network to predict the sequence of labels for the keyphrase extraction task.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 109,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "CRF Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "Given the input text sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{x} = \\{x_{1}, x_{2}, ..., x_{l}\\}$$\\end{document} and the sequence of output labels \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{y} = \\{y_{1}, y_{2}, ..., y_{l}\\}$$\\end{document}, the score is computed as:10\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} s(\\mathbf{x} ,\\mathbf{y} ) = \\sum _{t=0}^{l}{\\varvec{A}}_{y_{t},y_{t+1}} + \\sum _{t=1}^{l}{\\varvec{P}}_{t,y_{t}} , \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{P}}$$\\end{document} is the linearly transformed matrix from the output matrix of memory network \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{U}}$$\\end{document} such that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{P}}={\\varvec{U}}{\\varvec{W}}$$\\end{document}. The size of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{U}}$$\\end{document} is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l \\times d$$\\end{document}, where d is the size of the output vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}$$\\end{document}. The size of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}}$$\\end{document} is the weight matrix with the size of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d \\times k$$\\end{document}, where k is the number of labels. Thus, the size of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P $$\\end{document} is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l \\times k$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p_{i,j}$$\\end{document} represents the score of the j-th label of the i-th word of the input sequence. In addition, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{A}}$$\\end{document} is the matrix of transition scores, in which \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_{i,j}$$\\end{document} represents the score of a transition from the label i to label j.",
            "cite_spans": [],
            "section": "CRF Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "The probability of the label sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{y} $$\\end{document} can be calculated by the softmax function as follow:11where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{Y} _\\mathbf{x} $$\\end{document} represents all possible label sequences given a input sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{x} $$\\end{document}. In the training procedure, we maximize the log-probability of the correct label sequence:12This objective function and its gradients can be efficiently computed by dynamic programming algorithm. In order to find the best sequence of labels during decoding, the Viterbi algorithm is employed to decode the label sequence efficiently by maximizing the score :13\n",
            "cite_spans": [],
            "section": "CRF Layer ::: Methodology",
            "ref_spans": []
        },
        {
            "text": "To analyze the effectiveness of our model for keyphrase extraction, we conduct comparative experiments on two scientific publication datasets provided by Caragea et al. [3], which are from two top-tier machine learning conferences: ACM Knowledge Discovery and Data Mining (KDD) and ACM World Wide Web (WWW). Each dataset consists of the research paper titles, abstracts and corresponding author manually labeled keyphrases (gold standard).",
            "cite_spans": [
                {
                    "start": 170,
                    "end": 171,
                    "mention": "3",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Datasets ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "A detail description of datasets is summarized in Table 1, containing the total number of abstracts and keyphrases in the original dataset (#Abs/#KPs(All)), the number of abstracts for which at least one author-labeled keyphrase could be located and the total number of keyphrases located (#Abs/#KPs(Locatable)), the percentage of keyphrases not present in the abstracts (MissingKPs), the average number of keyphrase per paper (AvgKPs), and the number of keyphrases with one, two, three and more than three tokens found in these abstracts.",
            "cite_spans": [],
            "section": "Datasets ::: Experiments",
            "ref_spans": [
                {
                    "start": 56,
                    "end": 57,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Almost all previous works on keyphrase extraction use precision (P), recall (R) and F1-score (F1) to evaluate the results. Hence, we also keep our evaluation metric consistent. P, R and F1 are defined as follows:14\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} P = \\frac{\\mathbf {\\#}_{c}}{\\mathbf {\\#}_{e}}, R = \\frac{\\mathbf {\\#}_{c}}{\\mathbf {\\#}_{s}}, F1 = \\frac{2PR}{P+R} \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{\\# }_c$$\\end{document} is the number of correctly extracted keyphrases predicted by model, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{\\# }_e$$\\end{document} is the total number of keyphrases predicted by model and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{\\# }_s$$\\end{document} is the total number of standard keyphrases labeled by author.",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In our experiments, we partition the dataset into three groups using tenfold cross-validation: Onefold is used as the testing data; Onefold is used as the validating data; the remaining eightfolds are used as the training data. We report the average results of tenfold cross-validation.\n",
            "cite_spans": [],
            "section": "Evaluation Metrics ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In the experiment, we divide keyphrases into two categories: simple keyphrase (1-gram, referred to as SK) and complicated keyphrase (n-grams (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n>1$$\\end{document}), that is composed by two or more than two words, referred to as CK). Each word in dataset is labeled with non-keyphrase (O), simple keyphrase (SK) or complicated keyphrase (CK). For the complicated keyphrase, B-CK, M-CK and E-CK correspond to the beginning, middle and end word of CK, respectively. Thus, the number of labels is set to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=5$$\\end{document}.",
            "cite_spans": [],
            "section": "Implementation Details ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In order to eliminate the negative effects of different text formats, we convert the input text to lowercase in data pre-processing and employ two binary lexical features: whether the word contains digits or punctuation, which is similar to the works [11, 15], We use the 50-dimensional embeddings pre-trained by GloVe1 [18] on two corpora: Wikipedia20142 and Gigaword53. In addition, some parameters of model are empirically set as follows: the hyper-parameter in Eq. (8) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda =0.6$$\\end{document}, and the trainable parameters in Eq. (3) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overrightarrow{{\\varvec{W}}}_{m}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overleftarrow{{\\varvec{W}}}_{m} \\in \\mathbb {R}^{50\\times 50}$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{b}}_{m } \\in \\mathbb {R}^{50}$$\\end{document}. Dropout is applied to all GRU recurrent units on the input and output connections to avoid over-fitting, with a keep rate of 0.6 in the training procedure.",
            "cite_spans": [
                {
                    "start": 252,
                    "end": 254,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 256,
                    "end": 258,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 321,
                    "end": 323,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Implementation Details ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "To evaluate the performance of our method, we compare our method with four state-of-the-art keyphrase extraction methods, as follows:KEA [24], which employs a supervised Na\u00efve Bayes classifier to extract keyphrases using only two features: TF-IDF (i.e., term frequency-inverse document frequency) of a phrase and the distance of a phrase from the beginning of a document (i.e., its relative position).CeKE [3], which also uses a Na\u00efve Bayes classifier for extracting keyphrases from research papers embedded in citation networks. This work designs some novel features for keyphrase extraction based on citation context information and uses them in conjunction with traditional features which have been widely used in the previous supervised works of keyphrase extraction.EK-CRF [9], which is the state-of-the-art traditional supervised method, and uses the CRF algorithm based on sequence labeling to extract keyphrases from research papers. This method incorporates the expert-knowledge and domain-specific hints.CopyRNN [16], which is the first to employ sequence-to-sequence (Seq2Seq) framework with attention and copy mechanisms to generate keyphrases. This method is able to predict absent keyphrases that do not appear in the target document.M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{s}$$\\end{document}, which is the simplified model of our complete model MLM-CRF. M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{s}$$\\end{document} only uses the attention at sentence level. That is, the coverage of attention mechanism in memory network only depends on the length of the sentence, which the current word belongs to. In this model, the output memory vector and attention weight are calculated by the Eq. (5) and (6), respectively.M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{d}$$\\end{document}, which is the another simplified model of our complete model MLM-CRF. M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{d}$$\\end{document} only calculates attention weight between the current word and the whole input document. In this model, the output memory vector and attention weight are computed by the Eq. (5) and (7), respectively.\n",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 140,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 407,
                    "end": 408,
                    "mention": "3",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 779,
                    "end": 780,
                    "mention": "9",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1023,
                    "end": 1025,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Comparative Methods ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Table 2 shows the comparison of results of our model with other state-of-the-art supervised approaches. From Table 2, we can see that the overall results of KDD dataset is better than those of WWW, which is consistent with percentage of keyphrases not present in the abstracts (MissingKPs) in given research papers (MissingKPs = 51.12% on KDD, 56.39% on WWW), as given in Table 1. The benefit is that the experiment can reflect real application environment.",
            "cite_spans": [],
            "section": "Comparison with Supervised Prediction Methods ::: Experiments",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 115,
                    "end": 116,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 378,
                    "end": 379,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We first conduct experiments to compare the MLM-CRF with its two simplified models M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^s$$\\end{document} and M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^d$$\\end{document}. As the results given in Table 2, MLM-CRF gets the best results in terms of performance measures, and M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{d}$$\\end{document} achieves better results than M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{s}$$\\end{document}. These results indicate that long-range and more contextual information is more conducive to keyphrase extraction. More specifically, the contextual information captured by M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{d}$$\\end{document} is longer than by M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{s}$$\\end{document}, and MLM-CRF can capture the contextual information in the sentence level and document level.",
            "cite_spans": [],
            "section": "Comparison with Supervised Prediction Methods ::: Experiments",
            "ref_spans": [
                {
                    "start": 704,
                    "end": 705,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Secondly, we discuss the comparison of our model MLM-CRF with other comparative keyphrase prediction methods, including KEA, CeKE, EK-CRF and CopyRNN. As given in Table 2, MLM-CRF outperforms all comparative methods on two datasets, and even the M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{d}$$\\end{document} has a margin over the best performing extraction method CRF on the two test datasets. It is also worth mentioning that CeKE includes features based on the document-citation network, EK-CRF designs complex features integrating expert-knowledge and domain-specific hints during keyphrase extraction, and CopyRNN can generate absent keyphrases, whereas our model does not need to use extra knowledge, design complex features, and can not predict absent keyphrases.",
            "cite_spans": [],
            "section": "Comparison with Supervised Prediction Methods ::: Experiments",
            "ref_spans": [
                {
                    "start": 169,
                    "end": 170,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "In conclusion, the MLM-CRF can capture automatically much useful information from the source text for keyphrase extraction. Thus, our model gets the best results in terms of the performance measures, indicating that our method indeed outperforms the other approaches on all two datasets.\n",
            "cite_spans": [],
            "section": "Comparison with Supervised Prediction Methods ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We compare our different models in two different types of keyphrases: SK (including only single word, i.e., 1-gram) and CK (including several consecutive words, i.e., n-gram, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n \\ge 2$$\\end{document}). We first compare the MLM-CRF with its two simplified models M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^s$$\\end{document} and M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^d$$\\end{document} in SK and CK on both KDD and WWW datasets, respectively. As the results given in Table 3, both in SK and in CK, MLM-CRF gets the best results in terms of the performance measures, and M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{d}$$\\end{document} achieves better results than M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{s}$$\\end{document} on two datasets. These results indicate that long-range and more contextual information is more conducive to keyphrase extraction.\n",
            "cite_spans": [],
            "section": "Comparison in Different Types of Keyphrases ::: Experiments",
            "ref_spans": [
                {
                    "start": 1207,
                    "end": 1208,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Secondly, we compare the growth performance from the simplified method M-CRF\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^{s}$$\\end{document} to MLM-CRF in SK and CK. From Table 3, we can see that the growth of F1-score is 0.0781 for SK and 0.0450 for CK on KDD, and is 0.0305 for SK and 0.0190 for CK on WWW. It is obvious enough that the growth of F1-score in SK is more than that in CK on both KDD and WWW datasets. We can obtain the similar growth trends of Precision and Recall in SK and CK on two datasets. These results show that the 1-gram keyphrases have a stronger long-distance dependencies in text sequence than the n-gram (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n \\ge 2$$\\end{document}) keyphrases, which are more dependent on the local structural information. The experimental setup might be able to explain the main reason for these results. More specifically, for identifying keyphrases labeled by CK using the CRF model, different labels of CK are restricted, while for identifying keyphrases labeled by SK, the single SK label is totally unrestricted. For example, the B-CK label must be followed by the M-CK or E-CK label in the experiments.",
            "cite_spans": [],
            "section": "Comparison in Different Types of Keyphrases ::: Experiments",
            "ref_spans": [
                {
                    "start": 402,
                    "end": 403,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Finally, we discuss the performance of our models in different types of keyphrases SK and CK. As the results given in Table 3, we can see that our models can obtain better performance in CK than that in SK. The main reason may be that the percentage of 1-gram keyphrases in all keyphrases is significantly less than the percentage of n-gram (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n \\ge 2$$\\end{document}) keyphrases.",
            "cite_spans": [],
            "section": "Comparison in Different Types of Keyphrases ::: Experiments",
            "ref_spans": [
                {
                    "start": 124,
                    "end": 125,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "In this paper, we proposed a multi-level memory network with CRFs named MLM-CRF for extracting keyphrases from scientific research papers. In particular, we first extended the input memory of the memory network with two different levels (i.e., sentence level and document level) to capture the long-range contextual information hidden in text data. We then employed the CRF model to capture the structural dependencies between adjacent words in text sequence and determine whether a candidate phrase is a keyphrase. Our experimental results have shown that the proposed model MLM-CRF can significantly outperform the state-of-the-art supervised prediction approaches (including three extraction methods and one generation method) on both WWW and KDD datasets. In future, we plan to explore the more effective attention mechanism for taking much less computing costs in encoding the long document.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Statistics of the two benchmark datasets.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Comparison of the proposed models with other approaches\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Comparison of our different models in different type keyphrases.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: An overview of the MLM-CRF model for keyphrase extraction. It is shown only as a single hop.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "Automatic keyword extraction from documents using conditional random fields",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liao",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "J. Comput. Inf. Syst.",
            "volume": "4",
            "issn": "3",
            "pages": "1169-1180",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}