{
    "paper_id": "PMC7126336",
    "metadata": {
        "title": "Segmentation of DNA using simple recurrent neural network",
        "authors": [
            {
                "first": "Wei-Chen",
                "middle": [],
                "last": "Cheng",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Jau-Chi",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Cheng-Yuan",
                "middle": [],
                "last": "Liou",
                "suffix": "",
                "email": "cyliou@csie.ntu.edu.tw",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "DNA consists of nucleotides. Certain locations of the DNA possess special meanings. The beginning and the end of a gene are two important locations. Segment is the basic unit, or building block to interpret DNA. The intron, exon and transcription factor are sections of DNA and play different roles in the transcription process. A gene is also a segment that can be used for making protein. The collection of segmented DNA can be further analyzed to show how the genes regulate each other and how those segments works. However, the reason that the segments can only exist at certain locations and the rules behind them are still unclear.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "There are ways to accomplish the segmentation. One way to locate the beginning and the end of a segment is to search a similar sequence in the database. The idea behind this technique is that there exist similar patterns in different DNA sequences. In other words, the patterns in a strand of DNA sequence may have high possibility to be found in the strand of other DNA sequences. Researchers have dedicated to locate functional regions for decades. Statisticians try to locate the regions which satisfy the assumption of statistical models. Bernaola-Galvan et al. [1] provide a segmentation algorithm based on the Jensen\u2013Shannon entropic divergence. This algorithm is used to decompose long-range correlated DNA sequences into statistically significant, compositionally homogeneous patches. Fujiwara et al. [2] developed a hidden Markov model that represents known sequence characteristics of mitochondrial targeting signals to predict the existence of the mitochondrial targeting signals. The signal is the presequence that directs nascent proteins bearing it to mitochondria. Hidden Markov model were also used in extracting motifs for predicting the binding sites of unknown transcription factors, without a priori knowledge, from functionally related DNA sequences [3]. Machine learning methods are capable of building the models automatically and, then, the huge number of combinations of features can be tested [17], [18]. For example, Sonnenburg et al. [4] use the kernel weight to determine the exon start. Garc\u00eda-Pedrajas et al. [5] developed the methods to cope with class imbalance problems for decision tree and support vector machine [6], [7] in the problems of translation initiation site recognition.",
            "cite_spans": [
                {
                    "start": 566,
                    "end": 569,
                    "mention": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 809,
                    "end": 812,
                    "mention": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "mention": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1419,
                    "end": 1423,
                    "mention": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "mention": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1462,
                    "end": 1465,
                    "mention": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1540,
                    "end": 1543,
                    "mention": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1649,
                    "end": 1652,
                    "mention": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1654,
                    "end": 1657,
                    "mention": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A theory proposed that DNA sequences have language structures [8], [9]. There are also attempts [11], [12] to study the relationship between biological sequences and the Chomsky hierarchy [10]. The simple recurrent network (SRN) [13] is a hyper-Turing machine [14]. It has been shown [13], [15] that it can learn arbitrary underlying grammars and automata from the presentation of sentences. Such automata-like structure is extremely difficult to reach by any statistical ways, for example, hidden Markov model. It is also argued [16] that Elman network can accommodate quasi-regular structure and makes use of this structure for predictions and inferences. Such quasi-grammartical structure cannot be analyzed by any rule-based systems. We expect that the DNA sequence could contain such kind structures. So, this network is a potential candidate to analyze DNA sequence. Specifically, the large prediction errors indicate the segmentation points [13]. We show an example to reveal such quasi-regular structures in the end of Section 3.",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 65,
                    "mention": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 67,
                    "end": 70,
                    "mention": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 96,
                    "end": 100,
                    "mention": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 102,
                    "end": 106,
                    "mention": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 188,
                    "end": 192,
                    "mention": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 229,
                    "end": 233,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 260,
                    "end": 264,
                    "mention": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 284,
                    "end": 288,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 290,
                    "end": 294,
                    "mention": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 530,
                    "end": 534,
                    "mention": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 948,
                    "end": 952,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "SARS genome is used in the first experiment. Then we employ two types of SRN to analyze influenza A virus. One type uses the perceptrons in the hidden layer and the other type uses self-organizing neurons in the hidden layer. The former can be trained by the back-propagation algorithm (BP). The later can be trained by the self-organizing rule. We did extensive simulations to find suitable parameters for SRN. The reason why we analyze the influenza A virus is that its subtype H1N1 was the cause of human influenza in 2009. Its HA (Hemagglutinin) region is responsible for binding the virus to the cell and causes infection [19]. Since hemagglutinin is the major surface protein of the influenza A virus and is essential to the entry process into a cell, it is the primary target of neutralizing antibodies.",
            "cite_spans": [
                {
                    "start": 627,
                    "end": 631,
                    "mention": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "A simple recurrent network (or called Elman network) [13] is a three-layer neural network with the addition of a set of \u201ccontext neurons\u201d in the first layer, see Fig. 1\n. These context neurons assemble an inside self-reference layer. In each iteration, the previous state of the hidden layer saved in the context layer together with the input layer activates the hidden layer. This network maintains a stream of states which allows it to perform the sequence-prediction task. This network is proposed to model temporal human behaviors [13], like language. It can discover the underlying structure of words.",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 535,
                    "end": 539,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Architecture",
            "ref_spans": [
                {
                    "start": 162,
                    "end": 168,
                    "mention": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Elman generated sentences of varied lengths from fixed words. Those sentences were concatenated and formed a stream of words. Each word was represented as a combination of letters, and each letter was represented by a 5-bit randomly assigned binary vector. The network processed the concatenated binary vectors sequentially and was trained to predict the next letter by using the binary vector of the next letter as the desired output. Elman found that after training, the prediction error is very high at the beginning of a word and declines with the rest letters received. This implies that SRN has learned the various structures of words and is able to segment words from a sequence of letters.",
            "cite_spans": [],
            "section": "Architecture",
            "ref_spans": []
        },
        {
            "text": "Biologists use biotechnology (ex. polymerase chain reaction) to interact with a virus genome and look for interesting and meaningful regions (segments) of the sequence. Since genetic information is saved in the DNA sequence, we plan to use SRN to segment the sequence in a computational way. Based on the results Elman studied [13], we expect that SRN can learn the genome structure and detect the boundary of the protein coding region according to the prediction error. We further compare our findings with the protein coding regions found by other researchers.",
            "cite_spans": [
                {
                    "start": 327,
                    "end": 331,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Architecture",
            "ref_spans": []
        },
        {
            "text": "Consider a genome sequence {x(t), t\n = 0, 1, 2, \u2026}, where x(t) \u2208 {A(adenine), C(cytosine), T(thymine), G(guanine)}. Instead of using 2 bits to encode the four nucleotides, we use 4 bits to prevent non-uniform similarity (cosine or Euclidean distance) for each nucleotide pair because any nucleotide can be joined by ester bonds to the preceding nucleotide without bias. The four nucleotides are A \u2261 [1, \u22121, \u22121, \u22121]T, C \u2261 [\u22121, 1, \u22121, \u22121]T, T \u2261 [\u22121, \u22121, 1, \u22121]T, and G \u2261 [\u22121, \u22121, \u22121, 1]T. Each positive bit indicates one nucleotide. The number of dimensions of the context layer, which is the same as that of the hidden layer, is N. The number of dimensions of output layer is the same as that of the input layer. From extensive experiments, we set 20 hidden neurons in the first part of this work. The network has M\n = 4 input neurons, N\n = 20 hidden neurons, N\n = 20 context neurons, and M\n = 4 output neurons. Let the weight matrix W contain the set of synaptic weights that connects the input layer, context layer and the hidden layer, W\n \u2208 \nR\nN\u00d7(M+N+1). The weight matrix U contains the set of weights that connects the hidden layer and the output layer, U\n \u2208 \nR\nM\u00d7(N+1). The initial values of all synaptic weights in W and U are randomly assigned within the range [\u22120.2, 0.2]. The network is trained to predict the next nucleotide vector. For example, the input nucleotide at time t\n = 0 is x(0), and its desired output will be x(1). The input at time t\n = 1 is x(1), and the desired output will be x(2). The sequence of nucleotides is presented to the network one after another. For the convenience of mathematical expression, let the desired output d(0), \nd(1), \nd(2), \u2026 denote the input data at the next time step,(1)d(0)=x(1),d(1)=x(2),\u2026The error signal at the output of neuron i at time t is defined by(2)ei(t)=di(t)-yiout(t).The total error is obtained by summing over all neurons in the output layer,(3)\u03b6(t)=12\u2211i=14ei2(t).The input layer y\nin(t) consists of the input data at time t and the context layer which copies the activation of the hidden layer at the previous time step,(4)yin(t)=x(t)yhid(t-1).The initial activation of the context layer is set to zero, y\nin(0) = [x(0)T, 0 \u2026 0] T. The induced local field vihid(t) produced at the input of the activation function associated with hidden neuron i is(5)vihid(t)=\u2211j=0M+Nwijyjin(t),i\u2208{1,\u2026,N},where the synaptic weight w\ni0 (corresponding to the fixed input y0in=-1) is the bias. The induced local field viout(t) with the output neuron i is(6)viout(t)=\u2211j=0Nuijyjhid(t),i\u2208{1,\u2026,M}where the synaptic weight u\ni0 is the bias and y0hid=-1. Hence the function signal yihid appearing at the output of neuron i in the hidden layer at time t is(7)yihid=f(vihid(t)).The yiout appearing at the output of neuron i in the output layer is(8)yiout=f(viout(t)).In this work, we adopt the antisymmetric function, tanh(x)=e2x-1e2x+1, as the activation function of each neuron,(9)f(x)=tanh(x),and its derivative is(10)f\u2032(x)=(1+x)(1-x).Hence, the output of each neuron is in the range [\u22121, 1]. The initial error is equal to \u03b6(0) = 2. We expect that the nucleotide with a very large error could be the boundary of a protein coding region. The synaptic weights W and U are adjusted by the back-propagation algorithm [20] which performs gradient descent in error space. These weights are updated slightly in the direction that reduces error as much as possible to accomplish the expectation d(t) = \nx(t\n + 1) = \nE(x(t)) \u2248 \nx(t\n + 1). The correction for the weight in W is \u0394w\nij and it is proportional to the partial derivative,(11)\u0394wij(t)=-\u03b7(t)\u2202\u03b6(t)\u2202wij.where \u03b7 is a learning rate function. \u03b7 will be reduced to zero exponentially,(12)\u03b7(t)=\u03b70\u00d7e-a\u00d7(t-t0)t1-t0,where iteration t starts from t\n0. \u03b7\n0 is the initial value of the rate. Set \u03b7\n0\n = 0.5 and a\n = 6 in this work. The correction for the weight in U is \u0394u\nij,(13)\u0394uij(t)=-\u03b7(t)\u2202\u03b6(t)\u2202uij.\n",
            "cite_spans": [
                {
                    "start": 3259,
                    "end": 3263,
                    "mention": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Architecture",
            "ref_spans": []
        },
        {
            "text": "The SARS-CoV RNA has been detected frequently in respiratory specimens and convalescent-phase serum specimens from the patients having antibodies that react with SARS coronavirus. There is strong evidence that this virus is etiologically associated with the outbreak of SARS [21], [22], [23]. The genome has been analyzed by seeking the genes in the database. We select 11 complete genomes of SARS-CoV recorded in GenBank [24]. The accession numbers and their lengths (number of basepairs or bps in brief) are listed in Table 1\n. Note that the original record is a single-stranded positive sense RNA. Every selected sequence is the cDNA converted from its RNA. There is one-to-one correspondence between cDNA and RNA bases. We will use the cDNA sequence to train the network.",
            "cite_spans": [
                {
                    "start": 275,
                    "end": 279,
                    "mention": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 281,
                    "end": 285,
                    "mention": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 287,
                    "end": 291,
                    "mention": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 422,
                    "end": 426,
                    "mention": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Analysis of SARS genomes",
            "ref_spans": [
                {
                    "start": 520,
                    "end": 527,
                    "mention": "Table 1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "The network processes all 11 genome sequences which are concatenated in a long single sequence. We apply the BP algorithm to adjust its synaptic weights for 1000 epochs. The learning rate is reduced by (12) during the 1000 epochs. After training, we present the sequence again and record the prediction errors for all nucleotides. We repeat this training procedure for 300 times, hence, we obtain 300 trained SRNs and get 300 different prediction error sequences. Fig. 2\nplots the 300 learning curves during the training processes. Each error point in a curve is the average error of all nucleotides in the 11 sequences. The network initially outputs [\u22121, \u22121, \u22121, \u22121] for each input nucleotide pattern and the training makes the output to fit the next nucleotide in the sequence. Therefore, the training error is \u2211iei2(t)=2 at the beginning. The learning curve does not decrease monotonously because the algorithm updates the weights immediately after presenting one input nucleotide pattern. This figure shows that after 1000 epochs, the 300 networks reached to a local or global minimum in the weight space. Each procedure takes roughly 35 min and the whole experiment takes 175 hours per machine. Note that we use \u2211iei2(t) to be the error in this figure.",
            "cite_spans": [],
            "section": "Analysis of SARS genomes",
            "ref_spans": [
                {
                    "start": 464,
                    "end": 470,
                    "mention": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "In Fig. 3\n, we plot the averaged prediction error for each nucleotide along the genome of \u201cAY274119.3\u201d. Error magnitude is represented by the gray level, white represents the largest error and black represents no error. These prediction errors are the averaged error values obtained after the 300 training procedures. To give a clear picture, we further smooth the predicted errors over an interval of 501 nucleotides using a Gaussian function plotted on the top left corner of this figure. This genome has been analyzed in [25], its results are also illustrated in Fig. 3 by green color. The white vertical band near the 13 kB shows that this region has large errors and it is also detected by Marra as the boundary of S2 and S3. Note that kB is the abbreviation of kilo-basepairs. The large region from 26 to 29.5 kB corresponds to the fragments detected in [25], [26]. Marra\u2019s research shows that there are overlaps of the segments in this area [25]. For this genome, the maximal mean of prediction error is 2.1647, the minimal mean of prediction error is 1.1435, and the median of the mean is 1.7063.",
            "cite_spans": [
                {
                    "start": 524,
                    "end": 528,
                    "mention": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 860,
                    "end": 864,
                    "mention": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 866,
                    "end": 870,
                    "mention": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 948,
                    "end": 952,
                    "mention": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Analysis of SARS genomes",
            "ref_spans": [
                {
                    "start": 3,
                    "end": 9,
                    "mention": "Fig. 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 566,
                    "end": 572,
                    "mention": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Suppose that the 500 nucleotides which have the highest prediction errors are the boundaries of 501 segments. Fig. 4\n(a) plots the histogram of their length information. The shortest segment, which is \u201cCG\u201d, has only 2 base pairs. The longest segment has 364 base pairs. Note that all 500 peaks are cytosine. Most segments have short lengths. The segment which has a long length implies that this portion of the genome has fewer mutations than other parts. Some of the short segments are codons. These segments may reveal the structural information in the genome sequence. We plot several predicted segmentation points which near the protein coding region in Fig. 4(b). The blue vertical lines on the bottom of Fig. 4(b) indicate the boundaries of the segments obtained by [25]. There are five hits among thirteen known protein coding regions.",
            "cite_spans": [
                {
                    "start": 772,
                    "end": 776,
                    "mention": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Analysis of SARS genomes",
            "ref_spans": [
                {
                    "start": 110,
                    "end": 116,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 658,
                    "end": 664,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 710,
                    "end": 716,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "\nTable 2\nlists the detailed 15 genes of the identified SARS genome by the research [25]. The \u201chead\u201d means the beginning of a gene and the \u201ctail\u201d means the end of a gene along the genome location. The \u201cClosest Pt.\u201d indicates the closest point, segmented by SRN, to the head or tail point. The \u201cORF\u201d means the open reading frame. The work [25] focuses on the segments which begin with the start codon \u2018ATG\u2019 and end with the stop codon \u2018TGA\u2019, \u2018TAA\u2019, \u2018TAG\u2019. It then searches the biological meaning of such segments in various databases. Fig. 4(b) shows two biologically identified protein coding regions, spike glycoprotein and small envelope E protein. They belong to coronavirus and have nucleotides ATGTTTATTTT \u2026 ATTACACATAA and ATGTACTCATT \u2026 TTCTGGTCTAA. These two regions are marked by S5, S6, S11, and S12 in this figure. The SRN finds the stop codon \u2018TAAA\u2019 in three cases and the start codon \u2018CGAAC\u2019 in all four cases.",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 87,
                    "mention": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 337,
                    "end": 341,
                    "mention": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Analysis of SARS genomes",
            "ref_spans": [
                {
                    "start": 533,
                    "end": 539,
                    "mention": "Fig. 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 1,
                    "end": 8,
                    "mention": "Table 2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Among the 501 segments, we list all short segments of lengths shorter than seven base pairs, <7, in Table 3\nand construct a tree from them, see Fig. 5\n. From this tree, we see the number of nodes doesn\u2019t grow exponentially with tree layers. This means those segments aren\u2019t composed from \u201cA\u201d, \u201cC\u201d, \u201cT\u201d, \u201cG\u201d arbitrarily. They follow certain structural rules and need further biological studies.",
            "cite_spans": [],
            "section": "Analysis of SARS genomes",
            "ref_spans": [
                {
                    "start": 144,
                    "end": 150,
                    "mention": "Fig. 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 100,
                    "end": 107,
                    "mention": "Table 3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "We assume DNA sequences are structured like languages which are quasi-regular: they allow the combination of some members of syntactic categories, but not others. For example, the sentences: \u201cI gave food to the orphanage\u201d and \u201cI gave the orphanage food\u201d are both correct. However, if we replace \u201cgave\u201d with \u201cdonated\u201d, the sentence \u201cI donated the orphanage food\u201d is wrong. From the tree in Fig. 5, we find the combinations of nodes are not symmetrical. It means that SRN has the capability to extract quasi-regular rule from DNA sequences.",
            "cite_spans": [],
            "section": "Analysis of SARS genomes",
            "ref_spans": [
                {
                    "start": 389,
                    "end": 395,
                    "mention": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "For comparison, we further use the unsupervised SRN [31], [32] to process the H1N1 sequences. The results are plotted in Fig. 7 marked by SOR. This unsupervised SRN was proposed by Voegtlin. The self-organizing neurons are used in the hidden layer and context layer; see Fig. 1. The topology of these neurons is a grid square map. These neurons use time-delay feedback to represent the information hidden in time. This recursive feedback makes this network different from the original self-organizing map [33]. The synaptic weights are updated according to the self-organizing rule [33].",
            "cite_spans": [
                {
                    "start": 52,
                    "end": 56,
                    "mention": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 58,
                    "end": 62,
                    "mention": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 505,
                    "end": 509,
                    "mention": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 582,
                    "end": 586,
                    "mention": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Analysis H1N1 using unsupervised simple recurrent network ::: Analysis of H1N1 sequences",
            "ref_spans": [
                {
                    "start": 121,
                    "end": 127,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 271,
                    "end": 277,
                    "mention": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The 137 sequences are used to train this unsupervised network. After extensive trials, we set the network with 8 \u00d7 8 hidden neurons and use it to analyze H1N1 virus. The results are marked by SOR in Fig. 7. The training procedure is repeated 50 times. All 50 learning curves are recorded in Fig. 8\n(b). The learning 50 curves for the SRN with 40 hidden neurons and trained by BP are also plotted in Fig. 8(a). The BP learning curves show that the SRN tries to find information and rules in time and the rules compete against each other. We see that the curve jumps up and down rapidly. But the learning curve obtained by the self-organizing rule is relatively well behaved. The sequence closest to the center is used for calculating the prediction errors and these errors are plotted and marked with SOR in Fig. 7. There are 50 converged errors. We sort these 50 errors from top to bottom and show their prediction errors in Fig. 7(d). Note that Fig. 7(d) plot the smoothed prediction errors by a Gaussian low pass filter with a window size of 31. Stronger intensity indicates higher error in the figure. In supervised BP learning, the nucleotides in the high error regions are less predictable. In unsupervised learning, the high error regions show the nucleotides are away from the statistical center in time domain. The best converged error is plotted on the top of the image Fig. 7(d).",
            "cite_spans": [],
            "section": "Analysis H1N1 using unsupervised simple recurrent network ::: Analysis of H1N1 sequences",
            "ref_spans": [
                {
                    "start": 199,
                    "end": 205,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 291,
                    "end": 297,
                    "mention": "Fig. 8",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 399,
                    "end": 405,
                    "mention": "Fig. 8",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 807,
                    "end": 813,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 925,
                    "end": 931,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 946,
                    "end": 952,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 1379,
                    "end": 1385,
                    "mention": "Fig. 7",
                    "ref_id": "FIGREF6"
                }
            ]
        },
        {
            "text": "In order to visualize the structure in time, we employ the hierarchical clustering method [34] to classify the activations of the hidden layer of SRN. This method was used in Elman\u2019s work [13] to group the meanings of words. It aggregates the clusters, which have minimum distances, and constructs a binary tree by merging clusters. After constructing the tree, one can cut the leaf nodes by setting a threshold distance. In the communication between Plate and Elman, they have noticed that the activation of hidden neurons is affected by the input, \u201c\u2026 The hidden unit activation patterns are highly dependent upon preceding inputs\u2026 \u201d, see line 2 of page 199 in [13]. In Fig. 9\n, we generate the dendrogram with no more than eight leaf nodes instead of four in order to visualize more information. Setting eight clusters in this case means each one of the four clusters, corresponding to the four nucleotides, is further divided into two groups. The colors of the 8 leaf nodes are listed on the top of this figure. The cluster intensities are assigned by the levels of the leaf nodes. This is because nodes in the same cluster should have similar intensities. For example, in Fig. 9(a1), the node 5 has an intensity black which corresponds to grey code 1. Node 7 has an intensity as that of code 2 and node 6 has an intensity code 3 and so on. Similar structures can be found in the two different methods. Group (1, 7, 2) in (a2) is similar to group (5, 4, 6) in (b2). Without considering the link length, (a2) is isomorphic to (b2), this is because there is a bijective mapping from nodes (5, 6, 1, 7, 2, 3, 8, 4) in (a2) to nodes (3, 2, 5, 4, 6, 1, 8, 7) in (b2). This means these two methods catch similar structure in time.",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 94,
                    "mention": "[34]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 188,
                    "end": 192,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 662,
                    "end": 666,
                    "mention": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Clustering hidden activations ::: Analysis of H1N1 sequences",
            "ref_spans": [
                {
                    "start": 671,
                    "end": 677,
                    "mention": "Fig. 9",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 1176,
                    "end": 1182,
                    "mention": "Fig. 9",
                    "ref_id": "FIGREF8"
                }
            ]
        },
        {
            "text": "The hierarchical clustering process confines the representation of the relations in a tree-like structure. We use Isomap [35] and multidimensional scaling (MDS) [36] to visualize the hidden activations in a two dimensional space, see Fig. 10\n. The colors of leaf nodes obtained from hierarchical clustering are kept in this figure. The grey links between points show the adjacent temporal relations along the genome sequence. One activation follows the other activation if there is a link between them. In Isomap, the number of neighborhoods are set to 60, 300, 350 in Fig. 10 (a1), (b1) and (c1) respectively. The number of neighborhoods are also set to 60, 300, 350 in Fig. 10 (a2), (b2), and (c2). Fig. 10 (a1\u2013a4) are obtained from the best trained networks. We see that the best trained SRN, in Fig. 10(a1) and (a3), can resolve the activations according to their appearances in the genome sequence. This is, in some sense, similar to the polysemous of a word. Fig. 10(b1\u2013b4) are obtained from the best 15 trained networks. Fig. 10(c1\u2013c4) are obtained from all 50 trained networks.",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 125,
                    "mention": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 161,
                    "end": 165,
                    "mention": "[36]",
                    "ref_id": "BIBREF35"
                }
            ],
            "section": "Visualizing hidden activations in two dimensional space ::: Analysis of H1N1 sequences",
            "ref_spans": [
                {
                    "start": 234,
                    "end": 241,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 569,
                    "end": 576,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 671,
                    "end": 678,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 701,
                    "end": 708,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 799,
                    "end": 806,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 965,
                    "end": 972,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 1028,
                    "end": 1035,
                    "mention": "Fig. 10",
                    "ref_id": "FIGREF9"
                }
            ]
        },
        {
            "text": "The residual variances in Fig. 11\nshow how much information is captured with respect to dimensionality by the two dimension reduction algorithms, Isomap and MDS. The residual variances of Isomap may not decrease monotonously for SRN trained by the BP algorithm. The residual variance decreases as the dimensionality is increased for SRN trained by the self-organizing rule. Four dimensions are enough to catch most variances of the hidden activations for the H1N1 sequences.",
            "cite_spans": [],
            "section": "Visualizing hidden activations in two dimensional space ::: Analysis of H1N1 sequences",
            "ref_spans": [
                {
                    "start": 26,
                    "end": 33,
                    "mention": "Fig. 11",
                    "ref_id": "FIGREF10"
                }
            ]
        },
        {
            "text": "This work presents a new technology to study genome sequences. Without any prior biological knowledge and only processing the ATCG sequences, the result is strikingly consistent with the findings from biologists. This implies that we can use this new technology to study more complicated genomes which are still a mystery to biologists. The underlying structures detected by SRN provide new types of features for further biological studies. By ranking the errors, this technology provides the priorities for biologists to choose which part of the genomes is worth to study. The results of the proposed segmentation method can be used in distinguishing an artificial DNA segment from an natural segment, because the nucleotides joined together in the natural environment may be different from the one joined in the laboratory.",
            "cite_spans": [],
            "section": "Summary",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1: Information on the 11 SARS genomes.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2: Comparison of the segmentation locations.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3: List of short segments that have lengths less than 7.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4: Setting of parameters (a).\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5: Setting of parameters (b).\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6: Setting of parameters (c).\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1: The structure of the recurrent neural network used in the analysis of DNA sequence.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2: Recorded 300 learning curves. The colors of curves indicate their converged mean square errors. 287 curves reach to values lower than 1.8.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3: The averaged prediction errors of the SARS \u201cAY274119.3\u201d genome. Each vertical band in the image shows a value that is averaged over an interval of 501 nucleotides by Gaussian function. This function is plotted on the top left corner. S1 to S30 indicate the beginning points and ending points of biologically identified 15 segments in [25]. Five segments belong to coronavirus. The rest ten segments are still unknown.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 4: (a) There are 100 bins in the histogram. Each bin has an interval of length 40 base pairs. (b) The error peaks marked by green color that are near the boundaries of the protein coding regions. The boundaries are marked by blue vertical lines. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig. 5: Tree derived from short segments as listed in Table 3. The nodes which are the ends of protein coding regions are marked in blue color. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Fig. 6: The networks are trained by back-propagation. (a1) The learning curves with different numbers of hidden neurons. (a2) The histogram of the converged errors from (a1). (b1) The learning curves with different numbers of training DNA sequences. (b2) The histogram of the converged errors from (b1). (c1) The learning curves of different lengths of training DNA sequences. (c2) The histogram of the converged errors from (c1).",
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Fig. 7: This figure plots the averaged errors along the closest sequence. (a) The averaged prediction errors of the best trained SRN with lowest converged error. (b) The averaged prediction errors of the best 15 trained SRNs that have smallest 15 converged errors. (c) The averaged prediction errors over all 50 simulations. (d) All prediction errors of the 50 simulation. The performance of these 50 trained networks are sorted from top to bottom.",
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Fig. 8: The plots show all the 50 learning curves of two methods. (a) The learning curves by BP. (b) The learning curves by self-organizing rule.",
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Fig. 9: Hierarchical clustering diagram of the activations of hidden layer in influenza analysis. Each intensity indicates a cluster. The intensities are assigned according to the levels of the leaf nodes. The plots show the results of supervised (a) and unsupervised learning (b) for different trained networks. (1\u20133) are the trees constructed from the activations of the hidden neurons that have the minimum converged error, minimum 15 converged errors, and all 50 trained networks respectively.",
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Fig. 10: This figure shows the results of mapping the hidden activations in two dimensional space. The 8 colors are 8 clusters by hierarchical clustering method. The grey links show the transits of hidden states.",
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Fig. 11: The residual variances of Isomap (circles), MDS (cross) in different dimensions for the hidden activations of the trained SRN, Isomap 1 and MDS 1 are plots for the best trained network. Isomap 15 and MDS 15 are for the best 15 trained networks. Isomap 50 and Isomap 50 are for all 50 trained networks. Each curve is normalized within zero and one in the y-axis.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Compositional segmentation and long-range fractal correlations in dna sequences",
            "authors": [
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Bernaola-Galvan",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Roman-Roldan",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Oliver",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Physical Review E",
            "volume": "53",
            "issn": "5",
            "pages": "5181-5189",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Prediction of mitochondrial targeting signals using hidden markov models",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Fujiwara",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Asogawa",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Nakai",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Genome Informatics",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Automatic extraction of motifs represented in the hidden markov model from a number of dna sequences",
            "authors": [
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Yada",
                    "suffix": ""
                },
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Totoki",
                    "suffix": ""
                },
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Ishikawa",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Asai",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Nakai",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Bioinformatics",
            "volume": "14",
            "issn": "4",
            "pages": "317-325",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Constructing support vector machine ensemble",
            "authors": [
                {
                    "first": "H.-C.",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "H.-M.",
                    "middle": [],
                    "last": "Je",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Kim",
                    "suffix": ""
                },
                {
                    "first": "S.Y.",
                    "middle": [],
                    "last": "Bang",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Pattern Recognition",
            "volume": "36",
            "issn": "",
            "pages": "2757-2767",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Gene structure prediction by linguistic methods",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Searls",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Genomics",
            "volume": "23",
            "issn": "",
            "pages": "540-551",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "The language of genes",
            "authors": [
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Searls",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Nature",
            "volume": "420",
            "issn": "",
            "pages": "211-217",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [
                {
                    "first": "N.",
                    "middle": [],
                    "last": "Chomsky",
                    "suffix": ""
                }
            ],
            "year": 1957,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Durbin",
                    "suffix": ""
                },
                {
                    "first": "S.R.",
                    "middle": [],
                    "last": "Eddy",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Krogn",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Mitchison",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Baldi",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "ren Brunak",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Finding structure in time",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Elman",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Cognitive Science",
            "volume": "14",
            "issn": "",
            "pages": "179-211",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Analog computation via neural networks",
            "authors": [
                {
                    "first": "H.T.",
                    "middle": [],
                    "last": "Siegelmann",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Theoretical Computer Science",
            "volume": "131",
            "issn": "",
            "pages": "331-360",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Extracting finite structure from infinite language",
            "authors": [
                {
                    "first": "T.",
                    "middle": [],
                    "last": "McQueen",
                    "suffix": ""
                },
                {
                    "first": "A.",
                    "middle": [],
                    "last": "Hopgood",
                    "suffix": ""
                },
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Allen",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Tepper",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Knowledge-Based Systems",
            "volume": "18",
            "issn": "",
            "pages": "135-141",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "A data mining approach to discover unusual folding regions in genome sequences",
            "authors": [
                {
                    "first": "S.-Y.",
                    "middle": [],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "W.",
                    "middle": [],
                    "last": "min Liu",
                    "suffix": ""
                },
                {
                    "first": "J.V.",
                    "middle": [],
                    "last": "Maizel",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Knowledge-Based Systems",
            "volume": "15",
            "issn": "",
            "pages": "243-250",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Use of mobile phones as intelligent sensors for sound input analysis and sleep state detection",
            "authors": [
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Ondrej",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Jakub",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Dalibor",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Sensors",
            "volume": "11",
            "issn": "",
            "pages": "6037-6055",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Rumelhart",
                    "suffix": ""
                },
                {
                    "first": "G.",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Williams",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "",
            "volume": "Vol. 1",
            "issn": "",
            "pages": "318-362",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "A novel coronavirus associated with severe acute respiratory syndrome",
            "authors": [
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Ksiazek",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Erdman",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Goldsmith",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "The New England Journal of Medicine",
            "volume": "348",
            "issn": "",
            "pages": "1953-1966",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Coronavirus as a possible cause of severe acute respiratory syndrome",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Peiris",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "L.",
                    "middle": [],
                    "last": "Poon",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "The Lancet",
            "volume": "361",
            "issn": "",
            "pages": "1319-1325",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Identification of a novel coronavirus in patients with severe acute respiratory syndrome.",
            "authors": [
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Drosten",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "G\u00fcther",
                    "suffix": ""
                },
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Preiser",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "The New England Journal of Medicine",
            "volume": "348",
            "issn": "",
            "pages": "1967-1976",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "Genbank",
            "authors": [
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Benson",
                    "suffix": ""
                },
                {
                    "first": "I.",
                    "middle": [],
                    "last": "Karsch-Mizrachi",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Lipman",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Ostell",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Wheeler",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Nucleic Acids Research",
            "volume": "33",
            "issn": "",
            "pages": "D34-D38",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "The genome sequence of the sars-associated coronavirus",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Marra",
                    "suffix": ""
                },
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "C.",
                    "middle": [],
                    "last": "Astell",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Science",
            "volume": "300",
            "issn": "",
            "pages": "1399-1404",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [
                {
                    "first": "M.",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "K.",
                    "middle": [],
                    "last": "Holmes",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "The influenza virus resource at the national center for biotechnology information",
            "authors": [
                {
                    "first": "Y.",
                    "middle": [],
                    "last": "Bao",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Bolotov",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Dernovoy",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Journal of Virology",
            "volume": "82",
            "issn": "",
            "pages": "596-601",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "Manifold construction based on local distance invariance",
            "authors": [
                {
                    "first": "W.-C.",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "C.-Y.",
                    "middle": [],
                    "last": "Liou",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Memetic Computing",
            "volume": "2",
            "issn": "",
            "pages": "149-160",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "Multiple sequence alignment with the clustal series of programs",
            "authors": [
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Chenna",
                    "suffix": ""
                },
                {
                    "first": "H.",
                    "middle": [],
                    "last": "Sugawara",
                    "suffix": ""
                },
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Koike",
                    "suffix": ""
                },
                {
                    "first": "R.",
                    "middle": [],
                    "last": "Lopez",
                    "suffix": ""
                },
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Gibson",
                    "suffix": ""
                },
                {
                    "first": "D.",
                    "middle": [],
                    "last": "Higgins",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Thompson",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Nucleic Acids Research",
            "volume": "31",
            "issn": "",
            "pages": "3497-3500",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Voegtlin",
                    "suffix": ""
                },
                {
                    "first": "P.",
                    "middle": [],
                    "last": "Dominey",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Advances in Self-Organizing Maps",
            "volume": "",
            "issn": "",
            "pages": "210-215",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "Recursive self-organizing maps",
            "authors": [
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Voegtlin",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Neural Networks",
            "volume": "15",
            "issn": "",
            "pages": "979-991",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "Self-organized formation of topologically correct feature maps.",
            "authors": [
                {
                    "first": "T.",
                    "middle": [],
                    "last": "Kohonen",
                    "suffix": ""
                }
            ],
            "year": 1982,
            "venue": "Biological Cybernetics",
            "volume": "",
            "issn": "",
            "pages": "59-69",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "Hierarchical clustering schemes.",
            "authors": [
                {
                    "first": "S.",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                }
            ],
            "year": 1967,
            "venue": "Psychometrika",
            "volume": "32",
            "issn": "3",
            "pages": "241-254",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "A global geometric framework for nonlinear dimensionality reduction",
            "authors": [
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Tenenbaum",
                    "suffix": ""
                },
                {
                    "first": "S.V.",
                    "middle": [],
                    "last": "de",
                    "suffix": ""
                },
                {
                    "first": "J.",
                    "middle": [],
                    "last": "Langford",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Science",
            "volume": "290",
            "issn": "",
            "pages": "2319-2323",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "Multidimensional scaling: I theory and method",
            "authors": [
                {
                    "first": "W.",
                    "middle": [],
                    "last": "Torgerson",
                    "suffix": ""
                }
            ],
            "year": 1952,
            "venue": "Psychometrika",
            "volume": "17",
            "issn": "",
            "pages": "401-419",
            "other_ids": {
                "DOI": []
            }
        }
    }
}