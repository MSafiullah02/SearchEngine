{
    "paper_id": "PMC7206249",
    "metadata": {
        "title": "Improving Multi-turn Response Selection Models with Complementary Last-Utterance Selection by Instance Weighting",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Kun",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "email": "franciszhou@pku.edu.com",
                "affiliation": {}
            },
            {
                "first": "Wayne",
                "middle": [
                    "Xin"
                ],
                "last": "Zhao",
                "suffix": "",
                "email": "batmanfly@gmail.com",
                "affiliation": {}
            },
            {
                "first": "Yutao",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "email": "yutao.zhu@umontreal.ca",
                "affiliation": {}
            },
            {
                "first": "Ji-Rong",
                "middle": [],
                "last": "Wen",
                "suffix": "",
                "email": "jrwen@ruc.edu.cn",
                "affiliation": {}
            },
            {
                "first": "Jingsong",
                "middle": [],
                "last": "Yu",
                "suffix": "",
                "email": "yjs@ss.pku.edu.cn",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Recent years have witnessed remarkable progress in retrieval-based open-domain conversation systems [3, 6]. In the past few years, various methods have been proposed for response selection [1, 3, 16, 22]. A key problem in response selection is how to measure the matching degree between a conversation context and a response candidate. Many efforts have been made to construct an effective matching model with neural architectures [16, 22].",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 102,
                    "mention": "3",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 104,
                    "end": 105,
                    "mention": "6",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 190,
                    "end": 191,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 193,
                    "end": 194,
                    "mention": "3",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 196,
                    "end": 198,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 200,
                    "end": 202,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 432,
                    "end": 434,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 436,
                    "end": 438,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To construct the training data, a widely adopted approach is pairing a positive response with several randomly selected utterances as negative responses, since the labeling of true negative responses is very time-consuming. Although such method does not require labeled negative data, it is likely to bring noise during the random sampling process for negative responses. In real-world datasets, a randomly selected response is likely to be \u201cfalse negative\u201d, in which the sampled response can reply to the last-utterance but is considered as a negative response. For example, the general utterance \u201cOK!\u201d or \u201cIt\u2019s great.\u201d can safely respond to many conversations. As shown in existing studies [1, 7, 15], the noise from random sampling will severely affect the performance of the matching model.\n",
            "cite_spans": [
                {
                    "start": 693,
                    "end": 694,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 696,
                    "end": 697,
                    "mention": "7",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 699,
                    "end": 701,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "However, we do not have any labeled data related to true negative samples. To address this difficulty, we find inspiration from the recent progress made in complementary learning [14, 17]. We design a main-complementary task pair. As shown in Fig. 1, the left side is the main task (i.e., our focus) which selects the correct response given the last utterance and context, while the right side is the complementary task which selects the last utterance given the response and context. To implement such a connection, we derive a weighted margin-based optimization objective for the main task. This objective is general to work with various matching models. It elegantly utilizes different prospects in utterance selection, either last-utterance selection or response selection. The main task is assisted by the complementary task, and finally, its performance is improved.",
            "cite_spans": [
                {
                    "start": 180,
                    "end": 182,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 184,
                    "end": 186,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Introduction",
            "ref_spans": [
                {
                    "start": 248,
                    "end": 249,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "To summarize, the major novelty lies in that the proposed approach can capture different supervision signals from different perspectives, and it is effective to reduce the influence of noisy data. The approach is general and flexible to apply to various deep matching models. We conduct extensive experiments on two public data sets, and experimental results on both data sets indicate that the models learned with our approach can significantly outperform their counterparts learned with other strategies.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Recently, data-driven approaches for chatbots [3, 9] have achieved promising results. Existing work can be categorized into generation-based methods [6, 9, 11, 20] and retrieval-based methods [3, 18, 21]. The first group of approaches learn response generation from the data. Based on the sequence-to-sequence structure with attention mechanism [11], multiple extensions have been made to tackle the \u201csafe response\u201d problem and generate informative responses [6, 20]. The retrieval-based methods try to find the most reasonable response from a large repository of conversational data [3, 16]. Recent work pays more attention to context-response matching for multi-turn response selection [16, 18, 22].",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 48,
                    "mention": "3",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 50,
                    "end": 51,
                    "mention": "9",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 150,
                    "end": 151,
                    "mention": "6",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 153,
                    "end": 154,
                    "mention": "9",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 156,
                    "end": 158,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 160,
                    "end": 162,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 193,
                    "end": 194,
                    "mention": "3",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 196,
                    "end": 198,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 200,
                    "end": 202,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 346,
                    "end": 348,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 460,
                    "end": 461,
                    "mention": "6",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 463,
                    "end": 465,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 585,
                    "end": 586,
                    "mention": "3",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 588,
                    "end": 590,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 689,
                    "end": 691,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 693,
                    "end": 695,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 697,
                    "end": 699,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Instance weighting is a semi-supervised approach proposed by Grandvale et al. [2]. The key idea is to utilize weighted margin-based optimization to train the model with a weight function to produce a reward for each instance. Then, researchers used this method to promote the model in noisy training data [8], and extended this method to other tasks [1, 4]. A recent work showed that the instance weighting strategy can be extended to different machine learning models and validated the improvement in different tasks.",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 80,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 306,
                    "end": 307,
                    "mention": "8",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 351,
                    "end": 352,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 354,
                    "end": 355,
                    "mention": "4",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Our work is inspired by the work of using new learning strategies to distinguish the noise in training data [7, 10, 15]. Shang et al. [10] and Lison et al. [7] utilized instance weighting strategy in open domain dialog systems via simple methods. Wu et al. [15] altered the negative sampling strategy and utilized a sequence-to-sequence model to distinguish false negative samples. Feng et al. [1] proposed three co-teaching mechanisms to reduce noise.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 110,
                    "mention": "7",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 112,
                    "end": 114,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 116,
                    "end": 118,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 135,
                    "end": 137,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 157,
                    "end": 158,
                    "mention": "7",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 258,
                    "end": 260,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 395,
                    "end": 396,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Different from aforementioned works, we utilize the last-utterance selection task as the complementary task to assist the response selection task by computing the instance weights. This complementary task is similar to the main task since it just exchanges the last utterance with the response. Our method is similar to a dual-learning approach and the difference is that the complementary model is not optimized together with the main model but only provides the instance weights to assist the main task. Besides, the two tasks own the same neural architecture, but leverage different supervision signals from the data.",
            "cite_spans": [],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "We denote a conversation as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{u_1,\\cdots ,u_j,\\cdots ,u_n\\}$$\\end{document}, where each utterance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_j$$\\end{document} is a conversation sentence. A dialogue system is built to give the next utterance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{n+1}$$\\end{document} to reply \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_n$$\\end{document}. We refer to the last known utterance (i.e.,\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_n$$\\end{document}) as last-utterance, and the utterance to be predicted (i.e.,\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u_{n+1}$$\\end{document}) as response.",
            "cite_spans": [],
            "section": "Preliminaries",
            "ref_spans": []
        },
        {
            "text": "We assume a training set represented by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {D}=\\{\\langle U_{qi}, q_i, r_i, y_i \\rangle \\}^{N}_{i=1}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$U_{qi}$$\\end{document} denotes the previous utterances \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{u_1,\\cdots ,u_{n-1}\\}$$\\end{document}. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_{i}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_{i}$$\\end{document} denote the last-utterance and response respectively. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_i$$\\end{document} is a label indicating whether \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_i$$\\end{document} is an appropriate response to the entire conversation context consisting of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$U_{qi}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q_i$$\\end{document}.",
            "cite_spans": [],
            "section": "Preliminaries",
            "ref_spans": []
        },
        {
            "text": "A retrieval-based dialogue system is designed to select the correct response r from a candidate response pool \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {R}$$\\end{document} based on the context (namely \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$U_{q}$$\\end{document} and q). This is also commonly called multi-turn response selection task [16, 18]. Formally, we usually solve this task by learning a matching model between last utterance and response given the context to compute the conditional probability of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Pr}(y=1 | q, r, U_q)$$\\end{document}, which indicates the probability that r can appropriately reply to q. For simplification, we omit \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$U_q$$\\end{document} and represent the probability by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Pr}(y=1 | q, r)$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 801,
                    "end": 803,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 805,
                    "end": 807,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Preliminaries",
            "ref_spans": []
        },
        {
            "text": "A commonly adopted loss for the matching model is the Cross-Entropy as:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{CE}=-\\sum _{i=1}^{N} \\big [y_{i}\\cdot \\log \\big (\\text {Pr}(y_i | q_i,r_i)\\big )+(1-y_{i})\\cdot \\log \\big (1-\\text {Pr}(y_i | q_i,r_i)\\big )\\big ]. \\end{aligned}$$\\end{document}This is indeed a binary classification task. The optimization loss drives the probability of the positive utterance to be one and the negative utterance to be zero.\n",
            "cite_spans": [],
            "section": "Preliminaries",
            "ref_spans": []
        },
        {
            "text": "Previous methods treat all sampled responses equally, which is easily influenced by the noise in training data. To address this problem, we propose a general weighted-enhanced optimization objective. We consider a pairwise setting: each training instance consists of a positive response and a negative response for a last utterance, denoted by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r^{+}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r^{-}$$\\end{document}. For convenience, we assume each positive response is paired with a single negative sample.",
            "cite_spans": [],
            "section": "A Pairwise Weight-Enhanced Optimization Objective ::: Approach",
            "ref_spans": []
        },
        {
            "text": "The basic idea is to minimize the Weighted Margin-based Loss in a pairwise way, which is defined as:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{WM} = \\sum _{i=0}^{N} w_{i} \\cdot \\max \\big \\{ \\text {Pr}(y=1 | r^{-}_i , q_i )-\\text {Pr}(y=1 | r^{+}_i, q_i )-\\gamma ,0\\big \\}, \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_i$$\\end{document} is the weight for the i-instance consisting of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r^{+}_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r^{-}_i$$\\end{document}. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma \\ge 0$$\\end{document} is a parameter to control the threshold of difference. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Pr}(y=1 | r^{+}_i , q_i )$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Pr}(y=1 | r^{-}_i , q_i )$$\\end{document} denote the conditional probabilities of an utterance being an appropriate and inappropriate response for q. When the probability of a negative response is larger than a positive one, we penalize it by summing the difference into the loss. This objective is general to work with various matching methods.",
            "cite_spans": [],
            "section": "A Pairwise Weight-Enhanced Optimization Objective ::: Approach",
            "ref_spans": []
        },
        {
            "text": "A major difficulty in setting weights (shown in Eq. 1) is that there is no external supervision information. Inspired by the recent progress made in self-supervised learning and co-teaching [1, 7], we leverage supervision signals from the data itself. Since response selection aims to select a suitable response from a candidate response pool, we devise a complementary task (i.e., last-utterance selection) that is trained with an assistant signal for setting the weights.",
            "cite_spans": [
                {
                    "start": 191,
                    "end": 192,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 194,
                    "end": 195,
                    "mention": "7",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Instance Weighting with Last-Utterance Selection Model ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Last-Utterance Selection. Similar to response selection, here \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^{-}$$\\end{document} can be sampled negative utterances. The complementary task captures data characteristics from a different perspective, so that the learned complementary model can be used to set weights by providing evidence on instance importance.",
            "cite_spans": [],
            "section": "Instance Weighting with Last-Utterance Selection Model ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Instance Weighting. After learning the last-utterance selection model, we now utilize it to set weights for training instances. The basic idea is if an utterance is a proper response, it should well match the real last-utterance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^{+}$$\\end{document}. On the contrary, for a true negative response, it should be uninformative to predict the last-utterance. Therefore, we introduce a new measure \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta $$\\end{document} to compute the degree that an utterance is a true positive response as:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\varDelta _r=\\text {Pr}(y=1| q^{+}, r) -\\text {Pr}(y=1 | q^{-}, r), \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Pr}(y=1| q^{+}, r)$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Pr}(y=1 | q^{-}, r)$$\\end{document} are the conditional probabilities of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^{+}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^{-}$$\\end{document} learned by the last-utterance selection model. In this way, a false negative response tends to yield a large \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta $$\\end{document} value, since it is able to reply to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^{+}$$\\end{document} and contains useful information to discriminate between \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^{+}$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$q^{-}$$\\end{document}. With this measure, we introduce our solution to set the weights defined in Eq. 2. Recall that a training instance is a pair of positive and \u201cnegative\u201d utterances, and we want to assign a weighted score indicating how much attention the response selection model should pay. Intuitively, a good training instance should be able to provide useful information to discriminate between positive and negative responses. We define the instance weighting formula as:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} w_{i}= \\min \\big \\{\\max \\{\\varDelta _{r_i^{+}}-\\varDelta _{r_i^{-}}+\\epsilon ,0\\},1\\big \\}, \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon $$\\end{document} is a parameter to adjust the mean value of weights, and we constrain the weight \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_i$$\\end{document} to be less equal to 1. From this formula, we can see that a large weight \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_i$$\\end{document} tends to correspond to a large \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta _{r_i^{+}}$$\\end{document} (a more informative positive response) and a small \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta _{r_i^{-}}$$\\end{document} (a less discriminative negative utterance).",
            "cite_spans": [],
            "section": "Instance Weighting with Last-Utterance Selection Model ::: Approach",
            "ref_spans": []
        },
        {
            "text": "In this part, we present the complete learning approach.",
            "cite_spans": [],
            "section": "Complete Learning Approach and Optimization ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Instantiation of the Deep Matching Models. We instantiate matching models for response selection. Our learning algorithm can work with any deep matching models. Here, we consider two recently proposed attention-based matching models, namely SMN [16] and DAM [22]. The SMN model is an RNN-based model. It first constructs semantic representations for context and response by GRU. Then, the matching features are captured by word-level and sequence-level similarity matrix. Finally a convolution neural network is adopted to distill important matching information as a matching vector and an utterance-level GRU is used to compute the matching score. The DAM model is a deep attention-based model which constructs semantic representation for context and response by a multi-layer transformer. Then, the word-level matching features are captured by cross-attention and self-attention layers. Finally a 3D-convolution is adopted to compute the matching score. These two models are selected due to their state-of-the-art performance on multi-turn response selection. Besides, previous studies have also adapted them with techniques such as weak-supervised learning [16] and co-teach learning [1].",
            "cite_spans": [
                {
                    "start": 246,
                    "end": 248,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 259,
                    "end": 261,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1161,
                    "end": 1163,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1188,
                    "end": 1189,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Complete Learning Approach and Optimization ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Learning and Optimization. Given a matching model, we first pre-train it with the cross-entropy in Eq. 1. This step aims to obtain a basic model that will be further fine-tuned by our approach. For each instance consisting of a positive and a negative response, the last-utterance selection model computes the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta $$\\end{document} value for each response by Eq. 3. Then, the weights are derived by Eq. 4 and utilized in the fine-tuning process by Eq. 2. The gradient will back-propagate to optimize the parameters in the response selection model (the gradient to last-utterance selection model is obstructed). This training approach encourages the model to focus on more confident instances with the supervision signal from the complementary task.",
            "cite_spans": [],
            "section": "Complete Learning Approach and Optimization ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Discussions. In addition to the measure defined in Eq. 4, we consider using other alternatives to compute \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_{i}$$\\end{document}, such as Jaccard similarity and embedding cosine similarity between positive and negative responses. Indeed, it is also possible to replace our multi-turn last-utterance selection model with a single-turn last-utterance selection model to reduce the influence of the context information. Currently, we do not fine-tune the last-utterance selection model, since there is no significant improvement from this strategy in our early experiments. More details will be discussed in Sect. 5.3.",
            "cite_spans": [],
            "section": "Complete Learning Approach and Optimization ::: Approach",
            "ref_spans": []
        },
        {
            "text": "Construction of the Datasets. To evaluate the performance of our approach, we use two public open-domain multi-turn conversation datasets. The first dataset is Douban Conversation Corpus (Douban) which is a multi-turn Chinese conversation data set crawled from Douban group1. This dataset consists of one million context-response pairs for training, 50,000 pairs for validation, and 6,670 pairs for test. Another dataset is E-commerce Dialogue Corpus (ECD) [19]. It consists of real-world conversations between customers and customer service staff in Taobao2. There are one million context-response pairs in the training set, and 10,000 pairs in both the validation set and the test set. For both datasets, the negative responses in the training set and the validation set are randomly sampled and the ratio of the positive and the negative is 1:13. In the test set, each context has 10 response candidates retrieved from an index whose appropriateness regarding to the context is judged by human annotators.",
            "cite_spans": [
                {
                    "start": 458,
                    "end": 460,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Task Setting. We implement our method as Sect. 4.3. We select DAM [22] and SMN [16] as response selection models. We only select DAM [22] as our last-utterance selection model not only due to its strong feature extraction ability, but also for guaranteeing the gain only comes from the response selection model. The pre-training process follows the setting in [16, 22]. During the instance weighting, we choose 50 as the size of the mini-batches. We use Adam optimizer [5] with the learning rate as 1e-4. All gradients are clipped by 1.0 to stabilize the training process. We tune \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma $$\\end{document} in {0,1/8,2/8,3/8,4/8}, and finally choose 2/8 for Douban dataset, 4/8 for ECD dataset. And we test \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon $$\\end{document} in {0,1/4,2/4,3/4}, and find 2/4 is the best choice for both datasets.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 69,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 80,
                    "end": 82,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 134,
                    "end": 136,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 361,
                    "end": 363,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 365,
                    "end": 367,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 470,
                    "end": 471,
                    "mention": "5",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Following the works [16, 22], we use Mean Average Presion (MAP), Mean Reciprocal Rank (MRR) and Precision at position 1 (P@1) as evaluation metrics.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 23,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 25,
                    "end": 27,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Baseline Models. We combine our approach with SMN and DAM to validate the effect. Besides, we compare our models with a number of baseline models:",
            "cite_spans": [],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "SMN [16] and DAM [22]: We utilize the pre-training results of the two models as baselines to validate the promotion of our proposed method.",
            "cite_spans": [
                {
                    "start": 5,
                    "end": 7,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 18,
                    "end": 20,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Single-turn models: MV-LSTM [12] and match-LSTM [13] are the typical single-turn matching models. They concatenate all utterances in contexts as a long document for matching.",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 31,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 49,
                    "end": 51,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Multi-view [21]: It measures the matching degree between a context and a response candidate in both a word view and an utterance view.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 14,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "DL2R [18]: It represents each utterance in contexts by RNNs and CNNs, and the matching score is computed based on the concatenation of the representations.",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 8,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "In addition to these baseline models, we denote the model with our proposed weighting method as Model-WM.\n",
            "cite_spans": [],
            "section": "Experimental Setup ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "We present the results of all comparison methods in Table 1. First, these methods show a consistent trend on both datasets over all metrics, i.e., DAM-WM > DAM > SMN-WM > SMN > other models. We can conclude that DAM and SMN are the best baselines in this task than other models because they can capture more semantic features from word-level and sentence-level matching information. Second, our method yields improvement in SMN and DAM on two datasets, and most of these promotions are statistically significant (t-test with p-value < 0.05). This proves the effectiveness of our instance weighting method.",
            "cite_spans": [],
            "section": "Results and Analysis ::: Experiment",
            "ref_spans": [
                {
                    "start": 58,
                    "end": 59,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Third, the promotion on Douban dataset by our approach is larger than that on ECD dataset. The difference may stem from the distribution of test sets of the two data. The test set of Douban is built from random sampling, while that of the ECD dataset is constructed by a response retrieval system. Therefore, the negative samples are more semantically similar to the positive ones. It is difficult to yield improvement by our approach with SMN and DAM in ECD dataset. Fourth, our method yields less improvement in SMN than DAM. A possible reason is that DAM fits our method better than SMN because DAM is a deep attention-based network, which owns stronger learning capacity. Another possible reason is that DAM is less sensitive to noisy training data since we have observed that the convergence process of SMN is not as stable as DAM.\n",
            "cite_spans": [],
            "section": "Results and Analysis ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "In this section, we explore a series of variations of our method. We replace the multi-turn last-utterance selection with other models or replace the weight produced by Eq. 4 with other heuristic methods. In this part, our experiments are conducted on Douban dataset with DAM [22] as our base model.",
            "cite_spans": [
                {
                    "start": 277,
                    "end": 279,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Heuristic Method. We consider the following methods, which change the weight produced by Eq. 4 with heuristic methods.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "DAM-uniform: we fix the weight as one and follow the same procedure of our learning approach, to validate the effectiveness of our dynamic weight strategy.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "DAM-random: we replace the weight model as a random function to produce random values varied in [0,1].",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "DAM-Jaccard: we use the Jaccard similarity between positive response and negative response as the weight.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "DAM-embedding [7]: we use the cosine similarity between the representation of positive and negative response as the weight. For DAM model, we calculate the average hidden state of all the words in the response as its representation.",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 16,
                    "mention": "7",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Model-Based Method. We consider the following methods, which change the computing approach of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varDelta $$\\end{document} in Eq. 3 by substituting our complementary model with other similar models.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "DAM-last-WM replaces the multi-turn last-utterance selection model with a single-turn last-utterance selection model. This method is used to prove the effectiveness of the context information U in the last-utterance selection model. DAM-DAM replaces the last-utterance selection model by a response selection model. We utilize DAM model to produce \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Pr(y=1|q^{+},r)$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Pr(y=1|q^{-},r)$$\\end{document}.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "DAM-dual is a prime-dual approach. The response selection model is the prime model and the last-utterance selection model is the dual model. The two approaches learn instance weights for each other as Eq. 2.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Result Analysis. Table 2 reports the results of these different variations of our method on Douban dataset. First, most of these variants outperform DAM model. It demonstrates that these instance weight strategies are effective in noisy data training. Among them, DAM-WM achieves the best results for all the three evaluation metrics. It indicates that our proposed method is more effective. Second, the improvement yielded by heuristic methods is less than model-based methods. A possible reason is that neural networks own stronger semantic capacity and the weights produced by these models can better distinguish noise in training data. Third, heuristic methods achieve worse performance than DAM-uniform. It indicates that Jaccard similarity and cosine similarity of representation are not proper instance weighting functions and bring a negative effect on response selection model.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": [
                {
                    "start": 23,
                    "end": 24,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Moreover, all these model-based methods receive similar results in all three metrics and outperform DAM model. It indicates that these methods are effective but not as powerful as our proposed method. For DAM-DAM model, a possible reason is that it cannot provide more useful signal for this task than our proposed method. For DAM-last-WM, its last-utterance selection model only utilizes the last utterance therefore it cannot select positive last-utterance confidently4, therefore the distinguish ratio becomes noisy and low confident. For DAM-dual model, we observe that the dual-learning approach does not improve the performance of the last-utterance selection task, the reason may be that the response selection task and last-utterance selection task are not an appropriate dual-task or the dual-learning approach is not proper. We will conduct further investigation to find an appropriate dual-learning approach for this task.",
            "cite_spans": [],
            "section": "Variations of Our Method ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "Previously, we have shown the effectiveness of our method. In this section, we qualitatively analyze why our method can yield good performance.",
            "cite_spans": [],
            "section": "Case Study ::: Experiment",
            "ref_spans": []
        },
        {
            "text": "We calculate the weights of all the instances in training data of Douban dataset, and select the instances with maximum and minimum weight (1.0 and 0.0) respectively. We present some of them in Table 3 and annotate them manually. The first case receives a weight of 0.0, which demonstrates that the case is identified as inappropriate negative case by our last-utterance selection model. The last case receives a weight of 1.0, and we can identify the positive and negative responses. This case study shows that our instance weighting method can identify the false negative samples and punish them with less weight.\n",
            "cite_spans": [],
            "section": "Case Study ::: Experiment",
            "ref_spans": [
                {
                    "start": 200,
                    "end": 201,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Previous studies mainly focus on the neural architecture for multi-turn retrieval-based dialog systems, but neglect the fundamental problem from noisy training data. In this paper, we proposed a novel learning approach that was able to effectively reduce the influence of noisy data. We utilized a complementary task to learn the weights for training instances that were used by the main task. The main task was furthermore fine-tuned according to a weight-enhanced margin-based loss. Such an approach can force the model to focus on more confident training instances. Experimental results on two public datasets have demonstrated the effectiveness of our proposed method. As future work, we will design other instance weighting methods to detect noise in open domain multi-turn response selection task. Furthermore, we will consider combining our approach with more learning paradigms such as dual-learning and adversarial-learning.",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Results on two datasets. Numbers marked with * indicate that the improvement is statistically significant compared with the pre-trained baseline (t-test with p-value < 0.05). We copy the numbers from [16] for the baseline models. Because the first four baselines obtain similar results in Douban dataset, we only implement two of them in ECD dataset.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Evaluation of DAM with different weighting strategies on Douban dataset.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Samples with the maximum and minimum weight learned by our approach. Green checkmarks indicate that the response candidates are proper replies of the contexts from human annotated, while red cross marks indicate inappropriate replies. The first case receives a weight of 0.0 and the negative responses can respond to the contexts to some extent. The second case receives a weight of 1.0 and the negative responses are unrelated to contexts.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: The case of response and last-utterance selection model.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: The overall sketch of our approach. Our approach contains a main task (Loss Optimization Module) and a complementary task (Instance Weight Calculation Module). Last-utterance selection model \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M_{utte}$$\\end{document} is utilized to calculate the instance weight, while response selection model \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M_{res}$$\\end{document} is utilized to calculate the loss for optimization.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}