{
    "paper_id": "PMC7148212",
    "metadata": {
        "title": "Domain-Independent Extraction of Scientific Concepts from Research Articles",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Arthur",
                "middle": [],
                "last": "Brack",
                "suffix": "",
                "email": "arthur.brack@tib.eu",
                "affiliation": {}
            },
            {
                "first": "Jennifer",
                "middle": [],
                "last": "D\u2019Souza",
                "suffix": "",
                "email": "jennifer.dsouza@tib.eu",
                "affiliation": {}
            },
            {
                "first": "Anett",
                "middle": [],
                "last": "Hoppe",
                "suffix": "",
                "email": "anett.hoppe@tib.eu",
                "affiliation": {}
            },
            {
                "first": "S\u00f6ren",
                "middle": [],
                "last": "Auer",
                "suffix": "",
                "email": "soeren.auer@tib.eu",
                "affiliation": {}
            },
            {
                "first": "Ralph",
                "middle": [],
                "last": "Ewerth",
                "suffix": "",
                "email": "ralph.ewerth@tib.eu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Scholarly communication as of today is a document-centric process. Research results are usually conveyed in written articles, as a PDF file with text, tables and figures. Automatic indexing of these texts is limited and generally does not access their semantic content. There are thus severe limitations how current research infrastructures can support scientists in their work: finding relevant research works, comparing them, and compiling summaries is still a tedious and error-prone manual work. The heightened increase in the number of published research papers aggravates this situation [7].",
            "cite_spans": [
                {
                    "start": 594,
                    "end": 595,
                    "mention": "7",
                    "ref_id": "BIBREF46"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Knowledge graphs are recognised as an effective approach to facilitate semantic search [3]. For academic search engines, Xiong et al. [47] have shown that exploiting knowledge bases like Freebase can improve search results. However, the introduction of new scientific concepts occurs at a faster pace than knowledge base curation, resulting in a large gap in knowledge base coverage of scientific entities [1], e.g. the task geolocation estimation of photos from the Computer Vision field is neither present in Wikipedia nor in more specialised knowledge bases like Computer Science Ontology (CSO) [39] or \u201cPapers with code\u201d [36]. Information extraction from text helps to identify emerging entities and to populate knowledge graphs [3]. It then is a first vital step towards a fine-grained research knowledge graph in which research articles are described and interconnected through entities like tasks, materials, and methods. Our work is motivated by the idea of the automatic construction of a research knowledge graph.",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 89,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 135,
                    "end": 137,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 407,
                    "end": 408,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 599,
                    "end": 601,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 626,
                    "end": 628,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 734,
                    "end": 735,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Information extraction from scientific texts, obviously, differs from its general domain counterpart: Understanding a research paper and determining its most important statements demands certain expertise in the article\u2019s domain. Every domain is characterised by its specific terminology and phrasing which is hard to grasp for a non-expert reader. In consequence, extraction of scientific concepts from text would entail the involvement of domain experts and a specific design of an extraction methodology for each scientific discipline \u2013 both requirements are rather time-consuming and costly.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "At present, a systematic study of these assumptions is missing. We thus present the task of domain-independent scientific concept extraction. We examine the intuition that most research papers share certain core concepts such as the mentions of research tasks or methods. If so, these would allow a domain-independent information extraction system to support populating a research knowledge graph, which does not reach all semantic depths of the analysed article, but still provides some science-specific structure.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we introduce a set of common scientific concepts that we find are relevant over a set of 10 examined domains from Science, Technology, and Medicine (STM). These generic concepts have been identified in a systematic, joint effort of domain experts and non-domain experts. The inter-coder agreement is measured to ensure the adequacy and quality of concepts. A set of research abstracts has been annotated using these concepts and the results are discussed with experts from the corresponding fields. The resulting dataset serves as a basis to train two baseline deep learning classifiers. In particular, we present an active learning approach to reduce the number of required training data. The systems are evaluated in different experimental setups.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Our main contributions can be summarised as follows: (1) We introduce the novel task domain-independent scientific concept extraction, which aims at automatically extracting scientific entities in a domain-independent manner. (2) We release a new corpus that comprises 110 abstracts of 10 STM domains annotated at the phrasal level. (3) We present and evaluate a state-of-the-art deep learning approach for this task. Additionally, we employ active learning for an optimal selection of instances, which to our knowledge, is demonstrated for the first time on scholarly text. We find that strategic instance selection gives us the same performance with only about half of the training data. (4) We release a silver-labelled corpus with 62 K automatically annotated abstracts of Elsevier with CCBY license and 1.2 Mio. extracted unique concepts comprising 24 domains. (5) We make our corpora and source code publicly available to facilitate further research.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Sentence Level Annotation. Early approaches for semantic structuring of research papers focused on sentences as the basic unit of analysis. This enables, for instance, automatic highlighting of relevant paper passages to enable efficient assessment regarding quality and relevance. Several ontologies have been created that focus on the rhetorical [11, 19], argumentative [31, 46] or activity-based [37] structure of research papers.",
            "cite_spans": [
                {
                    "start": 349,
                    "end": 351,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 353,
                    "end": 355,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 373,
                    "end": 375,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 377,
                    "end": 379,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 400,
                    "end": 402,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                }
            ],
            "section": "Scientific Corpora ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Annotated datasets exist for several domains, e.g. PubMed200k [12] from biomedical randomized controlled trials, NICTA-PIBOSO [26] from evidence-based medicine, Dr. Inventor [15] from Computer Graphics, Core Scientific Concepts (CoreSC) [31] from Chemistry and Biochemistry, and Argumentative Zoning (AZ) [46] from Chemistry and Computational Linguistics, Sentence Corpus [8] from Biology, Machine Learning and Psychology. Most datasets cover only a single domain, while few other datasets cover three domains. Several machine learning methods have been proposed for scientific sentence classification [12, 15, 24, 30].",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 65,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 127,
                    "end": 129,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 175,
                    "end": 177,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 238,
                    "end": 240,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 306,
                    "end": 308,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 373,
                    "end": 374,
                    "mention": "8",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 603,
                    "end": 605,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 607,
                    "end": 609,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 611,
                    "end": 613,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 615,
                    "end": 617,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Scientific Corpora ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Phrase Level Annotation. More recent corpora have been annotated at phrasal level (e.g. noun phrases). SciCite [9] and ACL ARC [25] are datasets for citation intent classification from Computer Science, Medicine, and Computational Linguistics. ACL RD-TEC [20] from Computational Linguistics aims at extracting scientific technology and non-technology terms. ScienceIE-17 [2] from Computer Science, Material Sciences, and Physics contains three concepts Process, Task and Material. SciERC [32] from the machine learning domain contains six concepts Task, Method, Metric, Material, Other-ScientificTerm and Generic. Each corpus covers at most three domains.",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 113,
                    "mention": "9",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 128,
                    "end": 130,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 256,
                    "end": 258,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 372,
                    "end": 373,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 489,
                    "end": 491,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Scientific Corpora ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Experts vs. Non-experts. The aforementioned datasets were usually annotated by domain experts [2, 12, 20, 26, 31, 32]. In contrast, Teufel et al. [46] explicitly use non-experts in their annotation tasks, arguing that text understanding systems can use general, rhetorical and logical aspects also when qualifying scientific text. According to this line of thought, more researchers used (presumably cheaper) non-expert annotation as an alternative [8, 15].",
            "cite_spans": [
                {
                    "start": 95,
                    "end": 96,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 98,
                    "end": 100,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 102,
                    "end": 104,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 106,
                    "end": 108,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 110,
                    "end": 112,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 114,
                    "end": 116,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 147,
                    "end": 149,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 450,
                    "end": 451,
                    "mention": "8",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 453,
                    "end": 455,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                }
            ],
            "section": "Scientific Corpora ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Snow et al. [43] provide a study on expert versus non-expert performance for general, non-scientific annotation tasks. They state that about four non-experts (Mechanical Turk workers, in their case) were needed to rival the experts\u2019 annotation quality. However, systems trained on data generated by non-experts showed to benefit from annotation diversity and to suffer less from annotator bias. A recent study [38] examines the agreement between experts and non-experts for visual concept classification and person recognition in historical video data. For the task of face recognition, training with expert annotations lead to an increase of only 1.5% in classification accuracy.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 15,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 411,
                    "end": 413,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Scientific Corpora ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Active Learning in Natural Language Processing (NLP). To the best of our knowledge, active learning has not been utilised in classification approaches for scientific text yet. Recent publications demonstrate the effectiveness of active learning for NLP tasks such as Named Entity Recognition (NER) [41] and sentence classification [49]. Siddhant and Lipton [42] and Shen et. al. [41] compare several sampling strategies on NLP tasks and show that Maximum Normalized Log-Probability (MNLP) based on uncertainty sampling performs well in NER.",
            "cite_spans": [
                {
                    "start": 299,
                    "end": 301,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 332,
                    "end": 334,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 358,
                    "end": 360,
                    "mention": "42",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 380,
                    "end": 382,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                }
            ],
            "section": "Scientific Corpora ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Academic Search Engines. Academic search engines such as Google Scholar [18], Microsoft Academic [34] and Semantic Scholar [40] specialise in search of scholarly literature. They exploit graph structures such as the Microsoft Academic Knowledge Graph [35], SciGraph [45], or the Semantic Scholar Corpus [1]. These graphs interlink the papers through meta-data such as citations, authors, venues, and keywords, but not through deep semantic representation of the articles\u2019 content.",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 75,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 98,
                    "end": 100,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 124,
                    "end": 126,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 252,
                    "end": 254,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 267,
                    "end": 269,
                    "mention": "45",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 304,
                    "end": 305,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Applications for Domain-Independent Scientific Information Extraction ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "However, first attempts towards a more semantic representation of article content exist: Ammar et al. [1] interlink the Semantic Scholar Corpus with DBpedia [29] and Unified Medical Language System (UMLS) [6] using entity linking techniques. Yaman et al. [48] connect SciGraph with DBpedia person entities. Xiong et al. [47] demonstrate that academic search engines can greatly benefit from exploiting general-purpose knowledge bases. However, the coverage of science-specific concepts is rather low [1].",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 104,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 158,
                    "end": 160,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 206,
                    "end": 207,
                    "mention": "6",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 256,
                    "end": 258,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 321,
                    "end": 323,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 501,
                    "end": 502,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Applications for Domain-Independent Scientific Information Extraction ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Research Paper Recommendation Systems. Beel et al. [4] provide a comprehensive survey about research paper recommendation systems. Such systems usually employ different strategies (e.g. content-based and collaborative filtering) and several data sources (e.g. text in the documents, ratings, feedback, stereotyping). Graph-based systems, in particular, exploit citation graphs and genes mentioned in the papers [27]. Beel et al. conclude that it is not possible to determine the most effective recommendation approach at the moment. However, we believe that a fine-grained research knowledge graph can improve such systems. Although \u201cPapers with code\u201d [36] is not a typical recommendation system, it allows researchers to browse easily for papers from the field of machine learning that address a certain task.",
            "cite_spans": [
                {
                    "start": 52,
                    "end": 53,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 412,
                    "end": 414,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 653,
                    "end": 655,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                }
            ],
            "section": "Applications for Domain-Independent Scientific Information Extraction ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "The OA-STM corpus [14] is a set of open access (OA) articles from various domains in Science, Technology and Medicine (STM). It was published in 2017 as a platform for benchmarking methods in scholarly article processing, amongst other scientific information extraction. The dataset contains a selection of 110 articles from 10 domains, namely Agriculture (Agr), Astronomy (Ast), Biology (Bio), Chemistry (Che), Computer Science (CS), Earth Science (ES), Engineering (Eng), Materials Science (MS), Mathematics (Mat), and Medicine (Med). This first annotation cycle focuses on the articles\u2019 abstracts as they contain a condensed summary of the article.",
            "cite_spans": [
                {
                    "start": 19,
                    "end": 21,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "OA-STM Corpus ::: Corpus for Domain-Independent Scientific Concept Extraction",
            "ref_spans": []
        },
        {
            "text": "The OA-STM Corpus is used as a base for (a) the identification of potential domain-independent concepts; (b) a first annotated corpus for baseline classification experiments. The annotation task was mainly performed by two post-doctoral researchers with a background in Computer Science (acting as non-expert annotators); their basic annotation assumptions were checked by domain experts.\n",
            "cite_spans": [],
            "section": "Annotation Process ::: Corpus for Domain-Independent Scientific Concept Extraction",
            "ref_spans": []
        },
        {
            "text": "Pre-annotation. A literature review of annotation schemes [2, 11, 30, 31] provided a seed set of potential candidate concepts. Both non-experts independently annotated a subset of the STM abstracts with these concepts (non-overlapping) and discussed the outcome. In a three-step process, the concept set was pruned to only contain those which seemed suitably transferable between domains. Our set of generic scientific concepts consists of Process, Method, Material, and Data (see Table 1 for their definitions). We also identified Task [2], Object [30], and Results [11], however, in this study we do not consider nested span concepts, hence we leave them out since they were almost always nested with the other scientific entities (e.g. a Result may be nested with Data).",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 60,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 62,
                    "end": 64,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 66,
                    "end": 68,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 70,
                    "end": 72,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 538,
                    "end": 539,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 550,
                    "end": 552,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 568,
                    "end": 570,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Annotation Process ::: Corpus for Domain-Independent Scientific Concept Extraction",
            "ref_spans": [
                {
                    "start": 487,
                    "end": 488,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Phase I. Five abstracts per domain (i.e. 50 abstracts) were annotated by both annotators and the inter-annotator agreement was computed using Cohen\u2019s \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} [10] at exact annotated spans. Results showed a moderate inter-annotator agreement of 0.52 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document}.",
            "cite_spans": [
                {
                    "start": 444,
                    "end": 446,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Annotation Process ::: Corpus for Domain-Independent Scientific Concept Extraction",
            "ref_spans": []
        },
        {
            "text": "Phase II. The annotations were then presented to subject specialists who each reviewed (a) the choice of concepts and (b) annotation decisions on the respective domain corpus. The interviews mostly confirmed the concept candidates as generally applicable. The experts\u2019 feedback on the annotation was even more valuable: The comments allowed for a more precise reformulation of the annotation guidelines, including illustrating examples from the corpus.",
            "cite_spans": [],
            "section": "Annotation Process ::: Corpus for Domain-Independent Scientific Concept Extraction",
            "ref_spans": []
        },
        {
            "text": "Consolidation. Finally, the 50 abstracts from phase I were reannotated by the non-experts. Based on the revised annotation guidelines, a substantial agreement of 0.76 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} could be reached (see Table 2). Similar annotation tasks for scientific entities, i.e. SciERC [32] considering one domain and ScienceIE-17 [2] considering three domains achieved agreements of 0.76 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} and 0.6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document}, respectively. Subsequently, the remaining 60 abstracts (six per domain) were annotated by one annotator. This phase also involved reconciliation of the previously annotated 50 abstracts to obtain a gold standard corpus.\n",
            "cite_spans": [
                {
                    "start": 555,
                    "end": 557,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 600,
                    "end": 601,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Annotation Process ::: Corpus for Domain-Independent Scientific Concept Extraction",
            "ref_spans": [
                {
                    "start": 488,
                    "end": 489,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Table 3 shows some characteristics of the resulting corpus. The corpus has a total of 6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data concept entities. The number of entities per abstract in our corpus directly correlates with the length of the abstracts (Pearson\u2019s R 0.97). Among the concepts, Process and Material directly correlate with abstract length (R 0.8 and 0.83, respectively), while Data has only a slight correlation (R 0.35) and Method has no correlation (R 0.02). The domains Bio, CS, Ast, and Eng contain the most of Process, Method, Material, and Data concepts, respectively.\n",
            "cite_spans": [],
            "section": "Corpus Characteristics ::: Corpus for Domain-Independent Scientific Concept Extraction",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Using the above mentioned architecture, we train one model with data from all domains combined. We refer to this model as the domain-independent classifier. Similarly, we train 10 models for each domain in our corpus \u2013 the domain-specific classifier.",
            "cite_spans": [],
            "section": "Supervised Learning with Full Training Dataset ::: Automatic Domain-Independent Scientific Concept Extraction",
            "ref_spans": []
        },
        {
            "text": "To obtain a robust evaluation of models, we perform five-fold cross-validation experiments. In each fold experiment, we train a model on 8 abstracts per domain (i.e. 80 abstracts), tune hyperparameters on 1 abstract per domain (i.e. 10 abstracts), and test on the remaining 2 abstracts per domain (i.e. 20 abstracts) ensuring that the data splits are not identical between the folds. All results reported in the paper are averaged over the five folds. We still obtain reliably trained domain-specific classifiers since on average they are trained on 400 concepts.",
            "cite_spans": [],
            "section": "Supervised Learning with Full Training Dataset ::: Automatic Domain-Independent Scientific Concept Extraction",
            "ref_spans": []
        },
        {
            "text": "In this setting, we employ an active learning strategy [42, 49] to train a new domain-independent classifier. Active learning is usually applied to determine the optimal set of sufficiently distinct instances to minimise annotation costs. With our application of active learning we find which proportion of our annotations suffice for training a robust classifier. We decide to use the MNLP [41] sampling strategy. We prefer it over its contemporary, Bayesian Active Learning by Disagreement (BALD) [22], since it has less computational requirements. The MNLP objective involves greedy sampling of sentences preferring those with the least logarithmic likelihood of the predicted tag sequence output by the CRF tag decoder, normalised by the number of tokens to avoid preferring longer sentences. In our experiments, we found that adding 4% of the data to be the most discriminative selection of classifier performance. Therefore, we run 25 iterations of active learning in each stage adding 4% training data. We perform five-fold cross validation as before and the per-fold models are retrained after data resampling.",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 58,
                    "mention": "42",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 60,
                    "end": 62,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 392,
                    "end": 394,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 500,
                    "end": 502,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Active Learning with Training Data Subset ::: Automatic Domain-Independent Scientific Concept Extraction",
            "ref_spans": []
        },
        {
            "text": "Table 4 shows an overview of the domain-independent classifier results. The system achieves an overall F1 of 65.5% and has low standard deviation 1.26 across the five folds. For this classifier, Material was the easiest concept with an F1 of 71% (\u00b11.88), whereas Method was the hardest concept with an F1 of 43% (\u00b16.30). Method is also the most underrepresented in our corpus, which partly explains the poor extraction performance. Best reported results for similar datasets, ScienceIE17 [2] and SciERC [32] (both have 500 abstracts), have an F1 score of 65.6% [5] and 44.7% [32], respectively, indicating that the size of our dataset with only 110 abstracts is sufficient.\n",
            "cite_spans": [
                {
                    "start": 489,
                    "end": 490,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 504,
                    "end": 506,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 562,
                    "end": 563,
                    "mention": "5",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 576,
                    "end": 578,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": []
        },
        {
            "text": "Next, we compare and contrast the 10 domain-specific classifiers (see Fig. 1) by their capability to extract the concepts from their own domains and in other domains.",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 75,
                    "end": 76,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Most Robust Domain.\nBio (third bar in each domain in Fig. 1) extracts scientific concepts from its own domain at the same performance as the domain-independent classifier with an F1 score of 71% (\u00b19.0) demonstrating a robust domain. It comprises only 11% of the overall data, yet the domain-independent classifier trained on all data does not outperform it.",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 58,
                    "end": 59,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Most Generic Domain.\nMS (the third last bar in each domain in Fig. 1) exhibits a high degree of domain independence since it is among the top 3 classifiers for seven of the 10 domains (viz. ES, Che, CS, Ast, Agr, MS, and Bio).\n",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 67,
                    "end": 68,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Most Specialised Domain.\nMat (the second last bar in each domain in Fig. 1) shows the lowest performance in extracting scientific concepts from all domains except itself. Hence it shows to be the most specialised domain in our corpus. Notably, a characteristic feature of this domain is that it has short abstracts (nearly a third of the size of the longest abstracts), so it is also the most underrepresented in our corpus. Also, distinct from the other domains, Mat has triple the number of Data entities compared to each of its other concepts, where in the other domains Process and Material are consistently predominant.",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 73,
                    "end": 74,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Medical and Life Science Domains. The Med, Agr, and Bio domains show strong domain relatedness. Their respective domain-specific classifiers show top five system performances among the three domains, when applied to another domain. For instance, the Med domain shows the strongest domain relatedness and is classified best by Med (last bar), followed by Bio (third bar) and Agr (first bar).",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": []
        },
        {
            "text": "Domain-Independent vs. Domain-Specific Classifier. Except for Bio the domain-independent classifier clearly outperforms the domain-specific one in extracting concepts from their respective domains. We attribute this, in part, to the improved span-detection performance. Span-detection merely relies on syntactic regularity, thus the domain-independent classifier can benefit from more training data of other domains. E.g., the CS classifier shows a relative improvement of 49.5% domain-specific\nF1 score to 65.9% in the domain-independent setting, which is supported by the enhanced span-detection performance from 73.4% to 82.0% in F1. Accuracy on token-level also improves from 67.7% to 77.5% F1 for CS, that is correct labelling of the tokens also benefits from other domains. This is also supported by the results in the confusion matrix depicted in Fig. 2 for the CS and the domain-independent classifier on token-level.",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 859,
                    "end": 860,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Scientific Concept Extraction. Figure 3 depicts the 10 domain-specific classifier results for extracting each of the four scientific concepts. It can be observed that Agr, Med, Bio, and Ast classifiers are the best in extracting their respective Process, Method, Material, and Data concepts.\n",
            "cite_spans": [],
            "section": "Domain-Independent and Domain-Specific Classifiers: Full Training Dataset ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 38,
                    "end": 39,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "The results of the active learning experiment over the full dataset plotted over the 25 iterations are depicted in Fig. 4, showing that MNLP clearly outperforms the random baseline. While using only 52% of the training data, the best result of the domain-independent classifier trained with all training data is surpassed with an F1 score of 65.5% (\u00b11.0). The random baseline achieves an F1 score of only 62.5% (\u00b12.6) with the same proportion of training data. When 76% of the data are sampled by MNLP, the best active learning performance across all steps is achieved with an F1 score of 69.0% on the validation set, having the best F1 of 66.4% (\u00b12.0) on the test set. Thus, 76% of our annotated sentences suffice to train an optimal performing model.",
            "cite_spans": [],
            "section": "Domain-Independent Classifier with Active Learning ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 120,
                    "end": 121,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "Analysing the distribution of sentences in the training data sampled by MNLP, shows (Math, CS) as the most preferred domains and (Eng, MS) the least preferred ones. Nonetheless, all domains are represented, that is a non-uniformly mix of sentences sampled by MNLP yields the most generic model with less training data. In contrast, the random sampling strategy uniformly samples sentences from all domains.",
            "cite_spans": [],
            "section": "Domain-Independent Classifier with Active Learning ::: Experimental Results and Discussion",
            "ref_spans": []
        },
        {
            "text": "Further, we show in Table 5 the proportion of training data for MNLP when the performance using the entire training dataset is achieved for related SciERC [32] and ScienceIE-17 [2] datasets. The results indicate, that also for related datasets on scientific texts MNLP can significantly reduce the amount of labelled training data.\n\n",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 158,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 178,
                    "end": 179,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Domain-Independent Classifier with Active Learning ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 26,
                    "end": 27,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "In this section, we analyse the correlations (Pearson\u2019s R) of inter-coder agreement \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} and the number of annotated concepts per domain (#) on (1) the performance F1 and (2) variance resp. standard deviation (std) of the classifiers across five-fold cross validation.",
            "cite_spans": [],
            "section": "Correlations Between Inter-annotator Agreement and Performance ::: Experimental Results and Discussion",
            "ref_spans": []
        },
        {
            "text": "Table 6 summarises the results of our correlation analysis. The active learning classifier (AL-trained) has been trained with 52% training data sampled by MNLP since it is the point at which the performance of the full data trained model is surpassed (see Table 5). For the domain-specific, domain-independent and AL-trained classifier we observe a strong correlation between F1 and number of concepts per domain (R 0.70, 0.76, 0.68) and a weak correlation between \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} and F1 (R 0.20, 0.28, 0.23). Thus, we surmise that the number of annotated concepts in a particular domain has more influence on the performance than the inter-annotator agreement.\n",
            "cite_spans": [],
            "section": "Correlations Between Inter-annotator Agreement and Performance ::: Experimental Results and Discussion",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 262,
                    "end": 263,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "The correlation values for the variance are different between the classifier types. For the domain-specific classifier the correlation between \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} and std, and the number of concepts per domain and std are slightly positive (R 0.29, 0.28), i.e. the higher the agreement and the size of the domain, the higher the variance of the domain-specific classifier. For the domain-independent classifier, there is no correlation (R 0.11, \u22120.05) and for the AL-trained classifier, the correlations become negative (R \u22120.41, \u22120.72), i.e. higher agreement and more annotated concepts per domain lead to less variance for the AL-trained classifier. In summary, we hypothesise that more diverse training data from several domains lead to better performance and lower variance by introducing an inductive bias.",
            "cite_spans": [],
            "section": "Correlations Between Inter-annotator Agreement and Performance ::: Experimental Results and Discussion",
            "ref_spans": []
        },
        {
            "text": "In this paper, we have introduced the novel task of domain-independent concept extraction from scientific texts. During a systematic annotation procedure involving domain experts, we have identified four general core concepts that are relevant across the domains of Science, Technology and Medicine. To enable and foster research on these topics, we have annotated a corpus for the domains. We have verified the adequacy of the concepts by evaluating the human annotator agreement for our broad STM domain corpus. The results indicate that the identification of the generic concepts in a corpus covering 10 different scholarly domains is feasible by non-experts with moderate agreement and after consultation of domain experts with substantial agreement (0.76 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document}).",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "We evaluated a state-of-the-art system on our annotated corpus which achieved a fairly high F1 score (65.5% overall). The domain-independent system noticeably outperforms the domain-specific systems, which indicates that the model can generalise well across domains. We also observed a strong correlation between the number of annotated concepts per domain and classifier performance, and only a weak correlation between inter-annotator agreement per domain and the performance. It is assumed that more annotated data positively influence the performance in the respective domain.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "Furthermore, we have suggested active learning for our novel task. We have shown that only approx. 5 annotated abstracts per domain serving as training data are sufficient to build a performant model. Our active learning results for SciERC [32] and ScienceIE17 [2] datasets were similar. The promising results suggest that we do not need a large annotated dataset for scientific information extraction. Active learning can significantly save annotation costs and enable fast adaptation to new domains.",
            "cite_spans": [
                {
                    "start": 241,
                    "end": 243,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 262,
                    "end": 263,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "We make our annotated corpus, a silver-labelled corpus with 62K abstracts comprising 24 domains, and source code publicly available.1 Thereby, we hope to facilitate research on the task of scientific information extraction and its several applications, e.g. academic search engines or research paper recommendation systems.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "In the future, we plan to extend and refine the concepts for certain domains. We also intend to apply and evaluate our automatic scientific concept extraction system to expand an open research knowledge graph [23]. For this purpose, we plan to extend the corpus with additional relevant annotation layers such as with coreference links [28] and relations [16, 32].",
            "cite_spans": [
                {
                    "start": 210,
                    "end": 212,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 337,
                    "end": 339,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 356,
                    "end": 358,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 360,
                    "end": 362,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Conclusions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: The four core scientific concepts that were derived in this study\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Per-domain and overall inter-annotator agreement (Cohen\u2019s Kappa \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document}) for Process, Method, Material, and Method scientific concept annotation\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: The annotated corpus characteristics containing 11 abstracts per domain in terms of size and the number of scientific concept phrases\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: The domain-independent classifier results in terms of Precision (P), Recall (R), and F1-score on scientific concepts, respectively, and Overall\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Performance of active learning with MNLP and random sampling strategy for the fraction of training data when the performance with entire training dataset is achieved; for SciERC and ScienceIE-17 results are reported across 5 random restarts\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6.: Inter-annotator agreement (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document}) and the number of concept phrases (#) per domain; F1 and std of domain-specific classifiers on their domains; F1 and std of domain-independent and AL-trained classifier on each domain; the right side depicts correlation coefficients (R) of each row with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\kappa $$\\end{document} and the number of concept phrases\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: F1 per domain of the 10 domain-specific classifiers (as bar plots) and of the domain-independent classifier (as scatter plots) for scientific concept extraction; the x-axis represents the 10 test domains",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Confusion matrix for (a) the CS classifier and (b) domain-independent classifier on CS domain predicting concept-type of tokens",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: F1 scores of the 10 domain-specific classifiers (bar plots) and the domain-independent classifiers (scatter plots) for extracting each scientific concept; the x-axis represents the evaluated concepts",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 4.: Progress of active learning with MNLP and random sampling strategy; the areas represent the standard deviation (std) of the F1 score across 5 folds for MNLP and random sampling strategy, respectively",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "A coefficient of agreement for nominal scales",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cohen",
                    "suffix": ""
                }
            ],
            "year": 1960,
            "venue": "Educ. Psychol. Measur.",
            "volume": "20",
            "issn": "1",
            "pages": "37-46",
            "other_ids": {
                "DOI": [
                    "10.1177/001316446002000104"
                ]
            }
        },
        "BIBREF2": {
            "title": "The document components ontology (DoCO)",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Constantin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Peroni",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Pettifer",
                    "suffix": ""
                },
                {
                    "first": "DM",
                    "middle": [],
                    "last": "Shotton",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Vitali",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Semant. Web",
            "volume": "7",
            "issn": "",
            "pages": "167-181",
            "other_ids": {
                "DOI": [
                    "10.3233/SW-150177"
                ]
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural Comput.",
            "volume": "9",
            "issn": "",
            "pages": "1735-1780",
            "other_ids": {
                "DOI": [
                    "10.1162/neco.1997.9.8.1735"
                ]
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Measuring the evolution of a scientific field through citation frames",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jurgens",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hoover",
                    "suffix": ""
                },
                {
                    "first": "DA",
                    "middle": [],
                    "last": "McFarland",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Jurafsky",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Trans. Assoc. Comput. Linguist.",
            "volume": "6",
            "issn": "",
            "pages": "391-406",
            "other_ids": {
                "DOI": [
                    "10.1162/tacl_a_00028"
                ]
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "Relational retrieval using a combination of path-constrained random walks",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Lao",
                    "suffix": ""
                },
                {
                    "first": "WW",
                    "middle": [],
                    "last": "Cohen",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Mach. Learn.",
            "volume": "81",
            "issn": "",
            "pages": "53-67",
            "other_ids": {
                "DOI": [
                    "10.1007/s10994-010-5205-8"
                ]
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "DBpedia - a large-scale, multilingual knowledge base extracted from Wikipedia",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Semant. Web",
            "volume": "6",
            "issn": "",
            "pages": "167-195",
            "other_ids": {
                "DOI": [
                    "10.3233/SW-140134"
                ]
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Entity-oriented search",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "Automatic recognition of conceptualization zones in scientific articles and two life science applications",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Liakata",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saha",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dobnik",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Batchelor",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Rebholz-Schuhmann",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Bioinformatics",
            "volume": "28",
            "issn": "7",
            "pages": "991-1000",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/bts071"
                ]
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "Scholarly ontology: modelling scholarly practices",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Pertsas",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Constantopoulos",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int. J. Digit. Libr.",
            "volume": "18",
            "issn": "3",
            "pages": "173-190",
            "other_ids": {
                "DOI": [
                    "10.1007/s00799-016-0169-3"
                ]
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "Research-paper recommender systems: a literature survey",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Beel",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Gipp",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Langer",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Breitinger",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Digit. Libr.",
            "volume": "17",
            "issn": "4",
            "pages": "305-338",
            "other_ids": {
                "DOI": [
                    "10.1007/s00799-015-0156-0"
                ]
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF43": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "The unified medical language system (UMLS): integrating biomedical terminology",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Bodenreider",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Nucleic Acids Res.",
            "volume": "32",
            "issn": "Database issue",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}