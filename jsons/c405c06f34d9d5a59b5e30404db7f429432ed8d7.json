{
    "paper_id": "c405c06f34d9d5a59b5e30404db7f429432ed8d7",
    "metadata": {
        "title": "Advances in Collaborative Filtering and Ranking",
        "authors": [
            {
                "first": "Liwei",
                "middle": [],
                "last": "Wu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "B.S. City University of Hong",
                    "location": {
                        "postCode": "2014",
                        "settlement": "Kong"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [],
    "body_text": [
        {
            "text": "List of Tables [8, 74] ants. We did not report SSE-PT++ results for beauty, games and steam, as the input sequence lengths are very short (see Table 8 The synthesis dataset has 10, 000 users and 2, 000 items with user friendship graph of size 10, 000 \u00d7 10, 000. Note that the graph only contains at most 6-hop valid information. GRMF G 6 means GRMF with G + \u03b1 \u00b7 G 2 + \u03b2 \u00b7 G 3 + \u03b3 \u00b7 G 4 + \u00b7 G 5 + \u03c9 \u00b7 G 6 . GRMF DNA-d means depth d is used. . . . ",
            "cite_spans": [
                {
                    "start": 15,
                    "end": 18,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 19,
                    "end": 22,
                    "text": "74]",
                    "ref_id": "BIBREF73"
                }
            ],
            "ref_spans": [
                {
                    "start": 143,
                    "end": 150,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                }
            ],
            "section": ""
        },
        {
            "text": "In this dissertation, we cover some recent advances in collaborative filtering and ranking.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Advances in Collaborative Filtering and Ranking"
        },
        {
            "text": "In chapter 1, we give a brief introduction of the history and the current landscape of collaborative filtering and ranking; chapter 2 we first talk about pointwise collaborative filtering problem with graph information, and how our proposed new method can encode very deep graph information which helps four existing graph collaborative filtering algorithms; chapter 3 is on the pairwise approach for collaborative ranking and how we speed Ask yourself, if today were the last day, what is the most important thing that you want to do? Then just follow your heart and I believe everyone can achieve a meaningful life.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Advances in Collaborative Filtering and Ranking"
        },
        {
            "text": "Nowadays in online retail and online content delivery applications, it is commonplace to have embedded recommendation systems algorithms that recommend items to users based on previous user behaviors and ratings. The field of recommender systems has gained more and more popularity ever since the famous Netflix competition [7] , in which competitors utilize user ratings to predict ratings for each user-movie pair and the final winner takes home 1 million dollars. During the competition, 2 distinct approaches stand out: one being the Restricted Boltzmann Machines [93] and the other being matrix factorization [61, 73] .",
            "cite_spans": [
                {
                    "start": 324,
                    "end": 327,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 568,
                    "end": 572,
                    "text": "[93]",
                    "ref_id": "BIBREF92"
                },
                {
                    "start": 614,
                    "end": 618,
                    "text": "[61,",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 619,
                    "end": 622,
                    "text": "73]",
                    "ref_id": "BIBREF72"
                }
            ],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "The combination of both approaches work well during the competition, but due to the ease of training and inference, matrix factorization approaches have dominated the collaborative filtering field before the widespread adoption of deep learning methods [95] . Collaborative filtering refers to making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). Usually this does not require approaching the recommendation problem as a ranking problem but rather pretends it is a regression or classification problem. Of course, there may be some loss incurred when using a regression or classification loss for ultimately what is a ranking problem. Because at the end of day, the ordering is the most important thing, which is directly associated with the recommender system performance. Collaborative ranking approaches mitigate the concerns by using a ranking loss. The ranking loss can be either [8, 89] and temporal information [43, 56] . The ways of constructing graphs can vary. One way to define graph over users is to exploit the friendship relationships among friends. But there are many ways to define such graphs. If there is more than one type of relationship, then we call it knowledge graphs instead of graphs. Knowledge graphs widely exist: for example, between 2 movies, they could be starred by the same actors or they could be the same genre of movies. In the field of collaborative filtering List-wise Approach [50, 121] ",
            "cite_spans": [
                {
                    "start": 253,
                    "end": 257,
                    "text": "[95]",
                    "ref_id": "BIBREF94"
                },
                {
                    "start": 983,
                    "end": 986,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 987,
                    "end": 990,
                    "text": "89]",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "text": "[43,",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 1021,
                    "end": 1024,
                    "text": "56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 1514,
                    "end": 1518,
                    "text": "[50,",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "text": "121]",
                    "ref_id": "BIBREF120"
                }
            ],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "This dissertation summarizes several published and under-review works that advance the field of collaborative filtering and ranking. Our first main contribution is that we fill out the void in [115] . We not only contribute to the fundamental approaches but also to extended approaches that utilize extra information,",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 198,
                    "text": "[115]",
                    "ref_id": "BIBREF114"
                }
            ],
            "ref_spans": [],
            "section": "Contributions and Outline of This Thesis"
        },
        {
            "text": "including graph and temporal ordering information. On one hand, we propose a novel way to encode long range graph interactions without require any training using bloom filters as backbone [119] . On the other hand, with the help of a new embedding-layer regularization called Stochastic Shared Embeddings (SSE) [117] , we can also introduce personalization for the state-of-the-art sequential recommendation model and achieve much better ranking performance with our personalized model [118] , where personalization is crucial for the success of recommender systems unlike most natural language tasks. This new regularization not only helps existing collaborative filtering and collaborative ranking algorithms but also benefits methods in natural language processing in fields like machine translation and sentiment analysis [117] . ",
            "cite_spans": [
                {
                    "start": 188,
                    "end": 193,
                    "text": "[119]",
                    "ref_id": "BIBREF118"
                },
                {
                    "start": 311,
                    "end": 316,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                },
                {
                    "start": 486,
                    "end": 491,
                    "text": "[118]",
                    "ref_id": "BIBREF117"
                },
                {
                    "start": 826,
                    "end": 831,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Contributions and Outline of This Thesis"
        },
        {
            "text": "In this chapter, we consider the Collaborative Ranking (CR) problem for recommendation systems. Given a set of pairwise preferences between items for each user, collaborative ranking can be used to rank un-rated items for each user, and this ranking can be naturally used for recommendation. It is observed that collaborative ranking algorithms usually achieve better performance since they directly minimize the ranking loss; however, they are rarely used in practice due to the poor scalability. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary Chapter 3:"
        },
        {
            "text": "In this chapter, we propose a listwise approach for constructing user-specific rankings in recommendation systems in a collaborative fashion. We contrast the listwise approach to previous pointwise and pairwise approaches, which are based on treating either each rating or each pairwise comparison as an independent instance respectively. By extending the work of [15] , we cast listwise collaborative ranking as maximum likelihood under a permutation model which applies probability mass to permutations based on a low rank latent score matrix. We present a novel algorithm called SQL-Rank, which can accommodate ties and missing data and can run in linear time. We develop a theoretical framework for analyzing listwise ranking methods based on a novel representation theory for the permutation model. ",
            "cite_spans": [
                {
                    "start": 364,
                    "end": 368,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Summary Chapter 4:"
        },
        {
            "text": "Collaborative Filtering with Graph Encoding",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 2"
        },
        {
            "text": "Recommendation systems are increasingly prevalent due to content delivery platforms, e-commerce websites, and mobile apps [98] . Classical collaborative filtering algorithms use matrix factorization to identify latent features that describe the user preferences and item meta-topics from partially observed ratings [61] . In addition to rating information, many real-world recommendation datasets also have a wealth of side information in the form of graphs, and incorporating this information often leads to performance gains [67, 89, 128] .",
            "cite_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "text": "[98]",
                    "ref_id": "BIBREF97"
                },
                {
                    "start": 315,
                    "end": 319,
                    "text": "[61]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 527,
                    "end": 531,
                    "text": "[67,",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 532,
                    "end": 535,
                    "text": "89,",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 536,
                    "end": 540,
                    "text": "128]",
                    "ref_id": "BIBREF127"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "However, each of these only utilizes the immediate neighborhood information of each node in the side information graph. More recently, [8] incorporated graph information when learning features with a Graph Convolution Network (GCN) based recommendation algorithm. GCNs [58] constitute flexible methods for incorporating graph structure beyond first-order neighborhoods, but their training complexity typically scales rapidly with the depth, even with sub-sampling techniques [18] . Intuitively, exploiting higher-order neighborhood information could benefit the generalization performance, especially when the graph is sparse, which is usually the case in practice. The main caveat of exploiting higher-order graph information is the high computational and memory cost when computing higher-order neighbors since the number of t-hop neighbors typically grows exponentially with t.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 138,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 269,
                    "end": 273,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 475,
                    "end": 479,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We aim to utilize higher order graph information without introducing much computational and memory overhead. We propose a Graph Deep Neighborhood Aware (Graph DNA) encoding, which approximately captures the higher-order neighborhood information of each node via Bloom filters [9] . Bloom filters encode neighborhood sets as c dimensional 0/1 vectors, where c = O(log n) for a graph with n nodes, which approximately preserves membership information. This encoding can then be combined with both graph regularized or feature based collaborative filtering algorithms, with little computational and memory overhead. In addition to computational speedups, we find that Graph DNA achieves better performance over competitors. We show that our Graph DNA encoding can be used with several collaborative filtering algorithms: graph-regularized matrix factorization with explicit and implicit feedback [89, 128] , co-factoring [67] , and GCN-based recommendation systems [74] . In some cases, using information from deeper neighborhoods (like 4 th order)",
            "cite_spans": [
                {
                    "start": 276,
                    "end": 279,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 893,
                    "end": 897,
                    "text": "[89,",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 898,
                    "end": 902,
                    "text": "128]",
                    "ref_id": "BIBREF127"
                },
                {
                    "start": 918,
                    "end": 922,
                    "text": "[67]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 962,
                    "end": 966,
                    "text": "[74]",
                    "ref_id": "BIBREF73"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "yields a 15x increase in performance, with graph DNA encoding yielding a 6x speedup compared to directly using the 4 th power of the graph adjacency matrix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Matrix factorization has been used extensively in recommendation systems with both explicit [61] and implicit [49] feedback. Such methods compute low dimensional user and item representations; their inner product approximates the observed (or to be predicted) entry in the target matrix. To incorporate graph side information in these systems, [89, 128] used a graph Laplacian based regularization framework that forces a pair of node representations to be similar if they are connected via an edge in the graph. In [126] , this was extended to the implicit feedback setting. [67] proposed a method that incorporates first-order information of the rating bipartite graph into the model by considering item co-occurrences. More recently, GC-MC [8] used a GCN approach performing convolutions on the main bipartite graph by treating the first-order side graph information as features, and [74] proposed combining GCNs and RNNs for the same task.",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 96,
                    "text": "[61]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 110,
                    "end": 114,
                    "text": "[49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 344,
                    "end": 348,
                    "text": "[89,",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 349,
                    "end": 353,
                    "text": "128]",
                    "ref_id": "BIBREF127"
                },
                {
                    "start": 516,
                    "end": 521,
                    "text": "[126]",
                    "ref_id": "BIBREF125"
                },
                {
                    "start": 576,
                    "end": 580,
                    "text": "[67]",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 743,
                    "end": 746,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 887,
                    "end": 891,
                    "text": "[74]",
                    "ref_id": "BIBREF73"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Methods that use higher order graph information are typically based on taking random walks on the graphs [31] . [52] extended this method to include graph side information in the model. Finally, the PageRank [76] algorithm can be seen as computing the steady state distribution of a Markov network, and similar methods for recommender systems was proposed in [1, 122] .",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 109,
                    "text": "[31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 112,
                    "end": 116,
                    "text": "[52]",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 208,
                    "end": 212,
                    "text": "[76]",
                    "ref_id": "BIBREF75"
                },
                {
                    "start": 359,
                    "end": 362,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 363,
                    "end": 367,
                    "text": "122]",
                    "ref_id": "BIBREF121"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "For a complete list of related works of representation learning on graphs, we refer the interested user to [36] . For the collaborative filtering setting, [8, 74] use Graph Convolutional Neural Networks (GCN) [23] , but with some modifications. Standard GCN methods without substantial modifications cannot be directly applied to collaborative filtering rating datasets, including well-known approaches like GCN [58] and GraphSage [35] , because they are intended to solve semi-supervised classification problem over graphs with nodes' features. PinSage [123] is the GraphSage extension to non-personalized graphbased recommendation algorithm but is not meant for collaborative filtering problems.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 155,
                    "end": 158,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 159,
                    "end": 162,
                    "text": "74]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 209,
                    "end": 213,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 412,
                    "end": 416,
                    "text": "[58]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 431,
                    "end": 435,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 554,
                    "end": 559,
                    "text": "[123]",
                    "ref_id": "BIBREF122"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "GC-MC [8] extends GCN to collaborative filtering, albeit it is less scalable than [123] .",
            "cite_spans": [
                {
                    "start": 6,
                    "end": 9,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 82,
                    "end": 87,
                    "text": "[123]",
                    "ref_id": "BIBREF122"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Our Graph DNA scheme can be used to obtain graph features in these extensions. In contrast to the above-mentioned methods involving GCNs, we do not use the data driven loss function to train our graph encoder. This property makes our graph DNA suitable for both transductive as well as inductive problems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Bloom filters have been used in Machine Learning for multi-label classification [21] , and for hashing deep neural network models representations [22, 37, 99] . However, to the best of our knowledge, until now, they have not been used to encode graphs, nor has this encoding been applied to recommender systems. So it would be interesting to extend our work to other recommender systems settings, such as [118] and [117] .",
            "cite_spans": [
                {
                    "start": 80,
                    "end": 84,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 146,
                    "end": 150,
                    "text": "[22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 151,
                    "end": 154,
                    "text": "37,",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 155,
                    "end": 158,
                    "text": "99]",
                    "ref_id": "BIBREF98"
                },
                {
                    "start": 405,
                    "end": 410,
                    "text": "[118]",
                    "ref_id": "BIBREF117"
                },
                {
                    "start": 415,
                    "end": 420,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "We consider the recommender system problem with a partially observed rating matrix R and a Graph that encodes side information G. In this section, we will introduce the Graph DNA algorithm for encoding deep neighborhood information in G. In the next section, we will show how this encoded information can be applied to various graph based recommender systems.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "The Bloom filter [9] is a probabilistic data structure designed to represent a set of elements.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 20,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Bloom Filter"
        },
        {
            "text": "Thanks to its space-efficiency and simplicity, Bloom filters are applied in many real-world applications such as database systems [10, 16] . A Bloom filter B consists of k independent hash functions h t (x) \u2192 {1, . . . , c}. The Bloom filter B of size c can be represented as a length c bit-array b. More details about Bloom filters can be found in [12] . Here we highlight a few desirable properties of Bloom filters essential to our graph DNA encoding: ",
            "cite_spans": [
                {
                    "start": 130,
                    "end": 134,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 135,
                    "end": 138,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 349,
                    "end": 353,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Bloom Filter"
        },
        {
            "text": "is the shortest path distance between nodes i and j in G. As the last step, we stack array representations of all Bloom filters and form a sparse matrix B \u2208 {0, 1} n\u00d7c , where the i-th row of B is the bit representation of B[i]. As a practical measure, to prevent over-saturation of Bloom filters for popular nodes in the graph, we add a hyper-parameter \u03b8 to control the max saturation level allowed for Bloom filters. This would also prevent hub nodes dominating in graph DNA encoding. The pseudo-code for the proposed encoding algorithm is given in Algorithm 1. We use graph DNA-d to denote our obtained graph encoding after applying Algorithm 1 with s looping from 1 to d. We also give a simple example to illustrate how the graph DNA is encoded into Bloom filter representations in ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bloom Filter"
        },
        {
            "text": "Suppose we are given the sparse rating matrix R \u2208 R n\u00d7m with n users and m items, and a graph G \u2208 R n\u00d7n encoding relationships between users. For simplicity, we do not assume a graph on the m items, though including it should be straightforward. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Collaborative Filtering with Graph DNA"
        },
        {
            "text": "Explicit Feedback : The objective function of Graph Regularized Matrix Factorization (GRMF) [13, 89, 128] is:",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 96,
                    "text": "[13,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 97,
                    "end": 100,
                    "text": "89,",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 101,
                    "end": 105,
                    "text": "128]",
                    "ref_id": "BIBREF127"
                }
            ],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "where U \u2208 R n\u00d7r , V \u2208 R m\u00d7r are the embeddings associated with users and items respectively, n is the number of users and m is the number of items, R \u2208 R n\u00d7m is the sparse rating matrix, tr() is the trace operator, \u03bb, \u00b5 are tuning coefficients, and Lap(\u00b7) is the graph Laplacian operator.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "The last term is called graph regularization, which tries to enforce similar nodes (measured by edge weights in G) to have similar embeddings. One naive way [14] to extend this to higher-order graph regularization is to replace the graph G with K i=1 w i \u00b7 G i and then use the graph Laplacian of K i=1 w i \u00b7 G i to replace G in (2.1). Computing G i for even small i is computationally infeasible for most real-world applications, and we will soon lose the sparsity of the graph, leading to memory issues. Sampling or thresholding could mitigate the problem but suffers from performance degradation.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 161,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "In contrast, our graph DNA obtained from Algorithm 1 does not suffer from any of these issues. The space complexity of our method is only of order O(n log n) for a graph with n nodes, instead of O(n 2 ). The reduced number of non-zero elements using graph DNA leads to a significant speed-up in many cases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "We can easily use graph DNA in GRMF as follows: we treat the c bits as c new pseudo-nodes and add them to the original graph G. We then have n + c nodes in a modified graph\u0120:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "To account for the c new nodes, we expand U \u2208 R n\u00d7r toU \u2208 R (n+c)\u00d7r by appending parameters for the meta-nodes. The objective function for GRMF with Graph DNA with be the same as (2.1) except replacing U and G withU and\u0120. At the prediction stage, we discard the meta-node embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "Implicit Feedback : For implicit feedback data, when R is a 0/1 matrix, weighted matrix factorization is a widely used algorithm [48, 49] . The only difference is that the",
            "cite_spans": [
                {
                    "start": 129,
                    "end": 133,
                    "text": "[48,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 134,
                    "end": 137,
                    "text": "49]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "where \u03c1 < 1 is a hyper-parameter reflecting the confidence of zero entries. In this case, we can apply the Graph DNA encoding as before trivially.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization"
        },
        {
            "text": "Co-Factorization of Rating and Graph Information (Co-Factor) [67, 103] is ideologically very different from GRMF and GRWMF, because it does not use graph information as regularization term. Instead it treats the graph adjacency matrix as another rating matrix, sharing one-sided latent factors with the original rating matrix. Co-Factor minimizes the following objective function:",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 65,
                    "text": "[67,",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 66,
                    "end": 70,
                    "text": "103]",
                    "ref_id": "BIBREF102"
                }
            ],
            "ref_spans": [],
            "section": "Co-Factorization with Graph Information"
        },
        {
            "text": "We can extend Co-Factor to incorporate our DNA-d by replacing G with B in the equation above, where B \u2208 R n\u00d7c is the Bloom filter bipartite graph adjacency matrix of n real-user nodes and c pseudo-user nodes, similar to B as in (2.2). We call the extension Co-Factor DNA-d.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Co-Factorization with Graph Information"
        },
        {
            "text": "Graph Convolutional Matrix Completion (GC-MC) is a graph convolutional network (GCN) based geometric matrix completion method [8] . In [8] , the rating matrix R is treated as an adjacency matrix in GCN while side information G is treated as feature matrix for nodes -each user has an n-dimensional 0/1 feature that corresponds to a column of G. The GCN model then performs convolutions of these features on the bipartite rating graph. Convolutions of these features are performed on the bipartite rating graph.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 129,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 135,
                    "end": 138,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Graph Convolutional Matrix Completion"
        },
        {
            "text": "We find in our experiments that using these one-hot encodings of the graph as feature is an inferior choice both in terms of performance and speed. To capture higher order side graph information, it is better to use G + \u03b1G 2 for some constant \u03b1 and this alternate choice usually gives smaller generalization error than the original GC-MC method. However, it is hard to explicitly calculate G + \u03b1G 2 and store the entire matrix for a large graph for the same reason described in Section 2.4.1. Again, we can use graph DNA to efficiently encode and store the higher order information before feeding it into GC-MC. We show in our experiments that this outperforms current state-of-the-art GCN methods [8, 74] as well as GC-MC with graph encoding methods that require training, such as Node2vec [32] and Deepwalk [84] . Our encoding scheme does not require training and therefore is a lot faster than previous encoding methods. More details are discussed in the experiment section 2.5.3.",
            "cite_spans": [
                {
                    "start": 698,
                    "end": 701,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 702,
                    "end": 705,
                    "text": "74]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 791,
                    "end": 795,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 809,
                    "end": 813,
                    "text": "[84]",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [],
            "section": "Graph Convolutional Matrix Completion"
        },
        {
            "text": "We show that our Graph DNA encoding technique can improve the performance of 4 popular graph-based recommendation algorithms: graph-regularized matrix factorization, ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We first simulate a user/item rating dataset with user graph as side information, generate its graph DNA, and use it on a downstream task: matrix factorization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "We randomly generate user and item embeddings from standard Gaussian distributions, and construct an Erd\u0151s-R\u00e9nyi Random graph of users. User embeddings are generated using Algorithm 11 in Appendix: at each propagation step, each user's embedding is updated by an average of its current embedding and its neighbors' embeddings. Based on user and item embeddings after T = 3 iterations of propagation, we generate the underlying ratings for each user-item pairs according to the inner product of their embeddings, and then sample a small portion of the dense rating matrix as training and test sets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "We implement our graph DNA encoding algorithm in python using a scalable python library [3] to generate Bloom filter matrix B. We adapt the GRMF C++ code to solve the objective function of GRMF DNA-K with our Bloom filter enhanced graph\u0120. We compare the following variants:",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 91,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "1. MF: classical matrix factorization only with 2 regularization without graph information.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "2. GRMF G d : GRMF with 2 regularization and using G, G 2 , . . . , G d [14] .",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 76,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "3. GRMF DNA-d: GRMF with 2 but using our proposed graph DNA-d.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "We report the prediction performance with Root Mean Squared Error (RMSE) on test data. All results are reported on the test set, with all relevant hyperparameters tuned on a held-out validation set. To accurately measure how large the relative gain is from using deeper information, we introduce a new metric called Relative Graph Gain (RGG) for using information X, which is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "where RMSE is measured for the same method with different graph information. This metric would be 0 if only first order graph information is utilized and is only defined when the denominator is positive.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation Study"
        },
        {
            "text": "In Table 2 .1, we can easily see that using a deeper neighborhood helps the recommendation performances on this synthetic dataset. Graph DNA-3's gain is 166% larger than that of using first-order graph G. We can see an increase in performance gain for an increase in depth d when d \u2264 3. This is expected because we set T = 3 during our creation of this dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Simulation Study"
        },
        {
            "text": "Next, we show that graph DNA can improve the performance of GRMF for explicit feedback. We conduct experiments on two real datasets: Douban [70] and Flixster [127] .",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 144,
                    "text": "[70]",
                    "ref_id": "BIBREF69"
                },
                {
                    "start": 158,
                    "end": 163,
                    "text": "[127]",
                    "ref_id": "BIBREF126"
                }
            ],
            "ref_spans": [],
            "section": "Graph Regularized Matrix Factorization for Explicit Feedback"
        },
        {
            "text": "Both datasets contain explicit feedback with ratings from 1 to 5. We pre-processed Douban and Flixster following the same procedure in [89, 115] . The experimental setups and comparisons are almost identical to the synthetic data experiment (see details in section 2.5.1). Due to the exponentially growing non-zero elements in the graph as we go deeper (see Table 8 .2), we are unable to run full GRMF G 4 and GRMF G 5 for these datasets. In fact, GRMF G 3 itself is too slow so we thresholded G 3 by only considering entries whose values are equal to or larger than 4. For the Bloom filter, we set a false positive rate of 0.1 and use capacity of 500 for Bloom filters, resulting in c = 4, 796.",
            "cite_spans": [
                {
                    "start": 135,
                    "end": 139,
                    "text": "[89,",
                    "ref_id": "BIBREF88"
                },
                {
                    "start": 140,
                    "end": 144,
                    "text": "115]",
                    "ref_id": "BIBREF114"
                }
            ],
            "ref_spans": [
                {
                    "start": 358,
                    "end": 365,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                }
            ],
            "section": "Graph Regularized Matrix Factorization for Explicit Feedback"
        },
        {
            "text": "We can see from Table 2 .1 that deeper graph information always helps. For Douban, graph DNA-3 is most effective, giving a relative graph gain of 82.79% compared to only 2% gain when using G 2 or G 3 naively. Interestingly for Flixster, using G 2 is better than using G 3 . However, Graph DNA-3 and DNA-4 yield 10x and 15x performance improvements respectively, lending credence to the implicit regularization property of graph DNA. For a fixed size Bloom filter, the computational complexity of graph DNA scales linearly with depth d, as compared to exponentially for GRMF G d . We measure the speed in ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 16,
                    "end": 23,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Graph Regularized Matrix Factorization for Explicit Feedback"
        },
        {
            "text": "We show our graph DNA can improve Co-Factor [67, 103] as well. The results are in Table 2 .1. We find that applying DNA-3 to the Co-Factor method improves performance on both the datasets, more so for Flixster. This is consistent with our observations for GRMF in Table 2 .1: deep graph information is more helpful for Flixster than Douban.",
            "cite_spans": [
                {
                    "start": 44,
                    "end": 48,
                    "text": "[67,",
                    "ref_id": "BIBREF66"
                },
                {
                    "start": 49,
                    "end": 53,
                    "text": "103]",
                    "ref_id": "BIBREF102"
                }
            ],
            "ref_spans": [
                {
                    "start": 82,
                    "end": 89,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                },
                {
                    "start": 264,
                    "end": 271,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Co-Factorization with Graph for Explicit Feedback"
        },
        {
            "text": "Applying Graph DNA to Co-Factor is detailed in the Appendix. [8, 74] ). All the methods except GC-MC utilize side graph information. ",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 64,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 65,
                    "end": 68,
                    "text": "74]",
                    "ref_id": "BIBREF73"
                }
            ],
            "ref_spans": [],
            "section": "Co-Factorization with Graph for Explicit Feedback"
        },
        {
            "text": "We follow the same procedure as in [116] to set ratings of 4 and above to 1, and the rest to 0. We compare the baseline graph based weighted matrix factorization [48, 49] with our proposed weighted matrix factorization with DNA-3. We do not compare with Bayesian personalized ranking [91] and the recently proposed SQL-rank [116] as they cannot easily utilize graph information.",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 40,
                    "text": "[116]",
                    "ref_id": "BIBREF115"
                },
                {
                    "start": 162,
                    "end": 166,
                    "text": "[48,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 167,
                    "end": 170,
                    "text": "49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 284,
                    "end": 288,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 324,
                    "end": 329,
                    "text": "[116]",
                    "ref_id": "BIBREF115"
                }
            ],
            "ref_spans": [],
            "section": "Graph Regularized Weighted Matrix Factorization for Implicit Feedback"
        },
        {
            "text": "The results are summarized in Table 2 .3 with experimental details in the Appendix.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 30,
                    "end": 37,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Graph Regularized Weighted Matrix Factorization for Implicit Feedback"
        },
        {
            "text": "Again, using DNA-3 achieves better prediction results over the baseline in terms of every single metric on both Douban and Flixster datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Regularized Weighted Matrix Factorization for Implicit Feedback"
        },
        {
            "text": "We can use graph DNA instead to efficiently encode and store the higher order information before feeding it into GC-MC.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Convolutional Matrix Factorization"
        },
        {
            "text": "We use the same split of three real-world datasets and follow the exact procedures as in [8, 74] . We tuned hyperparameters using a validation dataset and obtain the best test results found within 200 epochs using optimal parameters. We repeated the experiments 6 times and report the mean and standard deviation of test RMSE. After some tuning, we use the capacity of 10 Bloom filters for Douban and 60 for Flixster, as the latter has a much denser second-order graph. With a false positive rate of 0.1, this implies that we use 96-bits Bloom filters for Douban and 960 bits for Flixster. We use the resulting bloom filter bitarrays as the node features, and pass that as the input to GC-MC. Using Graph DNA-2, the input feature dimensions are thus reduced from 3000 to 96 and 960, which leads to a significant speed-up. The original GC-MC method did not scale up well beyond 3000 by 3000 rating matrices with the user and the item side graphs as it requires using normalized adjacency matrix as user/item features. PinSage [123] , while scalable, does not utilize the user/item side graphs. Furthermore, it is not feasible to have O(n) dimensional features for the nodes, where n is the number of nodes in side graphs. In contrast, our method only requires O(log(n)) dimensional features. We can see from Table 2 .4 that we outperform both GCN-based methods [8] and [74] in terms of performance by a large margin.",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 92,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 93,
                    "end": 96,
                    "text": "74]",
                    "ref_id": "BIBREF73"
                },
                {
                    "start": 1025,
                    "end": 1030,
                    "text": "[123]",
                    "ref_id": "BIBREF122"
                },
                {
                    "start": 1360,
                    "end": 1363,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "text": "[74]",
                    "ref_id": "BIBREF73"
                }
            ],
            "ref_spans": [
                {
                    "start": 1307,
                    "end": 1314,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Graph Convolutional Matrix Factorization"
        },
        {
            "text": "Note that another potential way to improve over GC-MC is to use other graph encoding schemes like Node2Vec [32] and DeepWalk [84] to encode the user-user graph into node features. One clear drawback is that those graph embedding methods are time-consuming.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 125,
                    "end": 129,
                    "text": "[84]",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [],
            "section": "Graph Convolutional Matrix Factorization"
        },
        {
            "text": "Using the official Node2vec implementation, excluding reading and writing, it takes 416.13 seconds to encode the 3K by 3K subsampled Yahoo-Music item graph and obtain resulting 760-d node embeddings. For our method, it only takes 7.55 seconds to obtain the same 760d features. Similarly, it takes over 15 mins to run the official C++ codes for DeepWalk [84] using the same parameters as Node2Vec to encode the graph. In fact, fast encoding via hashing and bitwise-or that does not require training is one of the main advantages of our method.",
            "cite_spans": [
                {
                    "start": 353,
                    "end": 357,
                    "text": "[84]",
                    "ref_id": "BIBREF83"
                }
            ],
            "ref_spans": [],
            "section": "Graph Convolutional Matrix Factorization"
        },
        {
            "text": "Furthermore, even without considering the time overhead, we found our graph DNA encoding outperforms Node2Vec and DeepWalk in terms of test RMSE. Details can be found in Table 2 .4. This could be due to that encoding higher-order information is more important for graph-regularized recommendation tasks, and graph DNA is a better and more direct way to encode higher order information compared with Node2Vec and DeepWalk.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 170,
                    "end": 177,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Graph Convolutional Matrix Factorization"
        },
        {
            "text": "Speed Comparisons Next, we compare the speed-ups obtained by graph DNA-d with GRMF G d (a naive way to encode higher order information by computing powers of G). Figure 3 suggests that graph DNA-1 (which encodes hop-2 information) scales better than directly computing G 2 in GRMF.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 162,
                    "end": 170,
                    "text": "Figure 3",
                    "ref_id": "FIGREF12"
                }
            ],
            "section": "Graph Convolutional Matrix Factorization"
        },
        {
            "text": "Exploring Effects of Rank Finally, we investigate whether the proposed DNA coding can achieve consistent improvements when varying the rank in the GRMF algorithm. In Table 2 .5, we compare the proposed GRMF DNA-3 with GRMF G 2 , which achieves the best RMSE without using DNA coding in the previous tables. The results clearly show that the improvement of the proposed DNA coding is consistent over different ranks and works even better when rank is larger.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 166,
                    "end": 173,
                    "text": "Table 2",
                    "ref_id": "TABREF12"
                }
            ],
            "section": "Graph Convolutional Matrix Factorization"
        },
        {
            "text": "In this chapter, we proposed Graph DNA, a deep neighborhood aware encoding scheme for collaborative filtering with graph information. We make use of Bloom filters to incorporate higher order graph information, without the need to explicitly minimize a loss function. The resulting encoding is extremely space and computationally efficient, and lends itself well to multiple algorithms that make use of graph information, including Graph Convolutional",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Networks. Experiments show that Graph DNA encoding outperforms several baseline methods on multiple datasets in both speed and performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Large-scale Pairwise Collaborative Ranking in Near-Linear Time",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 3"
        },
        {
            "text": "In online retail and online content delivery applications, it is commonplace to have embedded recommendation systems-algorithms that recommend items to users based on previous user behaviors and ratings. Online retail companies develop sophisticated recommendation systems based on purchase behavior, item context, and shifting trends. The",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Netflix prize [7] , in which competitors utilize user ratings to recommend movies, accelerated research in recommendation systems. While the winning submissions agglomerated several existing methods, one essential methodology, latent factor models, emerged as a critical component. The latent factor model means that the approximated rating for user i and item j is given by",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 17,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "One interpretation is that there are k latent topics and the approximated rating can be reconstructed as a combination of factor weights. By minimizing the square error loss of this reconstruction we arrive at the incomplete SVD,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "where \u2126 contains sampled indices of the rating matrix, R.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Often the performance of recommendation systems is not measured by the quality of rating prediction, but rather the ranking of the items that the system returns for a given user. The task of finding a ranking based on ratings or relative rankings is called Collaborative Ranking. Recommendation systems can be trained with ratings, that may be passively or actively collected, or by relative rankings, in which a user is asked to rank a number of items. A simple way to unify the framework is to convert the ratings into rankings by making pairwise comparisons of ratings. Specifically, the algorithm takes as input the pairwise comparisons, Y i,j,k for each user i and item pairs j, k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "This approach confers several advantages. Users may have different standards for their ratings, some users are more generous with their ratings than others. This is known as the calibration drawback, and to deal with this we must make a departure from standard matrix factorization methods. Because we focus on ranking and not predicting ratings, we can expect improved performance when recommending the top items. Our goal in this chapter is to provide a collaborative ranking algorithm that can scale to the size of the full Netflix dataset, a heretofore open problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The existing collaborative ranking algorithms, (for a summary see section 3.2), are limited by the number of observed ratings per user in the training data and cannot scale to massive datasets, therefore, making the recommendation results less accurate and less useful in practice. This motivates our algorithm, which can make use of the entire Netflix dataset without sub-sampling. Our contribution can be summarized below:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 For input data in the form of pairwise preference comparisons, we propose a new algorithm Primal-CR that alternatively minimizes latent factors using Newton's method in the primal space. By carefully designing the computation of gradient and Hessian vector product, our algorithm reduces the sample complexity per iteration to O(|\u2126| + d 1d2 r), while the state-of-the-art approach [81] have O(|\u2126|r) complexity.",
            "cite_spans": [
                {
                    "start": 383,
                    "end": 387,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Here |\u2126| (total number of pairs), is much larger than d 1d2 (d 1 is number of users andd 2 is averaged number of items rated by a user). For the Netflix problem, |\u2126| = 2 \u00d7 10 10 while d 1d2 = 10 8 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "\u2022 For input data in the form of ratings, we can further exploit the structure to speedup the gradient and Hessian computation. The resulting algorithm, Primal-CR++, can further reduce the time complexity to O(d 1d2 (r + logd 2 )) per iteration. In this setting, our algorithm has time complexity near-linear to the input size, and have comparable speed with classical matrix factorization model that takes O(d 1d2 r) time, while we can achieve much better recommendation by minimizing the ranking loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We show that our algorithms outperform existing algorithms on real world datasets and can be easily parallelized.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Collaborative filtering methodologies are summarized in [95] (see [24] for an early work).",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 60,
                    "text": "[95]",
                    "ref_id": "BIBREF94"
                },
                {
                    "start": 66,
                    "end": 70,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Among them, matrix factorization [61] has been widely used due to the success in the Netflix Prize. Many algorithms have been developed based on matrix factorization [19, 48, 90, 91, 102] , and many scalable algorithms have been developed [29, 61] . However, they are not suitable for ranking top items for a user due to the fact that their goal is to minimize the mean-square error (MSE) instead of ranking loss. In fact, MSE is not a good metric for recommendation when we want to recommend the top K items to a user. This has been pointed out in several papers [5] which argue normalized discounted cumulative gain (NDCG) should be used instead of MSE, and our experimental results also confirm this finding by showing that minimizing the ranking loss results in better precision and NDCG compared with the traditional matrix factorization approach that is targeting squared error.",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 37,
                    "text": "[61]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 166,
                    "end": 170,
                    "text": "[19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 171,
                    "end": 174,
                    "text": "48,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 175,
                    "end": 178,
                    "text": "90,",
                    "ref_id": "BIBREF89"
                },
                {
                    "start": 179,
                    "end": 182,
                    "text": "91,",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 183,
                    "end": 187,
                    "text": "102]",
                    "ref_id": "BIBREF101"
                },
                {
                    "start": 239,
                    "end": 243,
                    "text": "[29,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 244,
                    "end": 247,
                    "text": "61]",
                    "ref_id": "BIBREF60"
                },
                {
                    "start": 564,
                    "end": 567,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Ranking is a well studied problem, and there has been a long line of research focuses on learning one ranking function, which is called Learning to Rank. For example, RankSVM [53] is a well-known pair-wise model, and an efficient solver has been proposed in [17] for solving rankSVM. [15] is a list-wise model implemented using neural networks.",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 179,
                    "text": "[53]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 258,
                    "end": 262,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 284,
                    "end": 288,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Another class of point-wise models fit the ratings explicitly but has the issue of calibration drawback (see [34] ).",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "The collaborative ranking (CR) problem is essentially trying to learn multiple rankings together, and several models and algorithms have been proposed in literature. The Cofirank algorithm [114] , which tailors maximum margin matrix factorization [105] for collaborative ranking, is a point-wise model for CR, and is regarded as the performance benchmark for this task. If the ratings are 1-bit, a weighting scheme is proposed to improve the usual point-wise Matrix Factorization approach [78] . List-wise models for Learning to Rank can also be extended to many rankings setting, [100] . However it is still quite similar to a point-wise approach since they only consider the top-1 probabilities.",
            "cite_spans": [
                {
                    "start": 189,
                    "end": 194,
                    "text": "[114]",
                    "ref_id": "BIBREF113"
                },
                {
                    "start": 247,
                    "end": 252,
                    "text": "[105]",
                    "ref_id": "BIBREF104"
                },
                {
                    "start": 489,
                    "end": 493,
                    "text": "[78]",
                    "ref_id": "BIBREF77"
                },
                {
                    "start": 581,
                    "end": 586,
                    "text": "[100]",
                    "ref_id": "BIBREF99"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "For pairwise models in collaborative ranking, it is well known that they do not encounter the calibration drawback as do point-wise models, but they are computationally intensive and cannot scale well to large data sets [100] . The scalability problem for pairwise models is mainly due to the fact that their time complexity is at least proportional to |\u2126|, the number of pairwise preference comparisons, which grows quadratically with number of rated items for each user. Recently, [81] proposed a new Collrank algorithm, and they showed that Collrank has better precision and NDCG as well as being much faster compared with other CR methods on real world datasets, including Bayesian Personalized Ranking (BPR) [91] .",
            "cite_spans": [
                {
                    "start": 220,
                    "end": 225,
                    "text": "[100]",
                    "ref_id": "BIBREF99"
                },
                {
                    "start": 483,
                    "end": 487,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 713,
                    "end": 717,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Unfortunately their scalability is still constrained by number of pairs, so they can only run on subsamples for large datasets, such as Netflix. In this chapter, our algorithm",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Primal-CR and Primal-CR++ also belong to the family of pairwise models, but due to cleverly re-arranging the computation, we are able to have much better time complexity than existing ones, and as a result our algorithm can scale to very large datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "There are many other algorithms proposed for many rankings setting but none of these mentioned below can scale up to the extent of using all the ratings in the full Netflix data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "There are a few using Bayesian frameworks to model the problem [91] , [79] , [111] , the last of which requires many specified parameters. Another one proposed retargeted matrix factorization to get ranking by monotonically transforming the ratings [62] . [33] proposes a similar model without making generative assumptions on ratings besides assuming low-rank and correctness of the ranking order.",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 67,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 70,
                    "end": 74,
                    "text": "[79]",
                    "ref_id": "BIBREF78"
                },
                {
                    "start": 77,
                    "end": 82,
                    "text": "[111]",
                    "ref_id": "BIBREF110"
                },
                {
                    "start": 249,
                    "end": 253,
                    "text": "[62]",
                    "ref_id": "BIBREF61"
                },
                {
                    "start": 256,
                    "end": 260,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "We first formally define the collaborative ranking problem using the example of item recommender system. Assume we have d 1 users and d 2 items, the input data is given in the form of \"for user i, item j is preferred over item k\" and thus can be represented by a set of tuples (i, j, k). We use \u2126 to denote the set of observed tuples, and the observed pairwise preferences are denoted as {Y ijk | (i, j, k) \u2208 \u2126}, where Y ijk = 1 denotes that item j is preferred over item k for a particular user i and Y ijk = \u22121 to denote that item k is preferred over item j for user i.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "The goal of collaborative ranking is to rank all the unseen items for each user i based on these partial observations, which can be done by fitting a scoring matrix X \u2208 R d 1 \u00d7d 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "If the scoring matrix has X ij > X ik , it implies that item j is preferred over item k by the particular user i and therefore we should give higher rank for item j than item k. After we estimate the scoring matrix X by solving the optimization problem described below, we can then recommend top k items for any particular user.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "The Collaborative Ranking Model referred to in this chapter is the one proposed recently in [81] . It belongs to the family of pairwise models for collaborative ranking because it uses pairwise training losses [5] . The model is given as",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 96,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 210,
                    "end": 213,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "where L(.) is the loss function, X * is the nuclear norm regularization defined by the sum of all the singular value of the matrix X, and \u03bb is a regularization parameter. The ranking loss defined in the first term of (3.2) penalizes the pairs when Y ijk = 1 but X ij \u2212 X ik is positive but small, and penalizes even more when the difference is negative. The second term in the loss function is based on the assumption that there are only a small number of latent factors contributing to the users' preferences which is analogous to the idea behind incomplete SVD for matrix factorization mentioned in the introduction. In general we can use any loss function, but since",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "gives the best performance in practice [81] and enjoys many nice properties, such as smoothness and differentiable, we will focus on L 2 -hinge loss in this chapter. In fact, our first algorithm Primal-CR can be applied to any loss function, while Primal-CR++ can only be applied to L 2 -hinge loss.",
            "cite_spans": [
                {
                    "start": 39,
                    "end": 43,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "Despite the advantage of the objective function in equation (3.2) being convex, it is still not feasible for large-scale problems since d 1 and d 2 can be very large so that the scoring matrix X cannot be stored in memory, not to mention how to solve it. Therefore, in practice people usually transform (3.2) to a non-convex form by replacing X = U V T , and in that case since",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "We use u i and v j denote columns of U and V respectively. Note that [81] also solves the non-convex form (3.4) in their experiments, and in the rest of the paper we will propose a faster algorithm for solving (3.4).",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 106,
                    "end": 111,
                    "text": "(3.4)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "Although collaborative ranking assumes that input data is given in the form of pairwise comparisons, in reality almost all the datasets (Netflix, Yahoo-Music, MovieLens, etc)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Overview"
        },
        {
            "text": "contain user ratings to items in the form of {R ij | (i, j) \u2208\u03a9}, where\u03a9 is the subset of observed user-item pairs. Therefore, in practice we have to transform the rating-based data into pair-wise comparisons by generating all the item pairs rated by the same user:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Overview"
        },
        {
            "text": "\u2208\u03a9} is the set of items rated by user i. Assume there are averagel\u0233 d 2 items rated by a user (i.e.,d 2 = mean(|\u03a9 i |)), then the collaborative ranking problem will have O(d 1d2 2 ) pairs and thus the size of \u2126 grows quadratically.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Overview"
        },
        {
            "text": "Unfortunately, all the existing algorithms have O(|\u2126|r) complexity, so they cannot scale to large number of items. For example, the AltSVM (or referred to as Collrank)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Overview"
        },
        {
            "text": "Algorithm in [81] will run out of memory when we subsample 500 rated items per user on Netflix dataset since its implementation 1 stores all the pairs in memory and therefore requires O(|\u2126|) memory. So it cannot be used for the full Netflix dataset which has more than 20 billion pairs and requires 300GB memory space. To the best of our knowledge, no collaborative ranking algorithms have been applied to the full Netflix data set. But in real life, we hope to make use of as much information as possible to make better recommendation. As shown in our experiments later, using full training data instead of sub-sampling (such as selecting a fixed number of rated items per user) achieves higher prediction and recommendation accuracy for the same test data.",
            "cite_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Motivation and Overview"
        },
        {
            "text": "To overcome this scalability issue, we propose two novel algorithms for solving prob- ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Overview"
        },
        {
            "text": "If the input file is given in ratings, we can further reduce the time complexity to O(d 1d2 r + d 1d2 logd 2 ) using exactly the same optimization algorithm but smarter ways to compute gradient and Hessian vector product. This time complexity is much smaller than the number of comparisons |\u2126| = O (d 1d   2 2 ), and we call this algorithm Primal-CR++. We will first introduce Primal-CR in Section 3.4.2, and then present Primal-CR++ in Section 3.4.3.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 298,
                    "end": 307,
                    "text": "(d 1d   2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Motivation and Overview"
        },
        {
            "text": "In the first setting, we consider the case where the pairwise comparisons {Y ijk | (i, j, k) \u2208 \u2126} are given as input. To solve problem (3.4), we alternatively minimize U and V in the primal space (see Algorithm 2) . First, we fix U and update V, and the subproblem for V while U is fixed can be written as follows:",
            "cite_spans": [
                {
                    "start": 201,
                    "end": 213,
                    "text": "Algorithm 2)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Primal-CR: the proposed algorithm for pairwise input data"
        },
        {
            "text": "In [81] , this subproblem is solved by stochastic dual coordinate descent, which requires O(|\u2126|r) time and O(|\u2126|) space complexity. Furthermore, the objective function decreases ",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 7,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Primal-CR: the proposed algorithm for pairwise input data"
        },
        {
            "text": "procedure Fix U and update V 4:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal-CR: the proposed algorithm for pairwise input data"
        },
        {
            "text": "while not converged do",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal-CR: the proposed algorithm for pairwise input data"
        },
        {
            "text": "Apply truncated Newton update (Algorithm 3)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "procedure Fix V and update U 7:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "while not converged do 8:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Apply truncated Newton update (Algorithm 3) 9: return U, V recover score matrix X for the dual problem sometimes does not imply the decrease of primal objective function value, which often results in slow convergence. We therefore propose to solve this subproblem for V using the primal truncated Newton method (Algorithm 3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Newton method is a classical second-order optimization algorithm. For minimizing a vector-valued function f (x), Newton method iteratively updates the solution by x \u2190",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "x \u2212 (\u2207 2 f (x)) \u22121 \u2207f (x). However, the matrix inversion is usually hard to compute, so a truncated Newton method computes the update direction by solving the linear system",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "up to a certain accuracy, usually using a linear conjugate gradient method. If we vectorized the problem for updating V in eq (3.6), the gradient is a (rd 2 )sized vector and the Hessian is an (rd 2 )-by-(rd 2 ) matrix, so explicitly forming the Hessian is impossible. Below we discuss how to apply the truncated Newton method to solve our problem, and discuss efficient computations for each part.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "When applying the truncated Newton method, the Compute the Hessian-vector product q = Hp k 8:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "\u03b4 k+1 = \u03b4 k + \u03b1 k p k 10:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "if ||r k+1 || 2 < ||r 0 || 2 \u00b7 10 \u22122 then 12:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "break 13:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "is a R r\u00d7d 2 matrix and can be computed explicitly:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "\u2208 \u2126} is the subset of pairs that associates with user i, and e j is the indicator vector used to add the u i vector to the j-th column of the output matrix. The first derivative for L 2 -hinge loss function (3.3) is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "For convenience, we define g := vec(\u2207f (V )) to be the vectorized form of gradient. One can easily see that computing g naively by going through all the pairwise comparisons (j, k) and adding up arrays is time-consuming and has O(|\u2126|r) time complexity, which is the same with Collrank [81] .",
            "cite_spans": [
                {
                    "start": 285,
                    "end": 289,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "Fast computation for gradient Fortunately, we can reduce the time complexity to O(|\u2126| + d 1d2 r) by smartly rearranging the computations, so that the time is only linear to |\u2126| and r, but not to |\u2126|r. The method is described below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "First, for each i, the first term of (3.7) can be represented by time for each i. To compute t j , we first compute u T i v j for all j \u2208d 2 (i) in O(d 2 (i)r) time, and then go through all the (j, k) pairs while keep adding the coefficient related to this pair to t j and t k . Since there is no vector operations when we go through all pairs, this step only takes O(\u2126 i ) time. After getting all t j , we can then conduct j\u2208d 2 (i) t j u i e T j in O(d 2 (i)r) time. Therefore, the overall complexity can be reduced to O(|\u2126| + d 1d2 r). The pseudo code is presented in Algorithm 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "Derivation of Hessian-vector product Now we derive the Hessian ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "for all j \u2208d 2 (i) do 4: precompute u T i v j and store in a vector m i",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 24,
                    "text": "4:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Derivation of Gradient."
        },
        {
            "text": "Initialize a zero array t of size d 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "for",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "for all j \u2208d 2 (i) do ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Taking derivative again we can obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "and the second derivative for L 2 hinge loss function is given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Note that if we write the full Hessian H as a (d 2 r) by (d 2 r) matrix, then \u2207 2 j,k f (V ) is an r \u00d7 r block in H, where there are totally d 2 2 of these blocks. In the CG update for solving H \u22121 g, we only need to compute H \u00b7 a for some a \u2208 R d 2 r . For convenience, we also partition this a into d 2 blocks, each subvector a j has size r, so a = [a 1 ; \u00b7 \u00b7 \u00b7 ; a j ]. Similarly we can use subscript to denote the subarray (H \u00b7 a) j of the array H \u00b7 a, which becomes",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "where E j is the projection matrix to the j-th block, indicating that we are only adding (H \u00b7 a) j to the j-th block of matrix, and setting 0 elsewhere.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Algorithm 5. Primal-CR: efficient way to compute Hessian vector product",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "for all j \u2208d 2 (i) do 4: precompute u T i a j and store it in array b",
            "cite_spans": [
                {
                    "start": 22,
                    "end": 24,
                    "text": "4:",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Initialize a zero array t of size d 2",
            "cite_spans": [],
            "ref_spans": [],
            "section": "5:"
        },
        {
            "text": "for O(|\u2126| + d 1d2 r) by pre-computing u T i a j and caching the coefficient using the array t. The detailed algorithm is given in Algorithm 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Note that in Algorithm 5, we can reuse the m (sparse array storing the current prediction) which has been pre-computed in the gradient computation (Algorithm 4), and that will cost only O(d 1d2 ) memory. Even without storing the m matrix, we can compute m in the loop of line 4 in Algorithm 5, which will not increase the overall computational complexity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Fix V and Update U After updating V by truncated Newton, we need to fix V and update U . The subproblem for U can be written as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Since u i , the i-th column of U , is independent from the rest of columns, equation 3.16",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "can be decomposed into d 1 independent problems for u i :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Eq (3.17) is equivalent to an r-dimensional rankSVM problem. Since r is usually small, the problems are easy to solve. In fact, we can directly apply an efficient rankSVM algorithm proposed in [17] to solve each r-dimensional rankSVM problem. This algorithm algorithm only needs to store size d 1 \u00d7 r and d 2 \u00d7 r matrices for gradient and conjugate gradient method. The m matrix in Algorithm 4 is not needed, but in practice we find it can speedup the code by around 25%, and it only takes d 1d2 \u2264 |\u2126| memory space (less than the input size). Therefore, our algorithm is very memory-efficient.",
            "cite_spans": [
                {
                    "start": 193,
                    "end": 197,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Before going to Primal-CR++, we discuss the time complexity of Primal-CR when the input data is the user-item rating matrix. Assumed 2 is the averaged number of rated items per user, then there will be |\u2126| = O (d 1d   2 2 ) pairs, leading to O(d 1d 2 2 + d 1d2 r) time complexity for Primal-CR. This is much better than the O(d 1d ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 210,
                    "end": 219,
                    "text": "(d 1d   2",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "6:"
        },
        {
            "text": "Do another scan j fromd 2 to 1 to compute t \u2212 [\u03c0(j)] for all j 16:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6:"
        },
        {
            "text": "Now we discuss a more realistic scenario, where the input data is a rating matrix {R ij | (i, j) \u2208\u03a9} and\u03a9 is the observed set of user-item ratings. We assume there are only L levels of ratings, so R ij \u2208 {1, 2, . . . , L}. Also, we used 2 (i) := {j | (i, j) \u2208\u03a9} to denote the rated items for user i.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal-CR++: the proposed algorithm for rating data"
        },
        {
            "text": "Given this data, the goal is to solve the collaborative ranking problem ( The algorithm of Primal-CR++ is exactly the same with Primal-CR, but we use a smarter algorithm to compute gradient and Hessian vector product in near-linear time, by exploiting the structure of the input data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal-CR++: the proposed algorithm for rating data"
        },
        {
            "text": "We first discuss how to speed up the gradient computation of (3.7), where the main computation is to compute (3.9) for each i. When the loss function is L2-hinge loss, we can explicitly write down the coefficients t j in (3.9) by (3.18) where m j := u T i v j and I[\u00b7] is an indicator function such that I[a \u2264 b] = 1 if a \u2264 b, and I[a \u2264 b] = 0 otherwise. By splitting the cases of Y ijk = 1 and Y ijk = \u22121, we get Since we scan from left to right, these numbers can be maintained in constant time at each step. Now assume we scan over the numbers m 1 + 1, m 2 + 1, . . . , then at each point we can compute",
            "cite_spans": [
                {
                    "start": 230,
                    "end": 236,
                    "text": "(3.18)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Primal-CR++: the proposed algorithm for rating data"
        },
        {
            "text": "Although we observe that O(L) time is already small in practice (since L usually smaller than 10), in the following we show there is a way to remove the dependency on L by using a simple Fenwick tree [27] , F+tree [125] or segment tree. If we store the set {s 1 , . . . , s L } in Fenwick tree, then each query of \u2265r s i can be done in in O(log L) time, and since each step we only need to change one element into the set, the updating time is also O(log L). Note that t \u2212 j can be computed in the same way by scanning from largest m j to the smallest one.",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 204,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 214,
                    "end": 219,
                    "text": "[125]",
                    "ref_id": "BIBREF124"
                }
            ],
            "ref_spans": [],
            "section": "Primal-CR++: the proposed algorithm for rating data"
        },
        {
            "text": "To sum up, the algorithm first computes all m j in O(d 2 r) time, then sort these numbers using O(d 2 logd 2 ) time, and then compute t j for all j using two linear scans in O(d 2 log L) time. Here log L is dominated by logd 2 since L can be the number of unique rating levels in the current setd 2 (i). Therefore, after computing this for all users i = 1, . . . , d 1 , the time complexity for computing gradient is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Primal-CR++: the proposed algorithm for rating data"
        },
        {
            "text": "A similar procedure can also be used for computing the Hessian-vector product, and the computation of updating U with fixed V is simplier since the problem becomes Compared with the classical matrix factorization, where both ALS and SGD requires O(|\u03a9|r) time per iteration [61] , our algorithm has almost the same complexity, since logd 2 is usually smaller than r (typically r = 100). Also, since all the temporary memory when computing user i can be released immediately, the only memory cost is still the same with",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 277,
                    "text": "[61]",
                    "ref_id": "BIBREF60"
                }
            ],
            "ref_spans": [],
            "section": "Primal-CR++: the proposed algorithm for rating data"
        },
        {
            "text": "Updating U while fixing V can be parallelized easily because each column of U is independent and we can actually solve d 1 independent subproblems at the same time. For the other side, updating V while fixing U can also be parallelized by parallelizing \"computing g\" part and \"computing Ha\" part respectively. We implemented the algorithm using parallel computing techniques in Julia by computing g and Ha distributedly and summing ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parallelization"
        },
        {
            "text": "In this section, we test the performance of our proposed algorithms Primal-CR and Primal-CR++ on real world datasets, and compare with existing methods. All experiments are conducted on the UC Davis Illidan server with an Intel Xeon E5-2640 2.40GHz CPU and 64G RAM. We compare the following methods:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "\u2022 Primal-CR and Primal-CR++: our proposed methods implemented in Julia. 2 \u2022 Collrank: the collaborative ranking algorithm proposed in [81] . We use the C++ code released by the authors, and they parallelized their algorithm using OpenMP.",
            "cite_spans": [
                {
                    "start": 72,
                    "end": 73,
                    "text": "2",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 134,
                    "end": 138,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "\u2022 Cofirank: the classical collaborative ranking algorithm proposed in [114] . We use the C++ code released by the authors.",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 75,
                    "text": "[114]",
                    "ref_id": "BIBREF113"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "\u2022 MF: the classical matrix factorization model in (3.1) solved by SGD [61] .",
            "cite_spans": [
                {
                    "start": 70,
                    "end": 74,
                    "text": "[61]",
                    "ref_id": "BIBREF60"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "We used three data sets (MovieLens1m, Movielens10m, Netflix data) to compare these algorithms. The dataset statistics are summarized in Table 3 .1. The regularization parameter \u03bb used for each datasets are chosen by a random sampled validation set. For the pair-wise based algorithms, we covert the ratings into pair-wise comparisons, by saying that item j is preferred over item k by user i if user i gives a higher rating to item j over item k, and there will be no pair between two items if they have the same rating.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 136,
                    "end": 143,
                    "text": "Table 3",
                    "ref_id": "TABREF18"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "We compare the algorithms in the following three different ways:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "\u2022 Objective function: since Collrank, Primal-CR, Primal-CR++ have the same objective function, we can compare the convergence speed in terms of the objective function (3.4) with squared hinge loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "\u2022 Predicted pairwise error: the proportion of pairwise preference comparisons that we predicted correctly out of all the pairwise comparisons in the testing data: (3.20) where T represents the test data set and |T | denotes the size of test data set.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 169,
                    "text": "(3.20)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "\u2022 NDCG@k: a standard performance measure of ranking, defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "where i represents i-th user and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In the DCG definition, \u03c0 i (l) represents the index of the l-th ranked item for user i in test data based on the score matrix X = U T V generated, M is the rating matrix and M ij is the rating given to item j by user i. \u03c0 * i is the ordering provided by the underlying ground truth of the rating.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "3.5.1 Compare single thread versions using the same subsamples Since Collrank cannot scale to the full dataset of Movielens10m and Netflix, we sub-sample data using the same approach in their paper [81] and compare all the methods using the smaller training sets. More specifically, for each data set, we subsampled N ratings for training data and used the rest of ratings as test data. For this subsampled data, we discard users with less than N + 10 ratings, since we need at least 10 ratings for test data to compute the NDCG@10.",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 202,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "As shown in Figure 3 .1, 3.2, 3.3, both Primal-CR and Primal-CR++ perform considerably better than the existing Collrank algorithm. As data size increases, the performance gap becomes larger. As one can see, for Netflix data where N = 200, the speedup is more than 10 times compared to Collrank.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 20,
                    "text": "Figure 3",
                    "ref_id": "FIGREF12"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "For Cofirank, we observe that it is even slower than Collrank, which confirms the experiments conducted in [81] . Furthermore, Cofirank cannot scale to larger datasets, so we omit the results in We also include the classical matrix factorization algorithm in the NDCG comparisons.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "As shown in our complexity analysis, our proposed algorithms are competitive with MF in terms of speed, and MF is much faster than other collaborative ranking algorithms. Also, we observe that MF converges to a slightly worse solution in MovieLens10m and Netflix datasets, and converges to a much worse solution in MovieLens1m. The reason is that MF minimizes a simple mean square error, while our algorithms are minimizing ranking loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Based on the experimental results, our algorithm Primal-CR++ should be able to replace MF in many real world recommender systems. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Since Collrank can be implemented in a parallel fashion, we also implemented the parallel version of our algorithm in Julia. We want to show our algorithm scales up well and is still much faster than Collrank in the multi-core shared memory setting. As shown in Using our algorithm, we have the ability to solve the full Netflix problem, so a natural question to ask is: Does using more training data help us predict and recommend better?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Compare parallel versions"
        },
        {
            "text": "The answer is yes! We conduct the following experiments to verify this: For all the users with more than 20 ratings, we randomly choose 10 ratings as test data and out of the rest ratings we randomly choose up to C ratings per user as training data. One can see in Figure 3 .6, for the same test data, more training data leads to better prediction performance in terms of pairwise error and NDCG. Using all available ratings (C = d 2 )",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 265,
                    "end": 273,
                    "text": "Figure 3",
                    "ref_id": "FIGREF12"
                }
            ],
            "section": "Compare parallel versions"
        },
        {
            "text": "gives lowest pairwise error and highest NDCG@10, using up to 200 ratings per user (C = 200) gives second lowest pairwise error and second highest NDCG@10, and using up to 100 ratings per user (C = 100) has the highest pairwise error and lowest NDCG@10.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Compare parallel versions"
        },
        {
            "text": "Similar phenomenon is observed for Netflix data in Figure 3 .7. Collrank code does not work for C = 200 and C = d 2 and even for C = 100, it takes more than 20, 000 secs to converge while our Primal-CR++ takes less than 5, 000 secs for the full Netflix data. The speedup of our algorithm will be even more for a larger C or larger data size d 1 and d 2 . We tried to create input file without subsampling for Collrank, we created 344GB input data file and Collrank reported memory error message \"Segmentation Fault\". We also tried C = 200, still got the same error message. It is possible to implement Collrank algorithm by directly working on the rating data, but the time complexity remains the same, so it is clear that our proposed Primal-CR and Primal-CR++ algorithms are much faster.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 51,
                    "end": 59,
                    "text": "Figure 3",
                    "ref_id": "FIGREF12"
                }
            ],
            "section": "Compare parallel versions"
        },
        {
            "text": "To the best of our knowledge, our algorithm is the first ranking-based algorithm that can scale to full Netflix data set using a single core, and without sub-sampling. Our ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Compare parallel versions"
        },
        {
            "text": "We considered the collaborative ranking problem setting in which a low-rank matrix is fitted to the data in the form of pairwise comparisons or numerical ratings. We proposed our new optimization algorithms Primal-CR and Primal-CR++ where the time complexity is much better than all the existing approaches. We showed that our algorithms are much faster than state-of-the-art collaborative ranking algorithms on real data sets (MovieLens1m, Movielens10m and Netflix) using same subsampling scheme, and moreover our algorithm is the only one that can scale to the full Movielens10m and Netflix data. We observed that our algorithm has the same efficiency with matrix factorization, while achieving better NDCG since we minimize ranking loss. As a result, we expect our algorithm to be able to replace matrix factorization in many real applications.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "We study a novel approach to collaborative ranking-the personalized ranking of items for users based on their observed preferences-through the use of listwise losses, which are dependent only on the observed rankings of items by users. We propose the SQL-Rank algorithm, which can handle ties and missingness, incorporate both explicit ratings and more implicit feedback, provides personalized rankings, and is based on the relative rankings of items. To better understand the proposed contributions, let us begin with a brief history of the topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recommendation systems, found in many modern web applications, movie streaming services, and social media, rank new items for users and are judged based on user engagement (implicit feedback) and ratings (explicit feedback) of the recommended items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "A high-quality recommendation system must understand the popularity of an item and infer a user's specific preferences with limited data. Collaborative filtering, introduced in [44] , refers to the use of an entire community's preferences to better predict the preferences of an individual (see [95] for an overview). In systems where users provide ratings of items, collaborative filtering can be approached as a point-wise prediction task, in which we attempt to predict the unobserved ratings [80] . Low rank methods, in which the rating distribution is parametrized by a low rank matrix (meaning that there are a few latent factors) provides a powerful framework for estimating ratings [59, 73] . There are several issues with this approach. One issue is that the feedback may not be representative of the unobserved entries due to a sampling bias, an effect that is prevalent when the items are only 'liked' or the feedback is implicit because it is inferred from user engagement.",
            "cite_spans": [
                {
                    "start": 177,
                    "end": 181,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 295,
                    "end": 299,
                    "text": "[95]",
                    "ref_id": "BIBREF94"
                },
                {
                    "start": 496,
                    "end": 500,
                    "text": "[80]",
                    "ref_id": "BIBREF79"
                },
                {
                    "start": 690,
                    "end": 694,
                    "text": "[59,",
                    "ref_id": "BIBREF58"
                },
                {
                    "start": 695,
                    "end": 698,
                    "text": "73]",
                    "ref_id": "BIBREF72"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "Augmenting techniques like weighting were introduced to the matrix factorization objective to overcome this problem [48, 49] . Many other techniques are also introduced [55, 112, 120] .",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "text": "[48,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 121,
                    "end": 124,
                    "text": "49]",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 169,
                    "end": 173,
                    "text": "[55,",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 174,
                    "end": 178,
                    "text": "112,",
                    "ref_id": "BIBREF111"
                },
                {
                    "start": 179,
                    "end": 183,
                    "text": "120]",
                    "ref_id": "BIBREF119"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "Another methodology worth noting is the CofiRank algorithm of [113] which minimizes a convex surrogate of the normalized discounted cumulative gain (NDCG). The pointwise framework has other flaws, chief among them is that in recommendation systems we are not interested in predicting ratings or engagement, but rather we must rank the items.",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 67,
                    "text": "[113]",
                    "ref_id": "BIBREF112"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "Ranking is an inherently relative exercise. Because users have different standards for ratings, it is often desirable for ranking algorithms to rely only on relative rankings and not absolute ratings. A ranking loss is one that only considers a user's relative preferences between items, and ignores the absolute value of the ratings entirely, thus deviating from the pointwise framework. Ranking losses can be characterized as pairwise and listwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "A pairwise method decomposes the objective into pairs of items j, k for a user i, and effectively asks 'did we successfully predict the comparison between j and k for user i?'. The comparison is a binary response-user i liked j more than or less than k-with possible missing values in the event of ties or unobserved preferences. Because the pairwise model has cast the problem in the classification framework, then tools like support vector machines were used to learn rankings; [54] introduces rankSVM and efficient solvers can be found in [17] . Much of the existing literature focuses on learning a single ranking for all users, which we will call simple ranking [2, 28, 77] . This work will focus on the personalized ranking setting, in which the ranking is dependent on the user.",
            "cite_spans": [
                {
                    "start": 480,
                    "end": 484,
                    "text": "[54]",
                    "ref_id": "BIBREF53"
                },
                {
                    "start": 542,
                    "end": 546,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 667,
                    "end": 670,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 671,
                    "end": 674,
                    "text": "28,",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 675,
                    "end": 678,
                    "text": "77]",
                    "ref_id": "BIBREF76"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "Pairwise methods for personalized ranking have seen great advances in recent years,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "with the AltSVM algorithm of [81] , Bayesian personalized ranking (BPR) of [91] , and the near linear-time algorithm of [115] . Nevertheless, pairwise algorithms implicitly assume that the item comparisons are independent, because the objective can be decomposed",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[81]",
                    "ref_id": "BIBREF80"
                },
                {
                    "start": 75,
                    "end": 79,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 120,
                    "end": 125,
                    "text": "[115]",
                    "ref_id": "BIBREF114"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "where each comparison has equal weight. Listwise losses instead assign a loss, via a generative model, to the entire observed ranking, which can be thought of as a permutation of the m items, instead of each comparison independently. The listwise permutation model, introduced in [15] , can be thought of as a weighted urn model, where items correspond to balls in an urn and they are sequentially plucked from the urn with probability proportional to \u03c6(X ij ) where X ij is the latent score for user i and item j and \u03c6 is some non-negative function. They proposed to learn rankings by optimizing a cross entropy between the probability of k items being at the top of the ranking and the observed ranking, which they combine with a neural network, resulting in the ListNet algorithm. [100] applies this idea to collaborative ranking, but uses only the top-1 probability because of the computational complexity of using top-k in this setting. This was extended in [50] to incorporate neighborhood information. [121] instead proposes a maximum likelihood framework that uses the permutation probability directly, which enjoyed some empirical success.",
            "cite_spans": [
                {
                    "start": 280,
                    "end": 284,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 784,
                    "end": 789,
                    "text": "[100]",
                    "ref_id": "BIBREF99"
                },
                {
                    "start": 963,
                    "end": 967,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1009,
                    "end": 1014,
                    "text": "[121]",
                    "ref_id": "BIBREF120"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "Very little is understood about the theoretical performance of listwise methods. [15] demonstrates that the listwise loss has some basic desirable properties such as monotonicity,",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 85,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "i.e. increasing the score of an item will tend to make it more highly ranked. [65] studies the generalizability of several listwise losses, using the local Rademacher complexity, and found that the excess risk could be bounded by a 1/ \u221a n term (recall, n is the number of users). Two main issues with this work are that no dependence on the number of items is given-it seems these results do not hold when m is increasing-and the scores are not personalized to specific users, meaning that they assume that each user is an independent and identically distributed observation. A simple open problem is: can we consistently learn preferences from a single user's data if we are given item features and we assume a simple parametric model? (n = 1, m \u2192 \u221e.)",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "[65]",
                    "ref_id": "BIBREF64"
                }
            ],
            "ref_spans": [],
            "section": "A brief history of collaborative ranking"
        },
        {
            "text": "We can summarize the shortcomings of the existing work: current listwise methods for collaborative ranking rely on the top-1 loss, algorithms involving the full permutation probability are computationally expensive, little is known about the theoretical performance of listwise methods, and few frameworks are flexible enough to handle explicit and implicit data with ties and missingness. This chapter addresses each of these in turn by proposing and analyzing the SQL-rank algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions of this work"
        },
        {
            "text": "\u2022 We propose the SQL-Rank method, which is motivated by the permutation probability, and has advantages over the previous listwise method using cross entropy loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions of this work"
        },
        {
            "text": "\u2022 We provide an O(iter \u00b7 (|\u2126|r)) linear algorithm based on stochastic gradient descent,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions of this work"
        },
        {
            "text": "where \u2126 is the set of observed ratings and r is the rank.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions of this work"
        },
        {
            "text": "\u2022 The methodology can incorporate both implicit and explicit feedback, and can gracefully handle ties and missing data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions of this work"
        },
        {
            "text": "\u2022 We provide a theoretical framework for analyzing listwise methods, and apply this to the simple ranking and personalized ranking settings, highlighting the dependence on the number of users and items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions of this work"
        },
        {
            "text": "The permutation probability, [15] , is a generative model for the ranking parametrized by latent scores. First assume there exists a ranking function that assigns scores to all the items. Let's say we have m items, then the scores assigned can be represented as a vector s = (s 1 , s 2 , ..., s m ). Denote a particular permutation (or ordering) of the m items as \u03c0,",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Permutation probability"
        },
        {
            "text": "which is a random variable and takes values from the set of all possible permutations S m (the symmetric group on m elements). \u03c0 1 denotes the index of highest ranked item and \u03c0 m is the lowest ranked. The probability of obtaining \u03c0 is defined to be",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Permutation probability"
        },
        {
            "text": "where \u03c6(.) is an increasing and strictly positive function. An interpretation of this model is that each item is drawn without replacement with probability proportional to \u03c6(s i ) for item i in each step. One can easily show that P s (\u03c0) is a valid probability distribution,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Permutation probability"
        },
        {
            "text": "i.e. \u03c0\u2208Sm P s (\u03c0) = 1, P s (\u03c0) > 0, \u2200\u03c0. Furthermore, this definition of permutation probability enjoys several favorable properties (see [15] ). For any permutation \u03c0 if you swap two elements ranked at i < j generating the permutation \u03c0 (\u03c0 i = \u03c0 j , \u03c0 j = \u03c0 i , \u03c0 k = \u03c0 k , k = i, j), if s \u03c0 i > s \u03c0 j then P s (\u03c0) > P s (\u03c0 ). Also, if permutation \u03c0 satisfies s \u03c0 i > s \u03c0 i+1 , \u2200i, then we have \u03c0 = arg max \u03c0 \u2208Sm P s (\u03c0 ). Both of these properties can be summarized: larger scores will tend to be ranked more highly than lower scores. These properties are required for the negative log-likelihood to be considered sound for ranking [121] .",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 141,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 632,
                    "end": 637,
                    "text": "[121]",
                    "ref_id": "BIBREF120"
                }
            ],
            "ref_spans": [],
            "section": "Permutation probability"
        },
        {
            "text": "In recommendation systems, the top ranked items can be more impactful for the performance. In order to focus on the top k ranked items, we can compute the partialranking marginal probability,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Permutation probability"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Permutation probability"
        },
        {
            "text": "It is a common occurrence that only a proportion of the m items are ranked, and in that case we will allowm \u2264 m to be the number of observed rankings (we assume that \u03c0 1 , . . . , \u03c0m are the complete list of ranked items). When k = 1, the first summation vanishes and top-1 probability can be calculated straightforwardly, which is why k = 1 is widely used in previous listwise approaches for collaborative ranking. Counter-intuitively, we demonstrate that using a larger k tends to improve the ranking performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(4.2)"
        },
        {
            "text": "We see that computing the likelihood loss is linear in the number of ranked items, which is in contrast to the cross-entropy loss used in [15] , which takes exponential time in k. The cross-entropy loss is also not sound, i.e. it can rank worse scoring permutations more highly, but the negative log-likelihood is sound. We will discuss how we can deal with ties in the following subsection, namely, when the ranking is derived from ratings and multiple items receive the same rating, then there is ambiguity as to the order of the tied items. This is a common occurrence when the data is implicit, namely the output is whether the user engaged with the item or not, yet did not provide explicit feedback.",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 142,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "(4.2)"
        },
        {
            "text": "Because the output is binary, the cross-entropy loss (which is based on top-k probability with k very small) will perform very poorly because there will be many ties for the top ranked items. To this end, we propose a collaborative ranking algorithm using the listwise likelihood that can accommodate ties and missingness, which we call Stochastic Queuing Listwise Ranking, or SQL-Rank. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(4.2)"
        },
        {
            "text": "The goal of collaborative ranking is to predict a personalized score X ij that reflects the preference level of user i towards item j, where 1 \u2264 i \u2264 n and 1 \u2264 j \u2264 m. It is reasonable to assume the matrix X \u2208 R n\u00d7m to be low rank because there are only a small number of latent factors contributing to users' preferences. The input data is given in the form of \"user i gives item j a relevance score R ij \". Note that for simplicity we assume all the users have the same numberm of ratings, but this can be easily generalized to the non-uniform case by replacingm with m i (number of ratings for user i).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "With our scores X and our ratings R, we can specify our collaborative ranking model using the permutation probability (4.2). Let \u03a0 i be a ranking permutation of items for user i (extracted from R), we can stack \u03a0 1 , . . . \u03a0 n , row by row, to get the permutation matrix \u03a0 \u2208 R n\u00d7m . Assuming users are independent with each other, the probability of observing a particular \u03a0 given the scoring matrix X can be written as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "We will assume that log \u03c6(x) = 1/(1 + exp(\u2212x)) is the sigmoid function. This has the advantage of bounding the resulting weights, \u03c6(X ij ), and maintaining their positivity without adding additional constraints.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "Typical rating data will contain many ties within each row. In such cases, the permutation \u03a0 is no longer unique and there is a set of permutations that coincides with rating because with any candidate \u03a0 we can arbitrarily shuffle the ordering of items with the same relevance scores to generate a new candidate matrix \u03a0 which is still valid (see To learn the scoring matrix X, we can naturally solve the following maximum likelihood estimator with low-rank constraint:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "where X is the structural constraint of the scoring matrix. To enforce low-rankness, we use the nuclear norm regularization X = {X : X * \u2264 r}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "Eq This upper bound is much easier to optimize and can be solved using Stochastic Gradient Descent (SGD).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "Next we discuss how to apply our model for explicit and implicit feedback settings. In the explicit feedback setting, it is assumed that the matrix R is partially observed and the observed entries are explicit ratings in a range (e.g., 1 to 5). We will show in the experiments that k =m (using the full list) leads to the best results. [50] also observed that increasing k is useful for their cross-entropy loss, but they were not able to increase k since their model has time complexity exponential to k.",
            "cite_spans": [
                {
                    "start": 336,
                    "end": 340,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "In the implicit feedback setting each element of R ij is either 1 or 0, where 1 means positive actions (e.g., click or like) and 0 means no action is observed. Directly solving (4.5)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "will be expensive sincem = m and the computation will involve all the mn elements at each iteration. Moreover, the 0's in the matrix could mean either a lower relevance score or missing, thus should contribute less to the objective function. Therefore, we adopt the idea of negative sampling [71] in our list-wise formulation. For each user (row of R), assume there arem 1's, we then sample \u03c1m unobserved entries uniformly from the same row and append to the back of the list. This then becomes the problem withm = (1 + \u03c1)m and then we use the same algorithm in explicit feedback setting to conduct updates. We then repeat the sampling process at the end of each iteration, so the update will be based on different set of 0's at each time.",
            "cite_spans": [
                {
                    "start": 292,
                    "end": 296,
                    "text": "[71]",
                    "ref_id": "BIBREF70"
                }
            ],
            "ref_spans": [],
            "section": "Deriving objective function for SQL-Rank"
        },
        {
            "text": "Despite the advantage of the objective function in equation (4.5) being convex, it is still not feasible for large-scale problems since the scoring matrix X \u2208 R n\u00d7m leads to high computational and memory cost. We follow a common trick to transform (4.5) to the non-convex form by replacing X = U T V : with U \u2208 R r\u00d7n , V \u2208 R r\u00d7m so that the objective ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Non-convex implementation"
        },
        {
            "text": "where u i , v j are columns of U, V respectively. We apply stochastic gradient descent to solve this problem. At each step, we choose a permutation matrix \u03a0 \u2208 S(R, \u2126) using the stochastic queuing process (Algorithm 8) and then update U, V by \u2207f (U, V ). For example, the gradient with respect to V is (g = log \u03c6 is the sigmoid function),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Non-convex implementation"
        },
        {
            "text": "where \u2126 j denotes the set of users that have rated the item j and rank i (j) is a function gives the rank of the item j for that user i. Because g is the sigmoid function, g = g \u00b7 (1 \u2212 g).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Non-convex implementation"
        },
        {
            "text": "The gradient with respect to U can be derived similarly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Non-convex implementation"
        },
        {
            "text": "As one can see, a naive way to compute the gradient of f requires O(nm 2 r) time, which is very slow even for one iteration. However, we show in Algorithm 12 (in the appendix) that there is a smart way to re-arranging the computation so that \u2207 V f (U, V ) can be computed in O(nmr) time, which makes our SQL-Rank a linear-time algorithm (with the same per-iteration complexity as classical matrix factorization).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Non-convex implementation"
        },
        {
            "text": "In this section, we compare our proposed algorithm (SQL-Rank) with other state-of-the-art algorithms on real world datasets. Note that our algorithm works for both implicit feedback and explicit feedback settings. In the implicit feedback setting, all the ratings are 0 or 1;",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "in the explicit feedback setting, explicit ratings (e.g., 1 to 5) are given but only to a subset of user-item pairs. Since many real world recommendation systems follow the implicit feedback setting (e.g., purchases, clicks, or checkins), we will first compare SQL-Rank on implicit feedback datasets and show it outperforms state-of-the-art algorithms. Then we will verify that our algorithm also performs well on explicit feedback problems. All experiments are conducted on a server with an Intel Xeon E5-2640 2.40GHz CPU and 64G RAM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In the implicit feedback setting we compare the following methods:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "\u2022 SQL-Rank: our proposed algorithm implemented in Julia 1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "1 https://github.com/wuliwei9278/SQL-Rank",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "\u2022 Weighted-MF: the weighted matrix factorization algorithm by putting different weights on 0 and 1's [48, 49] .",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 105,
                    "text": "[48,",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 106,
                    "end": 109,
                    "text": "49]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "\u2022 BPR: the Bayesian personalized ranking method motivated by MLE [91] . For both Weighted-MF and BPR, we use the C++ code by Quora 2 .",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 69,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                }
            ],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "Note that other collaborative ranking methods such as Pirmal-CR++ [115] and List-MF [100] do not work for implicit feedback data, and we will compare with them later in the explicit feedback experiments. For the performance metric, we use precision@k for k = 1, 5, 10 defined by",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 71,
                    "text": "[115]",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 84,
                    "end": 89,
                    "text": "[100]",
                    "ref_id": "BIBREF99"
                }
            ],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "where R is the rating matrix and \u03a0 il gives the index of the l-th ranked item for user i among all the items not rated by user i in the training set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "We use rank r = 100 and tune regularization parameters for all three algorithms using a random sampled validation set. For Weighted-MF, we also tune the confidence weights on unobserved data. For BPR and SQL-Rank, we fix the ratio of subsampled unobserved 0's versus observed 1's to be 3 : 1, which gives the best performance for both BPR and SQL-rank in practice.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "We experiment on the following four datasets. Note that the original data of Movie-lens1m, Amazon and Yahoo-music are ratings from 1 to 5, so we follow the procedure in [91, 124] to preprocess the data. We transform ratings of 4, 5 into 1's and the rest entries (with rating 1, 2, 3 and unknown) as 0's. Also, we remove users with very few 1's in the corresponding row to make sure there are enough 1's for both training and testing. For",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 173,
                    "text": "[91,",
                    "ref_id": "BIBREF90"
                },
                {
                    "start": 174,
                    "end": 178,
                    "text": "124]",
                    "ref_id": "BIBREF123"
                }
            ],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "Amazon, Yahoo-music and Foursquare, we discard users with less than 20 ratings and randomly select 10 1's as training and use the rest as testing. Movielens1m has more ratings than others, so we keep users with more than 60 ratings, and randomly sample 50 of them as training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "\u2022 Movielens1m: a popular movie recommendation data with 6, 040 users and 3, 952",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "items. \u2022 Yahoo-music: the Yahoo music rating data set 4 which contains 15, 400 users and 1, 000 items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "\u2022 Foursquare: a location check-in data 5 . The data set contains 3, 112 users and 3, 298 venues with 27, 149 check-ins. The data set is already in the form of \"0/1\" so we do not need to do any transformation.",
            "cite_spans": [
                {
                    "start": 39,
                    "end": 40,
                    "text": "5",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Implicit Feedback"
        },
        {
            "text": "The experimental results are shown in Table 4 .1. We find that SQL-Rank outperforms both Weighted-MF and BPR in most cases.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 38,
                    "end": 45,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                }
            ],
            "section": "Implicit Feedback"
        },
        {
            "text": "Next we compare the following methods in the explicit feedback setting:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "\u2022 SQL-Rank: our proposed algorithm implemented in Julia. Note that in the explicit feedback setting our algorithm only considers pairs with explicit ratings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "\u2022 List-MF: the listwise algorithm using the cross entropy loss between observed rating and top 1 probability [100] . We use the C++ implementation on github 6 .",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 114,
                    "text": "[100]",
                    "ref_id": "BIBREF99"
                }
            ],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "\u2022 MF: the classical matrix factorization algorithm in [59] utilizing a pointwise loss solved by SGD. We implemented SGD in Julia.",
            "cite_spans": [
                {
                    "start": 54,
                    "end": 58,
                    "text": "[59]",
                    "ref_id": "BIBREF58"
                }
            ],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "\u2022 Primal-CR++: the recently proposed pairwise algorithm in [115] . We use the Julia implementation released by the authors 7 .",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 64,
                    "text": "[115]",
                    "ref_id": "BIBREF114"
                }
            ],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "Experiments are conducted on Movielens1m and Yahoo-music datasets. We perform the same procedure as in implicit feedback setting except that we do not need to mask the ratings into \"0/1\".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "We measure the performance in the following two ways:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "\u2022 NDCG@k: defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "where i represents i-th user and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "In the DCG definition, \u03a0 il represents the index of the l-th ranked item for user i in test data based on the learned score matrix X. R is the rating matrix and R ij is the rating given to item j by user i. \u03a0 * i is the ordering provided by the ground truth rating.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "\u2022 Precision@k: defined as a fraction of relevant items among the top k recommended items: here we consider items with ratings assigned as 4 or 5 as relevant. R ij follows the same definitions above but unlike before \u03a0 il gives the index of the l-th ranked item for user i among all the items that are not rated by user i in the training set (including both rated test items and unobserved items).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Explicit Feedback"
        },
        {
            "text": "As shown in Table 4 ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 19,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                }
            ],
            "section": "Explicit Feedback"
        },
        {
            "text": "To illustrate the training speed of our algorithm, we plot precision@1 versus training time for the Movielen1m dataset and the Foursquare dataset. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training speed"
        },
        {
            "text": "One important innovation in our SQL-Rank algorithm is the Stochastic Queuing (SQ)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Effectiveness of Stochastic Queuing (SQ)"
        },
        {
            "text": "Process for handling ties. To illustrate the effectiveness of the SQ process, we compare As shown Table 4 .3 and Figure 8 .3 (in the appendix), the performance gain from SQ in terms of precision is substantial (more than 10%) on Movielen1m dataset. It verifies the claim that our way of handling ties and missing data is very effective and improves the ranking results by a lot. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 98,
                    "end": 105,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                },
                {
                    "start": 113,
                    "end": 121,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "Effectiveness of Stochastic Queuing (SQ)"
        },
        {
            "text": "Another benefit of our algorithm is that we are able to minimize top k probability with much larger k and without much overhead. Previous approaches [50] already pointed out increasing k leads to better ranking results, but their complexity is exponential to k so they were not able to have k > 1. To show the effectiveness of using permutation probability for full lists rather than using the top k probability for top k partial lists in the likelihood loss, we fix everything else to be the same and only vary k in Equation (4.5).",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 153,
                    "text": "[50]",
                    "ref_id": "BIBREF49"
                }
            ],
            "ref_spans": [],
            "section": "Effectiveness of using the Full List"
        },
        {
            "text": "We obtain the results in Table 4 .4 and Figure 8 .4 (in the appendix). It shows that the larger k we use, the better the results we can get. Therefore, in the final model, we set k to be the maximum number (length of the observed list.)",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 32,
                    "text": "Table 4",
                    "ref_id": "TABREF22"
                },
                {
                    "start": 40,
                    "end": 48,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "Effectiveness of using the Full List"
        },
        {
            "text": "In this chapter, we propose a listwise approach for collaborative ranking and provide an efficient algorithm to solve it. Our methodology can incorporate both implicit and explicit feedback, and can gracefully handle ties and missing data. In experiments, we demonstrate our algorithm outperforms existing state-of-the art methods in terms of top k recommendation precision. We also provide a theoretical framework for analyzing listwise methods highlighting the dependence on the number of users and items.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Chapter 5"
        },
        {
            "text": "Recently, embedding representations have been widely used in almost all AI-related fields, from feature maps [63] in computer vision, to word embeddings [71, 83] in natural language processing, to user/item embeddings [49, 73] in recommender systems. Usually, the embeddings are high-dimensional vectors. Take language models for example, in GPT [87] and Bert-Base model [25] , 768-dimensional vectors are used to represent words.",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "[63]",
                    "ref_id": "BIBREF62"
                },
                {
                    "start": 153,
                    "end": 157,
                    "text": "[71,",
                    "ref_id": "BIBREF70"
                },
                {
                    "start": 158,
                    "end": 161,
                    "text": "83]",
                    "ref_id": "BIBREF82"
                },
                {
                    "start": 218,
                    "end": 222,
                    "text": "[49,",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 223,
                    "end": 226,
                    "text": "73]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 346,
                    "end": 350,
                    "text": "[87]",
                    "ref_id": "BIBREF86"
                },
                {
                    "start": 371,
                    "end": 375,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "higher dimensions in their unreleased large models. In recommender systems, things are slightly different: the dimension of user/item embeddings are usually set to be reasonably small, 50 or 100. But the number of users and items is on a much bigger scale. Contrast this with the fact that the size of word vocabulary that normally ranges from 50,000 to 150,000, the number of users and items can be millions or even billions in large-scale real-world commercial recommender systems [6] .",
            "cite_spans": [
                {
                    "start": 483,
                    "end": 486,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Bert-Large model utilizes 1024-dimensional vectors and GPT-2 [88] may have used even"
        },
        {
            "text": "Given the massive number of parameters in modern neural networks with embedding layers, mitigating over-parameterization can play a big role in preventing over-fitting in deep learning. We propose a regularization method, Stochastic Shared Embeddings (SSE), that uses prior information about similarities between embeddings, such as semantically and grammatically related words in natural languages or real-world users who share social relationships. Critically, SSE progresses by stochastically transitioning between embeddings as opposed to a more brute-force regularization such as graph-based Laplacian regularization and ridge regularization. Thus, SSE integrates seamlessly with existing stochastic optimization methods and the resulting regularization is data-driven.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bert-Large model utilizes 1024-dimensional vectors and GPT-2 [88] may have used even"
        },
        {
            "text": "We will begin the paper with the mathematical formulation of the problem, propose SSE, and provide the motivations behind SSE. We provide a theoretical analysis of SSE that can be compared with excess risk bounds based on empirical Rademacher complexity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bert-Large model utilizes 1024-dimensional vectors and GPT-2 [88] may have used even"
        },
        {
            "text": "We then conducted experiments for a total of 6 tasks from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages and find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed methods can further reduce over-fitting, which often leads to more favorable generalization results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bert-Large model utilizes 1024-dimensional vectors and GPT-2 [88] may have used even"
        },
        {
            "text": "Regularization techniques are used to control model complexity and avoid over-fitting. 2 regularization [46] is the most widely used approach and has been used in many matrix factorization models in recommender systems; 1 regularization [109] is used when a sparse model is preferred. For deep neural networks, it has been shown that p regularizations are often too weak, while dropout [45, 106] is more effective in practice. There are many other regularization techniques, including parameter sharing [30] , max-norm regularization [104] , gradient clipping [82] , etc.",
            "cite_spans": [
                {
                    "start": 104,
                    "end": 108,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 237,
                    "end": 242,
                    "text": "[109]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 386,
                    "end": 390,
                    "text": "[45,",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 391,
                    "end": 395,
                    "text": "106]",
                    "ref_id": "BIBREF105"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 534,
                    "end": 539,
                    "text": "[104]",
                    "ref_id": "BIBREF103"
                },
                {
                    "start": 560,
                    "end": 564,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Our proposed SSE-graph is very different from graph Laplacian regularization [13] , in which the distances of any two embeddings connected over the graph are directly penalized.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 81,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Hard parameter sharing uses one embedding to replace all distinct embeddings in the same group, which inevitably introduces a significant bias. Soft parameter sharing [75] is similar to the graph Laplacian, penalizing the l 2 distances between any two embeddings.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 171,
                    "text": "[75]",
                    "ref_id": "BIBREF74"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "These methods have no dependence on the loss, while the proposed SSE-graph method is for each embedding E l [j i l ] \u2208 S i do 8:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Forward and backward pass with the new embeddings 10: Return embeddings {E 1 , . . . , E M }, and neural network parameters \u0398 data-driven in that the loss influences the effect of regularization. Unlike graph Laplacian regularization, hard and soft parameter sharing, our method is stochastic by nature. This allows our model to enjoy similar advantages as dropout [106] .",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 370,
                    "text": "[106]",
                    "ref_id": "BIBREF105"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Interestingly, in the original BERT model's pre-training stage [25] , a variant of SSE-SE is already implicitly used for token embeddings but for a different reason. In [25] , the authors masked 15% of words and 10% of the time replaced the [mask] token with a random token. In the next section, we discuss how SSE-SE differs from this heuristic.",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 67,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 169,
                    "end": 173,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Another closely related technique to ours is the label smoothing [107] , which is widely used in the computer vision community. We find that in the classification setting if we apply SSE-SE to one-hot encodings associated with output y i only, our SSE-SE is closely related to the label smoothing, which can be treated as a special case of our proposed method.",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 70,
                    "text": "[107]",
                    "ref_id": "BIBREF106"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Throughout this chapter, the network input x i and label y i will be encoded into indices The loss function can be written as the functions of embeddings:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Shared Embeddings"
        },
        {
            "text": "where y i is the label and \u0398 encompasses all trainable parameters including the embeddings,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stochastic Shared Embeddings"
        },
        {
            "text": "The loss function is a mapping from embedding spaces to the reals. For text input, each E l [j i l ] is a word embedding vector in the input sentence or document. For recommender systems, usually there are two embedding look-up tables: one for users and one for items [41] . So the objective function, such as mean squared loss or some ranking losses, will comprise both user and item embeddings for each input. We can more succinctly Suppose that we have access to knowledge graphs [66, 72] over embeddings, and we have a prior belief that two embeddings will share information and replacing one with the other should not incur a significant change in the loss distribution. For example, if two movies are both comedies and they are starred by the same actors, it is very likely that for the same user, replacing one comedy movie with the other comedy movie will result in little change in the loss distribution. In stochastic optimization, we can replace the loss gradient for one movie's embedding with the other similar movie's embedding, and this will not significantly bias the gradient if the prior belief is accurate. On the other hand, if this exchange is stochastic, then it will act to smooth the gradient steps in the long run, thus regularizing the gradient updates. ",
            "cite_spans": [
                {
                    "start": 268,
                    "end": 272,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 483,
                    "end": 487,
                    "text": "[66,",
                    "ref_id": "BIBREF65"
                },
                {
                    "start": 488,
                    "end": 491,
                    "text": "72]",
                    "ref_id": "BIBREF71"
                }
            ],
            "ref_spans": [],
            "section": "Stochastic Shared Embeddings"
        },
        {
            "text": "Instead of optimizing objective function R n (\u0398) in (5.1), SSE-Graph described in Algorithm 9, Figure 5 and suppress their indices.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 95,
                    "end": 103,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "General SSE with Knowledge Graphs: SSE-Graph"
        },
        {
            "text": "In the single embedding table case, M = 1, there are many ways to define transition probability from j to k. One simple and effective way is to use a random walk (with random restart and self-loops) on a knowledge graph G, i.e. when embedding j is connected with k but not with l, we can set the ratio of p(j, k|\u03a6) and p(j, l|\u03a6) to be a constant greater than 1. In more formal notation, we have j \u223c k, j \u223c l \u2212\u2192 p(j, k|\u03a6)/p(j, l|\u03a6) = \u03c1,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "General SSE with Knowledge Graphs: SSE-Graph"
        },
        {
            "text": "where \u03c1 > 1 and is a tuning parameter. It is motivated by the fact that embeddings connected with each other in knowledge graphs should bear more resemblance and thus be more likely replaced by each other. Also, we let p(j, j|\u03a6) = 1 \u2212 p 0 , where p 0 is called the SSE probability and embedding retainment probability is 1 \u2212 p 0 . We treat both p 0 and \u03c1 as tuning hyper-parameters in experiments. With (5.3) and k p(j, k|\u03a6) = 1, we can derive transition probabilities between any two embeddings to fill out the transition probability table.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "General SSE with Knowledge Graphs: SSE-Graph"
        },
        {
            "text": "When there are multiple embedding tables, M > 1, then we will force that the transition from j to k can be thought of as independent transitions from j l to k l within embedding table l (and index set I l ). Each table may have its own knowledge graph, resulting in its own transition probabilities p l (., .). The more general form of the SSE-graph objective is given below: This is equivalent to have a randomized embedding look-up layer as shown in Figure 5 .1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 452,
                    "end": 460,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "General SSE with Knowledge Graphs: SSE-Graph"
        },
        {
            "text": "We can also accommodate sequences of embeddings, which commonly occur in natural language application, by considering (j i l,1 , k l,1 ), . . . , (j i l,n i l , k l,n i l ) instead of (j i l , k l ) for l-th embedding table in (5.4), where 1 \u2264 l \u2264 M and n i l is the number of embeddings in table l that are associated with (x i , y i ). When there is more than one embedding look-up table, we sometimes prefer to use different p 0 and \u03c1 for different look-up tables in (5.3) and the SSE probability constraint. For example, in recommender systems, we would use p u , \u03c1 u for user embedding table and p i , \u03c1 i for item embedding table.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "General SSE with Knowledge Graphs: SSE-Graph"
        },
        {
            "text": "We find that SSE with knowledge graphs, i.e., SSE-Graph, can force similar embeddings to cluster when compared to the original neural network without SSE-Graph. In Figure 5 .3, one can easily see that more embeddings tend to cluster into 2 black holes after applying SSE-Graph when embeddings are projected into 3D spaces using PCA. Interestingly, a similar phenomenon occurs when assuming the knowledge graph is a complete graph, which we would introduce as SSE-SE below.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 164,
                    "end": 172,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "General SSE with Knowledge Graphs: SSE-Graph"
        },
        {
            "text": "One clear limitation of applying the SSE-Graph is that not every dataset comes with good-quality knowledge graphs on embeddings. For those cases, we could assume there is a complete graph over all embeddings so there is a small transition probability between every pair of different embeddings: where N is the size of the embedding table. The SGD procedure in Algorithm 9 can still be applied and we call this algorithm SSE-SE (Stochastic Shared Embeddings -Simple and Easy). It is worth noting that SSE-Graph and SSE-SE are applied to embeddings associated with not only input x i but also those with output y i . Unless there are considerably many more embeddings than data points and model is significantly overfitting, normally p 0 = 0.01",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simplified SSE with Complete Graph: SSE-SE"
        },
        {
            "text": "gives reasonably good results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simplified SSE with Complete Graph: SSE-SE"
        },
        {
            "text": "Interestingly, we found that the SSE-SE framework is related to several techniques used in practice. For example, BERT pre-training unintentionally applied a method similar to SSE-SE to input x i by replacing the masked word with a random word. This would implicitly introduce an SSE layer for input x i in Figure 5 .1, because now embeddings associated with input x i be stochastically mapped according to (5.5) . The main difference between this and SSE-SE is that it merely augments the input once, while SSE introduces randomization at every iteration, and we can also accommodate label embeddings. In experimental Section 5.4.4, we will show that SSE-SE would improve original BERT pre-training procedure as well as fine-tuning procedure.",
            "cite_spans": [
                {
                    "start": 407,
                    "end": 412,
                    "text": "(5.5)",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 307,
                    "end": 315,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Simplified SSE with Complete Graph: SSE-SE"
        },
        {
            "text": "We explain why SSE can reduce the variance of estimators and thus leads to better generalization performance. For simplicity, we consider the SSE-graph objective (5.2) where there is no transition associated with the label y i , and only the embeddings associated with the input x i undergo a transition. When this is the case, we can think of the loss as a function of the x i embedding and the label, (E[j i ], y i ; \u0398). We take this approach because it is more straightforward to compare our resulting theory to existing excess risk bounds.",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 167,
                    "text": "(5.2)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Theoretical Guarantees"
        },
        {
            "text": "The SSE objective in the case of only input transitions can be written as,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Guarantees"
        },
        {
            "text": "and there may be some constraint on \u0398. Let\u0398 denote the minimizer of S n subject to this constraint. We will show in the subsequent theory that minimizing S n will get us close to a minimizer of S(\u0398) = ES n (\u0398), and that under some conditions this will get us close to the Bayes risk. We will use the standard definitions of empirical and true risk,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Guarantees"
        },
        {
            "text": "Our results depend on the following decomposition of the risk. By optimality of\u0398,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Guarantees"
        },
        {
            "text": "where The high level idea behind the following results is that when the SSE protocol reflects the underlying distribution of the data, then the bias term B(\u0398) is small, and if the SSE transitions are well mixing then the SSE excess risk E(\u0398) will be of smaller order than the standard Rademacher complexity. This will result in a small excess risk.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Guarantees"
        },
        {
            "text": "Theorem 1. Consider SSE-graph with only input transitions. Transition matrices are contractive and will induce dependencies between the Rademacher random variables, thereby stochastically reducing the supremum. In the case of no label noise, namely that Y |X is a point mass, e(x, y; \u0398) = 0, and \u03c1 e,n = 0. The use of L as opposed to the losses, , will also make \u03c1 L,n of smaller order than the standard empirical Rademacher complexity. We demonstrate this with a partial simulation of \u03c1 L,n on the Movielens1m dataset in Figure 8 .5 of the Appendix.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 522,
                    "end": 530,
                    "text": "Figure 8",
                    "ref_id": null
                }
            ],
            "section": "Theoretical Guarantees"
        },
        {
            "text": "Suppose that 0 \u2264 (., .; \u0398) \u2264 b for some b > 0, then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 2. Let the SSE-bias be defined as"
        },
        {
            "text": "Remark 2. The price for 'smoothing' the Rademacher complexity in Theorem 1 is that SSE may introduce a bias. This will be particularly prominent when the SSE transitions have little to do with the underlying distribution of Y, X. On the other extreme, suppose that p(j, k) is non-zero over a neighborhood N j of j, and that for data x , y with encoding k \u2208 N j , x , y is identically distributed with x i , y i , then B = 0. In all likelihood, the SSE transition probabilities will not be supported over neighborhoods of iid random pairs, but with a well chosen SSE protocol the neighborhoods contain approximately iid pairs and B is small. We report the metric precision for top k recommendations as P @k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 2. Let the SSE-bias be defined as"
        },
        {
            "text": "Model P @1 P @5 P @10 P @1 P @5 P @10 P @1 P @5 P ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Movielens1m Yahoo Music Foursquare"
        },
        {
            "text": "We have conducted extensive experiments on 6 tasks, including 3 recommendation tasks (explicit feedback, implicit feedback and sequential recommendation) and 3 NLP tasks (neural machine translation, BERT pre-training, and BERT fine-tuning for sentiment classification) and found that our proposed SSE can effectively improve generalization performances on a wide variety of tasks. Note that the details about datasets and parameter settings can be found in the appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Matrix Factorization Algorithm (MF) [73] and Bayesian Personalized Ranking Algorithm (BPR) [91] can be viewed as neural networks with one hidden layer (latent features) and",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[73]",
                    "ref_id": "BIBREF72"
                },
                {
                    "start": 91,
                    "end": 95,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                }
            ],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer (Matrix Factorization and BPR)"
        },
        {
            "text": "are quite popular in recommendation tasks. MF uses the squared loss designed for explicit feedback data while BPR uses the pairwise ranking loss designed for implicit feedback data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer (Matrix Factorization and BPR)"
        },
        {
            "text": "First, we conduct experiments on two explicit feedback datasets: Movielens1m and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer (Matrix Factorization and BPR)"
        },
        {
            "text": "Movielens10m. For these datasets, we can construct graphs based on actors/actresses starring the movies. We compare SSE-graph and the popular Graph Laplacian Regularization (GLR) method [89] in Table 5 .1. The results show that SSE-graph consistently outperforms GLR. This indicates that our SSE-Graph has greater potentials over graph Laplacian regularization as we do not explicitly penalize the distances across embeddings, but rather we implicitly penalize the effects of similar embeddings on the loss. Furthermore, we show that even without existing knowledge graphs of embeddings, our SSE-SE performs only slightly worse than SSE-Graph but still much better than GLR and MF.",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 190,
                    "text": "[89]",
                    "ref_id": "BIBREF88"
                }
            ],
            "ref_spans": [
                {
                    "start": 194,
                    "end": 201,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "Neural Networks with One Hidden Layer (Matrix Factorization and BPR)"
        },
        {
            "text": "In general, SSE-SE is a good alternative when graph information is not available. We then show that our proposed SSE-SE can be used together with standard regularization techniques such as dropout and weight decay to improve recommendation results regardless of the loss functions and dimensionality of embeddings. This is evident in Table 5.2 and   Table 5 .3. With the help of SSE-SE, BPR can perform better than the state-of-art listwise approach SQL-Rank [116] in most cases. We include the optimal SSE parameters in the Table 5 .4. SSE-SE has two tuning parameters: probability p x to replace embeddings associated with input x i and probability p y to replace embeddings associated with output y i . We use the dropout probability of 0.1, weight decay of 1e \u22125 , and learning rate of 1e \u22123 for all experiments. ",
            "cite_spans": [
                {
                    "start": 459,
                    "end": 464,
                    "text": "[116]",
                    "ref_id": "BIBREF115"
                }
            ],
            "ref_spans": [
                {
                    "start": 334,
                    "end": 357,
                    "text": "Table 5.2 and   Table 5",
                    "ref_id": "TABREF12"
                },
                {
                    "start": 525,
                    "end": 532,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "Neural Networks with One Hidden Layer (Matrix Factorization and BPR)"
        },
        {
            "text": "SASRec [56] is the state-of-the-arts algorithm for sequential recommendation task. It applies the transformer model [110] , where a sequence of items purchased by a user can be viewed as a sentence in transformer, and next item prediction is equivalent to next word prediction in the language model. In Table 5 .4, we perform SSE-SE on input embeddings (p x = 0.1, p y = 0), output embeddings (p x = 0.1, p y = 0) and both embeddings (p x = p y = 0.1), and observe that all of them significantly improve over state-of-the-art SASRec (p x = p y = 0). The regularization effects of SSE-SE is even more obvious when we increase the number of self-attention blocks from 2 to 6, as this will lead to a more sophisticated model with many more parameters. This leads to the model overfitting terribly even with dropout and weight decay. We can see in Table 5 .4 that when both methods use dropout and weight decay, SSE-SE + SASRec is doing much better than SASRec without SSE-SE. Table 5 .5. Our proposed SSE-SE helps the Transformer achieve better BLEU scores on English-to-German in 10 out of 11 newstest data between 2008 and 2018. ",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 11,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 116,
                    "end": 121,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                }
            ],
            "ref_spans": [
                {
                    "start": 303,
                    "end": 310,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                },
                {
                    "start": 844,
                    "end": 851,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                },
                {
                    "start": 973,
                    "end": 980,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "Transformer Encoder Model for Sequential Recommendation"
        },
        {
            "text": "We use the transformer model [110] as the backbone for our experiments. The baseline model is the standard 6-layer transformer architecture and we apply SSE-SE to both encoder, and decoder by replacing corresponding vocabularies' embeddings in the source and target sentences. We trained on the standard WMT 2014 English to German dataset which consists of roughly 4.5 million parallel sentence pairs and tested on WMT 2008",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 34,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                }
            ],
            "ref_spans": [],
            "section": "Neural Machine Translation"
        },
        {
            "text": "to 2018 news-test sets. We use the OpenNMT implementation in our experiments. We use the same dropout rate of 0.1 and label smoothing value of 0.1 for the baseline model and our SSE-enhanced model. The only difference between the two models is whether or not we use our proposed SSE-SE with p 0 = 0.01 in (5.5) for both encoder and decoder embedding layers. We evaluate both models' performances on the test datasets using BLEU scores [85] .",
            "cite_spans": [
                {
                    "start": 435,
                    "end": 439,
                    "text": "[85]",
                    "ref_id": "BIBREF84"
                }
            ],
            "ref_spans": [],
            "section": "Neural Machine Translation"
        },
        {
            "text": "We summarize our results in Table 5 ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 28,
                    "end": 35,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "Neural Machine Translation"
        },
        {
            "text": "BERT's model architecture [25] is a multi-layer bidirectional Transformer encoder based on the Transformer model in neural machine translation. Despite SSE-SE can be used for both pre-training and fine-tuning stages of BERT, we want to mainly focus on pre-training as fine-tuning bears more similarity to the previous section. We use SSE probability of 0.015 for embeddings (one-hot encodings) associated with labels and SSE probability of We continue to pre-train Google pre-trained BERT model on our crawled IMDB movie reviews with and without SSE-SE and compare downstream tasks performances.",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 30,
                    "text": "[25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "BERT for Sentiment Classification"
        },
        {
            "text": "In Table 5 .6, we find that SSE-SE pre-trained BERT base model helps us achieve the state-of-the-art results for the IMDB sentiment classification task, which is better than the previous best in [47] . We report test set accuracy of 0.9542 after fine-tuning for one epoch only. For the similar SST-2 sentiment classification task in Table 5 ",
            "cite_spans": [
                {
                    "start": 195,
                    "end": 199,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                },
                {
                    "start": 333,
                    "end": 340,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "BERT for Sentiment Classification"
        },
        {
            "text": "In Figure 5 .4, it is clear to see that our one-hidden-layer neural networks with SSE-SE are achieving much better generalization results than their respective standalone versions.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Speed and convergence comparisons."
        },
        {
            "text": "One can also easily spot that SSE-version algorithms converge at much faster speeds with the same learning rate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Speed and convergence comparisons."
        },
        {
            "text": "We have proposed Stochastic Shared Embeddings, which is a data-driven approach to regularization, that stands in contrast to brute force regularization such as Laplacian and ridge regularization. Our theory is a first step towards explaining the regularization effect of SSE, particularly, by 'smoothing' the Rademacher complexity. The extensive experimentation demonstrates that SSE can be fruitfully integrated into existing deep learning applications.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Chapter 6 SSE-PT: Sequential Recommendation Via Personalized Transformer",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The sequential recommendation problem has been an important open research question, yet using temporal information to improve recommendation performance has proven to be challenging. SASRec, proposed by [56] for sequential recommendation problems, has achieved state-of-the-art results and enjoyed more than 10x speed-up when compared to earlier CNN/RNN-based methods. However, the model used in SASRec is the standard Transformer which is inherently an un-personalized model. In practice, it is important to include a personalized Transformer in SASRec especially for recommender systems, but [56] found that adding additional personalized embeddings did not improve the performance of their Transformer model, and postulate that the failure of adding personalization is due to the fact that they already use the user history and the user embeddings only contribute to overfitting. In this work, we propose a novel method, Personalized Transformer (SSE-PT), that successfully introduces personalization into self-attentive neural network architectures.",
            "cite_spans": [
                {
                    "start": 203,
                    "end": 207,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 594,
                    "end": 598,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Introducing user embeddings into the standard transformer model is intrinsically difficult with existing regularization techniques, as unavoidably a large number of user parameters are introduced, which is often at the same scale of the number of training data. But we show that personalization can greatly improve ranking performance with a recent regularization technique called Stochastic Shared Embeddings (SSE) [117] . The personalized Transformer (SSE-PT) model with SSE regularization works well for all 5 real-world datasets we consider without overfitting, outperforming previous state-of-the-art algorithm SASRec by almost 5% in terms of NDCG@10. Furthermore, after examining some random users' engagement history, we find our model is not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements.",
            "cite_spans": [
                {
                    "start": 416,
                    "end": 421,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Both session-based and sequential (i.e., next-basket) recommendation algorithms take advantage of additional temporal information to make better personalized recommendations. The main difference between session-based recommendations [43] and sequential recommendations [56] is that the former assumes that the user ids are not recorded and therefore the length of engagement sequences are relatively short. Therefore, session-based recommendations normally do not consider user factors. On the other hand, sequential recommendation treats each sequence as a user's engagement history [56] . Both settings, do not explicitly require time-stamps: only the relative temporal orderings are assumed known (in contrast to, for example, timeSVD++ [60] using time-stamps). Initially, sequence data in temporal order are usually modelled with Markov models, in which a future observation is conditioned on the last few observed items [92] . In [92] , a personalized Markov model with user latent factors is proposed for more personalized results.",
            "cite_spans": [
                {
                    "start": 233,
                    "end": 237,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 269,
                    "end": 273,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 584,
                    "end": 588,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 740,
                    "end": 744,
                    "text": "[60]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 925,
                    "end": 929,
                    "text": "[92]",
                    "ref_id": "BIBREF91"
                },
                {
                    "start": 935,
                    "end": 939,
                    "text": "[92]",
                    "ref_id": "BIBREF91"
                }
            ],
            "ref_spans": [],
            "section": "Session-based and Sequential Recommendation"
        },
        {
            "text": "In recent years, deep learning techniques, borrowed from natural language processing (NLP) literature, are getting widely used in tackling sequential data. Like word sentences in NLP, item sequences in recommendations can be similarly modelled by recurrent neural networks (RNN) [42, 43] and convolutional neural network (CNN) [108] models.",
            "cite_spans": [
                {
                    "start": 279,
                    "end": 283,
                    "text": "[42,",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 284,
                    "end": 287,
                    "text": "43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 327,
                    "end": 332,
                    "text": "[108]",
                    "ref_id": "BIBREF107"
                }
            ],
            "ref_spans": [],
            "section": "Session-based and Sequential Recommendation"
        },
        {
            "text": "Recently, attention models are increasingly used in both NLP [25, 110] and recommender systems [56, 68] . SASRec [56] is a recent method with state-of-the-art performance among the many deep learning models. Motivated by the Transformer model in neural machine translation [110] , SASRec utilizes a similar architecture to the encoder part of the Transformer model. Our proposed model, SSE-PT, is a personalized extension of the transformer model.",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 65,
                    "text": "[25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 66,
                    "end": 70,
                    "text": "110]",
                    "ref_id": "BIBREF109"
                },
                {
                    "start": 95,
                    "end": 99,
                    "text": "[56,",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 100,
                    "end": 103,
                    "text": "68]",
                    "ref_id": "BIBREF67"
                },
                {
                    "start": 113,
                    "end": 117,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 273,
                    "end": 278,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                }
            ],
            "ref_spans": [],
            "section": "Session-based and Sequential Recommendation"
        },
        {
            "text": "In deep learning, models with many more parameters than data points can easily overfit to the training data. This may prevent us from adding user embeddings as additional parameters into complicated models like the Transformer model [56] , which can easily have 20 layers with millions of parameters for a medium-sized dataset like Movielens10M [38] .",
            "cite_spans": [
                {
                    "start": 233,
                    "end": 237,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 345,
                    "end": 349,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Regularization Techniques"
        },
        {
            "text": "2 regularization [46] is the most widely used approach and has been used in many matrix factorization models in recommender systems; 1 regularization [109] is used when a sparse model is preferred. For deep neural networks, it has been shown that p regularizations are often too weak, while dropout [45, 106] is more effective in practice. There are many other regularization techniques, including parameter sharing [30] , max-norm regularization [104] , gradient clipping [82] , etc. Very recently, a new regularization technique called Stochastic Shared Embeddings (SSE) [117] is proposed as a new means of regularizing embedding layers. We find that the base version SSE-SE is essential to the success of our Personalized Transformer (SSE-PT) model.",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 21,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 150,
                    "end": 155,
                    "text": "[109]",
                    "ref_id": "BIBREF108"
                },
                {
                    "start": 299,
                    "end": 303,
                    "text": "[45,",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "106]",
                    "ref_id": "BIBREF105"
                },
                {
                    "start": 416,
                    "end": 420,
                    "text": "[30]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 447,
                    "end": 452,
                    "text": "[104]",
                    "ref_id": "BIBREF103"
                },
                {
                    "start": 473,
                    "end": 477,
                    "text": "[82]",
                    "ref_id": "BIBREF81"
                },
                {
                    "start": 573,
                    "end": 578,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Regularization Techniques"
        },
        {
            "text": "Given n users and each user engaging with a subset of m items in a temporal order, the goal of sequential recommendation is to learn a good personalized ranking of top K items out of total m items for any given user at any given time point. We assume data in the format of n item sequences:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sequential Recommendation"
        },
        {
            "text": "Sequences s i of length T contain indices of the last T items that user i has interacted with in the temporal order (from old to new). For different users, the sequence lengths can vary, but we can pad the shorter sequences so all of them have length T . We cannot simply randomly split data points into train/validation/test sets because they come in temporal orders. Instead, we need to make sure our training data is before validation data which is before test data temporally. We use last items in sequences as test sets, second-to-last items as validation sets and the rest as training sets. We use ranking metrics such as NDCG@K and Recall@K for evaluations, which are defined in the Appendix.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Sequential Recommendation"
        },
        {
            "text": "Our model, which we call SSE-PT, is motivated by the Transformer model in [110] and [56] .",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 79,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                },
                {
                    "start": 84,
                    "end": 88,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "It also utilizes a new regularization technique called stochastic shared embeddings [117] .",
            "cite_spans": [
                {
                    "start": 84,
                    "end": 89,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "In the following sections, we are going to examine each important component of our where d = d u + d i . So each input sequence s i \u2208 R T will be represented by the following embedding:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "where [v j it ; u i ] represents concatenating item embedding v j it \u2208 R d i and user embedding u i \u2208 R du into embedding E t \u2208 R d for time t. Note that the main difference between our model and [56] is that we introduce the user embeddings u i , making our model personalized.",
            "cite_spans": [
                {
                    "start": 196,
                    "end": 200,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "Transformer Encoder On top of the embedding layer, we have B blocks of selfattention layers and fully connected layers, where each layer extracts features for each time step based on the previous layer's outputs. Since this part is identical to the Transformer [56, 110] , we will skip the details.",
            "cite_spans": [
                {
                    "start": 261,
                    "end": 265,
                    "text": "[56,",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 266,
                    "end": 270,
                    "text": "110]",
                    "ref_id": "BIBREF109"
                }
            ],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "Prediction Layer At time t, the predicted probability of user i engaged item l is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "where \u03c3 is the sigmoid function and r itl is the predicted score of item l by user l at time point t, defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "where F B t\u22121 is the output hidden units associated with the transformer encoder at the last timestamp. Although we can use another set of user and item embedding look-up tables for the u i and v l , we find it better to use the same set of embedding look-up tables U, V as in the embedding layer. But regularization for those embeddings can be different. To distinguish the u i and v l in (6.4) from u i , v j in (6.2), we call embeddings in (6.4) output embeddings and those in (6.2) input embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "The binary cross entropy loss between predicted probability for the positive item l = j i(t+1) and one uniformly sampled negative item k \u2208 \u2126 is given as \u2212[log(p itl ) + log(1 \u2212 p itk )].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "Summing over s i and t, we obtain the objective function that we want to minimize is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "At the inference time, top-K recommendations for user i at time t can be made by sorting scores r itl for all items and recommending the first K items in the sorted list.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Personalized Transformer Architecture"
        },
        {
            "text": "The most important regularization technique to SSE-PT model is the Stochastic Shared Embeddings (SSE) [117] .",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 107,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Novel Application of Stochastic Shared Embeddings"
        },
        {
            "text": "The main idea of SSE is to stochastically replace embeddings with another embedding with some pre-defined probability during SGD, which has the effect of regularizing the embedding layers. Without SSE, all the existing well-known regularization techniques like layer normalization, dropout and weight decay fail and cannot prevent the model from over-fitting badly after introducing user embeddings. [117] develops two versions of SSE, SSE-Graph and SSE-SE. In the simplest uniform case, SSE-SE replaces one embedding with another embedding uniformly with probability p, which is called SSE probability in [117] . Since we don't have knowledge graphs for user or items, we simply apply the SSE-SE to our SSE-PT model. We find SSE-SE makes possible training this personalized model with O(nd u ) additional parameters.",
            "cite_spans": [
                {
                    "start": 400,
                    "end": 405,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                },
                {
                    "start": 606,
                    "end": 611,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Novel Application of Stochastic Shared Embeddings"
        },
        {
            "text": "There are 3 different places in our model that SSE-SE can be applied. We can apply SSE-SE to input/output user embeddings, input item embeddings, and output item embeddings with probabilities p u , p i and p y respectively. Note that input user embedding and output user embedding are always replaced at the same time with SSE probability p u .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Novel Application of Stochastic Shared Embeddings"
        },
        {
            "text": "Empirically, we find that SSE-SE to user embeddings and output item embeddings always helps, but SSE-SE to input item embeddings is only useful when the average sequence length is large, e.g., more than 100 in Movielens1M and Movielens10M datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Novel Application of Stochastic Shared Embeddings"
        },
        {
            "text": "Other Regularization Techniques Besides the SSE [117] , we also utilized other widely used regularization techniques, including layer normalization [4] , batch normalization [51] , residual connections [39] , weight decay [64] , and dropout [106] . Since they are used in the same way in the previous paper [56] , we omit the details to the Appendix. ",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 53,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                },
                {
                    "start": 148,
                    "end": 151,
                    "text": "[4]",
                    "ref_id": null
                },
                {
                    "start": 174,
                    "end": 178,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 202,
                    "end": 206,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 222,
                    "end": 226,
                    "text": "[64]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 241,
                    "end": 246,
                    "text": "[106]",
                    "ref_id": "BIBREF105"
                },
                {
                    "start": 307,
                    "end": 311,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "Novel Application of Stochastic Shared Embeddings"
        },
        {
            "text": "In this section, we compare our proposed algorithms, Personalized Transformer (SSE-PT) and SSE-PT++, with other state-of-the-art algorithms on real-world datasets. We implement our codes in Tensorflow and conduct all our experiments on a server with 40-core Intel Xeon E5-2630 v4 @ 2.20GHz CPU, 256G RAM and Nvidia GTX 1080 GPUs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Datasets We use 5 datasets. The first 4 have exactly the same train/dev/test splits as in [56] . The datasets are: Beauty and Games categories from Amazon product review datasets 1 ; Steam dataset introduced in [56] , which contains reviews crawled from a large video game distribution platform; Movielens1M dataset [38] , a widely used benchmark datasets containing one million user movie ratings; Movielens10M dataset with ten million user ratings cleaned by us. Detailed dataset statistics are given in Table 8 .7. One can easily see that the first 3 datasets have short sequences (average length \u00a1 12) while the last 2 datasets have very long sequences (\u00bf 10x longer).",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 94,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 211,
                    "end": 215,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 316,
                    "end": 320,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [
                {
                    "start": 506,
                    "end": 513,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "Evaluation Metrics The evaluation metrics we use are standard ranking metrics, namely NDCG and Recall for top recommendations (See Appendix). We follow the same evaluation setting as the previous paper [56] : predicting ratings at time point t + 1 given the previous t ratings. For a large dataset with numerous users and items, the evaluation procedure would be slow because (8.14) would require computing the ranking of all items based on their predicted scores for every single user. As a means of speed-up evaluations, we sample a fixed number C (e.g., 100) of negative candidates while always keeping the positive item that we know the user will engage next. This way, both R ij and \u03a0 i will be narrowed down to a small set of item candidates, and prediction scores will only be computed for those items through a single forward pass of the neural network.",
            "cite_spans": [
                {
                    "start": 202,
                    "end": 206,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Ideally, we want both NDCG and Recall to be as close to 1 as possible, because NDCG@K = 1 means the positive item is always put on the top-1 position of the top-K ranking list, and Recall@K = 1 means the positive item is always contained by the top-K recommendations the model makes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "Baselines We include 5 non-deep-learning and 6 deep-learning algorithms in our comparisons.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "The simplest baseline is PopRec, basically ranking items according to their popularity. More advanced methods such as matrix factorization based baselines include Bayesian personalized ranking for implicit feedback [91] , namely BPR;",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 219,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                }
            ],
            "ref_spans": [],
            "section": "Non-deep-learning Baselines"
        },
        {
            "text": "Factorized Markov Chains and Personalized Factorized Markov Chains models [92] also known as FMC and PFMC; and translation based method [40] called TransRec.",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 78,
                    "text": "[92]",
                    "ref_id": "BIBREF91"
                },
                {
                    "start": 136,
                    "end": 140,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Non-deep-learning Baselines"
        },
        {
            "text": "Deep-learning Baselines Recent years have seen many advances in deep learning for sequential recommendations. GRU4Rec is the first RNN-based method proposed for this problem [43] ; GRU4Rec + [42] later is proposed to address some shortcomings of the ",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 191,
                    "end": 195,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Non-deep-learning Baselines"
        },
        {
            "text": "initial version. Caser is the corresponding CNN-based method [108] . STAMP [68] utilizes the attention mechanism without using RNN or CNN as building blocks. Very recently,",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 66,
                    "text": "[108]",
                    "ref_id": "BIBREF107"
                },
                {
                    "start": 75,
                    "end": 79,
                    "text": "[68]",
                    "ref_id": "BIBREF67"
                }
            ],
            "ref_spans": [],
            "section": "Non-deep-learning Baselines"
        },
        {
            "text": "SASRec utilizes state-of-art Transformer encoder [110] with self-attention mechanisms.",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 54,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                }
            ],
            "ref_spans": [],
            "section": "Non-deep-learning Baselines"
        },
        {
            "text": "Hierarchical gating networks, also known as HGN [69] are also proposed to solve this problem.",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 52,
                    "text": "[69]",
                    "ref_id": "BIBREF68"
                }
            ],
            "ref_spans": [],
            "section": "Non-deep-learning Baselines"
        },
        {
            "text": "We use the same datasets as in [56] and follow the same procedure in the paper: use last items for each user as test data, second-to-last as validation data and the rest as training data. We implemented our method in Tensorflow and solve it with Adam Optimizer [57] with a learning rate of 0.001, momentum exponential decay rates \u03b2 1 = 0.9, \u03b2 2 = 0.98 and a batch size of 128. In Table 6 .1, since we use the same data, the performance of previous methods except STAMP have been reported in [56] . We tune the dropout rate, and SSE probabilities p u , p i , p y for input user/item embeddings and ",
            "cite_spans": [
                {
                    "start": 31,
                    "end": 35,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 261,
                    "end": 265,
                    "text": "[57]",
                    "ref_id": "BIBREF56"
                },
                {
                    "start": 491,
                    "end": 495,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [
                {
                    "start": 380,
                    "end": 387,
                    "text": "Table 6",
                    "ref_id": "TABREF35"
                }
            ],
            "section": "Experiment Setup"
        },
        {
            "text": "Apart from evaluating our SSE-PT against SASRec using well-defined ranking metrics on real-world datasets, we also visualize the differences between both methods in terms of their attention mechanisms. In Figure 6 .2, a random user's engagement history in",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 205,
                    "end": 213,
                    "text": "Figure 6",
                    "ref_id": "FIGREF18"
                }
            ],
            "section": "Attention Mechanism Visualization"
        },
        {
            "text": "Movielens1M dataset is given in temporal order (column-wise). We hide the last item whose index is 26 in test set and hope that a temporal collaborative ranking model can figure out item-26 is the one this user will watch next using only previous engagement history. One can see for a typical user; they tend to look at a different style of movies at different times. Earlier on, they watched a variety of movies, including Sci-Fi, animation, thriller, romance, horror, action, comedy and adventure. But later on, in the last two columns of Figure 6 .2, drama and thriller are the two types they like to watch most, especially the drama type. In fact, they watched 9 drama movies out of recent 10 movies.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 541,
                    "end": 549,
                    "text": "Figure 6",
                    "ref_id": "FIGREF18"
                }
            ],
            "section": "Attention Mechanism Visualization"
        },
        {
            "text": "For humans, it is natural to reason that the hidden movie should probably also be drama ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Attention Mechanism Visualization"
        },
        {
            "text": "In [56] , it has been shown that SASRec is about 11 times faster than Caser and 17 times faster than GRU4Rec + and achieves much better NDCG@10 results so we did for Movielens1M dataset. Given that we added additional user embeddings into our SSE-PT model, it is expected that it will take slightly longer to train our model than un-personalized SASRec. We find empirically that training speed of the SSE-PT and SSE-PT++ model are comparable to that of SASRec, with SSE-PT++ being the fastest and the best performing model. It is clear that our SSE-PT and SSE-PT++ achieve much better ranking performances than our baseline SASRec using the same training time.",
            "cite_spans": [
                {
                    "start": 3,
                    "end": 7,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                }
            ],
            "ref_spans": [],
            "section": "Training Speed"
        },
        {
            "text": "SSE probability Given the importance of SSE regularization for our SSE-PT model, we carefully examined the SSE probability for input user embedding in Table 8 .10 in Appendix. We find that the appropriate hyper-parameter SSE probability is not very sensitive: anywhere between 0.4 and 1.0 gives good results, better than parameter sharing and not using SSE-SE. This is also evident based on comparison results in Table 6 .3.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 151,
                    "end": 158,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                },
                {
                    "start": 413,
                    "end": 420,
                    "text": "Table 6",
                    "ref_id": "TABREF35"
                }
            ],
            "section": "Ablation Study"
        },
        {
            "text": "Recall that the sampling probability is unique to our SSE-PT++ model. We show in Table 8 .11 in Appendix using an appropriate sampling probability like 0.2 \u2192 0.3 would allow it to outperform SSE-PT when the same maximum length is used.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 81,
                    "end": 88,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                }
            ],
            "section": "Sampling Probability"
        },
        {
            "text": "We find for our SSE-PT model, a larger number of attention blocks is preferred. One can easily see in Table 8 ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 102,
                    "end": 109,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                }
            ],
            "section": "Number of Attention Blocks"
        },
        {
            "text": "In this chapter, we propose a novel neural network architecture called Personalized",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Transformer for the temporal collaborative ranking problem. It enjoys the benefits of being a personalized model, therefore achieving better ranking results for individual users than the current state-of-the-art. By examining the attention mechanisms during inference, the model is also more interpretable and tends to pay more attention to recent items in long sequences than un-personalized deep learning models. I was exploring directions on incorporating additional side information such as graphs and temporal orderings. In chapter 5, we came up with a new graph encoding method in chapter 2 to enhance existing graph-based collaborative filtering, allowing them to encode deep graph information and therefore achieve better recommendation performances. We made the temporal collaborative ranking model personalized in chapter 7 by incorporating user embeddings. In the process, motivated by the need to prevent over-fitting caused by the additional parameters, we introduced a general regularization technique for embedding layers in deep learning in chapter 6, which was shown to be useful for many other models with lots of embedding parameters both within and outside recommendations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Despite that the dissertation is lengthy and has contained many important research [6] . But to do that, we need a well-defined problem, dataset and metric, and lots of people participating both from academics and industry by combining strengths from both parties. total nm ratings. We choose T = 3 so the graph contains at most 6-hop information among n users. We use rank r = 50 for both user and item embeddings. We set influence weight w = 0.6, i.e. in each propagation step, 60% of one user's preference is decided by its friends (i.e. neighbors in the friendship graph). We set p = 0.001, which is the probability for each of the possible edges being chosen in Erd\u00f5s-R\u00e9nyi graph G. A small edge probability p, influence weight w < 1.0, and a not too-large T is needed, because we don't want that all users become more or less the same after T propagation steps.",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 86,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Future Work"
        },
        {
            "text": "We omit the definitions of RMSE, Precision@k, NDCG@k, MAP as those can be easily found online. HLU: Half-Life Utility [11, 98] is defined as:",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 122,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 123,
                    "end": 126,
                    "text": "98]",
                    "ref_id": "BIBREF97"
                }
            ],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "where n is the number of users and HLU i is given by:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "where R i\u03a0 il follows previous definition, d is the neural vote (usually the rating average), and \u03b1 is the viewing halflife. The halflife is the number of the item on the list such that there is a 50-50 chance the user will review that item [11] .",
            "cite_spans": [
                {
                    "start": 241,
                    "end": 245,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Metrics"
        },
        {
            "text": "Input: n users, m items, rank r, influence weight w, T propagation steps Output: R tr \u2208 R n\u00d7m , R te \u2208 R n\u00d7m , G \u2208 R n\u00d7n 1: Randomly initialize U \u2208 R n\u00d7r , V \u2208 R m\u00d7r from standard normal distribution for i = 1, ..., n do",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 11. Simulation of Synthetic Data"
        },
        {
            "text": "Set U =\u0168 7: Generate rating matrix R = U V T 8: Random sample observed user/item indices in training and test data: \u2126 tr , \u2126 te 9: Obtain R tr = \u2126 tr \u2022 R, R te = \u2126 te \u2022 R 10: return rating matrices R tr , R te , user graph G ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 11. Simulation of Synthetic Data"
        },
        {
            "text": "To reproduce results reported in the paper, one need to download data (douban and flixster) and third-party C++ Matrix Factorization library from the link https://www. csie.ntu.edu.tw/~cjlin/papers/ocmf-side/. One can simply follow README there to compile the codes in Matlab and run one-class matrix factorization library in different modes (both explicit feedback and implicit feedback works). The advantage of using this library is that the codes support multi-threading and runs quite fast with very efficient memory space allocations. It also supports with graph or other side information. All three methods' baseline can be simply run with the tuning parameters we reported in the As to simulation study, we will also provide python codes to repeat our Algorithm 11",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Reproducibility"
        },
        {
            "text": "to generate synthesis dataset. One can easily simulate the data before converting into Matlab data format and running the codes as before. The optimal parameters can be found in Table 8 In Table 8 .4, one can compare magnitude of optimal \u03b1 and \u03b2 to have a good idea of whether G or G 2 is more useful. G represents shallow graph information and G 2 represents deep graph information. If one already run GRMF G 2 , one can then use this as a preliminary test to decide whether to go deep with DNA-3 (d = 3) to capture deep graph information or simply go ahead with DNA-1 (d = 1) to fully utilize shallow information.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 178,
                    "end": 185,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                },
                {
                    "start": 189,
                    "end": 196,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                }
            ],
            "section": "Reproducibility"
        },
        {
            "text": "For douban dataset, we have \u03b1 = 0.05 > 0.0005 = \u03b2, which implies shallow information is important and we should fully utilize it. It explains why DNA-1 is performing well both in terms of performance and speed on douban dataset. It is worth noting that GRMF DNA-1's Bloom filter matrix B contains much more nnz than that of G in Table 8 The synthesis dataset has 10, 000 users and 2, 000 items with user friendship graph of size 10, 000\u00d710, 000. Note that the graph only contains at most 6-hop valid information. GRMF G 6 means GRMF with G + \u03b1 \u00b7 G 2 + \u03b2 \u00b7 G 3 + \u03b3 \u00b7 G 4 + \u00b7 G 5 + \u03c9 \u00b7 G 6 . GRMF DNA-d means depth d is used. Figure 5 .1 as a custom operator.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 329,
                    "end": 336,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                },
                {
                    "start": 624,
                    "end": 632,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Reproducibility"
        },
        {
            "text": "To run SSE-Graph, we need to construct good-quality knowledge graphs on embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "We managed to match movies in Movielens1m and Movielens10m datasets to IMDB websites, therefore we can extract plentiful information for each movie, such as the cast of the movies, user reviews and so on. For simplicity reason, we construct the knowledge graph on item-side embeddings using the cast of movies. Two items are connected by an edge when they share one or more actors/actresses. For user side, we do not have good quality graphs: we are only able to create a graph on users in Movielens1m dataset based on Algorithm 12. Compute gradient for V when U fixed Input: \u03a0, U , V , \u03bb, \u03c1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "Output: g g \u2208 R r\u00d7m is the gradient for f (V )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "For implicit feedback, it should be (1 + \u03c1) \u00b7m instead ofm, since \u03c1 \u00b7m 0's are appended to the back Initialize total = 0, tt = 0",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "Return g their age groups but we do not have any side information on users in Movielens10m dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "When running experiments, we do a parameter sweep for weight decay parameter and then fix it before tuning the parameters for SSE-Graph and SSE-SE. We utilize different \u03c1 and p for user and item embedding tables respectively. The optimal parameters are stated in In the second leg of experiments, we remove the constraints on the maximum number of ratings per user. We want to show that SSE-SE can be a good alternative when graph information is not available. We follow the same procedures in [115, 116] . In Table 5 .2,",
            "cite_spans": [
                {
                    "start": 494,
                    "end": 499,
                    "text": "[115,",
                    "ref_id": "BIBREF114"
                },
                {
                    "start": 500,
                    "end": 504,
                    "text": "116]",
                    "ref_id": "BIBREF115"
                }
            ],
            "ref_spans": [
                {
                    "start": 510,
                    "end": 517,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "we can see that SSE-SE can be used with dropout to achieve the smallest RMSE across Douban, Movielens10m, and Netflix datasets. In Table 5 .3, one can see that SSE-SE is more effective than dropout in this case and can perform better than STOA listwise approach SQL-Rank [116] on 2 datasets out of 3.",
            "cite_spans": [
                {
                    "start": 271,
                    "end": 276,
                    "text": "[116]",
                    "ref_id": "BIBREF115"
                }
            ],
            "ref_spans": [
                {
                    "start": 131,
                    "end": 138,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "In Table 5 .2, SSE-SE has two tuning parameters: probability p u to replace embeddings associated with user-side embeddings and probability p i to replace embeddings associated with item side embeddings because there are two embedding tables. But here for simplicity, we use one tuning parameter p s = p u = p i . We use dropout probability of p d , dimension of user/item embeddings d, weight decay of \u03bb and learning rate of 0.01 for all experiments, with the exception that the learning rate is reduced to 0.005 when both SSE-SE and Dropout are applied. For Douban dataset, we use d = 10, \u03bb = 0.08. For Movielens10m and Netflix dataset, we use d = 50, \u03bb = 0.1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 10,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "Neural Networks with One Hidden Layer"
        },
        {
            "text": "We use the transformer model [110] as the backbone for our experiments. The control group is the standard transformer encoder-decoder architecture with self-attention. In the experiment group, we apply SSE-SE towards both encoder and decoder by replacing corresponding vocabularies' embeddings in the source and target sentences. We trained on the standard WMT 2014 English to German dataset which consists of roughly 4.5",
            "cite_spans": [
                {
                    "start": 29,
                    "end": 34,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                }
            ],
            "ref_spans": [],
            "section": "Neural Machine Translation"
        },
        {
            "text": "million parallel sentence pairs and tested on WMT 2008 to 2018 news-test sets. Sentences were encoded into 32,000 tokens using a byte-pair encoding. We use the SentencePiece,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Machine Translation"
        },
        {
            "text": "OpenNMT and SacreBLEU implementations in our experiments. We trained the 6-layer ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Neural Machine Translation"
        },
        {
            "text": "In the first leg of experiments, we crawled one million user reviews data from IMDB and pre-trained the BERT-Base model (12 blocks) for 500, 000 steps using sequences of maximum length 512 and batch size of 8, learning rates of 2e \u22125 for both models using one NVIDIA V100 GPU. Then we pre-trained on a mixture of our crawled reviews and reviews in IMDB sentiment classification tasks (250K reviews in train and 250K reviews in test) for another 200, 000 steps before training for another 100, 000 steps for the reviews in IMDB sentiment classification task only. In total, both models are pre-trained on the same datasets for 800, 000 steps with the only difference being our model utilizes SSE-SE.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BERT"
        },
        {
            "text": "In the second leg of experiments, we fine-tuned the two models obtained in the first-leg experiments on two sentiment classification tasks: IMDB sentiment classification task and SST-2 sentiment classification task. The goal of pre-training on IMDB dataset but fine-tuning for SST-2 task is to explore whether SSE-SE can play a role in transfer learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BERT"
        },
        {
            "text": "The results are summarized in Table 5 .6 for IMDB sentiment task. In experiments, we use maximum sequence length of 512, learning rate of 2e \u22125 , dropout probability of 0.1 and we run fine-tuning for 1 epoch for the two pre-trained models we obtained before.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 30,
                    "end": 37,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "BERT"
        },
        {
            "text": "For the Google pre-trained BERT-base model, we find that we need to run a minimum of 2 epochs. This shows that pre-training can speed up the fine-tuning. We find that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BERT"
        },
        {
            "text": "Google pre-trained model performs worst in accuracy because it was only pre-trained on Wikipedia and books corpus while ours have seen many additional user reviews. We also find that SSE-SE pre-trained model can achieve accuracy of 0.9542 after fine-tuning for one epoch only. On the contrast, the accuracy is only 0.9518 without SSE-SE for embeddings associated with output y i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "BERT"
        },
        {
            "text": "For the SST-2 task, we use maximum sequence length of 128, learning rate of 2e \u22125 , dropout probability of 0.1 and we run fine-tuning for 3 epochs for all 3 models in Table 5 .7.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 167,
                    "end": 174,
                    "text": "Table 5",
                    "ref_id": "TABREF28"
                }
            ],
            "section": "BERT"
        },
        {
            "text": "We report AUC, accuracy and F1 score for dev data. For test results, we submitted our predictions to Glue website for the official evaluation. We find that even in transfer learning, our SSE-SE pre-trained model still enjoys advantages over Google pre-trained model and our pre-trained model without SSE-SE. Our SSE-SE pre-trained model achieves 94 .3% accuracy on SST-2 test set versus 93.6 and 93.8 respectively. If we are using SSE-SE for both pre-training and fine-tuning, we can achieve 94.5% accuracy on the SST-2 test set, which approaches the 94.9 score reported by the BERT-Large model. SSE probability of 0.01 is used for fine-tuning.",
            "cite_spans": [
                {
                    "start": 346,
                    "end": 348,
                    "text": "94",
                    "ref_id": "BIBREF93"
                }
            ],
            "ref_spans": [],
            "section": "BERT"
        },
        {
            "text": "Throughout this section, we will suppress the probability parameters, p(., .|\u03a6) = p(., .). Simulation of a bound on \u03c1 L,n for the movielens1M dataset. Throughout the simulation, L is replaced with (which will bound \u03c1 L,n by Jensen's inequality). The SSE probability parameter dictates the probability of transitioning. When this is 0 (box plot on the right), the distribution is that of the samples from the standard Rademacher complexity (without the sup and expectation). As we increase the transition probability, the values for \u03c1 L,n get smaller.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Let us break the variability term into two components Notice that we may write,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "Again we may introduce a second set of Rademacher random variables \u03c3 i , which results in Then the result follows from McDiarmid's inequality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proofs"
        },
        {
            "text": "\u2022 NDCG@K: defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "where i represents i-th user and DCG@K(i, \u03a0 i ) = K l=1 2 R i\u03a0 il \u2212 1 log 2 (l + 1) . (8.15) In the DCG definition, \u03a0 il represents the index of the l-th ranked item for user i in test data based on the learned score matrix X. R is the rating matrix and R ij is the rating given to item j by user i. \u03a0 * i is the ordering provided by the ground truth rating.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 92,
                    "text": "(8.15)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "\u2022 Recall@K: defined as a fraction of positive items retrieved by the top K recommendations the model makes:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "here we already assume there is only a single positive item that user will engage next and the indicator function 1{\u22031 \u2264 l \u2264 k : R i\u03a0 il = 1} is defined to indicate whether the positive item falls into the top K position in our obtained ranked list using scores predicted in (6.4).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "Layer Normalization Layer normalization [4] normalizes neurons within a layer. Previous studies [4] show it is more effective than batch normalization for training recurrent neural networks (RNNs). One alternative is the batch normalization [51] but we find it does not work as well as the layer normalization in practice even for a reasonable large batch size of 128. Therefore, our SSE-PT model adopts layer normalization.",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 43,
                    "text": "[4]",
                    "ref_id": null
                },
                {
                    "start": 96,
                    "end": 99,
                    "text": "[4]",
                    "ref_id": null
                },
                {
                    "start": 241,
                    "end": 245,
                    "text": "[51]",
                    "ref_id": "BIBREF50"
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "Residual Connections Residual connections are firstly proposed in ResNet for image classification problems [39] . Recent research finds that residual connections can help training very deep neural networks even if they are not convolutional neural networks [110] .",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 257,
                    "end": 262,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "Using residual connections allows us to train very deep neural networks here. For example, the best performing model for Movielens10M dataset in Table 8 .12 is the SSE-PT with 6 attention blocks, in which 1 + 6 * 3 + 1 = 20 layers are trained end-to-end.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 145,
                    "end": 152,
                    "text": "Table 8",
                    "ref_id": "TABREF39"
                }
            ],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "Weight Decay Weight decay [64] , also known as l 2 regularization [46] , is applied to all embeddings, including both user and item embeddings.",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 30,
                    "text": "[64]",
                    "ref_id": "BIBREF63"
                },
                {
                    "start": 66,
                    "end": 70,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "Dropout Dropout [106] is applied to the embedding layer E, self-attention layer and pointwise feed-forward layer by stochastically dropping some percentage of hidden units to prevent co-adaption of neurons. Dropout has been shown to be an effective way of regularizing deep learning models.",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 21,
                    "text": "[106]",
                    "ref_id": "BIBREF105"
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "In summary, layer normalization and dropout are used in all layers except prediction layer. Residual connections are used in both self-attention layer and pointwise feed-forward layer. SSE-SE is used in embedding layer and prediction layer. \u2022 PopRec: ranking items according to their popularity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "\u2022 BPR: Bayesian personalized ranking for implicit feedback setting [91] . It is a low-rank matrix factorization model with a pairwise loss function. But it does not utilize the temporal information. Therefore, it serves as a strong baseline for non-temporal methods.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 71,
                    "text": "[91]",
                    "ref_id": "BIBREF90"
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "\u2022 FMC: Factorized Markov Chains: a first-order Markov Chain method, in which predictions are made only based on previously engaged item.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "\u2022 PFMC: a personalized Markov chain model [92] that combines matrix factorization and first-order Markov Chain to take advantage of both users' latent long-term preferences as well as short-term item transitions.",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 46,
                    "text": "[92]",
                    "ref_id": "BIBREF91"
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "\u2022 TransRec: a first-order sequential recommendation method [40] in which items are embedded into a transition space and users are modelled as translation vectors SQL-Rank [116] and item-based recommendations [94] are omitted because the former is similar to BPR [91] except using the listwise loss function instead of the pairwise loss function and the latter has been shown inferior to TransRec [40] .",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 171,
                    "end": 176,
                    "text": "[116]",
                    "ref_id": "BIBREF115"
                },
                {
                    "start": 208,
                    "end": 212,
                    "text": "[94]",
                    "ref_id": "BIBREF93"
                },
                {
                    "start": 396,
                    "end": 400,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [],
            "section": "Appendix to Chapter 6"
        },
        {
            "text": "\u2022 GRU4Rec: the first RNN-based method proposed for the session-based recommendation problem [43] . It utilizes the GRU structures [20] initially proposed for speech modelling.",
            "cite_spans": [
                {
                    "start": 92,
                    "end": 96,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 130,
                    "end": 134,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Deep-learning baselines"
        },
        {
            "text": "\u2022 GRU4Rec + : follow-up work of GRU4Rec by the same authors: the model has a very similar architecture to GRU4Rec but has a more complicated loss function [42] .",
            "cite_spans": [
                {
                    "start": 155,
                    "end": 159,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                }
            ],
            "ref_spans": [],
            "section": "Deep-learning baselines"
        },
        {
            "text": "\u2022 Caser: a CNN-based method [108] which embeds a sequence of recent items in both time and latent spaces forming an 'image' before learning local features through horizontal and vertical convolutional filters. In [108] , user embeddings are included in the prediction layer only. On the contrast, in our Personalized Transformer, user embeddings are also introduced in the lowest embedding layer so they can play an important role in self-attention mechanisms as well as in prediction stages.",
            "cite_spans": [
                {
                    "start": 28,
                    "end": 33,
                    "text": "[108]",
                    "ref_id": "BIBREF107"
                },
                {
                    "start": 213,
                    "end": 218,
                    "text": "[108]",
                    "ref_id": "BIBREF107"
                }
            ],
            "ref_spans": [],
            "section": "Deep-learning baselines"
        },
        {
            "text": "\u2022 STAMP: a session-based recommendation algorithm [68] using attention mechanism.",
            "cite_spans": [
                {
                    "start": 50,
                    "end": 54,
                    "text": "[68]",
                    "ref_id": "BIBREF67"
                }
            ],
            "ref_spans": [],
            "section": "Deep-learning baselines"
        },
        {
            "text": "[68] only uses fully connected layers with one attention block that is not self-attentive.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep-learning baselines"
        },
        {
            "text": "\u2022 SASRec: a self-attentive sequential recommendation method [56] motivated by Transformer in NLP [110] . Unlike our method SSE-PT, SASRec does not incorporate user embedding and therefore is not a personalized method. SASRec paper [56] also does not utilize SSE [117] for further regularization: only dropout and weight decay are used.",
            "cite_spans": [
                {
                    "start": 60,
                    "end": 64,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 97,
                    "end": 102,
                    "text": "[110]",
                    "ref_id": "BIBREF109"
                },
                {
                    "start": 231,
                    "end": 235,
                    "text": "[56]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 262,
                    "end": 267,
                    "text": "[117]",
                    "ref_id": "BIBREF116"
                }
            ],
            "ref_spans": [],
            "section": "Deep-learning baselines"
        },
        {
            "text": "\u2022 HGN: hierarchical gating networks method to solve the sequential recommendation problem [69] , which incorporates the user embeddings and gating networks for better personalization than the SASRec model. ",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 94,
                    "text": "[69]",
                    "ref_id": "BIBREF68"
                }
            ],
            "ref_spans": [],
            "section": "Deep-learning baselines"
        },
        {
            "text": "In this dissertation, we cover some recent advances in collaborative filtering and ranking.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In chapter 1, we give a brief introduction of the history and the current landscape of collaborative filtering and ranking; chapter 2 we first talk about pointwise collaborative filtering problem with graph information, and how our proposed new method can encode very deep graph information which helps four existing graph collaborative filtering algorithms; chapter 3 is on the pairwise approach for collaborative ranking and how we speed up the algorithm to near-linear time complexity; chapter 4 is on the new listwise approach for collaborative ranking and how the listwise approach is a better choice of loss for both explicit and implicit feedback over pointwise and pairwise loss; chapter 5 is about the new regularization technique Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it is both theoretically sound and empirically effectively for 6 different tasks across recommendation and natural language processing; chapter 6 is how we introduce personalization for the state-of-the-art sequential recommendation model with the help of SSE, which plays an important role in preventing our personalized model from overfitting to the training data; chapter 7, we summarize what we have achieved so far and predict what the future directions can be; chapter 8 is the appendix to all the chapters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "A recommender system based on local random walks and spectral methods",
            "authors": [
                {
                    "first": "Zeinab",
                    "middle": [],
                    "last": "Abbassi",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vahab",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mirrokni",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 workshop on Web mining and social network analysis",
            "volume": "",
            "issn": "",
            "pages": "102--108",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Ranking on graph data",
            "authors": [
                {
                    "first": "Shivani",
                    "middle": [],
                    "last": "Agarwal",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Proceedings of the 23rd international conference on Machine learning",
            "volume": "",
            "issn": "",
            "pages": "25--32",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Scalable bloom filters",
            "authors": [
                {
                    "first": "Paulo",
                    "middle": [],
                    "last": "S\u00e9rgio Almeida",
                    "suffix": ""
                },
                {
                    "first": "Carlos",
                    "middle": [],
                    "last": "Baquero",
                    "suffix": ""
                },
                {
                    "first": "Nuno",
                    "middle": [],
                    "last": "Pregui\u00e7a",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Hutchison",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Information Processing Letters",
            "volume": "101",
            "issn": "6",
            "pages": "255--261",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Collaborative ranking",
            "authors": [
                {
                    "first": "Suhrid",
                    "middle": [],
                    "last": "Balakrishnan",
                    "suffix": ""
                },
                {
                    "first": "Sumit",
                    "middle": [],
                    "last": "Chopra",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the fifth ACM international conference on Web search and data mining",
            "volume": "",
            "issn": "",
            "pages": "143--152",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "The netflix prize",
            "authors": [
                {
                    "first": "James",
                    "middle": [],
                    "last": "Bennett",
                    "suffix": ""
                },
                {
                    "first": "Stan",
                    "middle": [],
                    "last": "Lanning",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "KDD Cup and Workshop in conjunction with ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The netflix prize",
            "authors": [
                {
                    "first": "James",
                    "middle": [],
                    "last": "Bennett",
                    "suffix": ""
                },
                {
                    "first": "Stan",
                    "middle": [],
                    "last": "Lanning",
                    "suffix": ""
                },
                {
                    "first": "Netflix",
                    "middle": [],
                    "last": "Netflix",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "KDD Cup and Workshop in conjunction with ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Graph convolutional matrix completion",
            "authors": [
                {
                    "first": "Rianne",
                    "middle": [],
                    "last": "Van Den",
                    "suffix": ""
                },
                {
                    "first": "Thomas",
                    "middle": [
                        "N"
                    ],
                    "last": "Berg",
                    "suffix": ""
                },
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Kipf",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1706.02263"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Space/time trade-offs in hash coding with allowable errors",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Burton",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bloom",
                    "suffix": ""
                }
            ],
            "year": 1970,
            "venue": "Communications of the ACM",
            "volume": "13",
            "issn": "7",
            "pages": "422--426",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Apache hadoop goes realtime at facebook",
            "authors": [
                {
                    "first": "Dhruba",
                    "middle": [],
                    "last": "Borthakur",
                    "suffix": ""
                },
                {
                    "first": "Jonathan",
                    "middle": [],
                    "last": "Gray",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Joydeep Sen",
                    "suffix": ""
                },
                {
                    "first": "Kannan",
                    "middle": [],
                    "last": "Sarma",
                    "suffix": ""
                },
                {
                    "first": "Nicolas",
                    "middle": [],
                    "last": "Muthukkaruppan",
                    "suffix": ""
                },
                {
                    "first": "Hairong",
                    "middle": [],
                    "last": "Spiegelberg",
                    "suffix": ""
                },
                {
                    "first": "Karthik",
                    "middle": [],
                    "last": "Kuang",
                    "suffix": ""
                },
                {
                    "first": "Dmytro",
                    "middle": [],
                    "last": "Ranganathan",
                    "suffix": ""
                },
                {
                    "first": "Aravind",
                    "middle": [],
                    "last": "Molkov",
                    "suffix": ""
                },
                {
                    "first": "Samuel",
                    "middle": [],
                    "last": "Menon",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rash",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 2011 ACM SIGMOD International Conference on Management of data",
            "volume": "",
            "issn": "",
            "pages": "1071--1080",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Empirical analysis of predictive algorithms for collaborative filtering",
            "authors": [
                {
                    "first": "David",
                    "middle": [],
                    "last": "John S Breese",
                    "suffix": ""
                },
                {
                    "first": "Carl",
                    "middle": [],
                    "last": "Heckerman",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kadie",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "43--52",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Network applications of bloom filters: A survey",
            "authors": [
                {
                    "first": "Andrei",
                    "middle": [],
                    "last": "Broder",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Mitzenmacher",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Internet mathematics",
            "volume": "1",
            "issn": "4",
            "pages": "485--509",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Graph regularized nonnegative matrix factorization for data representation",
            "authors": [
                {
                    "first": "Deng",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "Xiaofei",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Jiawei",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Thomas S",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE transactions on pattern analysis and machine intelligence",
            "volume": "33",
            "issn": "",
            "pages": "1548--1560",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Grarep: Learning graph representations with global structural information",
            "authors": [
                {
                    "first": "Shaosheng",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Qiongkai",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th ACM international on conference on information and knowledge management",
            "volume": "",
            "issn": "",
            "pages": "891--900",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Learning to rank: from pairwise approach to listwise approach",
            "authors": [
                {
                    "first": "Zhe",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Qin",
                    "suffix": ""
                },
                {
                    "first": "Tie-Yan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Ming-Feng",
                    "middle": [],
                    "last": "Tsai",
                    "suffix": ""
                },
                {
                    "first": "Hang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 24th international conference on Machine learning",
            "volume": "",
            "issn": "",
            "pages": "129--136",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Bigtable: A distributed storage system for structured data",
            "authors": [
                {
                    "first": "Fay",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                },
                {
                    "first": "Sanjay",
                    "middle": [],
                    "last": "Ghemawat",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wilson",
                    "suffix": ""
                },
                {
                    "first": "Deborah",
                    "middle": [
                        "A"
                    ],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "Mike",
                    "middle": [],
                    "last": "Wallach",
                    "suffix": ""
                },
                {
                    "first": "Tushar",
                    "middle": [],
                    "last": "Burrows",
                    "suffix": ""
                },
                {
                    "first": "Andrew",
                    "middle": [],
                    "last": "Chandra",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [
                        "E"
                    ],
                    "last": "Fikes",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gruber",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "ACM Transactions on Computer Systems (TOCS)",
            "volume": "26",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Efficient algorithms for ranking with svms",
            "authors": [
                {
                    "first": "Olivier",
                    "middle": [],
                    "last": "Chapelle",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sathiya Keerthi",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Information Retrieval",
            "volume": "13",
            "issn": "3",
            "pages": "201--215",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Stochastic training of graph convolutional networks with variance reduction",
            "authors": [
                {
                    "first": "Jianfei",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "Le",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "941--949",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Matrix completion with noisy side information",
            "authors": [
                {
                    "first": "Kai-Yang",
                    "middle": [],
                    "last": "Chiang",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "3447--3455",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "authors": [
                {
                    "first": "Junyoung",
                    "middle": [],
                    "last": "Chung",
                    "suffix": ""
                },
                {
                    "first": "Caglar",
                    "middle": [],
                    "last": "Gulcehre",
                    "suffix": ""
                },
                {
                    "first": "Kyunghyun",
                    "middle": [],
                    "last": "Cho",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.3555"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Robust bloom filters for large multilabel classification tasks",
            "authors": [
                {
                    "first": "Nicolas",
                    "middle": [],
                    "last": "Moustapha M Cisse",
                    "suffix": ""
                },
                {
                    "first": "Thierry",
                    "middle": [],
                    "last": "Usunier",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "Artieres",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gallinari",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1851--1859",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Binaryconnect: Training deep neural networks with binary weights during propagations",
            "authors": [
                {
                    "first": "Matthieu",
                    "middle": [],
                    "last": "Courbariaux",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "Jean-Pierre",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3123--3131",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "authors": [
                {
                    "first": "Micha\u00ebl",
                    "middle": [],
                    "last": "Defferrard",
                    "suffix": ""
                },
                {
                    "first": "Xavier",
                    "middle": [],
                    "last": "Bresson",
                    "suffix": ""
                },
                {
                    "first": "Pierre",
                    "middle": [],
                    "last": "Vandergheynst",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "3844--3852",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Item-based top-n recommendation algorithms",
            "authors": [
                {
                    "first": "Mukund",
                    "middle": [],
                    "last": "Deshpande",
                    "suffix": ""
                },
                {
                    "first": "George",
                    "middle": [],
                    "last": "Karypis",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "ACM Transactions on Information Systems (TOIS)",
            "volume": "22",
            "issn": "1",
            "pages": "143--177",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Pretraining of deep bidirectional transformers for language understanding",
            "authors": [
                {
                    "first": "Jacob",
                    "middle": [],
                    "last": "Devlin",
                    "suffix": ""
                },
                {
                    "first": "Ming-Wei",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                },
                {
                    "first": "Kenton",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Kristina",
                    "middle": [
                        "Toutanova"
                    ],
                    "last": "Bert",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1810.04805"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "authors": [
                {
                    "first": "John",
                    "middle": [],
                    "last": "Duchi",
                    "suffix": ""
                },
                {
                    "first": "Elad",
                    "middle": [],
                    "last": "Hazan",
                    "suffix": ""
                },
                {
                    "first": "Yoram",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Journal of Machine Learning Research",
            "volume": "12",
            "issn": "",
            "pages": "2121--2159",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "A new data structure for cumulative frequency tables. Software: Practice and Experience",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "M"
                    ],
                    "last": "Fenwick",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "An efficient boosting algorithm for combining preferences",
            "authors": [
                {
                    "first": "Yoav",
                    "middle": [],
                    "last": "Freund",
                    "suffix": ""
                },
                {
                    "first": "Raj",
                    "middle": [],
                    "last": "Iyer",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [
                        "E"
                    ],
                    "last": "Schapire",
                    "suffix": ""
                },
                {
                    "first": "Yoram",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Journal of machine learning research",
            "volume": "4",
            "issn": "",
            "pages": "933--969",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Large-scale matrix factorization with distributed stochastic gradient descent",
            "authors": [
                {
                    "first": "Rainer",
                    "middle": [],
                    "last": "Gemulla",
                    "suffix": ""
                },
                {
                    "first": "Erik",
                    "middle": [],
                    "last": "Nijkamp",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Peter",
                    "suffix": ""
                },
                {
                    "first": "Yannis",
                    "middle": [],
                    "last": "Haas",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sismanis",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "69--77",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Deep learning",
            "authors": [
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "1",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Itemrank: A random-walk based scoring algorithm for recommender engines",
            "authors": [
                {
                    "first": "Marco",
                    "middle": [],
                    "last": "Gori",
                    "suffix": ""
                },
                {
                    "first": "Augusto",
                    "middle": [],
                    "last": "Pucci",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Roma",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Siena",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "International Joint Conferences on Artificial Intelligence",
            "volume": "7",
            "issn": "",
            "pages": "2766--2771",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "node2vec: Scalable feature learning for networks",
            "authors": [
                {
                    "first": "Aditya",
                    "middle": [],
                    "last": "Grover",
                    "suffix": ""
                },
                {
                    "first": "Jure",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "855--864",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Preference completion from partial rankings",
            "authors": [
                {
                    "first": "Suriya",
                    "middle": [],
                    "last": "Gunasekar",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Oluwasanmi",
                    "suffix": ""
                },
                {
                    "first": "Joydeep",
                    "middle": [],
                    "last": "Koyejo",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ghosh",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1370--1378",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Matchin: eliciting user preferences with an online game",
            "authors": [
                {
                    "first": "Severin",
                    "middle": [],
                    "last": "Hacker",
                    "suffix": ""
                },
                {
                    "first": "Luis",
                    "middle": [],
                    "last": "Von Ahn",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
            "volume": "",
            "issn": "",
            "pages": "1207--1216",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Inductive representation learning on large graphs",
            "authors": [
                {
                    "first": "Will",
                    "middle": [],
                    "last": "Hamilton",
                    "suffix": ""
                },
                {
                    "first": "Zhitao",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "Jure",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "1024--1034",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Representation learning on graphs: Methods and applications",
            "authors": [
                {
                    "first": "Rex",
                    "middle": [],
                    "last": "William L Hamilton",
                    "suffix": ""
                },
                {
                    "first": "Jure",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1709.05584"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
            "authors": [
                {
                    "first": "Song",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Huizi",
                    "middle": [],
                    "last": "Mao",
                    "suffix": ""
                },
                {
                    "first": "William",
                    "middle": [
                        "J"
                    ],
                    "last": "Dally",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1510.00149"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "The movielens datasets: History and context",
            "authors": [
                {
                    "first": "Maxwell",
                    "middle": [],
                    "last": "Harper",
                    "suffix": ""
                },
                {
                    "first": "Joseph",
                    "middle": [
                        "A"
                    ],
                    "last": "Konstan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Acm transactions on interactive intelligent systems (tiis)",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Deep residual learning for image recognition",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Xiangyu",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Shaoqing",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "Jian",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "770--778",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Translation-based recommendation",
            "authors": [
                {
                    "first": "Ruining",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Wang-Cheng",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Julian",
                    "middle": [],
                    "last": "Mcauley",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the Eleventh ACM Conference on Recommender Systems",
            "volume": "",
            "issn": "",
            "pages": "161--169",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Neural collaborative filtering",
            "authors": [
                {
                    "first": "Xiangnan",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Lizi",
                    "middle": [],
                    "last": "Liao",
                    "suffix": ""
                },
                {
                    "first": "Hanwang",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Liqiang",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "Xia",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Tat-Seng",
                    "middle": [],
                    "last": "Chua",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 26th International Conference on World Wide Web",
            "volume": "",
            "issn": "",
            "pages": "173--182",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Recurrent neural networks with topk gains for session-based recommendations",
            "authors": [
                {
                    "first": "Bal\u00e1zs",
                    "middle": [],
                    "last": "Hidasi",
                    "suffix": ""
                },
                {
                    "first": "Alexandros",
                    "middle": [],
                    "last": "Karatzoglou",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 27th ACM International Conference on Information and Knowledge Management",
            "volume": "",
            "issn": "",
            "pages": "843--852",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks",
            "authors": [
                {
                    "first": "Bal\u00e1zs",
                    "middle": [],
                    "last": "Hidasi",
                    "suffix": ""
                },
                {
                    "first": "Alexandros",
                    "middle": [],
                    "last": "Karatzoglou",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1511.06939"
                ]
            }
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Recommending and evaluating choices in a virtual community of use",
            "authors": [
                {
                    "first": "Will",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                },
                {
                    "first": "Larry",
                    "middle": [],
                    "last": "Stead",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [],
                    "last": "Rosenstein",
                    "suffix": ""
                },
                {
                    "first": "George",
                    "middle": [],
                    "last": "Furnas",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems",
            "volume": "",
            "issn": "",
            "pages": "194--201",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Improving neural networks by preventing co-adaptation of feature detectors",
            "authors": [
                {
                    "first": "Nitish",
                    "middle": [],
                    "last": "Geoffrey E Hinton",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "Ruslan R",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1207.0580"
                ]
            }
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Ridge regression: Biased estimation for nonorthogonal problems",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Arthur",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [
                        "W"
                    ],
                    "last": "Hoerl",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kennard",
                    "suffix": ""
                }
            ],
            "year": 1970,
            "venue": "Technometrics",
            "volume": "12",
            "issn": "1",
            "pages": "55--67",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Universal language model fine-tuning for text classification",
            "authors": [
                {
                    "first": "Jeremy",
                    "middle": [],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Ruder",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1801.06146"
                ]
            }
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Pu learning for matrix completion",
            "authors": [
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "Nagarajan",
                    "middle": [],
                    "last": "Natarajan",
                    "suffix": ""
                },
                {
                    "first": "Inderjit",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "2445--2453",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "Collaborative filtering for implicit feedback datasets",
            "authors": [
                {
                    "first": "Yifan",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Yehuda",
                    "middle": [],
                    "last": "Koren",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Volinsky",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on",
            "volume": "",
            "issn": "",
            "pages": "263--272",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Listwise collaborative filtering",
            "authors": [
                {
                    "first": "Shanshan",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Shuaiqiang",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Tie-Yan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Zhumin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Jari",
                    "middle": [],
                    "last": "Veijalainen",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "343--352",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "authors": [
                {
                    "first": "Sergey",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "Christian",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1502.03167"
                ]
            }
        },
        "BIBREF51": {
            "ref_id": "b51",
            "title": "Trustwalker: a random walk model for combining trust-based and item-based recommendation",
            "authors": [
                {
                    "first": "Mohsen",
                    "middle": [],
                    "last": "Jamali",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Ester",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "397--406",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Optimizing search engines using clickthrough data",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Joachims",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "ACM SIGKDD",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF53": {
            "ref_id": "b53",
            "title": "Optimizing search engines using clickthrough data",
            "authors": [
                {
                    "first": "Thorsten",
                    "middle": [],
                    "last": "Joachims",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "133--142",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Fism: factored item similarity models for top-n recommender systems",
            "authors": [
                {
                    "first": "Santosh",
                    "middle": [],
                    "last": "Kabbur",
                    "suffix": ""
                },
                {
                    "first": "Xia",
                    "middle": [],
                    "last": "Ning",
                    "suffix": ""
                },
                {
                    "first": "George",
                    "middle": [],
                    "last": "Karypis",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "659--667",
            "other_ids": {}
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Self-attentive sequential recommendation",
            "authors": [
                {
                    "first": "Wang-Cheng",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Julian",
                    "middle": [],
                    "last": "Mcauley",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1808.09781"
                ]
            }
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "Adam: A method for stochastic optimization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Diederik",
                    "suffix": ""
                },
                {
                    "first": "Jimmy",
                    "middle": [],
                    "last": "Kingma",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ba",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.6980"
                ]
            }
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Semi-supervised classification with graph convolutional networks",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Thomas",
                    "suffix": ""
                },
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Kipf",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1609.02907"
                ]
            }
        },
        "BIBREF58": {
            "ref_id": "b58",
            "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
            "authors": [
                {
                    "first": "Yehuda",
                    "middle": [],
                    "last": "Koren",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "426--434",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Collaborative filtering with temporal dynamics",
            "authors": [
                {
                    "first": "Yehuda",
                    "middle": [],
                    "last": "Koren",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "447--456",
            "other_ids": {}
        },
        "BIBREF60": {
            "ref_id": "b60",
            "title": "Matrix factorization techniques for recommender systems",
            "authors": [
                {
                    "first": "Yehuda",
                    "middle": [],
                    "last": "Koren",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Bell",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Volinsky",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Computer",
            "volume": "",
            "issn": "8",
            "pages": "30--37",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Retargeted matrix factorization for collaborative filtering",
            "authors": [
                {
                    "first": "Oluwasanmi",
                    "middle": [],
                    "last": "Koyejo",
                    "suffix": ""
                },
                {
                    "first": "Sreangsu",
                    "middle": [],
                    "last": "Acharyya",
                    "suffix": ""
                },
                {
                    "first": "Joydeep",
                    "middle": [],
                    "last": "Ghosh",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 7th ACM conference on Recommender systems",
            "volume": "",
            "issn": "",
            "pages": "49--56",
            "other_ids": {}
        },
        "BIBREF62": {
            "ref_id": "b62",
            "title": "Imagenet classification with deep convolutional neural networks",
            "authors": [
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [
                        "E"
                    ],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "1097--1105",
            "other_ids": {}
        },
        "BIBREF63": {
            "ref_id": "b63",
            "title": "A simple weight decay can improve generalization",
            "authors": [
                {
                    "first": "Anders",
                    "middle": [],
                    "last": "Krogh",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [
                        "A"
                    ],
                    "last": "Hertz",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "950--957",
            "other_ids": {}
        },
        "BIBREF64": {
            "ref_id": "b64",
            "title": "Generalization analysis of listwise learning-to-rank algorithms",
            "authors": [
                {
                    "first": "Yanyan",
                    "middle": [],
                    "last": "Lan",
                    "suffix": ""
                },
                {
                    "first": "Tie-Yan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Zhiming",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Hang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "577--584",
            "other_ids": {}
        },
        "BIBREF65": {
            "ref_id": "b65",
            "title": "Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia",
            "authors": [
                {
                    "first": "Jens",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                },
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Isele",
                    "suffix": ""
                },
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Jakob",
                    "suffix": ""
                },
                {
                    "first": "Anja",
                    "middle": [],
                    "last": "Jentzsch",
                    "suffix": ""
                },
                {
                    "first": "Dimitris",
                    "middle": [],
                    "last": "Kontokostas",
                    "suffix": ""
                },
                {
                    "first": "Pablo",
                    "middle": [
                        "N"
                    ],
                    "last": "Mendes",
                    "suffix": ""
                },
                {
                    "first": "Sebastian",
                    "middle": [],
                    "last": "Hellmann",
                    "suffix": ""
                },
                {
                    "first": "Mohamed",
                    "middle": [],
                    "last": "Morsey",
                    "suffix": ""
                },
                {
                    "first": "Patrick",
                    "middle": [],
                    "last": "Van Kleef",
                    "suffix": ""
                },
                {
                    "first": "S\u00f6ren",
                    "middle": [],
                    "last": "Auer",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Semantic Web",
            "volume": "6",
            "issn": "2",
            "pages": "167--195",
            "other_ids": {}
        },
        "BIBREF66": {
            "ref_id": "b66",
            "title": "Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence",
            "authors": [
                {
                    "first": "Dawen",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "Jaan",
                    "middle": [],
                    "last": "Altosaar",
                    "suffix": ""
                },
                {
                    "first": "Laurent",
                    "middle": [],
                    "last": "Charlin",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [
                        "M"
                    ],
                    "last": "Blei",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 10th ACM conference on recommender systems",
            "volume": "",
            "issn": "",
            "pages": "59--66",
            "other_ids": {}
        },
        "BIBREF67": {
            "ref_id": "b67",
            "title": "Stamp: short-term attention/memory priority model for session-based recommendation",
            "authors": [
                {
                    "first": "Qiao",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Yifu",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "Refuoe",
                    "middle": [],
                    "last": "Mokhosi",
                    "suffix": ""
                },
                {
                    "first": "Haibin",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
            "volume": "",
            "issn": "",
            "pages": "1831--1839",
            "other_ids": {}
        },
        "BIBREF68": {
            "ref_id": "b68",
            "title": "Hierarchical gating networks for sequential recommendation",
            "authors": [
                {
                    "first": "Chen",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Peng",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "Xue",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1906.09217"
                ]
            }
        },
        "BIBREF69": {
            "ref_id": "b69",
            "title": "Recommender systems with social regularization",
            "authors": [
                {
                    "first": "Hao",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Dengyong",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Chao",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Irwin",
                    "middle": [],
                    "last": "Michael R Lyu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "King",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the fourth ACM international conference on Web search and data mining",
            "volume": "",
            "issn": "",
            "pages": "287--296",
            "other_ids": {}
        },
        "BIBREF70": {
            "ref_id": "b70",
            "title": "Distributed representations of words and phrases and their compositionality",
            "authors": [
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [
                        "S"
                    ],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3111--3119",
            "other_ids": {}
        },
        "BIBREF71": {
            "ref_id": "b71",
            "title": "Wordnet: a lexical database for english",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "George",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Miller",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Communications of the ACM",
            "volume": "38",
            "issn": "11",
            "pages": "39--41",
            "other_ids": {}
        },
        "BIBREF72": {
            "ref_id": "b72",
            "title": "Probabilistic matrix factorization",
            "authors": [
                {
                    "first": "Andriy",
                    "middle": [],
                    "last": "Mnih",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ruslan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "1257--1264",
            "other_ids": {}
        },
        "BIBREF73": {
            "ref_id": "b73",
            "title": "Geometric matrix completion with recurrent multi-graph neural networks",
            "authors": [
                {
                    "first": "Federico",
                    "middle": [],
                    "last": "Monti",
                    "suffix": ""
                },
                {
                    "first": "Michael",
                    "middle": [],
                    "last": "Bronstein",
                    "suffix": ""
                },
                {
                    "first": "Xavier",
                    "middle": [],
                    "last": "Bresson",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "3697--3707",
            "other_ids": {}
        },
        "BIBREF74": {
            "ref_id": "b74",
            "title": "Simplifying neural networks by soft weight-sharing",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Steven",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [
                        "E"
                    ],
                    "last": "Nowlan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 1992,
            "venue": "Neural computation",
            "volume": "4",
            "issn": "4",
            "pages": "473--493",
            "other_ids": {}
        },
        "BIBREF75": {
            "ref_id": "b75",
            "title": "The pagerank citation ranking: Bringing order to the web",
            "authors": [
                {
                    "first": "Lawrence",
                    "middle": [],
                    "last": "Page",
                    "suffix": ""
                },
                {
                    "first": "Sergey",
                    "middle": [],
                    "last": "Brin",
                    "suffix": ""
                },
                {
                    "first": "Rajeev",
                    "middle": [],
                    "last": "Motwani",
                    "suffix": ""
                },
                {
                    "first": "Terry",
                    "middle": [],
                    "last": "Winograd",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF76": {
            "ref_id": "b76",
            "title": "An efficient algorithm for learning to rank from preference graphs",
            "authors": [
                {
                    "first": "Tapio",
                    "middle": [],
                    "last": "Pahikkala",
                    "suffix": ""
                },
                {
                    "first": "Evgeni",
                    "middle": [],
                    "last": "Tsivtsivadze",
                    "suffix": ""
                },
                {
                    "first": "Antti",
                    "middle": [],
                    "last": "Airola",
                    "suffix": ""
                },
                {
                    "first": "Jouni",
                    "middle": [],
                    "last": "J\u00e4rvinen",
                    "suffix": ""
                },
                {
                    "first": "Jorma",
                    "middle": [],
                    "last": "Boberg",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Machine Learning",
            "volume": "75",
            "issn": "",
            "pages": "129--165",
            "other_ids": {}
        },
        "BIBREF77": {
            "ref_id": "b77",
            "title": "One-class collaborative filtering",
            "authors": [
                {
                    "first": "Rong",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Yunhong",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Bin",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Nathan",
                    "suffix": ""
                },
                {
                    "first": "Rajan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Lukose",
                    "suffix": ""
                },
                {
                    "first": "Qiang",
                    "middle": [],
                    "last": "Scholz",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on",
            "volume": "",
            "issn": "",
            "pages": "502--511",
            "other_ids": {}
        },
        "BIBREF78": {
            "ref_id": "b78",
            "title": "Gbpr: Group preference based bayesian personalized ranking for one-class collaborative filtering",
            "authors": [
                {
                    "first": "Weike",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Li",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "International Joint Conferences on Artificial Intelligence",
            "volume": "13",
            "issn": "",
            "pages": "2691--2697",
            "other_ids": {}
        },
        "BIBREF79": {
            "ref_id": "b79",
            "title": "Transfer learning for behavior ranking",
            "authors": [
                {
                    "first": "Weike",
                    "middle": [],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Qiang",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Yuchao",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "Ben",
                    "middle": [],
                    "last": "Tan",
                    "suffix": ""
                },
                {
                    "first": "Zhong",
                    "middle": [],
                    "last": "Ming",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)",
            "volume": "8",
            "issn": "5",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF80": {
            "ref_id": "b80",
            "title": "Preference completion: Large-scale collaborative ranking from pairwise comparisons",
            "authors": [
                {
                    "first": "Dohyung",
                    "middle": [],
                    "last": "Park",
                    "suffix": ""
                },
                {
                    "first": "Joe",
                    "middle": [],
                    "last": "Neeman",
                    "suffix": ""
                },
                {
                    "first": "Jin",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Sujay",
                    "middle": [],
                    "last": "Sanghavi",
                    "suffix": ""
                },
                {
                    "first": "Inderjit",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1907--1916",
            "other_ids": {}
        },
        "BIBREF81": {
            "ref_id": "b81",
            "title": "On the difficulty of training recurrent neural networks",
            "authors": [
                {
                    "first": "Razvan",
                    "middle": [],
                    "last": "Pascanu",
                    "suffix": ""
                },
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1310--1318",
            "other_ids": {}
        },
        "BIBREF82": {
            "ref_id": "b82",
            "title": "Glove: Global vectors for word representation",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF83": {
            "ref_id": "b83",
            "title": "Deepwalk: Online learning of social representations",
            "authors": [
                {
                    "first": "Bryan",
                    "middle": [],
                    "last": "Perozzi",
                    "suffix": ""
                },
                {
                    "first": "Rami",
                    "middle": [],
                    "last": "Al-Rfou",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Skiena",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "701--710",
            "other_ids": {}
        },
        "BIBREF84": {
            "ref_id": "b84",
            "title": "A call for clarity in reporting BLEU scores",
            "authors": [
                {
                    "first": "Matt",
                    "middle": [],
                    "last": "Post",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers",
            "volume": "",
            "issn": "",
            "pages": "186--191",
            "other_ids": {}
        },
        "BIBREF85": {
            "ref_id": "b85",
            "title": "An item/user representation for recommender systems based on bloom filters",
            "authors": [
                {
                    "first": "Manuel",
                    "middle": [],
                    "last": "Pozo",
                    "suffix": ""
                },
                {
                    "first": "Raja",
                    "middle": [],
                    "last": "Chiky",
                    "suffix": ""
                },
                {
                    "first": "Farid",
                    "middle": [],
                    "last": "Meziane",
                    "suffix": ""
                },
                {
                    "first": "Elisabeth",
                    "middle": [],
                    "last": "M\u00e9tais",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS)",
            "volume": "",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF86": {
            "ref_id": "b86",
            "title": "Improving language understanding by generative pre-training",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Karthik",
                    "middle": [],
                    "last": "Narasimhan",
                    "suffix": ""
                },
                {
                    "first": "Tim",
                    "middle": [],
                    "last": "Salimans",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF87": {
            "ref_id": "b87",
            "title": "Language models are unsupervised multitask learners",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Radford",
                    "suffix": ""
                },
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Rewon",
                    "middle": [],
                    "last": "Child",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Luan",
                    "suffix": ""
                },
                {
                    "first": "Dario",
                    "middle": [],
                    "last": "Amodei",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF88": {
            "ref_id": "b88",
            "title": "Collaborative filtering with graph information: Consistency and scalable methods",
            "authors": [
                {
                    "first": "Nikhil",
                    "middle": [],
                    "last": "Rao",
                    "suffix": ""
                },
                {
                    "first": "Hsiang-Fu",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Pradeep",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Ravikumar",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "2107--2115",
            "other_ids": {}
        },
        "BIBREF89": {
            "ref_id": "b89",
            "title": "Factorization machines",
            "authors": [
                {
                    "first": "Steffen",
                    "middle": [],
                    "last": "Rendle",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEEE 10th International Conference on",
            "volume": "",
            "issn": "",
            "pages": "995--1000",
            "other_ids": {}
        },
        "BIBREF90": {
            "ref_id": "b90",
            "title": "Bpr: Bayesian personalized ranking from implicit feedback",
            "authors": [
                {
                    "first": "Steffen",
                    "middle": [],
                    "last": "Rendle",
                    "suffix": ""
                },
                {
                    "first": "Christoph",
                    "middle": [],
                    "last": "Freudenthaler",
                    "suffix": ""
                },
                {
                    "first": "Zeno",
                    "middle": [],
                    "last": "Gantner",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "Schmidt-Thieme",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence",
            "volume": "",
            "issn": "",
            "pages": "452--461",
            "other_ids": {}
        },
        "BIBREF91": {
            "ref_id": "b91",
            "title": "Factorizing personalized markov chains for next-basket recommendation",
            "authors": [
                {
                    "first": "Steffen",
                    "middle": [],
                    "last": "Rendle",
                    "suffix": ""
                },
                {
                    "first": "Christoph",
                    "middle": [],
                    "last": "Freudenthaler",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [],
                    "last": "Schmidt-Thieme",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 19th international conference on World wide web",
            "volume": "",
            "issn": "",
            "pages": "811--820",
            "other_ids": {}
        },
        "BIBREF92": {
            "ref_id": "b92",
            "title": "Restricted boltzmann machines for collaborative filtering",
            "authors": [
                {
                    "first": "Ruslan",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                },
                {
                    "first": "Andriy",
                    "middle": [],
                    "last": "Mnih",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 24th international conference on Machine learning",
            "volume": "",
            "issn": "",
            "pages": "791--798",
            "other_ids": {}
        },
        "BIBREF93": {
            "ref_id": "b93",
            "title": "Item-based collaborative filtering recommendation algorithms",
            "authors": [
                {
                    "first": "George",
                    "middle": [],
                    "last": "Badrul Munir Sarwar",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Karypis",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Joseph",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Konstan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Riedl",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "World Wide Web Conference",
            "volume": "1",
            "issn": "",
            "pages": "285--295",
            "other_ids": {}
        },
        "BIBREF94": {
            "ref_id": "b94",
            "title": "Collaborative filtering recommender systems",
            "authors": [
                {
                    "first": "Ben",
                    "middle": [],
                    "last": "Schafer",
                    "suffix": ""
                },
                {
                    "first": "Dan",
                    "middle": [],
                    "last": "Frankowski",
                    "suffix": ""
                },
                {
                    "first": "Jon",
                    "middle": [],
                    "last": "Herlocker",
                    "suffix": ""
                },
                {
                    "first": "Shilad",
                    "middle": [],
                    "last": "Sen",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "The adaptive web",
            "volume": "",
            "issn": "",
            "pages": "291--324",
            "other_ids": {}
        },
        "BIBREF95": {
            "ref_id": "b95",
            "title": "Getting deep recommenders fit: Bloom embeddings for sparse binary input/output networks",
            "authors": [
                {
                    "first": "Joan",
                    "middle": [],
                    "last": "Serr\u00e0",
                    "suffix": ""
                },
                {
                    "first": "Alexandros",
                    "middle": [],
                    "last": "Karatzoglou",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the Eleventh ACM Conference on Recommender Systems",
            "volume": "",
            "issn": "",
            "pages": "279--287",
            "other_ids": {}
        },
        "BIBREF96": {
            "ref_id": "b96",
            "title": "Gossip algorithms. Foundations and Trends\u00ae in Networking",
            "authors": [
                {
                    "first": "Devavrat",
                    "middle": [],
                    "last": "Shah",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "3",
            "issn": "",
            "pages": "1--125",
            "other_ids": {}
        },
        "BIBREF97": {
            "ref_id": "b97",
            "title": "Mining recommendations from the web",
            "authors": [
                {
                    "first": "Guy",
                    "middle": [],
                    "last": "Shani",
                    "suffix": ""
                },
                {
                    "first": "Max",
                    "middle": [],
                    "last": "Chickering",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Meek",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 2008 ACM conference on Recommender systems",
            "volume": "",
            "issn": "",
            "pages": "35--42",
            "other_ids": {}
        },
        "BIBREF98": {
            "ref_id": "b98",
            "title": "Hash kernels for structured data",
            "authors": [
                {
                    "first": "Qinfeng",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Petterson",
                    "suffix": ""
                },
                {
                    "first": "Gideon",
                    "middle": [],
                    "last": "Dror",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Langford",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                },
                {
                    "first": "Svn",
                    "middle": [],
                    "last": "Vishwanathan",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Journal of Machine Learning Research",
            "volume": "10",
            "issn": "",
            "pages": "2615--2637",
            "other_ids": {}
        },
        "BIBREF99": {
            "ref_id": "b99",
            "title": "List-wise learning to rank with matrix factorization for collaborative filtering",
            "authors": [
                {
                    "first": "Yue",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "Martha",
                    "middle": [],
                    "last": "Larson",
                    "suffix": ""
                },
                {
                    "first": "Alan",
                    "middle": [],
                    "last": "Hanjalic",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the fourth ACM conference on Recommender systems",
            "volume": "",
            "issn": "",
            "pages": "269--272",
            "other_ids": {}
        },
        "BIBREF100": {
            "ref_id": "b100",
            "title": "User based collaborative filtering using bloom filter with mapreduce",
            "authors": [
                {
                    "first": "Anita",
                    "middle": [],
                    "last": "Shinde",
                    "suffix": ""
                },
                {
                    "first": "Ila",
                    "middle": [],
                    "last": "Savant",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of International Conference on ICT for Sustainable Development",
            "volume": "",
            "issn": "",
            "pages": "115--123",
            "other_ids": {}
        },
        "BIBREF101": {
            "ref_id": "b101",
            "title": "Goaldirected inductive matrix completion",
            "authors": [
                {
                    "first": "Si",
                    "middle": [],
                    "last": "Si",
                    "suffix": ""
                },
                {
                    "first": "Kai-Yang",
                    "middle": [],
                    "last": "Chiang",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "Nikhil",
                    "middle": [],
                    "last": "Rao",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF102": {
            "ref_id": "b102",
            "title": "Relational learning via collective matrix factorization",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ajit",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [
                        "J"
                    ],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining",
            "volume": "",
            "issn": "",
            "pages": "650--658",
            "other_ids": {}
        },
        "BIBREF103": {
            "ref_id": "b103",
            "title": "Maximum-margin matrix factorization",
            "authors": [
                {
                    "first": "Nathan",
                    "middle": [],
                    "last": "Srebro",
                    "suffix": ""
                },
                {
                    "first": "Jason",
                    "middle": [],
                    "last": "Rennie",
                    "suffix": ""
                },
                {
                    "first": "Tommi",
                    "middle": [
                        "S"
                    ],
                    "last": "Jaakkola",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "1329--1336",
            "other_ids": {}
        },
        "BIBREF104": {
            "ref_id": "b104",
            "title": "Maximum-margin matrix factorization",
            "authors": [
                {
                    "first": "Nathan",
                    "middle": [],
                    "last": "Srebro",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Jason",
                    "suffix": ""
                },
                {
                    "first": "Tommi",
                    "middle": [
                        "S"
                    ],
                    "last": "Rennie",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Jaakkola",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "17",
            "issn": "",
            "pages": "1329--1336",
            "other_ids": {}
        },
        "BIBREF105": {
            "ref_id": "b105",
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "authors": [
                {
                    "first": "Nitish",
                    "middle": [],
                    "last": "Srivastava",
                    "suffix": ""
                },
                {
                    "first": "Geoffrey",
                    "middle": [],
                    "last": "Hinton",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Krizhevsky",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Ruslan",
                    "middle": [],
                    "last": "Salakhutdinov",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "The Journal of Machine Learning Research",
            "volume": "15",
            "issn": "1",
            "pages": "1929--1958",
            "other_ids": {}
        },
        "BIBREF106": {
            "ref_id": "b106",
            "title": "Rethinking the inception architecture for computer vision",
            "authors": [
                {
                    "first": "Christian",
                    "middle": [],
                    "last": "Szegedy",
                    "suffix": ""
                },
                {
                    "first": "Vincent",
                    "middle": [],
                    "last": "Vanhoucke",
                    "suffix": ""
                },
                {
                    "first": "Sergey",
                    "middle": [],
                    "last": "Ioffe",
                    "suffix": ""
                },
                {
                    "first": "Jon",
                    "middle": [],
                    "last": "Shlens",
                    "suffix": ""
                },
                {
                    "first": "Zbigniew",
                    "middle": [],
                    "last": "Wojna",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "2818--2826",
            "other_ids": {}
        },
        "BIBREF107": {
            "ref_id": "b107",
            "title": "Personalized top-n sequential recommendation via convolutional sequence embedding",
            "authors": [
                {
                    "first": "Jiaxi",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Ke",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "565--573",
            "other_ids": {}
        },
        "BIBREF108": {
            "ref_id": "b108",
            "title": "Regression shrinkage and selection via the lasso",
            "authors": [
                {
                    "first": "Robert",
                    "middle": [],
                    "last": "Tibshirani",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological)",
            "volume": "",
            "issn": "",
            "pages": "267--288",
            "other_ids": {}
        },
        "BIBREF109": {
            "ref_id": "b109",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "Ashish",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "Noam",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "Niki",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "Llion",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "Aidan",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "Lukasz",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "Illia",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF110": {
            "ref_id": "b110",
            "title": "Collaborative ranking with 17 parameters",
            "authors": [
                {
                    "first": "Maksims",
                    "middle": [],
                    "last": "Volkovs",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Richard",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Zemel",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2294--2302",
            "other_ids": {}
        },
        "BIBREF111": {
            "ref_id": "b111",
            "title": "Irgan: A minimax game for unifying generative and discriminative information retrieval models",
            "authors": [
                {
                    "first": "Jun",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Lantao",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Weinan",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Yu",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "Yinghui",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "Benyou",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Peng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Dell",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "515--524",
            "other_ids": {}
        },
        "BIBREF112": {
            "ref_id": "b112",
            "title": "Cofi rank-maximum margin matrix factorization for collaborative ranking",
            "authors": [
                {
                    "first": "Markus",
                    "middle": [],
                    "last": "Weimer",
                    "suffix": ""
                },
                {
                    "first": "Alexandros",
                    "middle": [],
                    "last": "Karatzoglou",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Quoc",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [
                        "J"
                    ],
                    "last": "Le",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "1593--1600",
            "other_ids": {}
        },
        "BIBREF113": {
            "ref_id": "b113",
            "title": "Maximum margin matrix factorization for collaborative ranking",
            "authors": [
                {
                    "first": "Markus",
                    "middle": [],
                    "last": "Weimer",
                    "suffix": ""
                },
                {
                    "first": "Alexandros",
                    "middle": [],
                    "last": "Karatzoglou",
                    "suffix": ""
                },
                {
                    "first": "Quoc",
                    "middle": [],
                    "last": "Viet Le",
                    "suffix": ""
                },
                {
                    "first": "Alex",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF114": {
            "ref_id": "b114",
            "title": "Large-scale collaborative ranking in near-linear time",
            "authors": [
                {
                    "first": "Liwei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Sharpnack",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "515--524",
            "other_ids": {}
        },
        "BIBREF115": {
            "ref_id": "b115",
            "title": "Sql-rank: A listwise approach to collaborative ranking",
            "authors": [
                {
                    "first": "Liwei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Sharpnack",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of Machine Learning Research (35th International Conference on Machine Learning)",
            "volume": "80",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF116": {
            "ref_id": "b116",
            "title": "Stochastic shared embeddings: Data-driven regularization of embedding layers",
            "authors": [
                {
                    "first": "Liwei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Shuqing",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Sharpnack",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.10630"
                ]
            }
        },
        "BIBREF117": {
            "ref_id": "b117",
            "title": "Temporal collaborative ranking via personalized transformer",
            "authors": [
                {
                    "first": "Liwei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Shuqing",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Sharpnack",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1908.05435"
                ]
            }
        },
        "BIBREF118": {
            "ref_id": "b118",
            "title": "Graph dna: Deep neighborhood aware graph encoding for collaborative filtering",
            "authors": [
                {
                    "first": "Liwei",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Hsiang-Fu",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Nikhil",
                    "middle": [],
                    "last": "Rao",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Sharpnack",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1905.12217"
                ]
            }
        },
        "BIBREF119": {
            "ref_id": "b119",
            "title": "Collaborative denoising auto-encoders for top-n recommender systems",
            "authors": [
                {
                    "first": "Yao",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Christopher",
                    "middle": [],
                    "last": "Dubois",
                    "suffix": ""
                },
                {
                    "first": "Alice",
                    "middle": [
                        "X"
                    ],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Martin",
                    "middle": [],
                    "last": "Ester",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the Ninth ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "153--162",
            "other_ids": {}
        },
        "BIBREF120": {
            "ref_id": "b120",
            "title": "Listwise approach to learning to rank: theory and algorithm",
            "authors": [
                {
                    "first": "Fen",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "Tie-Yan",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Jue",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Wensheng",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Hang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 25th international conference on Machine learning",
            "volume": "",
            "issn": "",
            "pages": "1192--1199",
            "other_ids": {}
        },
        "BIBREF121": {
            "ref_id": "b121",
            "title": "Edge-weighted personalized pagerank: breaking a decade-old performance barrier",
            "authors": [
                {
                    "first": "Wenlei",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Bindel",
                    "suffix": ""
                },
                {
                    "first": "Alan",
                    "middle": [],
                    "last": "Demers",
                    "suffix": ""
                },
                {
                    "first": "Johannes",
                    "middle": [],
                    "last": "Gehrke",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "1325--1334",
            "other_ids": {}
        },
        "BIBREF122": {
            "ref_id": "b122",
            "title": "Graph convolutional neural networks for web-scale recommender systems",
            "authors": [
                {
                    "first": "Rex",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "Ruining",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Kaifeng",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Pong",
                    "middle": [],
                    "last": "Eksombatchai",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "William",
                    "suffix": ""
                },
                {
                    "first": "Jure",
                    "middle": [],
                    "last": "Hamilton",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
            "volume": "",
            "issn": "",
            "pages": "974--983",
            "other_ids": {}
        },
        "BIBREF123": {
            "ref_id": "b123",
            "title": "Selection of negative samples for one-class matrix factorization",
            "authors": [
                {
                    "first": "Hsiang-Fu",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Mikhail",
                    "middle": [],
                    "last": "Bilenko",
                    "suffix": ""
                },
                {
                    "first": "Chih-Jen",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 SIAM International Conference on Data Mining",
            "volume": "",
            "issn": "",
            "pages": "363--371",
            "other_ids": {}
        },
        "BIBREF124": {
            "ref_id": "b124",
            "title": "A scalable asynchronous distributed algorithm for topic modeling",
            "authors": [
                {
                    "first": "Hsiang-Fu",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Cho-Jui",
                    "middle": [],
                    "last": "Hsieh",
                    "suffix": ""
                },
                {
                    "first": "Hyokun",
                    "middle": [],
                    "last": "Yun",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Svn Vishwanathan",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 24th International Conference on World Wide Web",
            "volume": "",
            "issn": "",
            "pages": "1340--1350",
            "other_ids": {}
        },
        "BIBREF125": {
            "ref_id": "b125",
            "title": "A unified algorithm for one-class structured matrix factorization with side information",
            "authors": [
                {
                    "first": "Hsiang-Fu",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Hsin-Yuan",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Inderjit",
                    "suffix": ""
                },
                {
                    "first": "Chih-Jen",
                    "middle": [],
                    "last": "Dhillon",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "AAAI",
            "volume": "",
            "issn": "",
            "pages": "2845--2851",
            "other_ids": {}
        },
        "BIBREF126": {
            "ref_id": "b126",
            "title": "Social computing data repository at ASU",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zafarani",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF127": {
            "ref_id": "b127",
            "title": "Kernelized probabilistic matrix factorization: Exploiting graphs and side information",
            "authors": [
                {
                    "first": "Tinghui",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Hanhuai",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "Arindam",
                    "middle": [],
                    "last": "Banerjee",
                    "suffix": ""
                },
                {
                    "first": "Guillermo",
                    "middle": [],
                    "last": "Sapiro",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 2012 SIAM international Conference on Data mining",
            "volume": "",
            "issn": "",
            "pages": "403--414",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . of Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . 97 7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . to Chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 8.1.1 Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 8.1.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 8.1.3 Graph Regularized Weighted Matrix Factorization for Implicit feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 8.1.4 Reproducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 8.1.5 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 8.2 Appendix to Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 8.3 Appendix to Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 8.3.1 Neural Networks with One Hidden Layer . . . . . . . . . . . . . . 107 8.3.2 Neural Machine Translation . . . . . . . . . . . . . . . . . . . . . 110 8.3.3 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 8.3.4 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 8.4 Appendix to Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 List of Figures 2.1 Illustration of Algorithm 1: the graph DNA encoding procedure. The curly brackets at each node indicate the nodes encoded at a particular step. At d = 0 each node's Bloom filter only encodes itself, and multi-hop neighbors are included as d increases. . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2 Illustration of our proposed DNA encoding method (DNA-3), with the corresponding bipartite graph representation. . . . . . . . . . . . . . . . . 15 2.3 Compare Training Speed of GRMF, with and without Graph DNA. . . . 22 3.1 Comparing Primal-CR, Primal-CR++ and Collrank, MovieLens1m data, 200 ratings/user, rank 100, lambda = 5000 . . . . . . . . . . . . . . . . . 41 3.2 Comparing Primal-CR, Primal-CR++ and Collrank, MovieLens10m data, 500 ratings/user, rank 100, lambda = 7000 . . . . . . . . . . . . . . . . 42 3.3 Comparing Primal-CR, Primal-CR++ and Collrank, Netflix data, 200 ratings/user, rank 100, lambda = 10000 . . . . . . . . . . . . . . . . . . 42 3.4 Comparing parallel version of Primal-CR and Collrank, MovieLens10m data, 500 ratings/user, rank 100, lambda = 7000 . . . . . . . . . . . . . . 43 3.5 Speedup of Primal-CR, MovieLens10m data, 500 ratings/user, rank 100, lambda = 7000 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.6 Varies number of ratings per user in training data, MovieLens1m data, rank 100 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.7 Varies number of ratings per user in training data, Netflix data, rank 100 47 4.1 Demonstration of Stochastic Queuing Process-the rating matrix R (left) generates multiple possible rankings \u03a0's (right), \u03a0 \u2208 S(R, \u2126) by breaking ties randomly. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 4.2 Training time of implicit feedback methods. . . . . . . . . . . . . . . many such neural networks at the same time. . . . . . 71 5.2 Illustration of how SSE-Graph algorithm in Figure 5.1 works for a simple neural network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 5.3 Projecting 50-dimensional embeddings obtained by training a simple neural network without SSE (Left), and with SSE-Graph (Center) , SSE-SE (Right) into 3D space using PCA. . . . . . . . . . . . . . . . . . . . . . . . . . . 74 5.4 Compare Training Speed of Simple Neural Networks with One Hidden Layer, i.e. MF and BPR, with and without SSE-SE. . . . . . . . . . . . 82 6.1 Illustration of our proposed SSE-PT model . . . . . . . . . . . . . . . . . 87 6.2 Illustration of how SASRec (Left) and SSE-PT (Right) differs on utilizing the Engagement History of A Random User in Movielens1M Dataset. . . 93 6.3 Illustration of the speed of SSE-PT . . . . . . . . . . . . . . . . . . . . . 95 8.1 Compare Training Speed of GRMF, with and without Graph DNA. . . . . 101 8.2 Comparing implicit feedback methods. . . . . . . . . . . . . . . . . . . . 110 8.3 Effectiveness of Stochastic Queuing Process. . . . . . . . . . . . . . . . . . 111 8.4 Effectiveness of using full lists. . . . . . . . . . . . . . . . . . . . . . . . . 112 8.5 Simulation of a bound on \u03c1 L,n for the movielens1M dataset. Throughout the simulation, L is replaced with (which will bound \u03c1 L,n by Jensen's inequality). The SSE probability parameter dictates the probability of transitioning. When this is 0 (box plot on the right), the distribution is that of the samples from the standard Rademacher complexity (without the sup and expectation). As we increase the transition probability, the values for \u03c1 L,n get smaller. . . . . . . . . . . . . . . . . . . . . . . . . . 114",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "p s is the SSE probability for both user and item embedding tables and p d is the dropout probability. . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.3 SSE-SE outperforms dropout for Neural Networks with One Hidden Layer such as Bayesian Personalized Ranking Algorithm regardless of dimensionality we use. We report the metric precision for top k recommendations as P @k. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.4 SSE-SE has two tuning parameters: probability p x to replace embeddings associated with input x i and probability p y to replace embeddings associated with output y i . We use the dropout probability of 0.1, weight decay of 1e \u22125 , and learning rate of 1e \u22123 for all experiments. . . . . . . . . . . . . . 79 5.5 Our proposed SSE-SE helps the Transformer achieve better BLEU scores on English-to-German in 10 out of 11 newstest data between 2008 and 2018. 80 5.6 Our proposed SSE-SE applied in the pre-training stage on our crawled IMDB data improves the generalization ability of pre-trained IMDB model and helps the BERT-Base model outperform current SOTA results on the IMDB Sentiment Task after fine-tuning. . . . . . . . . . . . . . . . . . . . 81 5.7 SSE-SE pre-trained BERT-Base models on IMDB datasets turn out working better on the new unseen SST-2 Task as well. . . . . . . . . . . . . . . . 82 6.1 Comparing various state-of-the-art temporal collaborative ranking algorithms on various datasets. The (A) to (E) are non-deep-learning methods, the (F) to (K) are deep-learning methods and the (L) to (O) are our vari-",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": ", so there is no need for SSE-PT++. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 6.2 Comparing SASRec, SSE-PT and SSE-PT++ on Movielens1M Dataset while varying the maximum length allowed and dimension of embeddings. 92 6.3 Comparing Different Regularizations for SSE-PT on Movielen1M Dataset. NO REG stands for no regularization. PS stands for parameter sharing across all users while PS(AGE) means PS is used within each age group. SASRec is added to last row after all SSE-PT results as a baseline. . . . 92 8.1 Compare Bloom filters of different depths and sizes an on Synthesis Dataset. Note that the number of bits of Bloom filter is decided by Bloom filter's maximum capacity and tolerable error rate (i.e. false positive error, we use 0.2 as default). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 8.2 Compare nnz of different methods on Douban and Flixster datasets. GRMF G 4 and GRMF DNA-2 are using the same 4-hop information in the graph but in different ways. Note that we do not exclude potential overlapping among columns. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 8.3 Compare Matrix Factorization for Explicit Feedback on Synthesis Dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Compare Matrix Factorization methods for Explicit Feedback on Douban and Flixster data. We use rank r = 10. . . . . . . . . . . . . . . . . . . . 106 8.5 Compare Co-factor Methods for Explicit Feedback on Douban and Flixster Datasets. We use rank r = 10 for both methods. . . . . . . . . . . . . . . 107 8.6 Compare Weighted Matrix Factorization with Graph for Implicit Feedback on Douban and Flixster Datasets. We use rank r = 10 for both methods and all metric results are in %. . . . . . . . . . . . . . . . . . . . . . . . 107 8.7 Description of Datasets Used in Evaluations. . . . . . . . . . . . . . . . . 118 8.8 Comparing our SSE-PT, SSE-PT++ with SASRec on Movielen1M dataset. We use number of negatives C = 100, dropout probability of 0.2 and learning rate of 1e \u22123 for all experiments while varying others. p u , p i , p u are SSE probabilities for user embedding, input item embedding and output item embedding respectively. . . . . . . . . . . . . . . . . . . . . . . . . . 119 8.9 Comparing our SSE-PT with SASRec on Movielens10M dataset. Unlike Table 8.8, we use the number of negatives C = 500 instead of 100 as C = 100 is too easy for this dataset and it gets too difficult to tell the differences between different methods: Hit Ratio@10 approaches 1. . . . . 120 8.10 Comparing Different SSE probability for user embeddings for SSE-PT on Movielens1M Dataset. Embedding hidden units of 50 for users and 100 for items, attention blocks of 2, SSE probability of 0.01 for item embeddings, dropout probability of 0.2 and max length of 200 are used. . . . . . . . . . 121 8.11 Comparing Different Sampling Probability, p s , of SSE-PT++ on Movie-lens1M Dataset. Hyper-parameters the same as Table 8.10, except that the max length T allowed is set 100 instead of 200 to show effects of sampling sequences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 8.12 Comparing Different Number of Blocks for SSE-PT while Keeping The Rest Fixed on Movielens1M and Movielens10M Datasets. . . . . . . . . . 122 8.13 Varying number of negatives C in evaluation on Movielens1M dataset. Other hyper-parameters are fixed for a fair comparison. . . . . . . . . . 122",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "up the algorithm to near-linear time complexity; chapter 4 is on the new listwise approach for collaborative ranking and how the listwise approach is a better choice of loss for both explicit and implicit feedback over pointwise and pairwise loss; chapter 5 is about the new regularization technique Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it is both theoretically sound and empirically effectively for 6 different tasks across recommendation and natural language processing; chapter 6 is how we introduce personalization for the state-of-the-art sequential recommendation model with the help of SSE, which plays an important role in preventing our personalized model from overfitting to the training data; chapter 7, we summarize what we have achieved so far and predict what the future directions can be; chapter 8 is the appendix to all the chapters.Acknowledgments Being a PhD student at the Statistics Department of the University of California, Davis is an extremely fun experience. Choosing Davis among many other schools back in 2014 is definitely one of the best decisions I have ever made in my life. Choosing my two advisors is another great decision I made in my life: at Davis, I was fortunate to meet my 2 wonderful advisors Cho (who has now moved to UCLA) and James, who have both become fathers in recent years. It is an interesting fact that they both came to Davis one year later than me, so technically one can argue I am more senior at Davis than my advisors. During my study, I was given a lot of freedom to follow my heart and passion to explore diverse directions that I am most curious about and work on my own projects with lots of encouragements, and appropriate level of guidance whenever I need it. I have enjoyed every single bit of the past five and half years of my study. In the process, I was fortunate to obtain 2 master degrees in Statistics and Computer Science in 2016 and 2018 respectively. I got to take or sit-in courses from many wonderful professors from both departments, including Ethan, Alexander, Prabir, Fushing, Jiming, Thomas, Hans, Debashis, James, Jie, Wolfgang, Duncan, and Jane-Ling from Statistics Department, Cho, Ian, Vladimir, Yong Jae, Yu, Matthew, Nitta, Sam from Computer Science Department. I was blessed to do 4 distinct internships at 4 different companies, namely AT&T Labs at San Ramon, Facebook at Melno Park and LinkedIn at Mountain View, and Dataminr at midtown Manhattan, during which I have encountered many helpful and inspirational people, especially my colleagues, mentors and supervisors. Moreover, I got the precious opportunity to present my work at top conferences including KDD'17 at Halifax, ICML'18 at Stockholm and NeurIPS'19 at Vancouver and attend different conferences around the world. I also got to teach a lot at Davis: I would have taught as a teaching assistant for 16 quarters.I'm truly grateful for the professors that wrote me strong recommendation letters while I was in college. Without the help and encouragements, my journey of PhD study would not have started. Thank you, Betsy, Jun, Rick and Xiang. It is an interesting fact that except Rick has retired, all the rest have now got tenure. Congratulations and how time flies! I also want to thank my collaborators including Hsiang-Fu and Nikhil from Amazon, Mahdi from Rutgers, Shengli, Joel and Alex from Dataminr, and of course most importantly my girlfriend, collaborator and best friend Shuqing. It has been a great pleasure to work with Shuqing, travel together to different conferences, and present our works. She also helped to proof-read the paper. Her accompany during past few years made my PhD study enjoyable. Actually SQL-Rank algorithm presented in chapter 4 is also named after her. Last but not the least, I want to mourn for those over 1000 people who have passed away because of the novel coronavirus outbreak in China (including Dr Wenliang Li and other heroes I don't know their names), and pray for families still stuck in Wuhan, including my parents and grandmother. Born and raised up in Wuhan till age of 18, I sincerely wish we can recover from coronavirus soon together as humankind in 2020. I cannot imagine how much pain those who lost their parents, partner and loved ones have been bearing. Maybe this reminds us to spend every day just like the last day on earth.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Space efficiency: classic Bloom filters use 1.44 log 2 (1/ ) of space per inserted key, where is the false positive rate associated with this Bloom filter. 2. Support for the union operation of two Bloom filters: the Bloom filter for the union of two sets can be obtained by performing bitwise 'OR' operations on the underlying bit-arrays of the two Bloom filters. 3. Size of the Bloom filter can be approximated by the number of nonzeros in the underlying bit array: in particular, given a Bloom filter representation B(A) of a set A: the number of elements of A can be estimated as |A| \u2248 \u2212 c k log 1 \u2212 nnz(b) c , where nnz(b) is the number of non-zero elements in array b. As a result, the number of common nonzero bits of B(A 1 ) and B(A 2 ) can be used as a proxy for |A 1 \u2229 A 2 |. Algorithm 1. Graph DNA Encoding with Bloom Filters Input: G: a graph of n nodes, c: the length of codes, k: the number of hash functions, d: the number of iterations, \u03b8: tuning parameter to control the number of elements hashed. Output: B \u2208 {0, 1} n\u00d7c : a boolean matrix to denote the bipartite relationship between n nodes and c bits. \u2022 H \u2190 {h t (\u00b7) : t = 1, . . . , k} Pick k hash functions \u2022 for i = 1, . . . , n: GraphBloom Initialization -B 0 [i] \u2190 BloomFilter(c, H) -B 0 [i].add(i) \u2022 for s = 1, . . . , d: d times neighborhood propagations -for i = 1, . . . , n: * for j \u2208 N 1 (i): degree-1 neighbors",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Our usage of Bloom filters is very different from previous works in[86,96,101], which use Bloom filter for standard hashing and is unrelated to graph encoding.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Illustration of Algorithm 1: the graph DNA encoding procedure. The curly brackets at each node indicate the nodes encoded at a particular step. At d = 0 each node's Bloom filter only encodes itself, and multi-hop neighbors are included as d increases.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Illustration of our proposed DNA encoding method (DNA-3), with the corresponding bipartite graph representation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Compare Training Speed of GRMF, with and without Graph DNA.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "lem(3.4), and both of them significantly reduce the time complexity over existing methods.If the input file is in the form of |\u2126| pairwise comparisons, our proposed algorithm, Primal-CR, can reduce the time and space complexity from O(|\u2126|r) to O(|\u2126| + d 1d2 r), wher\u0113 d 2 is the average number of items compared by one user. If the input data is given as user-item ratings (e.g., Netflix, Yahoo-Music), the complexity is reduced from",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Truncated Newton Update for V (same procedure can be used for updating U ) Input: Current solution U, V Output: V 1: Compute g = vec(\u2207f (V )) 2: Let H = \u2207 2 f (V ) (do not explicitly compute H) 3: procedure Linear Conjugate Gradient(g,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "(i) := {j | \u2203k s.t. (i, j, k) \u2208 \u2126} and t j is some coefficient computed by summing over all the pairs in \u2126 i . If we have t j , the overall gradient can be computed by O(d 2 (i)r)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "Primal-CR: efficient way to compute \u2207f (V )",
            "latex": null,
            "type": "figure"
        },
        "FIGREF15": {
            "text": "+= t[j] \u00b7 u i 13: g = vec(g + \u03bbV ) vectorize matrix g \u2208 R r\u00d7d 2 14: Form a sparse matrix m = [m 1 . . . m d 1 ] m can be reused later 15: return g, m derivations. From the gradient derivation, we have",
            "latex": null,
            "type": "figure"
        },
        "FIGREF16": {
            "text": "time for solving each subproblem with respect to u i , so the overall complexity is O(|\u2126| + rd 1d2 ) time per iteration.Summary of time and space complexity When updating V , we first compute gradient by Algorithm 4, which takes O(|\u2126| + d 1d2 r) time, and each Hessian-vector product in 5 also takes the same time. The updates for U takes the same time complexity with updating V , so the overall time complexity is O(|\u2126| + d 1d2 r) per iteration. The whole",
            "latex": null,
            "type": "figure"
        },
        "FIGREF18": {
            "text": "Primal-CR++: compute gradient part for f (V )",
            "latex": null,
            "type": "figure"
        },
        "FIGREF19": {
            "text": "Initialize s[1], . . . , s[L] and c[1], . . . , c[L]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF20": {
            "text": "and the question is: Can we have an algorithm with near-linear time with respect to number of observed ratings |\u03a9| = d 1d2 ? We answer this question in the affirmative by proposing Primal-CR++, a near-linear time algorithm for solving problem (3.4) with L2-hinge loss.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF21": {
            "text": "Assume the indexes ind 2 (i) are sorted by the the ascending order of m j . Then we can scan from left to right, and maintain the current accumulated sum s 1 , ..., s L and the current index counts c 1 , ..., c L for each rating level. If the current pointer is p, then s [p] = j:m j \u2264p,R ij = m j and c [p] = |{j : m j \u2264 p, R ij = }|.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF22": {
            "text": "Comparing Primal-CR, Primal-CR++ and Collrank, MovieLens1m data, 200 ratings/user, rank 100, lambda = 5000",
            "latex": null,
            "type": "figure"
        },
        "FIGREF23": {
            "text": "Comparing Primal-CR, Primal-CR++ and Collrank, MovieLens10m data, 500 ratings/user, rank 100, lambda = Comparing Primal-CR, Primal-CR++ and Collrank, Netflix data, 200 ratings/user, rank 100, lambda = 10000 decomposable to d 1 independent problems, see eq (3.17). Due to the page limit we omit the details here; interesting readers can check our code on github.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF24": {
            "text": "Comparing parallel version of Primal-CR and Collrank, MovieLens10m data, 500 ratings/user, rank 100, lambda = Speedup of Primal-CR, MovieLens10m data, 500 ratings/user, rank 100, lambda = 7000 them up in the end. We show in section 3.5.2 that our parallel version of the proposed new algorithm works better than the paralleled version of Collrank algorithm[81].",
            "latex": null,
            "type": "figure"
        },
        "FIGREF25": {
            "text": "",
            "latex": null,
            "type": "figure"
        },
        "FIGREF26": {
            "text": "Varies number of ratings per user in training data, MovieLens1m data, rank Varies number of ratings per user in training data, Netflix data, rank 100",
            "latex": null,
            "type": "figure"
        },
        "FIGREF27": {
            "text": "Primal-CR is still much faster than Collrank when 8 cores are used. Comparing our Primal-CR algorithm in 1 core, 4 cores and 8 cores on the same machine inFigure 3.5, the speedup is desirable. The speedup of Primal-CR and Collrank is summarized in the",
            "latex": null,
            "type": "figure"
        },
        "FIGREF28": {
            "text": "proposed algorithm makes the Collaborative Ranking Model in (3.4) a clear better choice for large-scale recommendation system over standard Matrix Factorization techniques, since we have the same scalability but achieve much better accuracy. Also, our experiments suggest that in practice, when we are given a set of training data, we should try to use all the training data instead of doing sub-sampling as existing algorithms do, and only Primal-CR and Primal-CR++ can scale up to all the ratings.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF29": {
            "text": "Demonstration of Stochastic Queuing Process-the rating matrix R (left) generates multiple possible rankings \u03a0's (right), \u03a0 \u2208 S(R, \u2126) by breaking ties randomly.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF30": {
            "text": "1). We denote the set of valid permutations as S(R, \u2126), where \u2126 is the set of all pairs (i, j) such that R i,j is observed. We call this shuffling process the Stochastic Queuing Process, since one can imagine that by permuting ties we are stochastically queuing new \u03a0's for future use in the algorithm.The probability of observing R therefore should be defined as P (k,m) X (R) = \u03a0\u2208S(R,\u2126) P X (\u03a0).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF31": {
            "text": "(4.4)  is hard to optimize since there is a summation inside the log. But by Jensen's inequality and convexity of \u2212 log function, we can move the summation outside log and obtain an upper bound of the original negative log-likelihood,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF32": {
            "text": ") show that our algorithm SQL-Rank is faster than BPR and Weighted-MF.Note that our algorithm is implemented in Julia while BPR and Weighted-MF are highlyoptimized C++ codes (usually at least 2 times faster than Julia) released by Quora. This speed difference makes sense as our algorithm takes O(nmr) time, which is linearly to the observed ratings. In comparison, pair-wise model such as BPR has O(nm 2 ) pairs, so will take O(nm 2 r) time for each epoch.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF33": {
            "text": "Training time of implicit feedback methods.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF34": {
            "text": "Input: input x i , label y i , backpropagate T steps, mini-batch size m, knowledge graphs on embeddings {E 1 , . . . , E M } 2: Define p l (., .|\u03a6) based on knowledge graphs on embeddings, l = 1, . . . , M 3: for t = 1 to T do 4: Sample one mini-batch {x 1 , . . . Identify the set of embeddings S i = {E 1 [j i 1 ], . . . , E M [j i M ]} for input x i and label y i 7:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF35": {
            "text": ", . . . , j i M which are elements of I 1 \u00d7 . . . I M , the index sets of embedding tables. A typical choice is that the indices are the encoding of a dictionary for words in natural language applications, or user and item tables in recommendation systems. Each index, j l , within the lth table, is associated with an embedding E l [j l ] which is a trainable vector in R d l . The embeddings associated with label y i are usually non-trainable one-hot vectors corresponding to label look-up tables while embeddings associated with input x i are trainable embedding vectors for embedding look-up tables. In natural language applications, we appropriately modify this framework to accommodate sequences such as sentences.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF36": {
            "text": "write the matrix of all embeddings for the ith sample as E[j i ] = (E 1 [j i 1 ], . . . , E M [j i M ]) where j i = (j i 1 , .. . , j i M ) \u2208 I. By an abuse of notation we write the loss as a function of the embedding matrix, (E[j i ]|\u0398).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF37": {
            "text": "SSE-Graph described in Algorithm 9 andFigure 5.2 can be viewed as adding exponentially many distinct reordering layers above the embedding layer. A modified backpropagation procedure in Algorithm 9 is used to train exponentially many such neural networks at the same time.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF38": {
            "text": "Illustration of how SSE-Graph algorithm in Figure 5.1 works for a simple neural network.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF39": {
            "text": "(j, k|\u03a6) is the transition probability (with parameters \u03a6) of exchanging the encoding vector j \u2208 I with a new encoding vector k \u2208 I in the Cartesian product index set of all embedding tables. When there is a single embedding table (M = 1) then there are no hard restrictions on the transition probabilities, p(., .), but when there are multiple tables(M > 1) then we will enforce that p(., .) takes a tensor product form (see(5.4)). When we are assuming that there is only a single embedding table (M = 1) we will not bold j, E[j]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF40": {
            "text": "Projecting 50-dimensional embeddings obtained by training a simple neural network without SSE (Left), and with SSE-Graph (Center) , SSE-SE (Right) into 3D space using PCA.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF41": {
            "text": "B(\u0398) = |R(\u0398) \u2212 S(\u0398)|, and E(\u0398) = |S(\u0398) \u2212 S n (\u0398)|. We can think of B(\u0398) as representing the bias due to SSE, and E(\u0398) as an SSE form of excess risk. Then by another application of similar bounds, R(\u0398) \u2264 R(\u0398 * ) + B(\u0398) + B(\u0398 * ) + E(\u0398) + E(\u0398 * ). (5.8)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF42": {
            "text": "Y |\u0398) be the expected loss conditional on input x i and e(E[j i ], y; \u0398) = (E[j i ], y|\u0398) \u2212 L(E[j i ]|\u0398) be the residual loss. Define the conditional and residual SSE empirical Rademacher complexities to be\u03c1 L,n = E \u03c3 sup \u0398 i \u03c3 i k p(j i , k) \u00b7 L(E[k]|\u0398) , (5.9) \u03c1 e,n = E \u03c3 sup \u0398 i \u03c3 i k p(j i , k) \u00b7 e(E[k], y i ; \u0398) ,(5.10) respectively where \u03c3 is a Rademacher \u00b11 random vectors in R n . Then we can decompose the SSE empirical risk into E sup \u0398 |S n (\u0398) \u2212 S(\u0398)| \u2264 2E[\u03c1 L,n + \u03c1 e,n ]. (5.11) Remark 1. The transition probabilities in (5.9), (5.10) act to smooth the empiricalRademacher complexity. To see this, notice that we can write the inner term of (5.9)as (P \u03c3) L, where we have vectorized \u03c3 i , L(x i ; \u0398) and formed the transition matrix P .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF44": {
            "text": "BPR 0.7031 0.6548 0.6273 0.4080 0.3315 0.2847 0.0437 0.0184 0.0146 SSE-SE + BPR 0.7254 0.6813 0.6469 0.4297 0.3498 0.3005 0.0609 0.0262 0.0155",
            "latex": null,
            "type": "figure"
        },
        "FIGREF46": {
            "text": ".5 and find that SSE-SE helps improving accuracy and BLEU scores on both dev and test sets in 10 out of 11 years from 2008 to 2018. In particular, on the last 5 years' test sets from 2014 to 2018, the transformer model with SSE-SE improves BLEU scores by 0.92 on average when compared to the baseline model without SSE-SE.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF47": {
            "text": "Compare Training Speed of Simple Neural Networks with One Hidden Layer, i.e. MF and BPR, with and without SSE-SE.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF48": {
            "text": "Transformer (SSE-PT) model, especially the embedding layer, and the novel application of stochastic shared embeddings (SSE) regularization technique.Embedding Layer We define a learnable user embedding look-up table U \u2208 R n\u00d7du and item embedding look-up table V \u2208 R m\u00d7d i , where d u , d i are the number of hidden units for user and item respectively. We also specify learnable positional encoding table P \u2208 R T \u00d7d ,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF49": {
            "text": "Illustration of our proposed SSE-PT model encoder used in the original papers",
            "latex": null,
            "type": "figure"
        },
        "FIGREF50": {
            "text": "Long Sequences: SSE-PT++To handle extremely long sequences, a slight modification can be made on the base SSE-PT model in terms of how input sequences s i 's are fed into the SSE-PT neural network. We call the enhanced model SSE-PT++ to distinguish it from the previously discussed SSE-PT model, which cannot handle sequences longer than T .The motivation of SSE-PT++ over SSE-PT comes from: sometimes we want to make use of extremely long sequences, s i = (j i1 , j i2 , . . . , j it ) for 1 \u2264 i \u2264 n, where t > T , but our SSE-PT model can only handle sequences of maximum length of T . The simplest way is to sample starting index 1 \u2264 v \u2264 t uniformly and use s i = (j iv , j i(v+1) , . . . , j iz ), where z = min(t, v + T \u2212 1). Although sampling the starting index uniformly from[1, t]  can accommodate long sequences of length t > T , this does not work well in practice. Uniform sampling does not take into account the importance of recent items in a long sequence. To solve this dilemma, we introduce an additional hyper-parameter p s which we call sampling probability. It implies that with probability p s , we sample the starting index v uniformly from [1, t \u2212 T ] and use sequence s i = (j iv , j i(v+1) , . . . , j i(v+T \u22121) ) as input. With probability 1 \u2212 p s , we simply use the recent T items (j i(t\u2212T +1) , . . . , j it ) as input. If the sequence s i is already shorter than T , then we always use the recent input sequence for user i.Our proposed SSE-PT++ model can work almost as well as SSE-PT with a much smaller T . One can see inTable 6.2 with T = 100, SSE-PT++ can perform almost as well as SSE-PT. The time complexity of the SSE-PT model is of order O(T 2 d + T d 2 ). Therefore, reducing T by one half would lead to a theoretically 4x speed-up in terms of the training and inference speeds. As to the model's space complexity, both SSE-PT and SSE-PT++ are of order O(nd u + md i + T d + d 2 ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF51": {
            "text": "Illustration of how SASRec (Left) and SSE-PT (Right) differs on utilizing the Engagement History of A Random User in Movielens1M Dataset.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF52": {
            "text": "type. So what about the machine's reasoning?For our SSE-PT, the hidden item indexed 26 is put in the first place among its top-5 recommendations. Intelligently, the SSE-PT recommends 3 drama movies, 2 thriller movies and mixing them up in positions. Interestingly, the top recommendation is 'Othello', which like the recently watched 'Richard III', is an adaptation of a Shakespeare play, and this dependence is reflected in the attention weight. On the contrast, SASRec cannot provide top-5 recommendations that are personalized enough. It recommends a variety of action, Sci-Fi, comedy, horror, and drama movies but none of them match item-26. Although this user has watched all these types of movies in the past, they do not watch these anymore as one can easily tell from his recent history. Unfortunately, SASRec cannot capture this and does not provide personalized recommendations for this user by focusing more on drama and thriller movies. It is easy to see that in contrast, our SSE-PT model shares with human reasoning that more emphasis should be placed on recent movies.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF53": {
            "text": "Illustration of the speed of SSE-PT not include Caser and GRU4Rec + in our comparisons. InFigure 6.3, we only compare the training speeds and ranking performances among SASRec, SSE-PT and SSE-PT++",
            "latex": null,
            "type": "figure"
        },
        "FIGREF54": {
            "text": ".12 in Appendix, the optimal ranking performances are achieved at B = 4 or 5 for Movielens1M dataset and at B = 6 for Movielens10M dataset. Personalization and Number of Negatives Sampled Based on the results in Table 8.13 in Appendix, we are positive that the personalized model always outperforms the un-personalized one when we use the same regularization techniques. This holds true regardless of how many negatives sampled or what ranking metrics are used during evaluation.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF55": {
            "text": "of this thesis is to cover some recent advances in collaborative filtering and ranking research and demonstrate that there are various types of orthogonality in which one can contribute to the field. During my PhD study, I was fortunate to explore many directions within collaborative filtering and ranking research. For the first 2 years of my research, I conducted foundational collaborative ranking research on improving the optimization procedure of the pairwise ranking loss in chapter 3 and designing new listwise loss objective functions in chapter 4 for collaborative ranking. For my last 2 years of PhD,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF56": {
            "text": "Compare Training Speed of GRMF, with and without Graph DNA.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF57": {
            "text": "Generate a random undirected Erd\u00f5s-R\u00e9nyi graph G with each edge being chosen with probability p 3: for t = 1, ..., T do 4:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF58": {
            "text": "Comparing implicit feedback methods.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF59": {
            "text": "Effectiveness of Stochastic Queuing Process. transformer base model on a single machine with 4 NVIDIA V100 GPUs for 20,000 steps. We use the same dropout rate of 0.1 and label smoothing value of 0.1 for the baseline model and our SSE-enhanced model. Both models have dimensionality of embeddings as d = 512.When decoding, we use beam search with the beam size of 4 and length penalty of 0.6 and replace unknown words using attention. For both models, we average last 5 checkpoints (we save checkpoints every 10,000 steps) and evaluate the model's performances on the test datasets using BLEU scores. The only difference between the two models is whether or not we use our proposed SSE-SE with p = 0.01 in Equation 5.5 for both encoder and decoder embedding layers.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF60": {
            "text": "Effectiveness of using full lists.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF61": {
            "text": "Proof: [Proof of Theorem 1] Consider the following variability term, sup \u0398 |S(\u0398) \u2212 S n (\u0398)|. (8.3) Figure 8.5.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF62": {
            "text": "X,Y sup \u0398 S n (\u0398) \u2212 E Y |X [S n (\u0398)] + E X,Y sup \u0398 E Y |X [S n (\u0398)] \u2212 S(\u0398) ,where X, Y represent the random input and label. To control the first term, we introduce a ghost dataset (x i , y i ), where y i are independently and identically distributed accordingto y i |x i . Define S n (\u0398) = i k p(j i , k) (E[k], y i |\u0398) (8.4)be the empirical SSE risk with respect to this ghost dataset.We will rewrite E Y |X [S n (\u0398)] in terms of the ghost dataset and apply Jensen'Y |X [S n (\u0398) \u2212 S n (\u0398)] (8.6) \u2264 EE Y |X sup \u0398 |S n (\u0398) \u2212 S n (j i , k)( (E[k], y i |\u0398) \u2212 (E[k], y i |\u0398)) = i k p(j i , k)(e(E[k], y i |\u0398) \u2212 e(E[k], y i |\u0398)). Because y i , y i |X are independent the term ( k p(j i , k)(e(E[k], y i |\u0398) \u2212 e(E[k], y i |\u0398))) i is a vector of symmetric independent random variables. Thus its distribution is not effected by multiplication by arbitrary Rademacher vectors \u03c3 i \u2208 {\u22121, +1}.E sup \u0398 |S n (\u0398) \u2212 S n (\u0398)| = E sup j i , k)(e(E[k], y i |\u0398) \u2212 e(E[k], y i |\u0398)) .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF63": {
            "text": "j i , k)e(E[k], y i |\u0398) .For the second term,E sup \u0398 E Y |X [S n (\u0398)] \u2212 S(\u0398)we will introduce a second ghost dataset x i , y i drawn iid to x i , y i . Because we are augmenting the input then this results in a new ghost encoding oj i . LetS n (\u0398) = i k p(j i , k) (E[k], y i |\u0398) (8.9)be the empirical risk with respect to this ghost dataset. Then we have thatS(\u0398) = E X E Y |X S n (X E Y |X [S n (\u0398)] \u2212 E Y |X [S n (\u0398)] (8.11) \u2264 EE X sup \u0398 E Y |X [S n (\u0398)] \u2212 E Y |X [S n (Y |X [S n (\u0398)] \u2212 E Y |X [S n (\u0398)].(8.13)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF64": {
            "text": "j i , k) (E[k], y i |\u0398) by Jensen's inequality again. Proof: [Proof of Theorem 2] It is clear that 2B \u2265 B(\u0398) + B(\u0398 * ). It remains to show our concentration inequality. Consider changing a single sample, (x i , y i ) to (x i , y i ), thus resulting in the SSE empirical risk, S n,i (\u0398). Thus,S n (\u0398) \u2212 S n,i (\u0398) = k p(j i , k) \u00b7 (E[k], y i |\u0398) \u2212 k p(j i , k) \u00b7 (E[k], y i |\u0398) = k p(j i , k) \u00b7 ( (E[k], y i |\u0398) \u2212 (E[k], y i |\u0398)) + k p(j i , k) \u2212 p(j i , k) \u00b7 (E[k], y i |\u0398) j i , k) \u2264 2b.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "in terms of depth d for a fixed c. . . . . . . . . . . . . . . . . . . . . . . 20 2.3 Comparison of GRWMF Variants for Implicit Feedback on Douban and Flixster datasets. P stands for precision and N stands for NDCG. We use rank r = 10 and all results are in %. . . . . . . . . . . . . . . . . . . . . 20 2.4 Comparison of GCN Methods for Explicit Feedback on Douban, Flixster",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "). All the methods except GC-MC utilize side graph information. . . . . . . . . . . . . . . . . 21 2.5 Comparison of GRMF Methods of different ranks for Explicit Feedback on Flixster Dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.1 Datasets used for experiments . . . . . . . . . . . . . . . . . . . . . . . . 46 3.2 Scability of Primal-CR and Collrank on Movielens10m . . . . . . . . . . Comparing different k on Movielens1m data set using 50 training data per user. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 Compare SSE-Graph and SSE-SE against ALS-MF with Graph LaplacianRegularization. The p u and p i are the SSE probabilities for user and item embedding tables respectively, as in(5.5). Definitions of \u03c1 u and \u03c1 i can be",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "pairwise or listwise. The difference among pointwise, pairwise and listwise are rooted in the distinct interpretations of the same data points. Pointwise approaches assume each user-item rating datum is independent; pairwise approaches assume that pairwise comparisons for two items by the same user are independent; the listwise approaches view the list of item preferences as a whole and treats different users' list as independent data points. In a strict definition, the Collaborative Filtering approaches refers to the based on user engagement[49]. Recently, implicit feedback datasets, such as user clicks of web pages, check-in's of restaurants, likes of posts, listening behavior of music, watching history and purchase history, are increasingly prevalent. Unlike explicit feedback, implicit feedback datasets can be obtained without users noticing or active participation. All So far, we have not touched the features used in recommender systems. Most open datasets do not have good user-side features due to privacy concerns. Even for item side, most datasets including Netflix, Movielens datasets do not have features prepared. However, feature information is crucial for better recommendation and ranking performances besides the fundamental approaches and data. There are many useful features but among them, probably the most challenging ones are including graphs that encode item or user relationships",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "1. Summary of Different Fundamental Approaches Before This Dissertation.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "2. Summary of Different Fundamental Approaches After This Dissertation.to temporal information, it is motivated by the fact that user-item interactions do not happen at one time and then stay static. Instead, usually they occur in a temporal order.This means standard train/validation/test splits may not be realistic. Because during inference, we want to predict future interactions only based on historical interactions. In sequential recommendation[56], train/validation/test are split in temporal ordering so they do not overlap. It has been shown in such setting, temporal ordering plays a large",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 1.1. Before this dissertation (the chapter 4[116]), no one had successfully applied the listwise approach to the implicit feedback setting due to the difficulty of the problem, because in implicit feedback contain only 1's and 0's without different level of ratings. A second contribution is that we speed up the previous best pairwise approaches for explicit feedback significantly. We achieved near-linear time complexity, which allows us to scale up to the full Netflix dataset without subsampling",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "The outline of this thesis is as follows: chapter 2 we first talk about pointwise collaborative filtering problem with graph information, and how our proposed new method can encode very deep graph information which helps four existing graph collaborative filtering algorithms; chapter 3 is on the pairwise approach for collaborative ranking and how we speed up the algorithm to near-linear time complexity; chapter 4 is on the new listwise approach for collaborative ranking and how the listwise approach is a better choice of loss for both explicit and implicit feedback over pointwise and pairwise loss; chapter 5 is about the new regularization technique Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it is both theoretically sound and empirically effectively for 6 different tasks across recommendation and natural language processing; chapter 6 is how we introduce personalization for the state-of-the-art sequential recommendation model with the help of SSE, which plays an important role in preventing our personalized model from overfitting to the training data.In this chapter, we consider recommender systems with side information in the form of graphs. Existing collaborative filtering algorithms mainly utilize only immediate neighborhood information and do not efficiently take advantage of deeper neighborhoods beyond 1-2 hops. The main issue with exploiting deeper graph information is the rapidly growing time and space complexity when incorporating information from these neighborhoods. In this chapter, we propose using Graph DNA, a novel Deep Neighborhood Aware graph encoding algorithm, for exploiting multi-hop neighborhood information. DNA encoding computes approximate deep neighborhood information in linear time using Bloom filters, and results in a per-node encoding whose dimension is logarithmic in the number of nodes in the graph. It can be used in conjunction with both feature-based and graph-regularization-based collaborative filtering algorithms. Graph DNA has the advantages of being memory and time efficient and providing additional regularization when compared to directly using higher order graph information. Code is open-sourced at https://github.com/wuliwei9278/Graph-DNA. This work is going to be published at the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020).",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "All the existing CR algorithms have time complexity at least O(|\u2126|r) per iteration, where r is the target rank and |\u2126| is number of pairs which grows quadratically with number of ratings per user. For example, the Netflix data contains totally 20 billion rating pairs, and at this scale all the current algorithms have to work with significant subsampling, resulting in poor prediction on testing data.In this chapter, we propose a new collaborative ranking algorithm called Primal-CR that reduces the time complexity to O(|\u2126| + d 1d2 r), where d 1 is number of users andd 2 is the averaged number of items rated by a user. Note that d 1d2 is strictly smaller and often much smaller than |\u2126|.Furthermore, by exploiting the fact that most data is in the form of numerical ratings instead of pairwise comparisons, we propose Primal-CR++ with O(d 1d2 (r + logd 2 )) time complexity. Both algorithms have better theoretical time complexity than existing approaches and also outperform existing approaches in terms of NDCG and pairwise error on real data sets. To the best of our knowledge, this is the first collaborative ranking algorithm capable of working on the full Netflix dataset using all the 20 billion rating pairs, and this leads to a model with much better recommendation compared with previous models trained on subsamples. Finally, compared with classical matrix factorization algorithm which also requires O(d 1d2 r) time, our algorithm has almost the",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Applying this framework to collaborative ranking, we derive asymptotic statistical rates as the number of users and items grow together. We conclude by demonstrating that our SQL-Rank method often outperforms current state-of-the-art algorithms for implicit feedback such as Weighted-MF and BPR and achieve favorable results when compared to explicit feedback algorithms such as matrix factorization and collaborative ranking. Code is open-sourced at https://github.com/wuliwei9278/SQL-Rank. This work has been published at the Thirty-fifth International Conference on Machine Learning (ICML 2018).In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information.We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalizationresults. Code is open-sourced at https://github.com/wuliwei9278/SSE. This work has been published at the Thirty-third Annual Conference on Neural Information Processing Systems (NeurIPS 2019). Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/wuliwei9278/SSE-PT. This work is currently still under review.",
            "latex": null,
            "type": "table"
        },
        "TABREF12": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF13": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF14": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF16": {
            "text": "5. Comparison of GRMF Methods of different ranks for Explicit Feedback on Flixster Dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF17": {
            "text": "s jk (Ha)[(p \u2212 1) \u00b7 r + 1 : p \u00b7 r] += t[j] \u00b7 u i 13: return Ha Fast computation for Hessian-vector product Similar to the case of gradient computation, using a naive way to compute H \u00b7 a requires O(|\u2126|r) time since we need to go through all the (i, j, k) tuples, and each of them requires O(r) time. However, we can apply the similar trick in gradient computation to reduce the time complexity to",
            "latex": null,
            "type": "table"
        },
        "TABREF18": {
            "text": "1. Datasets used for experiments",
            "latex": null,
            "type": "table"
        },
        "TABREF19": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF20": {
            "text": "2. One can see from the table that the speedup of our Primal-CR algorithm is comparable to Collrank. Due to the O(|\u2126|k) complexity, existing algorithms cannot deal with large number of pairs, so they always sub-sample a limited number of pairs per user when solving MovieLens10m or Netflix data. For example, for Collrank, the authors fixed number of ratings per user in training as N and only reported N up to 100 for Netflix data. When we tried to apply their code for N = 200, the algorithm gets very slow and reports memory error for N = 500.",
            "latex": null,
            "type": "table"
        },
        "TABREF21": {
            "text": "Algorithm 7. SQL-Rank: General FrameworkInput: \u2126, {R ij : (i, j) \u2208 \u2126}, \u03bb \u2208 R + , ss, rate, \u03c1 Output: U \u2208 R r\u00d7n and V \u2208 R r\u00d7mInput: \u2126, {R ij : (i, j) \u2208 \u2126}, \u03c1 Output: \u03a0 \u2208 R n\u00d7m for i = 1 to n doSort items based on observed relevance levels R i Form \u03a0 i based on indices of items in the sorted list Shuffle \u03a0 i for items within the same relevance level if Dataset is implicit feedback then Uniformly sample \u03c1m items from unobserved items Append sampled indices to the back of \u03a0 i Stack \u03a0 i as rows to form matrix \u03a0",
            "latex": null,
            "type": "table"
        },
        "TABREF22": {
            "text": "1. Comparing implicit feedback methods on various datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF23": {
            "text": "2. Comparing explicit feedback methods on various datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF24": {
            "text": ".2, our proposed listwise algorithm SQL-Rank outperforms previous listwise method List-MF in both NDCG@10 and precision@1, 5, 10. It verifies the claim that log-likelihood loss outperforms the cross entropy loss if we use it correctly. When listwise algorithm SQL-Rank is compared with pairwise algorithm Primal-CR++, the performances between SQL-Rank and Primal-CR++ are quite similar, slightly lower for NDCG@10 but higher for precision@1, 5, 10. Pointwise method MF is doing okay in NDCG but really bad in terms of precision. Despite having comparable NDCG, the predicted top k items given by MF are quite different from those given by other algorithms utilizing a ranking loss. The ordered lists based on SQL-Rank, Primal-CR++ and List-MF, on the other hand, share a lot of similarity and only have minor difference in ranking of some items. It is an interesting phenomenon that we think is worth exploring further in the future.",
            "latex": null,
            "type": "table"
        },
        "TABREF25": {
            "text": "3. Effectiveness of Stochastic Queuing Process.",
            "latex": null,
            "type": "table"
        },
        "TABREF26": {
            "text": "4. Comparing different k on Movielens1m data set using 50 training data per user.",
            "latex": null,
            "type": "table"
        },
        "TABREF28": {
            "text": "1. Compare SSE-Graph and SSE-SE against ALS-MF with Graph LaplacianRegularization. The p u and p i are the SSE probabilities for user and item embed-",
            "latex": null,
            "type": "table"
        },
        "TABREF29": {
            "text": "2. SSE-SE outperforms Dropout for Neural Networks with One Hidden Layer such as Matrix Factorization Algorithm regardless of dimensionality we use. p s is the SSE probability for both user and item embedding tables and p d is the dropout probability.",
            "latex": null,
            "type": "table"
        },
        "TABREF30": {
            "text": "3. SSE-SE outperforms dropout for Neural Networks with One Hidden Layer such as Bayesian Personalized Ranking Algorithm regardless of dimensionality we use.",
            "latex": null,
            "type": "table"
        },
        "TABREF32": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF34": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF35": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF36": {
            "text": "2. Comparing SASRec, SSE-PT and SSE-PT++ on Movielens1M Dataset while varying the maximum length allowed and dimension of embeddings.",
            "latex": null,
            "type": "table"
        },
        "TABREF37": {
            "text": "3. Comparing Different Regularizations for SSE-PT on Movielen1M Dataset. NO REG stands for no regularization. PS stands for parameter sharing across all users while PS(AGE) means PS is used within each age group. SASRec is added to last row after all SSE-PT results as a baseline.length T = 200 for Movielens 1M and 10M dataset and T = 50 for other datasets. We use top-K with K = 10 and the number of negatives C = 100 in the evaluation procedure. In practice, using a different K and C does not affect our conclusions.",
            "latex": null,
            "type": "table"
        },
        "TABREF38": {
            "text": "directions, it is by no means complete and I feel there are still many interesting and important directions worth pursuing but not done within this dissertation. I imagine at the end of day, assuming the computing power catches up, it is very likely we can discard most of the feature engineering and directly learn from raw data formats such as texts, images and videos, just like what happened in computer vision and natural language processing fields with the advances of deep learning techniques. This has not yet happened in neither academics or industry. To make this happen, I believe that there are a few shortcomings in current research practices:\u2022 There do not exist unified metrics to evaluate across papers. Some papers use root mean square errors (RMSE) as accuracy metric while others use ranking metrics.Then even within the ranking metrics, people tend to use different metrics: from AUC score to NDCG, precision and recall, not to mention that different top k can be used for NDCG, precision and recall.\u2022 There do not exist unified datasets to test on across papers. There are many datasets outside there, from Netflix to Movielens, and Douban to Flixster, etc.. Different papers tend to use different training-test splits: some do random splits while others do splits based on temporal orderings. Moreover, the recommender systems community does not really have a shared and well-maintained leader-board on the winning solutions. The results of various proposed algorithms are very likely to perform differently on different datasets.\u2022 Most datasets do not come with good features nor complete raw input data. To some extent, it is understandable because of the strict privacy and copyright concerns.But this has put academic researches at a disadvantage and often industry applied research will not take as much risk to pursue long-term projects. On the other hand, academics are able to take more risks to pursue longer-term research topics but unfortunately are limited by the constraints of good datasets and powerful computing resources.While addressing these shortcomings, I think it would be very exciting to see different levels of interactions between the recommendation field and other AI fields, including natural language understanding and computer vision. News/Book recommendation, image recommendation, video recommendation and audio recommendation are some promising examples that may see breakthroughs of new models, just as what happened during Netflix competition about 10 years ago",
            "latex": null,
            "type": "table"
        },
        "TABREF39": {
            "text": "1. Compare Bloom filters of different depths and sizes an on Synthesis Dataset. Note that the number of bits of Bloom filter is decided by Bloom filter's maximum capacity and tolerable error rate (i.e. false positive error, we use 0.2 as default). max capacity c bits nnz ratio RMSE (\u00d710 \u22123 ) % Relative Graph Gain",
            "latex": null,
            "type": "table"
        },
        "TABREF40": {
            "text": "2. Compare nnz of different methods on Douban and Flixster datasets. GRMF G 4 and GRMF DNA-2 are using the same 4-hop information in the graph but in different ways. Note that we do not exclude potential overlapping among columns.8.1.3 Graph Regularized Weighted Matrix Factorization for Implicit feedbackWe use the rank r = 10, negatives' weight \u03c1 = 0.01 and measure the prediction performance with metrics MAP, HLU, Precision@k and NDCG@k (see definitions of metrics in Appendix 8.1.2).We follow the similar procedure to what is done before in GRMF and co-factor:we run all combinations of tuning parameters of \u03bb l \u2208 {0.01, 0.1, 1, 10, 100} and \u03bb g \u2208 {0.01, 0.1, 1, 10, 100} for each method on validation data for fixed number 40 epochs and choose the best combination as the parameters to use on test data. We then report the best prediction results during first 40 epochs on test data with the chosen parameter combination.",
            "latex": null,
            "type": "table"
        },
        "TABREF41": {
            "text": "To reproduce results of our DNA methods, one need to generate Bloom filter matrix B following Algorithm 1. We will provide our python codes implementing Algorithm 1 and Matlab codes converting into the formats the library requires.For baselines and our DNA methods, We perform a parameter sweep for \u03bb l \u2208 {0.01, 0.1, 1, 10, 100}, \u03bb g \u2208 {0.01, 0.1, 1, 10, 100}, \u03b1 \u2208 {0.0001, 0.001, 0.01, 0.1, 0.3, 0.7, 1}, for \u03b2 \u2208 {0.005, 0.01, 0.03, 0.05, 0.1} when needed. We run all combinations of tuning parameters for each method on validation set for 40 epochs and choose the best combinationas the parameters to use on test data. We then report the best test RMSE in first 40 epochs on test data with the chosen parameter combination. We provide all the chosen combinations of tuning parameters that achieves reported optimal results in results tables in theTable 8.4, 8.5, 8.6  in Appendix. One just need to exactly follow our procedures in Section 2.4 to construct new\u0120,U to replace the G, U in baseline methods before feeding into Matlab.",
            "latex": null,
            "type": "table"
        },
        "TABREF42": {
            "text": ".3. For all the methods, we select the best parameters \u03bb l and \u03bb g from {0.01, 0.1, 1, 10, 100}. For method GRMF G 2 , we tune an additional parameter \u03b1 \u2208 {0.0001, 0.001, 0.01, 0.1, 0.3, 0.7, 1}. For the thrid-order method GRMF G 3 , we tune \u03b2 \u2208 {0.005, 0.01, 0.03, 0.05, 0.1} in addition to \u03bb l , \u03bb G , \u03b1. Due to the speed constraint, we are not able to tune a broader range of choices for \u03b1 and \u03b2 as it is too time-consuming to do so especially for douban and flixster datasets. For example, it takes takes about 3 weeks using 16-cores CPU to tune both \u03b1, \u03b2 on flixster dataset. We run each method with every possible parameter combination for fixed 80 epochs on the same training data, tune the best parameter combination based on a small predefined validation data and report the best RMSE results on test data with the best tuning parameters during the first 80 epochs. Note that only on the small synthesis dataset, we calculate full G 3 and report the results. On real datasets, there is no way to calculate full G 4 to utilize the complete 4-hop information, because one can easily spot inTable 8.2 the number of non-zero elements (nnz) is growing exponentially when the hop increases by 1, which makes it impossible for one to utilize complete 3-hop and 4-hop information.",
            "latex": null,
            "type": "table"
        },
        "TABREF43": {
            "text": ".2 though 20% less than that of G 2 . On the other hand, for flixster dataset, we have \u03b1 = 0.01 < 0.1 = \u03b2, which implies in this dataset deeper information is more important and we should go deeper. That explains why here GRMF DNA-3 (6-hop) achieves about 10 times more gain than using 1-hop GRMF G. Part of our code is already made available on Github: https://github.com/wuliwei9278/ Graph-DNA. We include pseudo-codes for Algorithm 12, 13 and Figures 8.2, 8.3, 8.4 in the appendix to chapter 4.",
            "latex": null,
            "type": "table"
        },
        "TABREF44": {
            "text": "3. Compare Matrix Factorization for Explicit Feedback on Synthesis Dataset.",
            "latex": null,
            "type": "table"
        },
        "TABREF46": {
            "text": "4. Compare Matrix Factorization methods for Explicit Feedback on Douban and Flixster data. We use rank r = 10.",
            "latex": null,
            "type": "table"
        },
        "TABREF47": {
            "text": "5. Compare Co-factor Methods for Explicit Feedback on Douban and Flixster Datasets. We use rank r = 10 for both methods.Table 8.6. Compare Weighted Matrix Factorization with Graph for Implicit Feedback on Douban and Flixster Datasets. We use rank r = 10 for both methods and all metric results are in %.For experiments in Section 5.4.1, we use Julia and C++ to implement SGD. For experiments in Section 5.4.2, and Section 5.4.4, we use Tensorflow and SGD/Adam Optimizer. For experiments in Section 5.4.3, we use Pytorch and Adam with noam decay scheme and warm-up. We find that none of these choices affect the strong empirical results supporting the effectiveness of our proposed methods, especially the SSE-SE. In any deep learning frameworks, we can introduce stochasticity to the original embedding look-up behaviors and easily implement SSE-Layer in",
            "latex": null,
            "type": "table"
        },
        "TABREF48": {
            "text": "Table 5.1 and Table 5.2. We use the learning rate of 0.01 in all SGD experiments.In the first leg of experiments, we examine users with fewer than 60 ratings in Movie-lens1m and Movielens10m datasets. In this scenario, the graph should carry higher importance. One can easily see fromTable 5.1 that without using graph information,Algorithm 13. Gradient update for V (Same procedure for updating U ) Input: V, ss, rate rate refers to the decaying rate of the step size ss Output: V Compute gradient g for V see alg 12 V \u2212= ss \u00b7 g ss * = rate Return V our proposed SSE-SE is the best performing matrix factorization algorithms among all methods, including popular ALS-MF and SGD-MF in terms of RMSE. With Graph information, our proposed SSE-Graph is performing significantly better than the Graph Laplacian Regularized Matrix Factorization method. This indicates that our SSE-Graph has great potentials over Graph Laplacian Regularization as we do not explicitly penalize the distances across embeddings but rather we implicitly penalize the effects of similar embeddings on the loss.",
            "latex": null,
            "type": "table"
        },
        "TABREF49": {
            "text": "7. Description of Datasets Used in Evaluations.dataset #users #items avg sequence len max sequence len",
            "latex": null,
            "type": "table"
        },
        "TABREF50": {
            "text": "8. Comparing our SSE-PT, SSE-PT++ with SASRec on Movielen1M dataset. We use number of negatives C = 100, dropout probability of 0.2 and learning rate of 1e \u22123 for all experiments while varying others. p u , p i , p u are SSE probabilities for user embedding, input item embedding and output item embedding respectively. Dimensions Number of Blocks Sampling Probability SSE-SE Parameters operating on item sequences.",
            "latex": null,
            "type": "table"
        },
        "TABREF51": {
            "text": "9. Comparing our SSE-PT with SASRec on Movielens10M dataset. UnlikeTable 8.8, we use the number of negatives C = 500 instead of 100 as C = 100 is too easy for this dataset and it gets too difficult to tell the differences between different methods: Hit Ratio@10 approaches 1.",
            "latex": null,
            "type": "table"
        },
        "TABREF52": {
            "text": "10. Comparing Different SSE probability for user embeddings for SSE-PT on Movielens1M Dataset. Embedding hidden units of 50 for users and 100 for items, attention blocks of 2, SSE probability of 0.01 for item embeddings, dropout probability of 0.2 and max length of 200 are used.",
            "latex": null,
            "type": "table"
        },
        "TABREF53": {
            "text": "11. Comparing Different Sampling Probability, p s , of SSE-PT++ on Movie-lens1M Dataset. Hyper-parameters the same asTable 8.10, except that the max length T allowed is set 100 instead of 200 to show effects of sampling sequences.Table 8.12. Comparing Different Number of Blocks for SSE-PT while Keeping The Rest Fixed on Movielens1M and Movielens10M Datasets.",
            "latex": null,
            "type": "table"
        },
        "TABREF54": {
            "text": "13. Varying number of negatives C in evaluation on Movielens1M dataset. Other hyper-parameters are fixed for a fair comparison.Advances in Collaborative Filtering and Ranking",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "In the simulation we carried out, we set the number of users n = 10, 000 and the number of items m = 2, 000. We uniformly sample 5% for training and 2% for testing out of the",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation Study"
        }
    ]
}