{
    "paper_id": "85b46956de829e72a8f6cba6739f4c873fdb32fa",
    "metadata": {
        "title": "Conservative two-stage group testing",
        "authors": [
            {
                "first": "Matthew",
                "middle": [],
                "last": "Aldridge",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Inspired by applications in testing for covid-19, we consider a variant of two-stage group testing we call 'conservative' two-stage testing, where every item declared to be defective must be definitively confirmed by being tested by itself in the second stage. We study this in the linear regime where the prevalence is fixed while the number of items is large. We study various nonadaptive test designs for the first stage, and derive a new lower bound for the total number of tests required. We find that a first-stage design with constant tests per item and constant items per test due to Broder and Kumar is extremely close to optimal. Simulations back up the theoretical results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Group testing is the following problem. Suppose there are n individuals, some of whom are infected with a disease. If a test exists that reliably detects the disease, then each individual can be separately tested for the disease to find if they have it or not, requiring n tests. However, in theory, a pooled strategy can be better: we can take samples from a number of individuals, pool the samples together, and test this pooled sample. If none of the individuals are infected, the test should be negative, while if one or more are the individuals are positive then, in theory, the test should be positive. It might be possible to ascertain which individuals have the disease in fewer than n such pooled tests, thus saving resources when tests are expensive or limited.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Group testing"
        },
        {
            "text": "Recent experiments suggest that the group testing paradigm holds for SARS-CoV-2, the virus that causes the disease covid-19; that is, pools of samples with just one positive sample and many negative samples do indeed produce positive results, at least for pools of around 32 samples or fewer [1, 8, 29, 31] . This work has led to a great interest in group testing as a possible way to make use of limited tests for covid-19; such work includes [9, 18, 19, 20, 22, 27, 28] and other papers we cite later.",
            "cite_spans": [
                {
                    "start": 292,
                    "end": 295,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 296,
                    "end": 298,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 299,
                    "end": 302,
                    "text": "29,",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 303,
                    "end": 306,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 444,
                    "end": 447,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 448,
                    "end": 451,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 452,
                    "end": 455,
                    "text": "19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 456,
                    "end": 459,
                    "text": "20,",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 460,
                    "end": 463,
                    "text": "22,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 464,
                    "end": 467,
                    "text": "27,",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 468,
                    "end": 471,
                    "text": "28]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "Group testing"
        },
        {
            "text": "Many of these papers use a similar model that we also use here: the number of individuals n is large; the prevalence p is constant; each individual is infected independently with probability p (the 'i.i.d. prior'); we wish to reduce the average-case number of tests ET ; and we want to be certain that each individual is correctly classified (the 'zero-error' paradigm). We emphasise the fact that p is constant as n \u2192 \u221e puts us in the so-called 'linear regime', rather than the often-studied 'sparse regime' where p \u2192 0 as n gets large; the linear regime seems more relevant with applications to covid-19 and other widely spread diseases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Group testing"
        },
        {
            "text": "Later, it will sometimes be convenient to instead consider the 'fixed-k' prior, where there is a fixed number k = pn of infected individuals. We discuss this mathematical convenience further in Subsection 3.3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Group testing"
        },
        {
            "text": "For more background on group testing, we point readers to the recent survey [5] .",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 79,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Group testing"
        },
        {
            "text": "An important distinction is between nonadaptive testing, where all tests are designed in advance and can be carried out in parallel, and adaptive testing, where each test result is examined before the next test pool is chosen.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "Recall we are the linear regime, where p is constant. For nonadaptive testing, a result of Aldridge [2] shows that any nonadaptive scheme using T < n tests has error probability bounded away from 0. So simple individual testing will be the optimal nonadaptive strategy, unless errors are tolerated -and errors that don't even vanish in the large n limit at that. For adaptive testing, the best known scheme is a generalized binary splitting scheme studied by Zaman and Pippenger [32] and Aldridge [3] , based on ideas of Hwang [23] . This scheme is the optimal 'nested' strategy [32] , and is within 5% of optimal for all p \u2264 1/2 [3] . This algorithm (or special cases, or simplifications) was discussed in the context of covid-19 by [18, 19, 25] . However, adaptive schemes are unlikely to be suitable for testing for covid-19, as many tests must be performed one after the other, meaning results will take a very long time to come back.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 103,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 479,
                    "end": 483,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 497,
                    "end": 500,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 527,
                    "end": 531,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 579,
                    "end": 583,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 630,
                    "end": 633,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 734,
                    "end": 738,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 739,
                    "end": 742,
                    "text": "19,",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 743,
                    "end": 746,
                    "text": "25]",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "We propose, rather, using an adaptive strategy with only two stages. Within stages, tests are performed nonadaptively in parallel, so results can be returned in only the time it takes to perform two tests. This provides a good compromise between the speed but inevitable errors (or full n individual tests) of nonadaptive schemes and the fewer tests but unavoidable slowness of fully adaptive schemes. Two-stage testing goes back to the foundational work of Dorfman [13] , and has been discussed more recently in the context of covid-19 by [6, 8, 10, 15, 18, 21, 30] .",
            "cite_spans": [
                {
                    "start": 466,
                    "end": 470,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 540,
                    "end": 543,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 544,
                    "end": 546,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 547,
                    "end": 550,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 551,
                    "end": 554,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 555,
                    "end": 558,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 559,
                    "end": 562,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 563,
                    "end": 566,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "From now on, we adopt standard group testing terminology as in, for example, [14, 4, 5] . In particular, individuals are 'items' and infected individuals are (slightly unfortunately) 'defective items'.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 81,
                    "text": "[14,",
                    "ref_id": null
                },
                {
                    "start": 82,
                    "end": 84,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 85,
                    "end": 87,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "A two-stage algorithm that is certain to correctly classify every item works as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "1. In the first stage, we perform some number T 1 of nonadaptive tests. This will find some nondefective items: any item that appears in a negative tests is a definite nondefective (DND). This will also find some defective items: any item that appears in a positive tests in which every other item is DND is a definite defective (DD).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "2. In the second stage, we must individually test every item whose status we so not yet know -that is, all items except the DNDs and DDs. This requires T 2 = n \u2212 (# DNDs + # DDs) tests.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "Ruling out DNDs when they appear in a negative test is a simple procedure in practice: following a negative test, a laboratory must simply report the samples in that pool. Further, if the test procedure can be unreliable, the procedure can easily be changed to ruling out items after they appear in some number d > 1 of negative tests. However, 'ruling in' DDs is trickier: first information about all the DNDs must be circulated (potentially among many different laboratories, with the privacy problems that entails), then each positive test must be carefully checked to see if all but one of the samples has been ruled out as a DND. Confirming that an item is defective thus involves checking a long chain of test results and pool details, which is complicated, very susceptible to occasional testing errors, and can be difficult to prove to a clinician's or patient's satisfaction.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "With these problems in mind, we introduce a variant we call conservative two-stage group testing. This adds the rule that every defective item must be definitively 'certified' by appearing in a (necessarily positive) test in the second stage in which it is the sole item. This gives a very simple proof that an item is defective, with the 'gold standard' individual test that will not be susceptible to dilution from other samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "So in the first stage of conservative two-stage testing, a nonadaptive scheme is used only to rule out DNDs -that is, items that appear a negative tests and are thus definite nondefectives. In the second round, each remaining item is individually tested, requiring T 2 = n \u2212 # DNDs tests.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "Since in the first stage of two-stage testing, we want to discover DNDs and DDs, while in the first stage of conservative two-stage testing we can concentrate on simply discover DNDs, we can say that two-stage testing has a lot in common with the 'DD algorithm' of [4, 24, 5] , while conservative two-stage testing is more like the 'COMP algorithm' of [11, 4, 24, 5] .",
            "cite_spans": [
                {
                    "start": 265,
                    "end": 268,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 269,
                    "end": 272,
                    "text": "24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 273,
                    "end": 275,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 352,
                    "end": 356,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 357,
                    "end": 359,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 360,
                    "end": 363,
                    "text": "24,",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 364,
                    "end": 366,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "Two-stage testing has previously received attention in the sparse p = o(1) setting; we direct interested readers to [26] for more details, or [5, Section 5.2] for a high-level overview. In the sparse regime, recovery always requires order k log n tests, so the difference between testing up to k DDs or not -that is, the distinction between usual two-stage testing and conservative two-stage testing -makes up a negligible proportion of tests, and can be considered irrelevant. It is only in the linear regime we consider here that we have to worry about the costs of definitively confirming items we think are defective.",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Conservative two-stage testing"
        },
        {
            "text": "In this paper we consider five algorithms for nonconservative two-stage testing. Recall that the second stage is always 'test every item not ruled out as a DND', so we need only define the first stage.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "Individual testing tests nothing in the first stage and tests every item individually in the second stage. Although very simple, this is provably the best scheme for p \u2265 (3 \u2212 \u221a 5)/2 = 0.382, and is the best conservative two-stage scheme we consider here for",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "Dorfman's algorithm splits the items into sets of size s and tests each set in the first stage, then individually tests each item in the positive sets in the second stage. Dorfman's algorithm is the best scheme we consider here for p > 0.121, although for p > 0.307 the optimal value is s = 1, where it is equivalent to individual testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "Bernoulli first stage where, in the first stage, each item is placed in each test independently with the same probability. This scheme is suboptimal, but within 0.2 bits of optimal for all p. For p > 1/(e + 1) = 0.269, the optimal number of first-stage tests is 0, and we recover individual testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "Constant tests-per-item first stage where in the first stage, each item is placed in the same number r of tests, chosen at random. This scheme is suboptimal, but very close to optimal when p is small. For For p > 0.269, the optimal number of first-stage tests is 0, and we recover individual testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "Doubly constant first stage where where in the first stage, each item is placed in the same number r of tests and each test contains the same number r of items, chosen at random. This is the best scheme we consider for all p, and is extremely close to our lower bound. For p > 0.268, the optimal the optimal number of first-stage tests is 0 and we recover individual testing; while for p > 0.121, the optimal number of tests per item is r = 1, and we recover Dorfman's algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "We also give a lower bound for the number of tests required for conservative two-stage testing (Theorem 5). Along the way, we also find a new lower bound for usual non-conservative two-stage testing (Theorem 4), which may be of independent interest. Our main results on the average numbers of tests necessary are illustrated in Figure 1 . The top subfigure shows the 'aspect ratio' [3] : the expected number of tests normalised by the number of items ET /n (smaller is better) in the large n limit. We can compare the aspect ratio to individual testing with T /n = 1 and the counting bound (see, for example, [7, 5] ) which says that ET /n \u2265 H(p), where H(p) is the binary entropy.",
            "cite_spans": [
                {
                    "start": 382,
                    "end": 385,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 609,
                    "end": 612,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 613,
                    "end": 615,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 328,
                    "end": 336,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Main results"
        },
        {
            "text": "The middle subfigure shows the rate nH(p)/ET (higher is better) in the large n limit, which corresponds the average number of bits of information learned per test [7, 5] . The rate can be compared to individual testing, with nH(p)/T = H(p) and the counting bound nH(p)/ET \u2264 1.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 166,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 167,
                    "end": 169,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "The doubly constant design is so close to the lower bound for the number of tests (which becomes an upper bound on the rate), that it can be difficult to see both. The bottom subfigure shows a zoomed in section of the rate graph.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "While the expressions in our main theorems are smooth for fixed values of the parameters, sometimes the parameters must be integers, with sudden jumps in the optimal value from one integer to the next. This leads to 'crooked' lines in graphs of the aspect ratio, and 'bumpy' lines in graphs of the rate. The 'kink' in the lower bound at p = 0.171 is where the dominant lower bound of Theorem 4 switches from Bound 2 to Bound 3 of that theorem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Main results"
        },
        {
            "text": "Alongside our theoretical results for large n, we present evidence from simulations with n = 1000 items (or just above 1000, if convenient for rounding reasons) and prevalence p = 0.027. We picked this value of p as it is an estimate by the Imperial College covid-19 Response Team for the prevalence of covid-19 in the UK as of 28 March 2020 [17] .",
            "cite_spans": [
                {
                    "start": 342,
                    "end": 346,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Specifically, we used the following algorithms, with parameters suggested by the optimal value in the large-n limit:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Individual testing with n = 1000 items, so T = 1000 tests.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Dorfman's algorithm with n = 1001 items, and s = 7 items per test, so T 1 = n/s = 143 tests in the first stage.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Bernoulli first stage with n = 1000 items, Bernoulli parameter \u03c0 = 1/pn = 0.037, and T 1 = 190 tests in the first stage, so \u03c3 = 1/p = 37.0 items per test on average.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Constant tests-per-item first stage with n = 1000 items, r = 4 tests per item, and T 1 = 160 tests in the first stage, so \u03c3 = nr/T 1 = 25 items per test on average.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Doubly constant first stage with n = 1000 items, r = 4 tests per item, and s = 25 items per test, so T 1 = nr/s = 160 tests in the first stage.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "We simulated each algorithm 1000 times. Table 1 shows the results of the simulations, displaying the mean number of tests used, alongside the first and ninth deciles. These simulated results are compared with a 'theory' result, which takes the theoretical behaviour of ET as n \u2192 \u221e (from Section 3) and plugs in n = 1000. We see that, compared to individual testing, the other four algorithms give at least a three-fold reduction in the number of tests required on average, with constant tests-per-item and doubly constant designs giving a four-fold reduction. The Bernoulli first stage was a significant improvement on Dorfman's algorithm, while the constant tests-per-item and doubly constant designs were a large improvement further. The difference between a constant tests-per-item and doubly constant first stage was small; this is not surprising, as our theoretical results show that constant tests-per-item is very close to optimal for p this small (see Figure 1) .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 40,
                    "end": 47,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 960,
                    "end": 969,
                    "text": "Figure 1)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Simulations"
        },
        {
            "text": "We see that Dorfman's algorithm performs on average very close to theoretical predictions. The Bernoulli, constant tests-per-item and doubly constant designs require about 6 more tests on average than the n \u2192 \u221e asymptotics imply; this is presumably because pn = 27 is sufficiently small that rare large defective populations drive up the average number of tests in a way that becomes increasingly unlikely as pn \u2192 \u221e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "3 Algorithms for conservative two-stage testing 3 ",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 49,
                    "text": "3",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Simulations"
        },
        {
            "text": "Individual testing has no first round T 1 = 0 then tests every item in the second round T 2 = n. This is a conservative algorithm with T = 0 + n = n.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ".1 Individual testing"
        },
        {
            "text": "It is proved in [2] that individual testing is the optimal one-stage algorithm for all p \u2208 (0, 1). It is proved in [16] that individual testing is the optimal adaptive algorithm for all p > (3 \u2212 \u221a 5)/2 = 0.369.",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 19,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 115,
                    "end": 119,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": ".1 Individual testing"
        },
        {
            "text": "Dorfman's algorithm [13] was the first group testing algorithm. We split the items into n/s groups of size s. (Here s has to be an integer, but since we are assuming n is large we don't have to worry about n/s being an integer.) If a group is positive, we test all its items individually in stage two.",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 24,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Dorfman's algorithm"
        },
        {
            "text": "Work that discusses Dorfman's algorithm in the context of testing for covid-19 include [6, 8, 10, 21] This has T 1 = n/s tests in stage 1. In stage 2, a group is positive with probability 1 \u2212 q s , so the expected number of tests is ET 2 = s(1 \u2212 q s )n/s = (1 \u2212 q s )n. This is a total of ET = n s + (1 \u2212 q s )n = n 1 s + 1 \u2212 q s tests on average. Dorfman's algorithm outperforms individual testing for all p < 1 \u2212 1/ 3 \u221a 3 = 0.307. Interestingly, Dorfman's algorithm with s = 2 is never optimal.",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 90,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 91,
                    "end": 93,
                    "text": "8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 94,
                    "end": 97,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 98,
                    "end": 101,
                    "text": "21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Dorfman's algorithm"
        },
        {
            "text": "There's no closed form for the optimal value of s, although it's approximately 1/ \u221a p when p is small.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dorfman's algorithm"
        },
        {
            "text": "The Bernoulli design is the most commonly used nonadaptive design and the mathematically simplest. In a Bernoulli design, each item is placed in each test independently with probability \u03c0. Here we suggest a Bernoulli design for the first stage of a two-stage algorithm. Bernoulli designs have been studied for nonadaptive group testing in the p = o(1) regime by [11, 4, 5] and others. It will be convenient to write \u03c3 = \u03c0n for the average number of items per test.",
            "cite_spans": [
                {
                    "start": 362,
                    "end": 366,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 367,
                    "end": 369,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 370,
                    "end": 372,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "Although the Bernoulli first stage is not optimal (see Figure 1 ), it is close to optimal, and the mathematical simplicity allows us to explicitly find the optimal design parameter \u03c0 = 1/np and the optimal number T 1 of first-stage tests. For models with slightly better performance, these can only be found numerically.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 55,
                    "end": 63,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Bernoulli first stage"
        },
        {
            "text": "It will be convenient here to work here and for the following algorithms with the so-called 'fixed k' prior, where we assume there are exactly k = pn defective items, chosen uniformly at random from the n items. Since we are assuming the number of items n is large, standard concentration inequalities imply the true number of defectives under the i.i.d. prior will in fact be very close to k = pn. We also note that none of the algorithms we consider here will actual take advantage of exact knowledge of k; it is merely a mathematical convenience to make proving theorems easier. The results we prove under this 'fixed k' prior do indeed hold for the i.i.d. prior also in the large n limit; see [5, Appendix to Chapter 1] for formal details of how to transfer results between the different prior models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "Throughout we write \u223c for asymptotic equivalence: a(n) \u223c b(n) means that a(n) = (1 + o(1))b(n) as n \u2192 \u221e. Theorem 1. Using a Bernoulli(\u03c0) first stage with an average of \u03c3 = \u03c0n items per test, conservative two-stage testing can be completed in",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "tests on average. When the prevalence p is known, the optimum value of \u03c0 is 1/pn, and we can succeed with ET \u223c np e ln",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "tests on average when p \u2264 1/(e + 1) = 0.269, or ET = n tests otherwise.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "Proof. We need to work out how many nondefective items are discovered by the Bernoulli design. A given nondefective item is discovered by a test if that item is in the test but the test is negative. This happens with with probability",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "When the p is known, simple calculus shows that this is maximised at \u03c3 = 1/p, where it takes the value e \u22121 /pn. Thus the probability a nondefective item is not discovered is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "Therefore, the total number of tests used by this algorithm on average is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "At the optimal \u03c3 = 1/p, this is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "We differentiate to find the optimum value of T 1 , giving",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "from which we get the optimal value Otherwise, T 1 = 0 is optimal, and we have individual testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Bernoulli first stage"
        },
        {
            "text": "In a constant tests-per-item nonadaptive design, we have a constant number r tests per item. For convenience, we arrange these in r rounds of T 1 /r tests, one test per item in each round. Rounds can be conducted in parallel, so this is not adding extra stages to our two-stage algorithm. The test for an item in a given round is chosen uniformly at random from the T 1 /r tests, independently from other items. It will be convenient to write \u03c3 = nr/T 1 for the average number of items per test. Constant tests-per-item designs are optimal nonadaptive designs in the sparse p = o(n) regime [12, 24] , so are a good candidate for the nonadaptive stage of a two-stage scheme. It is therefore not surprising that its performance is very close to optimal when p is small (see 1).",
            "cite_spans": [
                {
                    "start": 590,
                    "end": 594,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 595,
                    "end": 598,
                    "text": "24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Constant tests-per-item first stage"
        },
        {
            "text": "Theorem 2. Using a first stage with a constant number r of tests per item and an average number of \u03c3 items per test, conservative two-stage testing can be completed in",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Constant tests-per-item first stage"
        },
        {
            "text": "tests on average. When the prevalence p is known, r and \u03c3 can be numerically optimised.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Constant tests-per-item first stage"
        },
        {
            "text": "Proof. A nondefective item appearing in a given test sees a positive result with probability",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Constant tests-per-item first stage"
        },
        {
            "text": "as we get a positive result unless in that round all k defective items avoid the test that the given item is in. Thus all r tests are positive with probability (1 \u2212 e \u2212p\u03c3 ) r , since splitting the tests into rounds and using the fixed-k prior ensures these events are independent. Therefore the number of tests required is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Constant tests-per-item first stage"
        },
        {
            "text": "We now consider a first stage with both constant tests-per-item and constant items-per-test. We take r tests per item and s items per test. Note that doublecounting tells us we must have T 1 s = nr. Note also that r and s must be integers. Taking r = 1 and s = 1 gives individual testing. Taking r = 1 and s > 1 gives Dorfman's algorithm. Taking r = 2 gives the 'double pooling' algorithm of Broder and Kumar [10] . Taking r > 2 gives Broder and Kumar's more general 'r-pooling' algorithm [10] . Work to discuss doubly constant designs in the context of testing for covid-19 includes [8, 10, 30] .",
            "cite_spans": [
                {
                    "start": 409,
                    "end": 413,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 489,
                    "end": 493,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 584,
                    "end": 587,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 588,
                    "end": 591,
                    "text": "10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 592,
                    "end": 595,
                    "text": "30]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "Theorem 3. Using a first stage with a constant number r of tests per item and a constant number s of items per test, conservative two-stage testing can be completed in",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "tests on average, where q = 1 \u2212 p. When the prevalence p is known, r and s can be numerically optimised.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "We note that the expression here is the same as that heuristically demonstrated by Broder and Kumar [10] , who use the i.i.d prior but assume independence within rounds. (They say that they will discuss the accuracy of this approximation in the final version of [10] ). By using the fixed-k prior here, we actually do have independence within rounds, so can formally prove the result. In the large n limit, this then transfers to the i.i.d. prior, as discussed earlier and in [5, Appendix to Chapter 1].",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 104,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 262,
                    "end": 266,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "Proof. The probability that a given test containing a given nondefective item is negative is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "Since with the fixed-k prior we have independence between rounds, we have that the probability all the tests containing the nondefective item are positive, meaning the item requires retesting in the second stage, is (1 \u2212 q s\u22121 ) r .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "Over all, the expected number of tests required is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "Note that putting r = 1 does indeed give",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "as for Dorfman's algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Doubly constant first stage"
        },
        {
            "text": "In order to see how good out conservative two-stage algorithms are, we will compare the number of tests they require to a theretical lower bound (Theorem 4). It will be convenient to start with a lower bound for usual non-conservative two-stage testing (Theorem 5), which may be of independent interest. We will then show how to adapt the argument to conservative two-stage testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bounds"
        },
        {
            "text": "Let us start by thinking about a lower bound on the number of tests necessary for usual two-stage testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "Theorem 4. The expected number of tests required for two-stage testing is at least",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "Proof. Our goal is to bound the expected number T 2 of items that are not classified as DND or DD. A nondefective item fails to be classified DND if and only if it only appears in positive tests -that is, if for each test it is in, one of the other items is defective. A defective item fails to be classified DD if -but not only if -for each test it is in, one of the other items is defective. (It's not 'only if' because we also require one of these tests to contain solely definite nondefectives.) Let us call an item hidden if every test it is in contains at least one other defective item. Then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "where H i is the event that item i is a hidden nondefective.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "We seek a bound at least as good as individual testing T = n. Then without loss of generality we may assume there are no tests of weight w t = 1 in the first stage. If there is one, remove it and the item it tests; this leaves p the same, does not increase the error probability, and reduces the number of available tests per item.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "It will be convenient to write x ti = 1 if item i is in test t, and x ti = 0 if it is not. With this notation, the probability that item i is hidden is bounded by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "where q = 1 \u2212 p, due to a result of [2] . Note that 1 \u2212 q wt\u22121 is the probability of the event that i gets hidden in test t, and the bound (1) follows by applying the FKG inequality to these increasing events; see [2] for details. It will be useful later to write L(i) for the logarithm of the bound (1), so P(H i ) \u2265 e L(i) , where",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 39,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 214,
                    "end": 217,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "x ti ln(1 \u2212 q wt\u22121 ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for two-stage testing"
        },
        {
            "text": "We now use the arithmetic mean-geometric mean inequality in the form",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "to get the bound",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "We now need to bound term inside the exponential. By manipulations similar to those in [2] we have",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 90,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "as in the statement of the theorem. (We introduce the minus sign so that f (p) is positive.) Putting this back into (2), we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "Thus the total expected number of tests required is at least",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "To find the optimal value of T 1 , we differentiate this, to get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "with optimum",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "and we are done.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The expected number of hidden items is"
        },
        {
            "text": "We can now use the machinery of the previous result to prove a lower bound for conservative two-stage testing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "Theorem 5. For conservative two-stage group testing we have the following bounds:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "(ln g(p) + 1);",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "3. ET \u2265 n p + 1 f It may be useful to know that Bound 2 dominates for p < 0.171, and Bound 3 dominates for 0.171 < p < 0.382.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "Here, f is as in Theorem 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "Proof. Bound 1 is a universal bound of Fischer, Klasner and Wegenera [16] that applies to any group testing algorithm. It's left to prove 2 and 3. The proof bound for conservative two-stage testing proceeds in a similar way to that of Theorem 4. There are two different ways we can count the number of items that require testing in the second stage. For Bound 2, we count every item that appears solely in positive tests -such an item is either defective or a hidden nondefective. For Bound 3, we count all the defective items, of which there are pn on average, plus all nondefective items that appear solely in positive tests.",
            "cite_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "For Bound 2, the probability a test of weight w is positive is 1 \u2212 q w , where q = 1 \u2212 p. We use the same argument as before -this time in less detail. (For conservative two-sage testing we don't have to be so careful about ruling out individual tests in the first round: they can simply be moved into the second round.) The probability an item i appears in only positive tests is P(E i ) \u2265 t:xti=1 (1 \u2212 q wt ), by the FKG inequality. Going through exactly the same argument, the expected number of items in only positive tests is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "where g(p) = max w=2,3,...",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "giving an average number of tests ET \u2265 T 1 + n exp \u2212g(p) T 1 n .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "Optimising T 1 the same way gives the final bound ET \u2265= n 1 g(p)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "(ln g(p) + 1) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "For Bound 3, we must test the average of pn defective items, plus the average of (1 \u2212 p)nP(H i ) hidden nondefectives; here (1 \u2212 p)n is the average number of nondefectives, and P(H i ) is the probability a given nondefective is hidden. We can use from before the bound",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lower bound for conservative two-stage testing"
        },
        {
            "text": "ET \u2265 T 1 + pn + (1 \u2212 p)n exp \u2212f (p) T 1 n .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "This gives"
        },
        {
            "text": "Optimising in the same way gives",
            "cite_spans": [],
            "ref_spans": [],
            "section": "This gives"
        },
        {
            "text": "and hence",
            "cite_spans": [],
            "ref_spans": [],
            "section": "This gives"
        },
        {
            "text": "Comparing these bound with the results of our algorithms (see Figure 1 ), we see that testing with a doubly constant first stage is extremely close to optimal for all p.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 62,
                    "end": 70,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "This gives"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Assessment of specimen pooling to conserve SARS-CoV-2 testing resources",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Abdalhamid",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Bilder",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Mccutchen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Hinrichs",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Koepsell",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "C"
                    ],
                    "last": "Iwen",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "American Journal of Clinical Pathology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Individual testing is optimal for nonadaptive group testing in the linear regime",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aldridge",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "65",
            "issn": "4",
            "pages": "2058--2061",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Rates of adaptive group testing in the linear regime",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aldridge",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE International Symposium on Information Theory (ISIT)",
            "volume": "",
            "issn": "",
            "pages": "236--240",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Group testing algorithms: bounds and simulations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aldridge",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Baldassini",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "60",
            "issn": "6",
            "pages": "3671--3687",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Group testing: an information theory perspective. Foundations and Trends in Communications and Information Theory",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aldridge",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Scarlett",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "15",
            "issn": "",
            "pages": "196--392",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Optimization of group size in pool testing strategy for SARS-CoV-2: A simple mathematical model",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Arag\u00f3n-Caqueo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fern\u00e1ndez-Salinas",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Laroze",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Medical Virology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "The capacity of adaptive group testing",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Baldassini",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aldridge",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "2013 IEEE International Symposium on Information Theory Proceedings (ISIT)",
            "volume": "",
            "issn": "",
            "pages": "2676--2680",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Pooled RNA extraction and PCR assay for efficient SARS-CoV-2 detection. medRxiv",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Ben-Ami",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Increasing testing capacity for SARS-CoV-2 by pooling specimens. Significance",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "R"
                    ],
                    "last": "Bilder",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "C"
                    ],
                    "last": "Iwen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Abdalhamid",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Tebbs",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "S"
                    ],
                    "last": "Mcmahan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A note on double pooling tests. arXiv, 2020",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "Z"
                    ],
                    "last": "Broder",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Non-adaptive probabilistic group testing with noisy measurements: near-optimal bounds with efficient algorithms",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Chan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "H"
                    ],
                    "last": "Che",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jaggi",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Saligrama",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "49th Annual Allerton Conference on Communication, Control, and Computing",
            "volume": "",
            "issn": "",
            "pages": "1832--1839",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Information-theoretic and algorithmic thresholds for group testing",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Coja-Oghlan",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Gebhard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hahn-Klimroth",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Loick",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "The detection of defective members of large populations",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dorfman",
                    "suffix": ""
                }
            ],
            "year": 1943,
            "venue": "The Annals of Mathematical Statistics",
            "volume": "14",
            "issn": "4",
            "pages": "436--440",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Multi-stage group testing improves efficiency of large-scale COVID-19 screening",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Eberhardt",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Breuckmann",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Eberhardt",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Clinical Virology",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "On the cut-off point for combinatorial group testing",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Fischer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Klasner",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Wegenera",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Discrete Applied Mathematics",
            "volume": "91",
            "issn": "1",
            "pages": "83--92",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Estimating the number of infections and the impact of non-pharmaceutical interventions on COVID-19 in 11 European countries",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Flaxman",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Report",
            "volume": "13",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Early detection of superspreaders by mass group pool testing can mitigate COVID-19 pandemic. medRxiv",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "B"
                    ],
                    "last": "Gongalsky",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Group testing against COVID-19",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Gossner",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Is group testing ready for prime-time in disease identification? arXiv",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Haber",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Malinovsky",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Albert",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Boosting test-efficiency by pooled testing strategies for SARS-CoV-2. arXiv",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hanel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Thurner",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "A massively parallel COVID-19 diagnostic assay for simultaneous testing of 19200 patient samples",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hossain",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Reis",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rahman",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Salis",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A method for detecting all defective members in a population by group testing",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "K"
                    ],
                    "last": "Hwang",
                    "suffix": ""
                }
            ],
            "year": 1972,
            "venue": "Journal of the American Statistical Association",
            "volume": "67",
            "issn": "339",
            "pages": "605--608",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Performance of group testing algorithms with near-constant tests per item",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Johnson",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aldridge",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Scarlett",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "65",
            "issn": "2",
            "pages": "707--723",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Analysis and applications of adaptive group testing methods for covid-19. medRxiv",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mentus",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Romeo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dipaola",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Group testing with random pools: optimal two-stage algorithms",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "M\u00e9zard",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Toninelli",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "57",
            "issn": "3",
            "pages": "1736--1745",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "On accelerated testing for COVID-19 using group testing",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "R"
                    ],
                    "last": "Narayanan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Heidarzadeh",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Laxminarayan",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Everybody in the pool: Researchers use algorithms to tackle the coronavirus test shortage",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "S"
                    ],
                    "last": "Perry",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "IEEE Spectrum",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Efficient high throughput SARS-CoV-2 testing to detect asymptomatic carriers. medRxiv",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Evaluation of group testing for SARS-CoV-2 RNA. medRxiv",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sinnott-Armstrong",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Hickey",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Evaluation of COVID-19 RT-qPCR test in multi-sample pools. medRxiv",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Yelin",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Asymptotic analysis of optimal nested grouptesting procedures",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Zaman",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Pippenger",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Probability in the Engineering and Informational Sciences",
            "volume": "30",
            "issn": "4",
            "pages": "547--552",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Theoretical performance of conservative two-stage algorithms, compared to the lower bound of Theorem 5.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "e \u22121 (1 \u2212 p)/p \u2265 1. Then, ET \u223c epn ln e \u22121 1 \u2212 p p + pn + epn = np e ln e \u22121",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "ln 1 \u2212 (1 \u2212 p) w .",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Simulation results for conservative two-stage algorithms with n = 1000 and p = 0.027, compared to theoretical results suggested by the n \u2192 \u221e limit.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}