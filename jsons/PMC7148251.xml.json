{
    "paper_id": "PMC7148251",
    "metadata": {
        "title": "ReadNet: A Hierarchical Transformer Framework for Web Article Readability Analysis",
        "authors": [
            {
                "first": "Joemon",
                "middle": [
                    "M."
                ],
                "last": "Jose",
                "suffix": "",
                "email": "joemon.jose@glasgow.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Emine",
                "middle": [],
                "last": "Yilmaz",
                "suffix": "",
                "email": "emine.yilmaz@ucl.ac.uk",
                "affiliation": {}
            },
            {
                "first": "Jo\u00e3o",
                "middle": [],
                "last": "Magalh\u00e3es",
                "suffix": "",
                "email": "jm.magalhaes@fct.unl.pt",
                "affiliation": {}
            },
            {
                "first": "Pablo",
                "middle": [],
                "last": "Castells",
                "suffix": "",
                "email": "pablo.castells@uam.es",
                "affiliation": {}
            },
            {
                "first": "Nicola",
                "middle": [],
                "last": "Ferro",
                "suffix": "",
                "email": "ferro@dei.unipd.it",
                "affiliation": {}
            },
            {
                "first": "M\u00e1rio",
                "middle": [
                    "J."
                ],
                "last": "Silva",
                "suffix": "",
                "email": "mjs@inesc-id.pt",
                "affiliation": {}
            },
            {
                "first": "Fl\u00e1vio",
                "middle": [],
                "last": "Martins",
                "suffix": "",
                "email": "flaviomartins@acm.org",
                "affiliation": {}
            },
            {
                "first": "Changping",
                "middle": [],
                "last": "Meng",
                "suffix": "",
                "email": "meng40@purdue.edu",
                "affiliation": {}
            },
            {
                "first": "Muhao",
                "middle": [],
                "last": "Chen",
                "suffix": "",
                "email": "muhaochen@ucla.edu",
                "affiliation": {}
            },
            {
                "first": "Jie",
                "middle": [],
                "last": "Mao",
                "suffix": "",
                "email": "mjmjmtl@gmail.com",
                "affiliation": {}
            },
            {
                "first": "Jennifer",
                "middle": [],
                "last": "Neville",
                "suffix": "",
                "email": "neville@purdue.edu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Readability is an important linguistic measurement that indicates how easily readers can comprehend a particular document. Due to the explosion of web and digital information, there are often hundreds of articles describing the same topic, but vary in levels of readability. This can make it challenging for users to find the articles online that better suit their comprehension abilities. Therefore, an automated approach to assessing readability is a critical component for the development of recommendation strategies for web information systems, including digital libraries and web encyclopedias.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Text readability is defined as the overall effect of language usage and composition on readers\u2019 ability to easily and quickly comprehend the document [14]. In this work, we focus on evaluating document difficulty based on the composition of words and sentences. Consider the following two descriptions of the concept rainbow as an example.",
            "cite_spans": [
                {
                    "start": 151,
                    "end": 153,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "\nA more rigid scientific definition from English Wikipedia: A rainbow is a meteorological phenomenon that is caused by reflection, refraction and dispersion of light in water droplets resulting in a spectrum of light appearing in the sky.A more generic description from the Simple English Wikipedia: A rainbow is an arc of color in the sky that can be seen when the sun shines through falling rain. The pattern of colors starts with red on the outside and changes through orange, yellow, green, blue, to violet on the inside.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Clearly, the first description provides more rigidly expressed contents, but is more sophisticated due to complicated sentence structures and the use of professional words. In contrast, the second description is simpler, with respect to both grammatical and document structures. From the reader\u2019s perspective, the first definition is more appropriate for technically sophisticated audiences, while the second one is suitable for general audiences, such as parents who want to explain rainbows to their young children.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The goal of Readability Analysis is to provide a rating regarding the difficulty of an article for average readers. As the above example illustrates that, many approaches for automatically judging the difficulty of the articles are rooted in two factors: the difficulty of the words or phrases, and the complexity of syntax [11]. To characterize these factors, existing works [3, 29] mainly rely on some explicit features such as Average Syllables Per Word, Average Words Per Sentence, etc. For example, the Flesch-Kincaid index is a representative empirical measure defined as a linear combination of these factors [4]. Some later approaches mainly focus on proposing new features with the latest CohMetrix 3.0 [36] providing 108 features, and they combine and use the features using either linear functions or statistical models such as Support Vector Machines or multilayer perceptron [12, 40, 41, 43, 51]. While these approaches have shown some merits, they also lead to several drawbacks. Specifically (1) they do not consider sequential and structural information, and (2) they do not capture sentences-level or document-level semantics that are latent but essential to the task [11].",
            "cite_spans": [
                {
                    "start": 325,
                    "end": 327,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 377,
                    "end": 378,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 380,
                    "end": 382,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 617,
                    "end": 618,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 713,
                    "end": 715,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 889,
                    "end": 891,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 893,
                    "end": 895,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 897,
                    "end": 899,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 901,
                    "end": 903,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 905,
                    "end": 907,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 1186,
                    "end": 1188,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To address these issues, we propose ReadNet, a comprehensive readability classification framework that uses a hierarchical transformer network. The self-attention portion of the transformer encoder is better able to model long-range and global dependencies among words. The hierarchical structure can capture how words form sentences, and how sentences form documents, meanwhile reduce the model complexity exponentially. Moreover, explicit features indicating the readability of different granularities of text can be leveraged and aggregated from multiple levels of the model. We compare our proposed model to a number of widely-adopted document encoding techniques, as well as traditional readability analysis approaches based on explicit features. Experimental results on three benchmark datasets show that our work properly identifies the document representation techniques, and achieves the state-of-the-art performance by significantly outperform previous approaches.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Existing computational methods for readability analysis [3, 11, 29, 40, 53] mainly use empirical measures on the symbolic aspects of the text, while ignoring the sequence of words and the structure of the article. The Flesch-Kincaid index [28] and related variations use a linear combination of explicit features.",
            "cite_spans": [
                {
                    "start": 57,
                    "end": 58,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 60,
                    "end": 62,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 64,
                    "end": 66,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 68,
                    "end": 70,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 72,
                    "end": 74,
                    "mention": "53",
                    "ref_id": "BIBREF48"
                },
                {
                    "start": 240,
                    "end": 242,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Although models based on these traditional features are helpful to the quantification of readability for small and domain-specific groups of articles, they are far from generally applicable for a larger body of web articles [10, 17, 45]. Because those features or formulas generated from a small number of training text specifically selected by domain experts, they are far from generally representing the readability of large collections of corpora. Recent machine learning methods on readability evaluation are generally in the primitive stage. [18] proposes to combine language models and logistic regression. The existing way to integrate features is through a statistical learning method such as SVM [12, 20, 40, 41, 43, 51]. These approaches ignore the sequential or structural information on how sentences construct articles. Efforts have also been made to select optimal features from current hundreds of features [15]. Some computational linguistic methods have been developed to extract higher-level language features. The widely-adopted Coh-Metrix [22, 37] provides multiple features based on cohesion such as referential cohesion and deep cohesion.",
            "cite_spans": [
                {
                    "start": 225,
                    "end": 227,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 229,
                    "end": 231,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 233,
                    "end": 235,
                    "mention": "45",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 548,
                    "end": 550,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 706,
                    "end": 708,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 710,
                    "end": 712,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 714,
                    "end": 716,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 718,
                    "end": 720,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 722,
                    "end": 724,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 726,
                    "end": 728,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 923,
                    "end": 925,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1060,
                    "end": 1062,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1064,
                    "end": 1066,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Plenty of works have been conducted on utilizing neural models for sentimental or topical document classification or ranking, while few have paid attention to the readability analysis task. The convolutional neural network (CNN) [27] is often adopted in sentence-level classification which leverages local semantic features of sentence composition that are provided by word representation approaches. In another line of approaches, a recursive neural network [46] is adopted, which focuses on modeling the sequence of words or sentences. Hierarchical structures of such encoding techniques are proposed to capture structural information of articles, and have been widely used in tasks of document classification [7, 32, 48], and sequence generation [30] and sub-article matching [6]. Hierarchical attention network [52] is the current state-of-the-art method for document classification, which employs attention mechanisms on both word and sentence levels to capture the uneven contribution of different words and sentences to the overall meaning of the document. The Transformer model [50] uses multi-head self-attention to perform sequence-to-sequence translation. Self-attention is also adopted in text summarization, entailment and representation [31, 38]. Unlike topic and sentiment-related document classification tasks that focus on leveraging portions of lexemes that are significant to the overall meanings and sentiment of the document, readability analysis requires the aggregation of difficulty through all sentence components. Besides, precisely capturing the readability of documents requires the model to incorporate comprehensive readability-aware features, including difficulty, sequence and structure information, to the corresponding learning framework.",
            "cite_spans": [
                {
                    "start": 230,
                    "end": 232,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 460,
                    "end": 462,
                    "mention": "46",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 713,
                    "end": 714,
                    "mention": "7",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 716,
                    "end": 718,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 720,
                    "end": 722,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 750,
                    "end": 752,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 780,
                    "end": 781,
                    "mention": "6",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 816,
                    "end": 818,
                    "mention": "52",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1087,
                    "end": 1089,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 1252,
                    "end": 1254,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1256,
                    "end": 1258,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "The readability analysis problem is defined as an ordinal regression problem for articles. Given an article with up to n sentences and each sentence with up to m words, an article can be represented as a matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{A}}$$\\end{document} whose i-th row \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{A}}_{i,:}$$\\end{document} corresponds to the i-th sentences, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A_{i,j}$$\\end{document} denotes the j-th word of the i-th sentence. Given an article \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{A}}$$\\end{document}, a label will be provided to indicate the readability of this article.",
            "cite_spans": [],
            "section": "Problem Definition ::: Preliminary",
            "ref_spans": []
        },
        {
            "text": "We consider the examples introduced in Sect. 1, where two articles describe the same term \u201crainbow\u201d. The first rigorous scientific article can be classified as \u201cdifficult\u201d, and the second general description article can be classified as \u201ceasy\u201d.",
            "cite_spans": [],
            "section": "Problem Definition ::: Preliminary",
            "ref_spans": []
        },
        {
            "text": "Instead of classifying articles into binary labels like \u201ceasy\u201d or \u201cdifficult\u201d, more fine-grained labels can help people better understand the levels of readability. For instance, we can map the articles in standardization systems of English tests such as 5-level Cambridge English Exam (CEE), where articles from professional level English exam (CPE) are regarded than those from introductory English exam (KET).",
            "cite_spans": [],
            "section": "Problem Definition ::: Preliminary",
            "ref_spans": []
        },
        {
            "text": "Previous works [11, 21, 22, 24, 25, 28, 34] have proposed empirical features to evaluate readability. Correspondingly, we divide these features into sentence-level features and document-level features. Sentence-level features seek to evaluate the difficulty of sentences. For instance, the sentence-level feature \u201cnumber of words\u201d for sentences can be averaged into \u201cnumber of words per sentence\u201d to evaluate the difficulty of documents. Document-level features include the traditional readability indices and cohesion\u2019s proposed by Coh-Metrix [22]. These features are listed in Table 1.\n",
            "cite_spans": [
                {
                    "start": 16,
                    "end": 18,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 20,
                    "end": 22,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 24,
                    "end": 26,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 28,
                    "end": 30,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 32,
                    "end": 34,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 36,
                    "end": 38,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 40,
                    "end": 42,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 545,
                    "end": 547,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Explicit Features ::: Preliminary",
            "ref_spans": [
                {
                    "start": 585,
                    "end": 586,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Current approaches [12, 41, 43] average the sentence-level features of each sentence to construct document level features. Furthermore, these features are concatenated with document-level features, and use an SVM to learn on these features. The limitation lies in failing to capture the structure information of sentences and documents. For instance, in order to get the sentence level features for the document, it averages all these features of each sentence. It ignores how these sentences construct an article and which parts of the document more significantly decides the readability of the document. While cohesion features provided by Coh-Metrix tries to captures relationships between sentences, these features mainly depend on the repeat of words across multiple sentences. They did not directly model how these sentences construct a document in perspectives of structure and sequence.\n",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 22,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 24,
                    "end": 26,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 28,
                    "end": 30,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                }
            ],
            "section": "Explicit Features ::: Preliminary",
            "ref_spans": []
        },
        {
            "text": "Briefly speaking, existing works are mainly contributing more features as shown in Table 1. But the current models used to aggregate these features are based on SVM and linear models. In this work, we target to propose a more advanced model to better combine these features with document information.",
            "cite_spans": [],
            "section": "Explicit Features ::: Preliminary",
            "ref_spans": [
                {
                    "start": 89,
                    "end": 90,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "In this subsection, we introduce the encoding process of sentences in hierarchical mutli-head self-attention. The encoding process has three steps: (1) the self-attention encoder transforms the input sequence into a series of latent vectors; (2) the attention layer aggregates the encoded sequential information based on the induced significance of input units; (3) The encoded information is combined with the explicit features.",
            "cite_spans": [],
            "section": "From Words to Sentences ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "Transformer Self-attention Encoder. This encoder is adapted from the vanilla Transformer encoder [50]. The input for this encoder is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{A}}_{i,:}$$\\end{document}, which represents the i-th sentence.",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 100,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                }
            ],
            "section": "From Words to Sentences ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "The Embedding layer encodes each word \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A_{i,j}$$\\end{document} into a d-dimensional vector based on word embedding. The output is a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m \\times d$$\\end{document}-dimensional matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{B}}$$\\end{document} where d is the embedding dimension and m is the number of words.",
            "cite_spans": [],
            "section": "From Words to Sentences ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "The position encoding layer indicates the relative position of each word \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A_{i,j}$$\\end{document}. The elements of positional embedding matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{P}}$$\\end{document} where values in the i-th row j-th column is defined as follows.1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} P_{i, j}= {\\left\\{ \\begin{array}{ll} \\sin (i / 10^{4j/d})&{} { j {\\text { is even}} }\\\\ \\cos (i / 10^{4(j-1)/d})&{} { j {\\text { is odd}}} \\end{array}\\right. } \\end{aligned}$$\\end{document}The embedded matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{B}}$$\\end{document} and positional embedding matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{P}}$$\\end{document} are added into the initial hidden state matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}^{(0)} = {\\varvec{B}}+ {\\varvec{P}}$$\\end{document}. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}^{(0)}$$\\end{document} will go through a stack of p identical layers. Each layer contains two parts: (i) the Multi-Head Attention donated as function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ f_{MHA} $$\\end{document} defined in Eq. 2, and (ii) the Position-wise Feed-Forward \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ f_{FFN}$$\\end{document} defined in Eq. 4. Layer normalization is used to avoid gradient vanishing or explosion.",
            "cite_spans": [],
            "section": "From Words to Sentences ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "Multi-head Self-Attention function (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_{MHA}$$\\end{document}) [50] encodes the relationship among query matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{Q}}$$\\end{document}, key matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{K}}$$\\end{document} and value matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{V}}$$\\end{document} from different representation subspaces at different positions. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_k = d/h$$\\end{document}. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}}$$\\end{document} is a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d \\times d$$\\end{document} weight matrix. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\oplus $$\\end{document} denotes concatenation. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}}_{Ki}, {\\varvec{W}}_{Vi}, {\\varvec{W}}_{Qi}$$\\end{document} are \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d \\times d_k$$\\end{document} weight matrix for head function \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$g_i$$\\end{document}.2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} f_{MHA} ({\\varvec{Q}}, {\\varvec{K}}, {\\varvec{V}}) = (g_1({\\varvec{Q}}, {\\varvec{K}}, {\\varvec{V}}))\\,\\oplus \\,\\ldots \\,\\oplus \\,g_h({\\varvec{Q}}, {\\varvec{K}}, {\\varvec{V}}) ) {\\varvec{W}}\\end{aligned}$$\\end{document}\n3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} g_i({\\varvec{Q}}, {\\varvec{K}}, {\\varvec{V}}) = \\mathrm {softmax}(\\frac{ {\\varvec{Q}}{\\varvec{W}}_{Qi} ({\\varvec{K}}{\\varvec{W}}_{Ki})^{T}}{\\sqrt{d_k}}) ({\\varvec{V}}{\\varvec{W}}_{Vi}) \\end{aligned}$$\\end{document}Position-wise Feed-Forward Function\n\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_{FFN}$$\\end{document} [50] adopts two 1-Dimensional convolution layers with kernel size 1 to encode input matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{X}}$$\\end{document}.4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} f_{FFN}({\\varvec{X}}) = \\mathrm {Conv1D}( \\mathrm {ReLU} ( \\mathrm {Conv1D}({\\varvec{X}}))) \\end{aligned}$$\\end{document}For the l-th encoder layer, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}^{(l)}$$\\end{document} is encoded into \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}^{(l+1)}$$\\end{document} according to Eq. 55\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{H}}^{(l+1)} = f_{FFN}( f_{MHA} ({\\varvec{H}}^{(l)}, {\\varvec{H}}^{(l)}, {\\varvec{H}}^{(l)})) \\end{aligned}$$\\end{document}Attention Aggregation Layer. After p transformer encoder layers, each sentence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{A}}_{i,:}$$\\end{document} is encoded into a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m \\times d$$\\end{document}-dimensional matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}^{(p)}$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 331,
                    "end": 333,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 4891,
                    "end": 4893,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                }
            ],
            "section": "From Words to Sentences ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "We first pass \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}^{(p)}$$\\end{document} through a feed forward layer with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d\\ \\times \\ d$$\\end{document} dimensional weights \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}}_1$$\\end{document} and bias term \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b_1$$\\end{document} to obtain a hidden representation as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{U}}$$\\end{document}:\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ {\\varvec{U}}= \\tanh ({\\varvec{H}}^{(p)} {\\varvec{W}}_1 + b_1), $$\\end{document}then compute the similarity between \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{U}}$$\\end{document} and the trainable \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d \\times 1$$\\end{document} dimensional context matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{C}}$$\\end{document} via\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ {\\varvec{w}}= \\mathrm {softmax} ({\\varvec{U}}{\\varvec{C}}), $$\\end{document}which we use as importance weights to obtain the final embedding of the sentence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{A}}_{i,:}$$\\end{document}:6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{h}}_i = \\sum _{byRow} {\\varvec{H}}^{(p)} \\cdot {\\varvec{w}}\\end{aligned}$$\\end{document}Combination of Explicit Features. The sentence level features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{u}}_i$$\\end{document} introduced in Sect. 3.2 Table 1 for i-th sentence are concatenated by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{h}}_i^*} = {{\\varvec{h}}_i} \\oplus {{\\varvec{u}}_i}$$\\end{document}.",
            "cite_spans": [],
            "section": "From Words to Sentences ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": [
                {
                    "start": 4440,
                    "end": 4441,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "The second level of the hierarchical learning architecture is on top of the first layer. n encoded vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{h}}_i^*} (1 \\le i \\le n)$$\\end{document} are concatenated as the input for this layer. The structure of second level is the same as the first level. The output of this level is a vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{y}}$$\\end{document} as the overall embedding of this article.",
            "cite_spans": [],
            "section": "From Sentences to Articles ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "The goal of the transfer layer is to improve prediction quality on a target task where training data are scarce, while a large amount of other training data are available for a set of related tasks.",
            "cite_spans": [],
            "section": "Transfer Layer ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "The readability analysis problem suffers from the lack of labeled data. Traditional benchmark datasets labeled by domain experts typically contain a small number of articles. For instance, CEE contains 800 articles and Weebit contains around 8 thousand articles. Such quantities of articles are far smaller than those for sentiment or topic-related document classification tasks which typically involve over ten thousand articles even for binary classification [7, 27]. On the other hand, with the emerging of online encyclopedia applications such as Wikipedia, it provides a huge amount of training dataset. For instance, English Wikipedia and Simple-English Wikipedia contain more than 100 thousand articles which can be used to train a deep learning model.",
            "cite_spans": [
                {
                    "start": 462,
                    "end": 463,
                    "mention": "7",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 465,
                    "end": 467,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Transfer Layer ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "One fully connected layer combines the article embedding vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{y}}$$\\end{document} and document-level features \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{v}}}$$\\end{document} from Table 1 to output the readability label vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{r}}$$\\end{document} after a Softmax function. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}}_t$$\\end{document} is the weight of the fully connected layer. For dataset with m categories of readability ratings, each document is embedded into \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{r}}$$\\end{document} with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m-1$$\\end{document} dimensions.\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {{\\varvec{r}}} = \\mathrm {softmax}({{\\varvec{W}}_t}({\\varvec{y}}\\oplus {{\\varvec{v}}})) \\end{aligned}$$\\end{document}If transfer learning is needed, instead of random initialization, this network is initialized with a pre-trained network based on a larger corpus. During the training process, update the transfer layer while keeping all other layers frozen. If transfer learning is not needed, all layers are updated during the training process.",
            "cite_spans": [],
            "section": "Transfer Layer ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": [
                {
                    "start": 701,
                    "end": 702,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Given dataset with m categories of readability ratings, the goal is to minimize ordinal regression loss [42] defined as Eq. 7. \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{r}}_{k}$$\\end{document} represents the k-th dimension of the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{r}}$$\\end{document} vector. y is the true label. The threshold parameter \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta _1, \\theta _2,\\ldots \\theta _{m-1}$$\\end{document} are also learned automatically from the data.7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L({\\varvec{r}};y) = -\\sum _{k=1}^{m-1}f(s(k;y)(\\theta _k - {\\varvec{r}}_k)), \\quad where \\quad s(k;y)={\\left\\{ \\begin{array}{ll} -1 &{} k<y\\\\ +1 &{} k\\ge y \\end{array}\\right. } \\end{aligned}$$\\end{document}Here, the objective of learning the readability analysis model is essentially different from that of a regular document classification model, since the classes here do form a partial-order. However, the case of two classes degenerates the learning to the same as that of a binary classifier.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 107,
                    "mention": "42",
                    "ref_id": "BIBREF36"
                }
            ],
            "section": "Learning Objective ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "For self-attention, the path length in the computation graph between long-range dependencies in the network is O(1) instead of O(n) for recurrent models such as LSTM. Shorter path length in the computation graph makes it easier to learn the interactions between any elements in the sequence. For readability analysis, modeling the overall interaction between words is more important than modeling the consequent words. For semantic understanding, the consequence of two words such as \u201cvery good\u201d and \u201cnot good\u201d make distinct semantic meanings. While for readability analysis, it does not make difference in difficulty to understand it. The overall evaluation of the words difficulties in the sentences matters.",
            "cite_spans": [],
            "section": "Why Hierarchical Self-attention ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "The hierarchical learning structure benefits in two ways. First, it mimics human reading behaviors, since the sentence is a reasonable unit for people to read, process and understand. People rarely check the interactions between arbitrary words across different sentences in order to understand the article. Second, the hierarchical structure can reduce parameter complexity. For a document with n sentences, m words per sentence, d dimension per word, the parameter complexity of the model is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$O((nm)^2 d)$$\\end{document} for single level structure. While for the hierarchical structure, the parameter complexity is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$O(m^2d + n^2d)$$\\end{document}.",
            "cite_spans": [],
            "section": "Why Hierarchical Self-attention ::: Hierarchical Transformer for Readability Analysis",
            "ref_spans": []
        },
        {
            "text": "We use the following three datasets in our experiment. Table 2 reports the statistics of the three datasets including the average number of sentences per article \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n_{sent}$$\\end{document} and the average number of words per sentence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n_{word}$$\\end{document}.",
            "cite_spans": [],
            "section": "Datasets ::: Experiments",
            "ref_spans": [
                {
                    "start": 61,
                    "end": 62,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Wiki dataset [26] contains English Wikipedia and Simple English Wikipedia. Simple English Wikipedia thereof is a simplified version of English Wikipedia which only uses simple English words and grammars. This dataset contains 59,775 English Wikipedia articles and 59,775 corresponding Simple English Wikipedia articles.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 16,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Datasets ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Cambridge English Exam (CEE) [51] categorizes articles based on the criteria of five Cambridge English Exam level (KET, PET, FCE, CAE, CPE). The five ratings are sequentially from the easiest KET to the hardest CPE. In total, it contains 110 KET articles, 107 PET articles, 153 FCE articles, 263 CAE articles and 155 CPE articles. Even though this dataset designed for non-native speakers may differ from materials for native English speakers, the difficulty between five levels is still comparable. We test our model on this dataset in order to check whether our model can effectively evaluate the difficulty of English articles according to an existing standard.",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 32,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                }
            ],
            "section": "Datasets ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Weebit [49] is one of the largest dataset for readability analysis. It contains 7,676 articles targeted at different age group readers from Weekly Reader magazine and BBC-Bitesize website. Weekly Reader magazine categorizes articles according to the ages of targeted readers in 7\u20138, 8\u20139 and 9\u201310 years old. BBC-Bitesize has two levels for age 11\u201314 and 15\u201316. The targeted age is used to evaluate readability levels.\n\n\n",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 10,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                }
            ],
            "section": "Datasets ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "In this subsection, we provide a detailed evaluation of the proposed approach.",
            "cite_spans": [],
            "section": "Evaluation ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Baseline Approaches. We compare our proposed approach (denoted ReadNet) against the following baseline methods.",
            "cite_spans": [],
            "section": "Evaluation ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "\nStatistical classification algorithms based on explicit features: this category of baselines including the statistical classification algorithms that are widely adopted in a line of previous works [12, 20, 40, 41, 43, 51], such as multi-class Logistic Regression, the Linear SVM, and the Multilayer Perceptron (MLP) [49]. Explicit features on which these models are trained have been introduced in Sect. 3.2. Since this work targets at proposing a more advanced model to utilize features instead of proposing new features, all these features from Table 1 are used.Neural document classifiers: this category of baselines represents the other line of previous works that adopt variants of neural document models for sentence or document classification. Corresponding approaches including the Convolutional Neural Networks (CNN) [27], the Hierarchical Gated Neural Network with Long Short-term Memory (LSTM) [48], and the Hierarchical Attention Network (HATT) [52].The Hierarchical Attention Network combined with explicit features (HATT+), for which we use the same mechanism as our proposed approach to incorporate the explicit features into the representation of each sentence by the attentive RNN.Model Configurations. For article encoding, we limit the number of sentences of each article to up to 50, zero-pad short ones and truncate over-length ones. According to the data statistics in Table 2, 50 sentences are enough to capture the majority of information of articles in the datasets. For each sentence, we also normalize the number of words to be fed into the model as 50, also via zero-padding and truncating. We fix the batch size to 32, and use Adam [16] as the optimizer with a learning rate 0.001. The epochs of training for the neural models are limited to 300. We set the number of encoder layers p and q to 6. The embedding dimension \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d=100$$\\end{document}. Number of heads h in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_{MHA}$$\\end{document} is 3. CNN adopts the same configuration as [27]. Other statistical classification algorithms are trained until converge. Source code will be available in the final version.",
            "cite_spans": [
                {
                    "start": 199,
                    "end": 201,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 203,
                    "end": 205,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 207,
                    "end": 209,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 211,
                    "end": 213,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 215,
                    "end": 217,
                    "mention": "43",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 219,
                    "end": 221,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                },
                {
                    "start": 318,
                    "end": 320,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                },
                {
                    "start": 828,
                    "end": 830,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 907,
                    "end": 909,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 959,
                    "end": 961,
                    "mention": "52",
                    "ref_id": "BIBREF47"
                },
                {
                    "start": 1663,
                    "end": 1665,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 2501,
                    "end": 2503,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Evaluation ::: Experiments",
            "ref_spans": [
                {
                    "start": 554,
                    "end": 555,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1398,
                    "end": 1399,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Evaluation Protocol. We formalize the task as a classification task following previous works on the three benchmark datasets. In order to provide a valid quantitative evaluation, we have to follow the existing evaluation method to show the advantage of our proposed model compared with the baselines. We adopt 5-fold cross-validation to evaluate the proposed model and baselines. We report the classification accuracy that is aggregated on all folds of validation.",
            "cite_spans": [],
            "section": "Evaluation ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Results. The results are reported in Table 3. Traditional explicit features can provide satisfying results. Since the multi-class logistic regression, SVM and MLP models can combine the features number of words per sentence and number of syllabi per word which are included in Flesch-Kincaid score, they provide the reasonable result. CNN is only slightly better than random guess. We assume that this is because CNN does not capture the sequential and structural information of documents. The HATT approach provides the best among models without explicit features. The reasons root in the structure of the model which is able to capture length and structural information of the article. Since it also adopted a hierarchical structure, the conciseness of each sentence and that of the overall article structure is captured, which appears to be significant to the task. The explicit features further improve the results of HATT as shown by HATT+. Even without explicit features, our proposed approach is better than HATT+. HATT has appeared to be successful at highlighting some lexemes and sentence components that are significant to the overall meanings or sentiment of a document. However, unlike topic and sentiment-related document classification tasks, readability does not rely on several consecutive lexemes, but the aggregation of all sentence components. The path length in the computation graph between arbitrary components dependencies in ReadNet is O(1) instead of O(n) for HATT. Shorter path length in the computation graph makes it easier to learn the interactions between any arbitrary words in sentence level, or sentences in document-level.",
            "cite_spans": [],
            "section": "Evaluation ::: Experiments",
            "ref_spans": [
                {
                    "start": 43,
                    "end": 44,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Compared with traditional approaches, the main advantage of the proposed approach is that it uses the document encoder to learn how words are connected into sentences and how sentences are connected into documents. Baseline approaches only use the averaged explicit features of all the sentences. For these datasets, several extremely difficult and complicated sentences usually determine the readability of a document. This useful information is averaged and weakened by the total number of sentences in baselines.",
            "cite_spans": [],
            "section": "Evaluation ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "As shown in Table 3, the standard deviation of the CEE task is large compared with those in Wiki and Weebit tasks since the quantity of CEE articles is not enough to train a complex deep learning model. Transfer layer in ReadNet is utilized in three steps. First is to train and save the model from larger datasets such as Wiki or Weebit. Then, we initialize the model for CEE task and load the parameter weights from the saved model except for the transfer layer. Eventually on the target task, the transfer layer is trained while keeping all other layers fixed. As shown in Table 5, loading a pre-trained model based on Weebit or Wiki can increase the accuracy and decrease standard deviation on the CEE task. It is shown that a more accurate and stable model can be achieved by utilizing the transfer layer and well-trained models from related tasks.\n",
            "cite_spans": [],
            "section": "Analysis on Transfer Learning ::: Experiments",
            "ref_spans": [
                {
                    "start": 18,
                    "end": 19,
                    "mention": "3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 582,
                    "end": 583,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "Besides directly training and evaluating the same dataset, we also tried the model trained using Wikipedia dataset and evaluate on Cambridge English dataset. 10 articles are randomly selected from each level of Cambridge English Test. The probability of being classified as regular English Wikipedia instead of simple English Wikipedia is treated as the difficulty score. The average difficulty scores predicted by the model are shown in Table 4, which shows that our produced readability score implies correctly the difficulty of English documents for different levels of exams. A larger score indicates higher difficulty. These scores correctly indicate the difficulty levels of these exams.",
            "cite_spans": [],
            "section": "Analysis on Transfer Learning ::: Experiments",
            "ref_spans": [
                {
                    "start": 444,
                    "end": 445,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "We have proposed a model to evaluate the readability of articles which can make great contributions to a variety of applications. Our proposed Hierarchical Self-Attention framework outperforms existing approaches by combining hierarchical document encoders with the explicit features proposed by linguistics. For future works, we are interested in providing the personalized recommendation of articles based on the combination of article readability and the understanding ability of the user. Currently, readability of articles only evaluate the texts of articles, other modalities such as images [39] and taxonomies [8] considered to improve readers\u2019 understanding. More comprehensive document encoders such as RCNN [5] and tree LSTM [47] may also be considered.",
            "cite_spans": [
                {
                    "start": 598,
                    "end": 600,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 618,
                    "end": 619,
                    "mention": "8",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 718,
                    "end": 719,
                    "mention": "5",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 736,
                    "end": 738,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                }
            ],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Explicit features\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Statistics of datasets Wiki, Cambridge English Exam and Weebit\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Cross-validation classification accuracy and standard deviation (in parentheses) on Wikipedia (Wiki), Cambridge English Exam (CEE) and Weebit dataset. We report accuracy on three groups of models: (1) Statistical classification algorithms including multi-class logistic regression, Linear SVM and Multilayer Perceptron (MLP); (2) Three types of document classifier CNN, hierarchical GRNN using LSTM cells (LSTM), Hierarchical Attention Network (HATT); (3) Hierarchical Attention Network combined with explicit features (HATT+), and our proposed approach which combines explicit features and semantics with Hierarchical Self-Attention (ReadNet). Transfer learning is not used, and all parameters in the model are initialized randomly (transfer learning is evaluated separately in Table 5).\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Average readability scores of 10 randomly selected articles in Cambridge English Test predicted by our model trained using Wikipedia. PET, KET, FCE, CPE and CAE have increasing difficulty levels according to Cambridge English. The scores are the confidence scores of classified as regular English Wikipedia instead of simple English Wikipedia.\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Accuracy for CEE classification using the transfer layer. Original is the model not using transfer learning, and without loading trained weights from other dataset. Load Weebit is to load the parameters weights trained in Weebit except the transfer layer. Load Wiki is to load the parameters weights trained in Wiki except the transfer layer.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: ReadNet: proposed hierarchical transformer model specialized for readability analysis",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Lix and Rix: variations on a little-known readability index",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Anderson",
                    "suffix": ""
                }
            ],
            "year": 1983,
            "venue": "J. Read.",
            "volume": "26",
            "issn": "6",
            "pages": "490-496",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Computational assessment of text readability: a survey of current and future research",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Collins-Thompson",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ITL-Int. J. Appl. Linguist.",
            "volume": "165",
            "issn": "2",
            "pages": "97-135",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Predicting reading difficulty with statistical language models",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Collins-Thompson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Callan",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "J. Am. Soc. Inform. Sci. Technol.",
            "volume": "56",
            "issn": "13",
            "pages": "1448-1462",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "A new academic word list",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Coxhead",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "TESOL Q.",
            "volume": "34",
            "issn": "2",
            "pages": "213-238",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "The concept of readability",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Dale",
                    "suffix": ""
                },
                {
                    "first": "JS",
                    "middle": [],
                    "last": "Chall",
                    "suffix": ""
                }
            ],
            "year": 1949,
            "venue": "Elem. Engl.",
            "volume": "26",
            "issn": "1",
            "pages": "19-26",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "All mixed up? Finding the optimal feature set for general readability prediction and its application to English and Dutch",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "De Clercq",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Hoste",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Comput. Linguist.",
            "volume": "42",
            "issn": "3",
            "pages": "457-490",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Duchi",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Hazan",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Singer",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Mach. Learn. Res.",
            "volume": "12",
            "issn": "Jul",
            "pages": "2121-2159",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "A readability formula that saves time",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Fry",
                    "suffix": ""
                }
            ],
            "year": 1968,
            "venue": "J. Read.",
            "volume": "11",
            "issn": "7",
            "pages": "513-578",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "The varied uses of readability measurement today",
            "authors": [
                {
                    "first": "EB",
                    "middle": [],
                    "last": "Fry",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "J. Read.",
            "volume": "30",
            "issn": "4",
            "pages": "338-343",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Linguistic complexity: locality of syntactic dependencies",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Gibson",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Cognition",
            "volume": "68",
            "issn": "1",
            "pages": "1-76",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Coh-Metrix: analysis of text on cohesion and language",
            "authors": [
                {
                    "first": "AC",
                    "middle": [],
                    "last": "Graesser",
                    "suffix": ""
                },
                {
                    "first": "DS",
                    "middle": [],
                    "last": "McNamara",
                    "suffix": ""
                },
                {
                    "first": "MM",
                    "middle": [],
                    "last": "Louwerse",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Behav. Res. Methods Instrum. Comput.",
            "volume": "36",
            "issn": "2",
            "pages": "193-202",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "The fog index after twenty years",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gunning",
                    "suffix": ""
                }
            ],
            "year": 1969,
            "venue": "J. Bus. Commun.",
            "volume": "6",
            "issn": "2",
            "pages": "3-13",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "The measurement of readability: useful information for communicators",
            "authors": [
                {
                    "first": "GR",
                    "middle": [],
                    "last": "Klare",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "ACM J. Comput. Doc. (JCD)",
            "volume": "24",
            "issn": "3",
            "pages": "107-121",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "An analytic and cognitive parametrization of coherence relations",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Louwerse",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Cogn. Linguist.",
            "volume": "12",
            "issn": "3",
            "pages": "291-316",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "SMOG grading-a new readability formula",
            "authors": [
                {
                    "first": "GH",
                    "middle": [],
                    "last": "Mc Laughlin",
                    "suffix": ""
                }
            ],
            "year": 1969,
            "venue": "J. Read.",
            "volume": "12",
            "issn": "8",
            "pages": "639-646",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "",
            "authors": [
                {
                    "first": "DS",
                    "middle": [],
                    "last": "McNamara",
                    "suffix": ""
                },
                {
                    "first": "AC",
                    "middle": [],
                    "last": "Graesser",
                    "suffix": ""
                },
                {
                    "first": "PM",
                    "middle": [],
                    "last": "McCarthy",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Automated Evaluation of Text and Discourse with Coh-Metrix",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "Coh-Metrix: capturing linguistic features of cohesion",
            "authors": [
                {
                    "first": "DS",
                    "middle": [],
                    "last": "McNamara",
                    "suffix": ""
                },
                {
                    "first": "MM",
                    "middle": [],
                    "last": "Louwerse",
                    "suffix": ""
                },
                {
                    "first": "PM",
                    "middle": [],
                    "last": "McCarthy",
                    "suffix": ""
                },
                {
                    "first": "AC",
                    "middle": [],
                    "last": "Graesser",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Discourse Process.",
            "volume": "47",
            "issn": "4",
            "pages": "292-330",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF43": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "Multifaceted protein-protein interaction prediction based on Siamese residual RCNN",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Bioinformatics",
            "volume": "35",
            "issn": "14",
            "pages": "i305-i314",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF45": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF47": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF49": {
            "title": "Neural article pair modeling for Wikipedia sub-article matching",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Meng",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zaniolo",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Machine Learning and Knowledge Discovery in Databases",
            "volume": "",
            "issn": "",
            "pages": "3-19",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF50": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF51": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF52": {
            "title": "A computer readability formula designed for machine scoring",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Coleman",
                    "suffix": ""
                },
                {
                    "first": "TL",
                    "middle": [],
                    "last": "Liau",
                    "suffix": ""
                }
            ],
            "year": 1975,
            "venue": "J. Appl. Psychol.",
            "volume": "60",
            "issn": "2",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}