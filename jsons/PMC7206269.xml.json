{
    "paper_id": "PMC7206269",
    "metadata": {
        "title": "GAIM: Game Action Information Mining Framework for Multiplayer Online Card Games (Rummy as Case Study)",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sharanya",
                "middle": [],
                "last": "Eswaran",
                "suffix": "",
                "email": "sharanya.eswaran@games24x7.com",
                "affiliation": {}
            },
            {
                "first": "Vikram",
                "middle": [],
                "last": "Vimal",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Deepanshi",
                "middle": [],
                "last": "Seth",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Tridib",
                "middle": [],
                "last": "Mukherjee",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "With the proliferation of digital platforms, and low-cost availability of mobile devices and data, online multi-player card games, are becoming increasingly popular among adults and teenagers, with a market value of $4 billion in 2019 [1, 2]. Much of this popularity can be attributed to the unique blend of recreational and intellectual entertainment offered by such chanced-based and strategy-oriented card games as Rummy, Poker, etc. Such games greatly test a player\u2019s ability to navigate through the enormous state space, recall the moves of other players, and probabilistically estimate the missing information arising from imperfect and asymmetric knowledge (since a player\u2019s cards are usually hidden from others). Artificial intelligence and machine learning research in gaming have typically focused on bots exceeding human expertise in several games, e.g., Go, chess, backgammon, checkers, Jeopardy! and Atari games [4, 6, 14, 20, 22, 25]. However, in real cash card games, it is required to consciously ensure that there is no interference during the game play via any kind of automated decision making (beyond the random dealing of cards), to ensure fairness. Therefore, this work primarily focuses on leveraging machine learning to understand, benchmark, and profile the individual players with respect to their playing behavior, strategies and longitudinal evolution of game play.",
            "cite_spans": [
                {
                    "start": 236,
                    "end": 237,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 239,
                    "end": 240,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 926,
                    "end": 927,
                    "mention": "4",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 929,
                    "end": 930,
                    "mention": "6",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 932,
                    "end": 934,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 936,
                    "end": 938,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 940,
                    "end": 942,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 944,
                    "end": 946,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Mining the game states in multi-player, skill-based card games yields valuable information about the game and players, their intentions and root causes, for example, whether a player is conservative, skilled, or a risk-taker, or has become aggressive or disengaged. Another important use case for this analysis is to evaluate game prudence, i.e., detect and preempt addictive and gambling tendencies, which is especially important in cash games. However, player behavioral profiling via game state mining is challenging because of factors such as large state space and subjective reactions conditional to chance factors. This makes this effort fundamentally different from various player rating systems in other games (e.g., ELO rating in chess [5]), which are objective. For example, in Rummy (Sect. 3), one important decision for a player is whether to play or drop the game based on the cards he was dealt. For a clearly very good or bad set of cards the decision is straight forward to play or drop, respectively. However, most of the hands typically fall in intermediate category (over \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$55\\%$$\\end{document}). While a conservative player may drop such hands, a more aggressive player may play and possibly end up winning depending on the cards of his opponents, and how the game evolves by chance. Both kinds of players may end up with very similar ratings, based on any established schemes which look at their overall standing in the scoreboard across all players and games in the system. However, we strive for more fine-grained observation of player behaviors.",
            "cite_spans": [
                {
                    "start": 746,
                    "end": 747,
                    "mention": "5",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this work, we present Game Action Information Miner (GAIM), a Convolutional Neural Network based framework for card game analytics. The contributions of this work are: (i) A novel representation of the card game state as an image, which encapsulates all pertinent information in a generic and extensible manner. (ii) An efficient CNN model that out-performs other well-known methods to predict critical game decision, (iii) Calibrate the goodness of the state in a chance-independent, continuous and deterministic manner using the model. This is then leveraged to benchmark players, with respect to their response to a game state. (iv) Our model and derived metrics are remarkably valuable for end-to-end player behavioral analyses and gethering insights across game dynamics, that were not possible before due to the close coupling of chance and strategy.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Although the GAIM framework is applicable to any skill-based card game, we describe its utility within the context of one particular game, Rummy.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Deep Learning in Multi-player Games: Most of the literature in the space of deterministic games focuses on building agents that can learn and predict next moves and effectively play the game as skillfully as or better than a human, using techniques such as Monte Carlo Tree Search and deep reinforcement learning [10, 21, 22]. Unlike Chess or Go, multi player cards games are mostly partially observable markov decision processes or stochastic games (POMDP or POSG), where the game tree is not known fully. [8] uses neural fictitious self-play agent to approximate Nash equilibrium in poker. [17] generates probable subsets of true states prior to performing a tree search for decision making in games like Rack-O and Game of Pure Strategy. Other works include [27] which employs computer vision techniques to recognize Poker players\u2019 intentions and predict their actions based on their face expressions. [15] estimates the value of holding any possible private cards in any poker situation, using recursive reasoning, decomposition and a form of intuition that is learned from self-play using deep learning. [9] builds a convolution neural network model to predict the discarded tile in Mahjong using a data structure, which is a function of only a single game state, namely, the tile type, and is not generic or extensible like ours. The scope and objective of this work is not to develop a game playing agent, but rather to mine game intelligence of players that enables end-to-end behavioral analytics.",
            "cite_spans": [
                {
                    "start": 314,
                    "end": 316,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 318,
                    "end": 320,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 322,
                    "end": 324,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 508,
                    "end": 509,
                    "mention": "8",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 593,
                    "end": 595,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 762,
                    "end": 764,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 906,
                    "end": 908,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1110,
                    "end": 1111,
                    "mention": "9",
                    "ref_id": "BIBREF27"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Player Behavior Analysis: In [16], Clustering is used on game-specific features for online tennis to segment different types of players, which is used to introduce realistic diversity in the bots to play against humans. [19] uses non-negative tensor factorization to identify players who are similar not only by playing patterns and strategies but also by temporal trajectories. Our method precludes the need for game-specific features, and we use model-based profiling instead of clustering for identifying player patterns. In [28], for a game where the actions and their rewards are known, the player\u2019s motivation and reasoning behind actions are learned using inverse reinforcement learning. Our work is orthogonal to [28] because we focus on defining and determining the optimal policy.",
            "cite_spans": [
                {
                    "start": 30,
                    "end": 32,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 221,
                    "end": 223,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 529,
                    "end": 531,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 722,
                    "end": 724,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Rummy is a game of skill played with 2 to 6 players with the objective of forming valid melds, i.e., groups of sets and sequences with all the cards at hand. A set is a group of cards with the same rank but different suits (e.g.,\n\n); a sequence is a group of cards in sequential order of ranks all with the same suit (e.g.,\n\n. Each meld must be at least 3 cards long. A randomly selected card is designated as joker which the players can use to complete a set or sequence. For example, if\n\nis drawn as the wild card joker, then 2 of any suit can be used to form melds (a printed joker also plays the same role). A sequence/set that contains a joker is called an impure sequence/set, and otherwise it is a pure sequence/set.",
            "cite_spans": [],
            "section": "The Game of Rummy",
            "ref_spans": []
        },
        {
            "text": "Declaration: The player who declares first with all valid sequences and sets wins the game. It is mandatory to have at least two sequences, one of which must be pure, and the other can be pure or impure. Figure 1 shows an example of a winning declaration. Winner gets 0 points, other players get points equal to sum value of cards (face cards carry 10 points, jokers and pure sequences fetch 0 points). All players aim to minimize their points.",
            "cite_spans": [],
            "section": "The Game of Rummy",
            "ref_spans": [
                {
                    "start": 211,
                    "end": 212,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Drop: At any time during the game, the player has the option of dropping from the game. First drop is one of the most important decisions to be made in the game, because it helps conserve points. If the hand that was dealt was poor, then it is better to drop (and get away with 20 points) rather than end up with a higher score. Learning which hand to drop is an important skill that a good rummy player must acquire.",
            "cite_spans": [],
            "section": "The Game of Rummy",
            "ref_spans": []
        },
        {
            "text": "\n\n\n",
            "cite_spans": [],
            "section": "The Game of Rummy",
            "ref_spans": []
        },
        {
            "text": "The quest for a player is to strategize and progressively move towards creating a winning hand before his opponents. For the curious readers, we have summarized the odds and state space involved in a game of rummy in Table 1. It may be noted that, like most of the multi-player card games, Rummy is characterized as an imperfect information, non zero-sum game, where an optimal strategy equilibrium is a hard problem ranging from NP-hard to PPAD-complete [7, 8, 18].",
            "cite_spans": [
                {
                    "start": 456,
                    "end": 457,
                    "mention": "7",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 459,
                    "end": 460,
                    "mention": "8",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 462,
                    "end": 464,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                }
            ],
            "section": "The Game of Rummy",
            "ref_spans": [
                {
                    "start": 223,
                    "end": 224,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "The Game of Rummy",
            "ref_spans": []
        },
        {
            "text": "We represent a player\u2019s hand as a kx\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(n+1)$$\\end{document}x3 array, for k suits and n ranks. This paper uses 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times $$\\end{document} 14 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times $$\\end{document} 3 array where, each of the 4 rows represents a suit; each of 14 columns represents a rank, in the order \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\{A,2,3 \\ldots Q,K,A\\}$$\\end{document}. Ace is repeated because it can be both the lowest or the highest card in a sequence. The first \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$4 \\times 14$$\\end{document} plane represents the count of cards in hand. For example, if the player has one 4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\spadesuit $$\\end{document}, then the value in the corresponding cell is 1. The second plane is used to represent special properties of individual cards. We use it to denote if the card is a wild card joker. The third plane represents properties that are common to the entire hand. We use it to denote the total number of jokers in hand. This also allows for the printed joker card (which does not have a place in the 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times $$\\end{document} 14 array) to be counted. As an example, the array representation for the following hand, and the corresponding image format are shown in Fig. 3:\n\n, and the wild card joker is 9. We can now visualize the hand as an RGB image, wherein pure sequences are seen as horizontal consecutive blocks and sets are seen as vertical blocks; multiple cards of the same rank and suit result in a brighter red block; jokers emerge as a greenish block; the number of jokers reflect as a blue-tinge in the background.",
            "cite_spans": [],
            "section": "Hand Image Representation ::: GAIM Framework",
            "ref_spans": [
                {
                    "start": 2978,
                    "end": 2979,
                    "mention": "3",
                    "ref_id": "FIGREF7"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Hand Image Representation ::: GAIM Framework",
            "ref_spans": []
        },
        {
            "text": "When mapped to an RGB image, each plane can hold 8 bits of information (values 0 to 255), this image representation is sufficient for games played with up to 256 decks, with 8 card-level states (e.g., isJoker, isOpenCard, isDiscardedByPlayerA, fromClosedStack, etc.) and 8 hand-level states (e.g., total jokers (3 bits), game variant, turn number, time of play etc.). Without loss of generality, more planes can be included (and the depth of the convolution filters changed accordingly), as long as the separation between card-level and hand-level information is preserved. It may also be noted that this representation can be used for any card game, not just Rummy. For instance, in Bridge [7], we can use the first layer to denote the location of the card (with the declarer, the dummy or played in a trick), the second layer to denote all cards in the trump suit, and the third layer to indicate the trick number.",
            "cite_spans": [
                {
                    "start": 692,
                    "end": 693,
                    "mention": "7",
                    "ref_id": "BIBREF25"
                }
            ],
            "section": "Hand Image Representation ::: GAIM Framework",
            "ref_spans": []
        },
        {
            "text": "When the cards are dealt to a player, he needs to respond to the hand depending on whether it is a good, bad or mediocre hand. In case of a clearly strong hand (e.g., with a pure sequence already) or a clearly bad hand (e.g., with no jokers, and cards that are far apart), it is an easy decision to play or drop, respectively. But, with mediocre hands the decision is much more difficult and would depend on the skill, experience and behavioral tendencies of a player. We utilize the significance of the first drop decision to build a supervised convolutional neural network model for HQE (HandNet).",
            "cite_spans": [],
            "section": "Hand Quality Estimation Model ::: GAIM Framework",
            "ref_spans": []
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Hand Quality Estimation Model ::: GAIM Framework",
            "ref_spans": []
        },
        {
            "text": "We train HandNet to learn the first-drop decision of highly skilled players from the image representation of hands. The highly skilled are most likely to take the correct first drop decision. Hence, the output of this model, i.e., the probability of first drop given a hand, serves as an effective proxy for hand quality, thereby calibrating a given hand of cards on a continuous scale, in a deterministic manner (a hand always gets the same quality measure) abstracting extraneous factors such as luck, opponent skills and game state.",
            "cite_spans": [],
            "section": "Hand Quality Estimation Model ::: GAIM Framework",
            "ref_spans": []
        },
        {
            "text": "The architecture of HandNet is shown in Fig. 4, consisting of three convolution layers (with ReLU activation), followed by one average pooling layer (2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times $$\\end{document} 2), one drop out layer (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$20\\%$$\\end{document}), one fully connected dense layer (512 nodes, hyperbolic tangent activation), and finally a softmax layer for binary classification. The kernel size is taken as 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times $$\\end{document} 4 (with stride 1) because we are interested in an average meld size of 4. We use average pooling because, unlike conventional \u201cimage recognition\u201d applications of CNN (where max pooling is preferred to capture the most invariant features), we are more interested in understanding the what blocks are present, and more importantly, not present in a region and its neighborhood. Hence we use average pooling to reduce the loss of information while summarizing the region.",
            "cite_spans": [],
            "section": "Hand Quality Estimation Model ::: GAIM Framework",
            "ref_spans": [
                {
                    "start": 45,
                    "end": 46,
                    "mention": "4",
                    "ref_id": "FIGREF8"
                }
            ]
        },
        {
            "text": "Dataset and Training: To curate the training dataset, we define skilled player as one who has played at least 500 cash games and has an average differential end score of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge 5$$\\end{document} (which is 3 standard deviations away from mean), defined as\n\n. Differential end score is a better measure than the end score because it takes into account the margin of victory. The lower bound on the number of games played ensures that there is no luck involved. These filters resulted in a skilled-player base of 3956 users out of the total player base of 282,102 users. The initial hands of all the games played by these users in two months is taken as input, and the player\u2019s decision of drop (1) or play (0) is taken as the ground truth label. The model was trained for 100 epochs, with a stochastic gradient descent optimizer, 10-fold cross validation and categorical cross entropy as the loss function. The first-drop decision of a player will depend on the number of players in a game - the tendency to drop is higher in a 6-player game than a 2-player game, since there is higher chance of not getting the cards needed. Hence, we trained separate models for 2-, 3-, 4-, 5- and 6-player games, each with about 2.9 million records. The proportion of drops ranged from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$12\\%$$\\end{document} in 2-player to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$35\\%$$\\end{document} in 6-player games (Table 6).",
            "cite_spans": [],
            "section": "Hand Quality Estimation Model ::: GAIM Framework",
            "ref_spans": [
                {
                    "start": 2156,
                    "end": 2157,
                    "mention": "6",
                    "ref_id": "FIGREF10"
                }
            ]
        },
        {
            "text": "In this section, we evaluate how well the HQE model learns the ground truth (drop decision), the quality of a given hand, and other game nuances.",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": []
        },
        {
            "text": "Goodness of Architecture: The performance metrics (Area under ROC curve, Area under Precision Recall curve, Accuracy, Precision and Recall at break even point) of the 2-player model on the validation set are shown in Fig. 5. The HQE model is shown to out-perform (i) a Feed Forward neural network (multi-layer perceptron), where the image array is flattened and fed to 3 fully-connected hidden layers with 1024 nodes each, with a softmax layer at the end (FFNN-1) (ii) VGG16 [23], (iii) DenseNet [12] and (iv) ResNet [11]. The reason for our model\u2019s superior performance over other well-known CNN architectures is that our architecture and convolution filter sizes enable the model to detect the features of interest better.2 Figure 7 shows the evolution of training and validation accuracy and loss with each epoch. It may be noted that we use the break even point where precision = recall to optimize the threshold, rather than other metrics such as F1 score because, our objective is to effectively identify the good, bad and intermediate hands, rather than focusing on correctly predicting the drop class alone. The performance metrics of n-player models against the validation set are shown in Fig. 6. For the rest of the evaluation, we use the 2-player model because that is the most frequent scenario, but the conclusions can be extended without loss of generality to n-player games (also verified empirically but omitted due to space constraints).",
            "cite_spans": [
                {
                    "start": 476,
                    "end": 478,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 497,
                    "end": 499,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 518,
                    "end": 520,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Evaluation of HQE Model",
            "ref_spans": [
                {
                    "start": 222,
                    "end": 223,
                    "mention": "5",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 733,
                    "end": 734,
                    "mention": "7",
                    "ref_id": "FIGREF11"
                },
                {
                    "start": 1204,
                    "end": 1205,
                    "mention": "6",
                    "ref_id": "FIGREF10"
                }
            ]
        },
        {
            "text": "\n\n\n\n",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": []
        },
        {
            "text": "Goodness of Game State Representation: Next, we use rummy-specific features that can be potentially indicative of the goodness of a hand, and use these features instead of our game state input to train models using (i) Random Forest, (ii) XGBoost and (iii) Feed forward network (FFNN-2). The features used are number of jokers, number of pure sequence of length three and four, number of bits (e.g., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$6\\spadesuit $$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$7\\spadesuit $$\\end{document}), number of connected cards (e.g., \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$6\\spadesuit $$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$8\\spadesuit $$\\end{document}), number of sets of length three and four, number of cards away from a winning hand and hand score (i.e, points). The poor performance of these models (Fig. 5) show that these features are not sufficient to calibrate the hand quality.",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": [
                {
                    "start": 1789,
                    "end": 1790,
                    "mention": "5",
                    "ref_id": "FIGREF9"
                }
            ]
        },
        {
            "text": "Learning the Hand Quality: We now evaluate if the model provides continuous-scale quality estimation of hands as envisioned. Figure 8 shows the hands with highest and lowest drop prob (0.97, 2.76e\u221207, respectively) in a validation set. We see that the least probability hand is already a winning hand; the maximum probability hand has no jokers, and is at least 2 cards away from a pure sequence and hence is clearly a bad hand. We also compare the end scores of the true negatives TN (i.e., player played when drop prob is high) with the false positives FP, to study how the players fared when they differed from the model. The two groups were significantly different (average p-value of 0.0003), implying that playing a bad hand, as per the model, yields bad score. Next we consider a set of thumb rules for obviously good (Play hands with pure sequence and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\ge }1$$\\end{document} joker), obviously bad (Drop hands with 0 joker and 0 pure seq) and other (Not Sure) hands.",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": [
                {
                    "start": 132,
                    "end": 133,
                    "mention": "8",
                    "ref_id": "FIGREF12"
                }
            ]
        },
        {
            "text": "The objective of this exercise was three fold: (i) to verify the deterministic continuous calibration of our model, (ii) to show why such thumb rules are not sufficient to estimate hand quality and (iii) to understand if and when our model mispredicts.",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": []
        },
        {
            "text": "The average drop probabilities (output by model) for the 3 categories are 0.0065, 0.332 and 0.687, respectively, which are <, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\approx $$\\end{document}, > threshold 0.35, verifying objective i. Majority of hands (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$56\\%$$\\end{document}) fall under Not Sure category, indicating thumb rules are insufficient (objective ii). Next, we bucket the samples misclassified during training (which is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$10\\%$$\\end{document} since model accuracy is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$90\\%$$\\end{document}) into these 3 categories. We see in Fig. 9 that \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$99.1\\%$$\\end{document} of Play mismatches (constitutes \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3.8\\%) $$\\end{document} are FN (i.e., model recommends play but player dropped), implying player mistake; \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$82.3\\%$$\\end{document} of Drop mismatches (4.2%) are FP (i.e., model recommends drop but player plays), again implying player mistake; there is an almost equal split between FP and FN under Not Sure (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$5.4\\%$$\\end{document}), with near-threshold drop prob, implying intermediate hand, not necessarily a player mistake nor model\u2019s incompetence.",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": [
                {
                    "start": 1571,
                    "end": 1572,
                    "mention": "9",
                    "ref_id": "FIGREF13"
                }
            ]
        },
        {
            "text": "\n\n\n\n",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": []
        },
        {
            "text": "Learning the Effect of Joker: Next, we assess how well the model learns the importance of the joker. We retrain our model, with (i) second plane always 0, (ii) third plane always 0 and (iii) both set to 0. The precision and recall of the model reduced by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1.5\\%$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$35\\%$$\\end{document}\nand \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$49\\%$$\\end{document}, respectively compared to Fig. 5, substantially validating that the model learns value of joker from our input. Next, we took a randomly selected hand\n\nthat does not contain any joker. Its drop probability is 0.549 (not playable). As a rule of thumb, having a joker improves the quality of hand. To validate that, we treated each of these cards as a joker card, and the drop probability reduces in all cases and the hand becomes playable (Table 2).",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": [
                {
                    "start": 1162,
                    "end": 1163,
                    "mention": "5",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 1576,
                    "end": 1577,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Learning the Advantage of Left-Heavy Hands: Since the objective in a rummy game is to minimize the points, a skilled rummy player drops a hand with too many high value cards. In order to verify that the model has learned this, we selected 50 right-heavy images (i.e., most of the cards are on the right half of the image, implying many high value cards) with no pure sequences or jokers and computed their drop probabilities from the model; next we compared these with that of their left-heavy counterparts (by horizontally flipping each image). A one-tailed, paired two sample t-test shows that the mean drop probability is significantly lower after flipping (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=0.004$$\\end{document}).",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": []
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Evaluation of HQE Model",
            "ref_spans": []
        },
        {
            "text": "How a player reacts to a hand, given its quality, sheds light on various player characteristics and playing tendencies. For instance, Fig. 10 shows the initial hand quality (drop probabilities) of the last few hundred games of two different players. From this figure, we clearly understand that Player A is a very conservative player, dropping most of the hands he could have played, while Player B is a very aggressive player, playing most of the hands he should have dropped. To further aid such analyses, we derive relevant metrics (referred to as HQE Metrics), some of which are listed in Table 3. It may also be noted that the HQE model has been deployed in real time and the data used for the analyses in this section are from production pipeline (deployment details in supplementary material Section S2). \n",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": [
                {
                    "start": 139,
                    "end": 141,
                    "mention": "10",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 599,
                    "end": 600,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": []
        },
        {
            "text": "Measuring Player Engagement: The change in playing pattern over time is a precursor to important events in a player\u2019s journey. We use such longitudinal analysis of DA for churn prediction. We have an existing XGBoost model to predict churners (who become disengaged and leave the platform). The features used in this model include number of active days, win amount, deposit amount, etc. We augment this model with DA and its time series features. We observe that the AUC of the model improves by \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$11.3\\%$$\\end{document} from 0.634 to 0.706. To understand why DA improved churn prediction, we look into a player who churned and was correctly predicted by HQE-enhanced model, but not baseline churn model. Figure 11 shows his drop action in the last 500 games. We see that in the last 100 games, the player has not dropped a single game (sudden sharp decrease in DA), indicating onset of aggressiveness or disengagement. Similarly, we observed that DA was one of the most important features for predicting if a player will convert from practice to paid player, improving the accuracy of the conversion prediction model from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$90\\%$$\\end{document} to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$94\\%$$\\end{document}.\n",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": [
                {
                    "start": 979,
                    "end": 981,
                    "mention": "11",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "Enhancing Game Prudence: One of the biggest responsibilities in providing a real-money gaming platform is ensuring responsible game play among all players. Typical addiction indicators include spending excessive amount of time and money on the platform, and displaying desperate behavior (when they begin to chase losses) [3]. Drop behavior is helpful in observing such desperation. We have an anomaly detection model that identifies players who are on the trajectory of becoming irresponsible or addicted. The flagged players are blocked after verification by a counselor. The coefficient of variation (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{\\sigma }{\\mu }$$\\end{document}) in DA of blocked players is significantly higher than non-blocked players, per one-tailed Kruskal-Wallis rank hypothesis test (p = 0.017). This is an important revelation, because it implies that the HQE model and DA not only reflect the skill of the player (because, in most cases, the addicted players know the game well), but it also identifies clean, responsible and prudent skill.",
            "cite_spans": [
                {
                    "start": 323,
                    "end": 324,
                    "mention": "3",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Player Behavioral Analyses",
            "ref_spans": []
        },
        {
            "text": "Platform-Level KPIs: Within the gaming platform, there are several key performance indicators of players that help in all business decisions. Table 4 shows that there is very high correlation between DA and the KPIs listed. It is of great importance to note that the KPIs consistently increase monotonically with drop adherence. This implies that the playing mindset, propensity to spend, time taken to get engaged with the platform are all correlated with DA. The remarkable implication of this result is two fold. Firstly, we can use drop adherence as a reliable metric for early prediction of these KPIs. Secondly, if the drop adherence of the players can be improved then all these KPIs of interest would also improve organically, along with players win ratio (Table 3). This finding has been folded into several product campaigns, aiming to up-skill players to drop correctly.",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": [
                {
                    "start": 148,
                    "end": 149,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 771,
                    "end": 772,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": []
        },
        {
            "text": "Player Segmentation and Persona Clustering: Next we observe the drop behavior across different cohorts of players. Table 5 lists the segments considered. The drop metrics for 2000 players in each segment were computed. Figure 12 shows that the HQE metrics are different across different segments, implying that the playing behavior is different across the segments. We see that skilled and PC players are similar; PS and NUNew are similar, as expected intuitively (results from ANOVA and Tukey HSD test [26] are omitted due to space constraints).",
            "cite_spans": [
                {
                    "start": 504,
                    "end": 506,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                }
            ],
            "section": "Player Behavioral Analyses",
            "ref_spans": [
                {
                    "start": 226,
                    "end": 228,
                    "mention": "12",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 121,
                    "end": 122,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": []
        },
        {
            "text": "Next we group players based on their game play to identify previously unknown segments or player persona buckets. We randomly selected 10000 players, generated their HQE metrics for games played over 2 months, and then performed k-means clustering (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=5$$\\end{document} as per scree plot). The cluster centroids are shown in Fig. 13 (rows 1\u20135). For retrospective verification, we also present some extraneous meta-data about the players in the clusters (rows 6\u20139). The observations from these results are given in Fig. 14. We can now use such clustering to predict the persona bucket of new players too.",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": [
                {
                    "start": 598,
                    "end": 600,
                    "mention": "13",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 787,
                    "end": 789,
                    "mention": "14",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "\n\n\n",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": []
        },
        {
            "text": "As a summary, we compare the features of HQE-based DA as a skill metric with other metrics - end score (ES), average differential end score (Diff ES, Sect. 4), win ratio (WR), ELO [5], and drop adherence derived from two other indicators of hand quality that we have discussed already - thumbrule (DA Rule) and number of cards away from declaration (DA Dist in Table 6. We see that HQE-based DA is the most robust providing insights that others are unable to in a timely and accurate manner.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 182,
                    "mention": "5",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Player Behavioral Analyses",
            "ref_spans": [
                {
                    "start": 367,
                    "end": 368,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "\n\n",
            "cite_spans": [],
            "section": "Player Behavioral Analyses",
            "ref_spans": []
        },
        {
            "text": "We presented GAIM, a deep-learning framework that enables benchmarking and profiling of players. We develop an efficient model for GAIM that uses first drop action of skilled players to calibrate the goodness of the hand. The model is then used to mine game play characteristics of players, by monitoring drop adherence. Future work includes developing persuasion strategies to up-skill the players to improve first drop behavior and to look into more game play actions beyond the first drop so that a more fine-grained assessment of skill can be made.",
            "cite_spans": [],
            "section": "Conclusions and Future Directions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Rummy features for #cards = 13, #decks = 2, #suits = 4. i is the meld length\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Drop prob when jokers are introduced\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: HQE metrics with definitions\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: Pearson correlation coefficient (r) between DA and KPIs, along with significance value p\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Segments of players\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6.: Skill metrics. Y: Yes, N: No, E: Eventually\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Example of a winning declaration",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 10.: Hand quality helps identify different playing styles. The green dots are the correct plays, i.e., true negatives; blue: correct drops (TP); orange: wrong drops (FN); pink: wrong plays (FP).",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 11.: HQE for churn prediction (green:correct plays, blue: correct drops, orange: wrong drops, pink: wrong plays)",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig. 12.: HQE metrics for different player segments",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Fig. 13.: Cluster centroids and meta-data (GC: game count, Rev:revenue generated, Days: days on the system, ID: inactive days.",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Fig. 14.: Observation of cluster characteristics",
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Fig. 2.: Block diagram of GAIM",
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Fig. 3.: Array representation",
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Fig. 4.: Architecture of HandNet",
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Fig. 5.: Performance of HandNet",
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Fig. 6.: Performance of n-player models. The thresholds are 0.35, 0.3, 0.52, 0.61, 0.65 and ground truth drop ratios are 0.12, 0.17, 0.23, 0.30, 0.35, for 2,3,4,5,6p respectively",
            "type": "figure"
        },
        "FIGREF11": {
            "text": "Fig. 7.: Convergence of accuracy and loss",
            "type": "figure"
        },
        "FIGREF12": {
            "text": "Fig. 8.: Drop probability reflects hand quality",
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Fig. 9.: Bucketing of misclassified samples",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Human-level control through deep reinforcement learning",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Mnih",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Nature",
            "volume": "518",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/nature14236"
                ]
            }
        },
        "BIBREF6": {
            "title": "DeepStack: expert-level artificial intelligence in heads-up no-limit poker",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Moravcik",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Science",
            "volume": "356",
            "issn": "6337",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1126/science.aam6960"
                ]
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "Non-negative tensor factorization for behavioral pattern mining in online games",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sapienza",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Information",
            "volume": "9",
            "issn": "3",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.3390/info9030066"
                ]
            }
        },
        "BIBREF11": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Chinook the world man-machine checkers champion",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schaeffer",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "AI Mag.",
            "volume": "17",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Mastering the game of go with deep neural networks and tree search",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Silver",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Nature",
            "volume": "529",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/nature16961"
                ]
            }
        },
        "BIBREF14": {
            "title": "Mastering the game of go without human knowledge",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Silver",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nature",
            "volume": "550",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/nature24270"
                ]
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "Temporal difference learning and TD-gammon",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Tesauro",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Commun. ACM",
            "volume": "38",
            "issn": "3",
            "pages": "58-68",
            "other_ids": {
                "DOI": [
                    "10.1145/203330.203343"
                ]
            }
        },
        "BIBREF18": {
            "title": "Comparing individual means in analysis of variance",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tukey",
                    "suffix": ""
                }
            ],
            "year": 1949,
            "venue": "JSTOR Biometr.",
            "volume": "5",
            "issn": "2",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.2307/3001913"
                ]
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "How do gamblers start gambling: identifying behavioral markers for high-risk internet gambling",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Braverman",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Eur. J. Public Health",
            "volume": "22",
            "issn": "2",
            "pages": "273-278",
            "other_ids": {
                "DOI": [
                    "10.1093/eurpub/ckp232"
                ]
            }
        },
        "BIBREF22": {
            "title": "Deep blue",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Campbell",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Artif. Intell.",
            "volume": "134",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1016/S0004-3702(01)00129-1"
                ]
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Elo",
                    "suffix": ""
                }
            ],
            "year": 1978,
            "venue": "The Rating of Chessplayers Past and Present",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "Introduction to \u201cthis is watson\u201d",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Ferrucci",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IBM J. Res. Dev.",
            "volume": "56",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1147/JRD.2012.2184356"
                ]
            }
        },
        "BIBREF25": {
            "title": "Search in games with incomplete information: a case study using bridge card play",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Frank",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Artif. Intell.",
            "volume": "100",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1016/S0004-3702(97)00082-9"
                ]
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}