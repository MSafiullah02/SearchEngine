{
    "paper_id": "PMC7206329",
    "metadata": {
        "title": "On the Performance of Oversampling Techniques for Class Imbalance Problems",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Jiawen",
                "middle": [],
                "last": "Kong",
                "suffix": "",
                "email": "j.kong@liacs.leidenuniv.nl",
                "affiliation": {}
            },
            {
                "first": "Thiago",
                "middle": [],
                "last": "Rios",
                "suffix": "",
                "email": "thiago.rios@honda-ri.de",
                "affiliation": {}
            },
            {
                "first": "Wojtek",
                "middle": [],
                "last": "Kowalczyk",
                "suffix": "",
                "email": "w.j.kowalczyk@liacs.leidenuniv.nl",
                "affiliation": {}
            },
            {
                "first": "Stefan",
                "middle": [],
                "last": "Menzel",
                "suffix": "",
                "email": "Stefan.Menzel@honda-ri.de",
                "affiliation": {}
            },
            {
                "first": "Thomas",
                "middle": [],
                "last": "B\u00e4ck",
                "suffix": "",
                "email": "t.h.w.baeck@liacs.leidenuniv.nl",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "The classification problem under class imbalance has caught growing attention from both academic and industrial field. Due to recent advances, the progress in technical assets for data storage and management as well as in data science enables practitioners from industry and engineering to collect a large amount of data with the purpose of extracting knowledge and acquire hidden insights. An example may be illustrated from the field of computational design optimization where product parameters are modified to generate digital prototypes which performances are evaluated by numerical simulations, or based on equations expressing human heuristics and preferences. Here, many parameter variations usually result in valid and producible geometries but in the final steps of the optimization, i.e. in the area where the design parameters converge to a local/global optimum, some geometries are generated which violate given constraints. Under this circumstance, a database would contain a large number of designs which are according to specs (even if some may be of low performance) and a smaller number of designs which eventually violate pre-defined product requirements. By far, the resampling techniques have proven to be efficient in handling imbalanced benchmark datasets. However, the empirical study and application work in the imbalanced learning domain are mostly focusing on the \u201cclassical\u201d resampling techniques (SMOTE, ADASYN, and MWMOTE etc.) [11, 15, 20], although there are many recently developed resampling techniques.",
            "cite_spans": [
                {
                    "start": 1459,
                    "end": 1461,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1463,
                    "end": 1465,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1467,
                    "end": 1469,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In this paper, we set up several experiments on 19 benchmark datasets to study the efficiency of six powerful oversampling techniques, including SMOTE, ADASYN, MWMOTE, RACOG, wRACOG and RWO-Sampling. For each dataset, we also calculate seven data complexity measures to investigate the relationship between data complexity measures and the choice of resampling techniques, since researchers have pointed out that studying the data complexity of the imbalanced datasets is of vital importance [15] and it may affect the choice of resampling techniques [20]. We also perform the experiment on our real-world inspired vehicle dataset. Results of our experiments demonstrate that oversampling techniques that consider the minority class distribution (RACOG, wRACOG, RWO-Sampling) perform better in most cases and RACOG gives the best performance among the six reviewed approaches. Results on our real-world inspired vehicle dataset further validate this conclusion. No obvious relationship between data complexity measures and the choice of resampling techniques is found in our experiment. However, we find F1v value, a measure for evaluating the overlap which most researchers ignore [15, 20], has a strong negative correlation with the potential AUC value (after resampling).",
            "cite_spans": [
                {
                    "start": 493,
                    "end": 495,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 552,
                    "end": 554,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1183,
                    "end": 1185,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1187,
                    "end": 1189,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "The remainder of this paper is organized as follows. In Sect. 2, the research related to our work are presented, also including the relevant background knowledge on six resampling approaches and data complexity measures. In Sect. 3, the experimental setup is introduced in order to understand how the results are generated. Section 4 gives the results of our experiments. Further exploration through data from a real-world inspired digital vehicle model is presented in Sect. 5. Section 6 concludes the paper and outlines further research.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In the following, the six established resampling techniques SMOTE, ADASYN, MWMOTE, RACOG, wRACOG and RWO-Sampling are introduced.",
            "cite_spans": [],
            "section": "Resampling Technique ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "SMOTE and ADASYN. The synthetic minority oversampling technique (SMOTE) is the most famous resampling technique [3]. SMOTE produces synthetic minority samples based on the randomly chosen minority samples and their K-nearest neighbors. The new synthetic sample can be generated by using the randomized interpolation scheme above for minority samples. The main improvement in the adaptive synthetic (ADASYN) sampling technique is that the samples which are harder to learn are given higher importance and will be oversampled more often in ADASYN [7].",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 114,
                    "mention": "3",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 546,
                    "end": 547,
                    "mention": "7",
                    "ref_id": "BIBREF21"
                }
            ],
            "section": "Resampling Technique ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "MWMOTE. The majority weighted minority oversampling techniques (MWMOTE) improves the sample selection scheme and the synthetic sample generation scheme [2]. MWMOTE first finds the informative minority samples (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{imin}$$\\end{document}) by removing the \u201cnoise\u201d minority samples and finding the borderline majority samples. Then, every sample in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{imin}$$\\end{document} is given a selection weight (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_w$$\\end{document}), according to the distance to the decision boundary, the sparsity of the located minority class cluster and the sparsity of the nearest majority class cluster. These weights are converted in to selection probability (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_p$$\\end{document}) in the synthetic sample generation stage. The cluster-based synthetic sample generation process proposed in MWMOTE can be described as, 1). cluster all samples in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{imin}$$\\end{document} into M groups; 2). select a minority sample x from \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{imin}$$\\end{document} according to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_p$$\\end{document} and randomly select another sample y from the same cluster of x; 3). use the same equation employed in k-NN-based approach to generate the synthetic sample; 4). repeat 1)\u20133) until the required number of synthetic samples is generated.",
            "cite_spans": [
                {
                    "start": 153,
                    "end": 154,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Resampling Technique ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "RACOG and wRACOG. The oversampling approaches can effectively increase the number of minority class samples and achieve a balanced training dataset for classifiers. However, the oversampling approaches introduced above heavily reply on local information of the minority class samples and do not take the overall distribution of the minority class into account. Hence, the global information of the minority samples cannot be guaranteed. In order to tackle this problem, Das et al. [5] proposed RACOG (RApidy COnverging Gibbs) and wRACOG (Wrapper-based RApidy COnverging Gibbs).",
            "cite_spans": [
                {
                    "start": 482,
                    "end": 483,
                    "mention": "5",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Resampling Technique ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "In these two algorithms, the n-dimensional probability distribution of minority class is optimally approximated by Chow-Liu\u2019s dependence tree algorithm and the synthetic samples are generated from the approximated distribution using Gibbs sampling. Instead of running an \u201cexhausting\u201d long Markov chain, the two algorithms produce multiple relatively short Markov chains, each starting with a different minority class sample. RACOG selects the new minority samples from the Gibbs sampler using a predefined lag and this selection procedure does not take the usefulness of the generated samples into account. On the other hand, wRACOG considers the usefulness of the generated samples and selects those samples which have the highest probability of being misclassified by the existing learning model [5].",
            "cite_spans": [
                {
                    "start": 799,
                    "end": 800,
                    "mention": "5",
                    "ref_id": "BIBREF19"
                }
            ],
            "section": "Resampling Technique ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "RWO-Sampling. Inspired by the central limit theorem, Zhang et al. [24] proposed the random walk oversampling (RWO-Sampling) approach to generate the synthetic minority class samples which follows the same distribution as the original training data.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 69,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                }
            ],
            "section": "Resampling Technique ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "In order to add m synthetic examples to the n original minority examples (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m < n$$\\end{document}), we first select at random m examples from the minority class and then for each of the selected examples \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf {x} = (x_1,\\ldots , x_m)$$\\end{document} we generate its synthetic counterpart by replacing \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_i(j)$$\\end{document} (the ith attribute in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_j$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j \\in {1,2,\\ldots ,m}$$\\end{document}) with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu _i - r_i \\cdot \\sigma _{i}/ \\sqrt{n}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu _i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _i$$\\end{document} denote the mean and the standard deviation of the ith feature restricted to the original minority class, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r_i$$\\end{document} is a random value drawn from the standard normal distribution. When \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m > n$$\\end{document}, we can repeat the above process until we reach the required amount of synthetic examples. Since the synthetic sample is achieved by randomly walking from one real sample, so this oversampling is called random walk oversampling.",
            "cite_spans": [],
            "section": "Resampling Technique ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "In this section, we introduce the feature overlapping measures and linearity measures among various data complexity measures (Table 1).\n",
            "cite_spans": [],
            "section": "Data Complexity Measures ::: Related Works",
            "ref_spans": [
                {
                    "start": 132,
                    "end": 133,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Feature Overlapping Measures. F1 measures the highest discriminant ratio among all the features in the dataset [14]. F1v is a complement of F1 and a higher value of F1v indicates there exists a vector that can separate different class samples after these samples are projected on it [19]. F2 calculates the overlap ratio of all features (the width of the overlap interval to the width of the entire interval) and returns the product of the ratios of all features [19]. F3 measures the individual feature efficiency and returns the maximum value among all features.",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 114,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 284,
                    "end": 286,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 464,
                    "end": 466,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Data Complexity Measures ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "Linearity Measures. L1 and L2 both measure to what extent the classes can be linearly separated using an SVM with a linear kernel [19], where L1 returns the sum of the distances of the misclassified samples to the linear boundary and L2 returns the error rate of the linear classifier. L3 returns the error rate of an SVM with linear kernel on a test set, where the SVM is trained on training samples and the test set is manually created by performing linear interpolation on the two randomly chosen samples from the same class.",
            "cite_spans": [
                {
                    "start": 131,
                    "end": 133,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                }
            ],
            "section": "Data Complexity Measures ::: Related Works",
            "ref_spans": []
        },
        {
            "text": "The experiments reported in this paper are based on 19 two-class imbalanced datasets from the KEEL-collection [1] and six powerful oversampling approaches (using R package imbalance [4]), which have been reviewed in Sect. 2.1. The collected datasets are divided into 5 stratified folds (for cross-validation) and only the training set is oversampled, where the stratified fold is to ensure the imbalance ratio in the training set is consistent with the original dataset and only oversampling the training set is to avoid over-optimism problem [14].\n",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 112,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 183,
                    "end": 184,
                    "mention": "4",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 544,
                    "end": 546,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Experimental Setup",
            "ref_spans": []
        },
        {
            "text": "The 19 collected datasets can be simply divided into 4 groups, ecoli, glass, vehicle and yeast (Table 2). IR indicates the imbalance ratio, which is the ratio of the number of majority class samples to the number of minority class samples. In this paper, we aim to study the efficiency of different oversampling approaches and investigate the relationship between data complexity measures and the choice of oversampling techniques. Therefore, we need to calculate the 7 data complexity measures (shown in Table 1) for each dataset. In our 20 experiments for each dataset, we calculate the 7 data complexity measures for every training set (using R package ECoL [14]). Since we use 5 stratified cross-validations, we average each data complexity measures for these 5 training sets and make it the data complexity measure for the dataset.",
            "cite_spans": [
                {
                    "start": 662,
                    "end": 664,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Experimental Setup",
            "ref_spans": [
                {
                    "start": 102,
                    "end": 103,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 511,
                    "end": 512,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "In a binary classification problem, the confusion matrix (see Table 3) can provide intuitive classification results. In the class imbalance domain, it is widely admitted that Accuracy tends to give deceptive evaluation for the performance. Instead of Accuracy, the Area Under the ROC Curve (AUC) can be used to evaluate the performance [13] and can be computed as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$AUC = \\frac{1+ TP_{rate} - FP_{rate}}{2}$$\\end{document}, where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$TP_{rate} = \\frac{TP}{TP + FN}, FP_{rate} = \\frac{FP}{FP + TN}$$\\end{document}. Apart from the AUC value, there are some other measures to assess the performance for imbalanced datasets, such as geometric mean (GM) and F-measure (FM) [13].\n",
            "cite_spans": [
                {
                    "start": 337,
                    "end": 339,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1201,
                    "end": 1203,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                }
            ],
            "section": "Experimental Setup",
            "ref_spans": [
                {
                    "start": 68,
                    "end": 69,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "Due to the limited space, only the AUC results for C5.0 decision tree in our experiments are presented in Table 4. We can observe that RACOG outperforms the other 5 oversampling techniques in 9 out of 19 datasets and MWMOTE is the 2nd best oversampling approaches. From our experimental results, we can conclude that, in most cases, oversampling approaches which consider the minority class distribution (RACOG, wRACOG and RWO-Sampling) perform better. It was expected that data complexity can provide some guidance for choosing the oversampling technique, however, from our experimental results, no obvious relationship between data complexity and the choice of oversampling approaches can be concluded. This is because the 6 introduced oversampling approaches are designed for common datasets and do not take a specific data characteristic into account.\n",
            "cite_spans": [],
            "section": "Simulation Analysis and Discussions",
            "ref_spans": [
                {
                    "start": 112,
                    "end": 113,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "According to our experimental results, although the data complexity measures cannot provide guidance for choosing the oversampling approaches, we find there is a strong correlation between the potential best AUC (after oversample) and some of the data complexity measures. From Fig. 1 and Table 5, it can be concluded that the potential best AUC value that can be achieved through oversampling techniques has an extreme negative correlation with the F1v value and linearity measures. In the imbalanced learning domain, there are many researchers focus on studying data complexity measures. In [14], the authors propose that the potential best AUC value after resampling can be predicted through various data complexity measures. In [10], the authors demonstrate that F1 value has an influence on the potential improvement brought by oversampling approaches. However, they did not consider the F1v measure, which has the strongest correlation with AUC value. Hence, we recommend using F1v to evaluate the overlap in imbalanced dataset.",
            "cite_spans": [
                {
                    "start": 594,
                    "end": 596,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 733,
                    "end": 735,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Simulation Analysis and Discussions",
            "ref_spans": [
                {
                    "start": 283,
                    "end": 284,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 295,
                    "end": 296,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "For the experiments we adopted the computer fluid dynamics (CFD) simulation of a configuration of the TUM DrivAer model [8]. The simulation model is deformed using the discussed FFD algorithm, using a lattice with 7 planes in x- and z-directions, and 10 in y-direction (Fig. 3). The planes closer to the boundaries of the control volume are not displaced in order to enable a smooth transition from the region affected by the deformations to the original domain. Assuming symmetry of the shape with respect to the vertical plane (xz) and deformations caused by displacement of entire control planes only in the direction of their normal vectors, it yields a design space with 9 parameters. To generate the data set, the displacements \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_i$$\\end{document} were sampled from a random uniform distribution and constrained to the volume of the lattice, allowing the overlap of planes.\n",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 122,
                    "mention": "8",
                    "ref_id": "BIBREF22"
                }
            ],
            "section": "Generation of a Synthetic Data Set ::: Efficient Oversampling Strategies for Improved Vehicle Mesh Quality Classification",
            "ref_spans": [
                {
                    "start": 275,
                    "end": 276,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "The initial mesh was generated using the algorithms blockMesh and snappyHexMesh of OpenFOAM\u00ae1. We automatically generated 300 meshes based on the FFD algorithm implemented in python and evaluated them using the OpenFOAM checkMesh rounting. The quality of the meshes was verified using the checkMesh routine, also available in OpenFOAM\u00ae, and we generated 300 deformed meshes. In the process, 6 meshes were discarded due to errors in the meshing process. The metrics used to define the quality of the meshes were the number of warnings raised by the meshCheck algorithm, the maximum skewness and maximum aspect ratio. We manually labeled the feasible meshes according to the rules shown in Table 6. The imbalance ratios after manually labeling are also given in Table 6. Please note that the input attributes are exactly the same for all three sets of datasets, only the \u201cclass\u201d labels are different. In this way, the values of data complexity measures for the three datasets vary from each other.\n",
            "cite_spans": [],
            "section": "Generation of a Synthetic Data Set ::: Efficient Oversampling Strategies for Improved Vehicle Mesh Quality Classification",
            "ref_spans": [
                {
                    "start": 694,
                    "end": 695,
                    "mention": "6",
                    "ref_id": "TABREF5"
                },
                {
                    "start": 766,
                    "end": 767,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "The experimental results on the digital vehicle dataset are given in Table 7. It is consistent with the conclusion we draw in Sect. 4 that, RACOG outperforms the other 5 oversampling techniques in 2 out of 3 datasets. Therefore, combining our experimental results on both benchmark and real-world inspired datasets, we can conclude RACOG is the most powerful one of the considered 6 oversampling approaches. Moreover, we find that applying the oversampling techniques can improve the performance by around 10% for our digital vehicle datasets. We also calculate the data complexity measures for our digital vehicle datasets, our findings on the correlation between the potential AUC value and the data complexity measures remains consistent with the conclusion in Sect. 4.\n",
            "cite_spans": [],
            "section": "Results and Discussion ::: Efficient Oversampling Strategies for Improved Vehicle Mesh Quality Classification",
            "ref_spans": [
                {
                    "start": 75,
                    "end": 76,
                    "mention": "7",
                    "ref_id": "TABREF6"
                }
            ]
        },
        {
            "text": "In this work, we reviewed six powerful oversampling techniques, including \u201cclassical\u201d ones (SMOTE, ADASYN and MWMOTE) and new ones (RACOG, wRACOG and RWO-Sampling), in which the new ones consider the minority class distribution while the \u201cclassical\u201d ones not. The six reviewed oversampling approaches were performed on 19 benchmark imbalanced datasets and an imbalanced real-world inspired vehicle dataset to investigate their efficiency. Seven data complexity measures were considered in order to find the relationship between data complexity measures and the choice of resampling techniques. According to our experimental results, two main conclusions can be derived: In our experiment, in most cases, oversampling approaches which consider the minority class distribution (RACOG, wRACOG and RWO-Sampling) perform better. For both benchmark datasets and our real-world inspired dataset, RACOG performs best and MWMOTE comes to the second.No obvious relationship between data complexity measures and the choice of resampling techniques can be abstracted from our experimental results. However, we find F1v value has a strong correlation with the potential best AUC value (after resampling) while rare researchers in the imbalance learning domain do not consider F1v value for evaluating the overlap between classes.\n",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        },
        {
            "text": "We only simply apply the oversampling techniques for our digital vehicle dataset and evaluate their efficiency in this paper. In future work, we will focus on adjusting the imbalance learning algorithms to solve the proposed engineering problem. Additionally, the effect of the interaction between various data complexity measures on the choice of resampling technique will be studied.",
            "cite_spans": [],
            "section": "Conclusion and Future Work",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Complexity measures information. \u201cPositive\u201d and \u201cNegative\u201d indicate the positive and negative relation between measure value and data complexity respectively.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Information on datasets in 4 groups\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Confusion matrix for a binary classification problem\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4.: AUC results for C5.0 decision tree.\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table 5.: Results of hypothesis test.\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table 6.: Feasible meshes labeling rule.\n",
            "type": "table"
        },
        "TABREF6": {
            "text": "Table 7.: Experimental results (AUC) on digital vehicle dataset.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: Correlation matrix.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Example of free form deformation applied to a configuration of the TUM DrivAer model [8] using a lattice with four planes in each direction.",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig. 3.: Free form deformation lattice used to generate the data set for the experiments.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "KEEL data-mining software tool: data set repository, integration of algorithms and experimental analysis framework",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Alcal\u00e1-Fdez",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "J. Multiple-Valued Logic Soft Comput.",
            "volume": "17",
            "issn": "2",
            "pages": "255-287",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Adaptive swarm balancing algorithms for rare-event prediction in imbalanced healthcare data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "PLoS ONE",
            "volume": "12",
            "issn": "7",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0180830"
                ]
            }
        },
        "BIBREF3": {
            "title": "PolyCut: monotone graph-cuts for PolyCube base-complex construction",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Livesu",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Vining",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sheffer",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gregson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Scateni",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Trans. Graph.",
            "volume": "32",
            "issn": "6",
            "pages": "171:1-171:12",
            "other_ids": {
                "DOI": [
                    "10.1145/2508363.2508388"
                ]
            }
        },
        "BIBREF4": {
            "title": "An insight into classification with imbalanced data: empirical results and current trends on using data intrinsic characteristics",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "L\u00f3pez",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Garc\u00eda",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Palade",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Herrera",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Inf. Sci.",
            "volume": "250",
            "issn": "",
            "pages": "113-141",
            "other_ids": {
                "DOI": [
                    "10.1016/j.ins.2013.07.007"
                ]
            }
        },
        "BIBREF5": {
            "title": "How complex is your classification problem? A survey on measuring classification complexity",
            "authors": [
                {
                    "first": "AC",
                    "middle": [],
                    "last": "Lorena",
                    "suffix": ""
                },
                {
                    "first": "LP",
                    "middle": [],
                    "last": "Garcia",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                },
                {
                    "first": "MC",
                    "middle": [],
                    "last": "Souto",
                    "suffix": ""
                },
                {
                    "first": "TK",
                    "middle": [],
                    "last": "Ho",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM Comput. Surv. (CSUR)",
            "volume": "52",
            "issn": "5",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1145/3347711"
                ]
            }
        },
        "BIBREF6": {
            "title": "Addressing data complexity for imbalanced data sets: analysis of SMOTE-based oversampling and evolutionary undersampling",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Luengo",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Garc\u00eda",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Herrera",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Soft. Comput.",
            "volume": "15",
            "issn": "10",
            "pages": "1909-1936",
            "other_ids": {
                "DOI": [
                    "10.1007/s00500-010-0625-8"
                ]
            }
        },
        "BIBREF7": {
            "title": "Application of free form deformation techniques in evolutionary design optimisation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Menzel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Olhofer",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sendhoff",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "6th World Congress on Structural and Multidisciplinary Optimization (WCSM 2006)",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Representing the change - free form deformation for evolutionary design optimization",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Menzel",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sendhoff",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Evolutionary Computation in Practice",
            "volume": "",
            "issn": "",
            "pages": "63-86",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "MWMOTE-majority weighted minority oversampling technique for imbalanced data set learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Barua",
                    "suffix": ""
                },
                {
                    "first": "MM",
                    "middle": [],
                    "last": "Islam",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Murase",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Trans. Knowl. Data Eng.",
            "volume": "26",
            "issn": "2",
            "pages": "405-425",
            "other_ids": {
                "DOI": [
                    "10.1109/TKDE.2012.232"
                ]
            }
        },
        "BIBREF12": {
            "title": "Cross-validation for imbalanced datasets: avoiding overoptimistic and overfitting approaches [research frontier]",
            "authors": [
                {
                    "first": "MS",
                    "middle": [],
                    "last": "Santos",
                    "suffix": ""
                },
                {
                    "first": "JP",
                    "middle": [],
                    "last": "Soares",
                    "suffix": ""
                },
                {
                    "first": "PH",
                    "middle": [],
                    "last": "Abreu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Araujo",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Santos",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Comput. Intell. Mag.",
            "volume": "13",
            "issn": "4",
            "pages": "59-76",
            "other_ids": {
                "DOI": [
                    "10.1109/MCI.2018.2866730"
                ]
            }
        },
        "BIBREF13": {
            "title": "Free-form deformation of solid geometric models",
            "authors": [
                {
                    "first": "TW",
                    "middle": [],
                    "last": "Sederberg",
                    "suffix": ""
                },
                {
                    "first": "SR",
                    "middle": [],
                    "last": "Parry",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "ACM SIGGRAPH Comput. Graph.",
            "volume": "20",
            "issn": "4",
            "pages": "151-160",
            "other_ids": {
                "DOI": [
                    "10.1145/15886.15903"
                ]
            }
        },
        "BIBREF14": {
            "title": "On shape deformation techniques for simulation-based design optimization",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sieger",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Menzel",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Botsch",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "New Challenges in Grid Generation and Adaptivity for Scientific Computing",
            "volume": "",
            "issn": "",
            "pages": "281-303",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "RWO-sampling: a random walk over-sampling approach to imbalanced data classification",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Inf. Fusion",
            "volume": "20",
            "issn": "",
            "pages": "99-116",
            "other_ids": {
                "DOI": [
                    "10.1016/j.inffus.2013.12.003"
                ]
            }
        },
        "BIBREF17": {
            "title": "SMOTE: synthetic minority over-sampling technique",
            "authors": [
                {
                    "first": "NV",
                    "middle": [],
                    "last": "Chawla",
                    "suffix": ""
                },
                {
                    "first": "KW",
                    "middle": [],
                    "last": "Bowyer",
                    "suffix": ""
                },
                {
                    "first": "LO",
                    "middle": [],
                    "last": "Hall",
                    "suffix": ""
                },
                {
                    "first": "WP",
                    "middle": [],
                    "last": "Kegelmeyer",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "J. Artif. Intell. Res.",
            "volume": "16",
            "issn": "",
            "pages": "321-357",
            "other_ids": {
                "DOI": [
                    "10.1613/jair.953"
                ]
            }
        },
        "BIBREF18": {
            "title": "Imbalance: oversampling algorithms for imbalanced classification in R",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Cord\u00f3n",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Garc\u00eda",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Herrera",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Knowl.-Based Syst.",
            "volume": "161",
            "issn": "",
            "pages": "329-341",
            "other_ids": {
                "DOI": [
                    "10.1016/j.knosys.2018.07.035"
                ]
            }
        },
        "BIBREF19": {
            "title": "RACOG and wRACOG: two probabilistic oversampling techniques",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "NC",
                    "middle": [],
                    "last": "Krishnan",
                    "suffix": ""
                },
                {
                    "first": "DJ",
                    "middle": [],
                    "last": "Cook",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Knowl. Data Eng.",
            "volume": "27",
            "issn": "1",
            "pages": "222-234",
            "other_ids": {
                "DOI": [
                    "10.1109/TKDE.2014.2324567"
                ]
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fern\u00e1ndez",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Garc\u00eda",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Galar",
                    "suffix": ""
                },
                {
                    "first": "RC",
                    "middle": [],
                    "last": "Prati",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Krawczyk",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Herrera",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Learning from Imbalanced Data Sets",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}