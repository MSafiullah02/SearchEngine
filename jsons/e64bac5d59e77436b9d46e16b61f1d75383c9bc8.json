{
    "paper_id": "e64bac5d59e77436b9d46e16b61f1d75383c9bc8",
    "metadata": {
        "title": "Graph-Embedding Empowered Entity Retrieval",
        "authors": [
            {
                "first": "Emma",
                "middle": [
                    "J"
                ],
                "last": "Gerritse",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Radboud University",
                    "location": {
                        "settlement": "Nijmegen",
                        "country": "The Netherlands"
                    }
                },
                "email": "emma.gerritse@ru.nl"
            },
            {
                "first": "Faegheh",
                "middle": [],
                "last": "Hasibi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Radboud University",
                    "location": {
                        "settlement": "Nijmegen",
                        "country": "The Netherlands"
                    }
                },
                "email": ""
            },
            {
                "first": "Arjen",
                "middle": [
                    "P"
                ],
                "last": "De Vries",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Radboud University",
                    "location": {
                        "settlement": "Nijmegen",
                        "country": "The Netherlands"
                    }
                },
                "email": "a.devries@cs.ru.nl"
            }
        ]
    },
    "abstract": [
        {
            "text": "In this research, we improve upon the current state of the art in entity retrieval by re-ranking the result list using graph embeddings. The paper shows that graph embeddings are useful for entity-oriented search tasks. We demonstrate empirically that encoding information from the knowledge graph into (graph) embeddings contributes to a higher increase in effectiveness of entity retrieval results than using plain word embeddings. We analyze the impact of the accuracy of the entity linker on the overall retrieval effectiveness. Our analysis further deploys the cluster hypothesis to explain the observed advantages of graph embeddings over the more widely used word embeddings, for user tasks involving ranking entities.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Many information needs are entity-oriented, and with the rise of knowledge graphs in Web and enterprise search [20] , the role of entities has gained importance, both in the UI/UX where so-called entity cards are shown in response to entity-oriented queries, and in the ranking, where presence and absence of entity mentions is weighted differently from traditional term occurrences.",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 115,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Recently, word embeddings have been shown to be helpful for a number of information retrieval problems. In the case of entity retrieval, a natural representation would however not just represent words in context of their textual neighborhood, but in context of the knowledge graph instead. Here, we would want to apply graph embeddings instead of word embeddings, where the semantic space constructed by graph embeddings does not only encode the textual context of an entity mention, but also the context as defined through the knowledge graph. Considering Wikipedia as the knowledge graph to define the entities of interest, for example, creating a graph embedding representation does not just take the entity's page itself as context, but also its anchor text, presence in lists and/or tables, etc. It is therefore likely that graph embeddings capture more of the entity's semantic roles and as a result may distinguish better between ambiguous entities than a plain word embedding based representation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Exploring the use of graph embeddings in entity retrieval, we have studied a two-stage entity retrieval approach where the second stage employs graph embeddings for re-ranking the retrieval results of state-of-the-art entity ranking methods. We investigate the following research questions: RQ1: Does adding graph embeddings improve entity retrieval methods? RQ2: Which queries are helped the most?",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To our knowledge, we are the first to investigate how the structural information captured in graph embeddings can contribute to improved retrieval effectiveness in entity-oriented search. The contributions of this paper are as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We have build graph embeddings from Wikipedia as a knowledge graph 1 and evaluated the contribution of these embeddings as a representation of entities in the ranking algorithm, using the DBpedia-Entity V2 collection [12] . For every query, we re-rank the results of state-of-the-art entity retrieval methods using the similarity between the entity embeddings of the candidate entities retrieved in stage one with the entity embeddings of the entities identified in the query (using an off-the-shelf entity linker). We show that re-ranking using graph embeddings improves retrieval effectiveness, and investigate how to explain this result by comparing the structure of the two types of embeddings. We also analyze why some queries are helped by this method while others are not.",
            "cite_spans": [
                {
                    "start": 217,
                    "end": 221,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Distributional representations of language have been object of study for many years in natural language processing (NLP), because of their promise to represent words not in isolation, but 'semantically', with their immediate context. Algorithms like Word2Vec [19] and Glove [21] construct a vector space of word domains where similar words are mapped together (based on their linguistic context). Word2Vec uses neural networks to predict words based on the context (continuous bag of words) or context based on a word (skip gram). These word embedding representations have turned out to be highly effective in a wide variety of NLP tasks.",
            "cite_spans": [
                {
                    "start": 259,
                    "end": 263,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 274,
                    "end": 278,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Word and Graph Embeddings"
        },
        {
            "text": "Word embeddings have been shown to help effectiveness in document retrieval [6, 7] . In [7] , locally trained word embeddings are used for query expansion. Here queries are expanded with terms highly similar to the query, and it is shown that this method beats several other neural methods. In [6] , embeddings are used for weak supervision of documents. This paper uses query embeddings and document embeddings to predict relevance between queries and documents, when given BM25 scores as labels. It is able to improve on BM25.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 79,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 80,
                    "end": 82,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 88,
                    "end": 91,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 294,
                    "end": 297,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Word and Graph Embeddings"
        },
        {
            "text": "Word embeddings consider the immediate linguistic context of the word occurrences. Going beyond just the text itself, researchers have proposed to develop so-called graph embeddings to encode not just words in text, but words in context of semi-structured documents represented as graphs -for example, to distinguish the occurrence of a word in the title of a document from its occurrences in a paragraph, or in a document's anchor text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Word and Graph Embeddings"
        },
        {
            "text": "Different methods to produce graph embeddings have been proposed. Methods like Deepwalk [22] expect non-labeled edges and can be considered extensions of the word embedding approaches discussed before. Other approaches include the well-known method Trans-E [4] , where edges in the graph are denoted as triples (head, label, tail), where label is the value of the edge. Adding graph embedding vectors of the head and the label should result in the vector of the tail. The embeddings here are learned by gradient descent.",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 92,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 257,
                    "end": 260,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Word and Graph Embeddings"
        },
        {
            "text": "Wikipedia2Vec [26] applies graph embeddings to Wikipedia, creating embeddings that jointly capture link structure and text. The Wikipedia knowledge graph is indeed a natural resource for using graph embeddings, because it represents entities in a graph of interlinked Wikipedia pages and their text. The method proposed in [26] embeds words and entities in the same vector space by using word context and graph context. The word-word context is modeled using the Word2Vec approach, entity-entity context considers neighboring entities in the link graph, and word-entity context takes the words in the context of the anchor that links to an entity. The authors of Wikipedia2Vec demonstrate performance improvements on a variety of NLP tasks, although they did not consider entity retrieval in their work.",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 18,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 323,
                    "end": 327,
                    "text": "[26]",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Word and Graph Embeddings"
        },
        {
            "text": "An entity is an object or concept in the real world that can be distinctly identified [2] . Knowledge graphs like Wikipedia enrich the representation of entities by modeling the relations between them. Methods for document retrieval such as BM25 have been applied successfully to entity retrieval. However, since knowledge bases are semi-structured resources, this structural information may be used as well, for example by viewing entities as fielded documents extracted from the knowledge graph. A well-known example of this approach applies the fielded probabilistic model (BM25F [23] ), where term frequencies between different fields in documents are normalized to the length of each field. Another effective model for entity retrieval uses the fielded sequential dependence model (FSDM [27] ), which estimates the probability of relevance using information from single terms and bigrams, normalized per field.",
            "cite_spans": [
                {
                    "start": 86,
                    "end": 89,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 583,
                    "end": 587,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 792,
                    "end": 796,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Entity Retrieval"
        },
        {
            "text": "Linking entities mentioned in the query to the knowledge graph [3, 9] enables the use of relationships encoded in the knowledge graph, helping improve the estimation of relevance of candidate entities. Previous work has shown empirically that entity linking can increase effectiveness of entity retrieval. In [10] , for example, entity retrieval has been combined with entity linking to improve retrieval effectiveness over state-of-the-art methods like FSDM.",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 66,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 67,
                    "end": 69,
                    "text": "9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 309,
                    "end": 313,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Using Entity Linking for Entity Retrieval"
        },
        {
            "text": "Our research uses the Tagme entity linker [8] because it is especially suited to annotate short and poorly composed text like the queries we need to link to. Tagme adds Wikipedia hyperlinks to parts of the text, together with a confidence score.",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 45,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Using Entity Linking for Entity Retrieval"
        },
        {
            "text": "Very recent work has applied Trans-E graph embeddings to the problem of entity retrieval, and shown consistent but small improvements [15] . However, Trans-E graph embeddings are not a good choice if the graph has 1-to-many, transitive or symmetric relations, which is the case in knowledge graphs [1] . In our research, we also look into improving entity retrieval using graph embeddings, but use the Wikipedia2Vec representation to address these shortcomings.",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 138,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 298,
                    "end": 301,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Using Embeddings for Entity Retrieval"
        },
        {
            "text": "We base the training of our entity embeddings on Wikipedia2Vec [25, 26] . Taking a knowledge graph as the input, Wikipedia2Vec extends the skip-gram variant of Word2Vec [18, 19] and learns word and entity embeddings jointly. The objective function of this model is composed of three components. The first component infers optimal embeddings for words W in the corpus. Given a sequence of words w 1 w 2 ...w T and a context window of size c, the word-based objective function is:",
            "cite_spans": [
                {
                    "start": 63,
                    "end": 67,
                    "text": "[25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 68,
                    "end": 71,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 169,
                    "end": 173,
                    "text": "[18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 174,
                    "end": 177,
                    "text": "19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Graph Embeddings"
        },
        {
            "text": "where matrices U and V represent the input and output vector representations, deriving the final embeddings from matrix V.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Embeddings"
        },
        {
            "text": "The two other components of the objective function take the knowledge graph into account. One addition considers a link-based measure estimated from the knowledge graph (i.e., Wikipedia). This measure captures the relatedness between entities in the knowledge base, based on the similarity between their incoming links:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Embeddings"
        },
        {
            "text": "Here, C e denotes entities linked to an entity e, and E represents all entities in the knowledge graph.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Embeddings"
        },
        {
            "text": "The last addition to the objective function places similar entities and words near each other by considering the context of the anchor text. The intuition is the same as in classic Word2Vec, but here, words in the vicinity of the anchor text have to predict the entity mention. Considering a knowledge graph with anchors A and an entity e the goal is to predict context words of the entity:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Embeddings"
        },
        {
            "text": "where a(e) gives the previous and next c words of the referent entity e. These three components (word context, link structure, and anchor context) are then combined linearly into the following objective function:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Graph Embeddings"
        },
        {
            "text": "Training the Wikipedia2Vec model on a Wikipedia knowledge graph results in a single graph embedding vector for every Wikipedia entity. The next question to answer is how to use these graph embeddings in the setting of entity retrieval. We propose a two-stage ranking model, where we first produce a ranking of candidate entities using state-of-the-art entity retrieval models (see Sect. 2.2), and then use the graph embeddings to reorder these entities based on their similarity to the query entities, as measured in the derived graph embedding space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Re-ranking Entities"
        },
        {
            "text": "Following the related work discussed in Sect. 2.3, we use the Tagme entity linker to identify the entities mentioned in the query. Given input query Q, we obtain a set of linked entities E(Q) and a confidence score s(e) for each entity, which represents the strength of the relationship between the query and the linked entity. We then compute an embedding-based score for every query Q and entity E:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Re-ranking Entities"
        },
        {
            "text": "where \u2212 \u2192 E , \u2212 \u2192 e denote the embeddings vectors for entities E and e. The rationale for this approach is the hypothesis that relevant entities for a given query are situated close (in graph embedding space) to the query entities identified by the entity linker.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Re-ranking Entities"
        },
        {
            "text": "Consider for example the query \"Who is the daughter of Bill Clinton married to.\" Tagme links the query to entities Bill Clinton with a confidence of 0.66, Daughter with a confidence of 0.13, and Same-sex marriage with a confidence score of 0.21. Highly ranked entities then have a large similarity to these entities, where similarity to Bill Clinton adds more to the score than similarity to Daughter or Same-sex marriage (as the confidence score of Bill Clinton is higher than the other two). The relevant entities for this query (according to the DBpedia-Entity V2 test collection [12] ) are Chelsea Clinton, who is Bill Clinton's daughter, and Clinton Family. We can reasonably expect these entities to have similarity to the linked entities, confirming our intuition.",
            "cite_spans": [
                {
                    "start": 583,
                    "end": 587,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Re-ranking Entities"
        },
        {
            "text": "To produce our final score, we interpolate the embedding-based score computed using Eq. (5) with the score of the state-of-the-art entity retrieval model used to produce the candidate entities in stage one:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Re-ranking Entities"
        },
        {
            "text": "In our experiments, we used the DBpedia-Entity V2 test collection [12] . The collection consists of 467 queries and relevance assessments for 49280 query-entity pairs, where the entities are drawn from the DBpedia 2015-10 dump. The relevance assessments are graded values of 2, 1, and 0 for highly relevant, relevant, and not relevant entities, respectively. The queries are categorized into 4 different groups: SemSearch ES consisting of short and ambiguous keyword queries (e.g.,\"Nokia E73\"), INEX-LD containing IR-Style keyword queries (e.g., \"guitar chord minor\"), ListSearch consisting of queries seeking for a list of entities (e.g., \"States that border Oklahoma\"), and QALD-2 containing entity-bearing natural language queries (e.g., \"Which country does the creator of Miffy come from\"). Following the baseline runs curated with the DBpedia-Entity V2 collection, we used the stopped version of queries, where stop patterns like \"which\" and \"who\" are removed from the queries.",
            "cite_spans": [
                {
                    "start": 66,
                    "end": 70,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Test Collection"
        },
        {
            "text": "Wikipedia2Vec provides pre-trained embeddings. These embeddings, however, are not available for all entities in Wikipedia; e.g., 25% of the assessed entities in DBpedia-Entity V2 collection have no pre-trained embedding. The reasons for these missing embeddings are two-fold: (i) \"rare\" entities were excluded from the training data, and, (ii) entity identifiers evolve over time, resulting in entity mismatches with those in the DBpedia-Entity collection.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Embedding Training"
        },
        {
            "text": "For training new graph embeddings, we used Wikipedia 2019-07 dump. This was the newest version at the time of training. We address the entity mismatch problem by identifying the entities that have been renamed in the new Wikipedia dump. Some of these entities were obtained using the redirect API of Wikipedia. 2 Others were found by matching the Wikipedia page IDs of the two Wikipedia dumps. The page IDs of Wikipedia 2019-07 were available on the Wikipedia website. For the dump where DBpedia-Entity is based on, however, these IDs are not available anymore; we obtained them from the Nordlys package [11] .",
            "cite_spans": [
                {
                    "start": 604,
                    "end": 608,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Embedding Training"
        },
        {
            "text": "To avoid excluding rare entities and generate embeddings for a wide range of entities, we changed several Wikipedia2Vec settings. The two settings that resulted in the highest coverage of entities are: (i) minimum number of times an entity appears as a link in Wikipedia, (ii) whether to include or exclude disambiguation pages. Table 1 shows the effect of these settings on the number of missing entities; specifically the number of entities that are assessed in the DBpedia-Entity collection, but have missing embeddings. We categorize these missing entities into two groups:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 329,
                    "end": 336,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Embedding Training"
        },
        {
            "text": "-No-page: Entities without any pages. These entities neither were found by the Wikipedia redirect API nor could be matched by their page IDs. -No-emb: Entities that could be found by their identifiers, but were not included in the Wikipedia2Vec embeddings.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Embedding Training"
        },
        {
            "text": "The first line in Table 1 corresponds to the default setting of Wikipedia2Vec, which covers only 75% of assessed entities in the DBpedia-Entity collection. When considering all entities in the knowledge graph, this setting discards an even larger number of entities, which is not an ideal setup for entity ranking. By choosing the right settings (the last line of Table 1 ), we increased the coverage of entities to 97.6%.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 25,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 364,
                    "end": 371,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Embedding Training"
        },
        {
            "text": "We trained two versions of embeddings: with and without link graph; i.e., using Eq. (4) with and without the L e component.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Embedding Training"
        },
        {
            "text": "Our entity re-ranking approach involves free parameter \u03bb that needs to be estimated (see Eq. (6)). To set this parameter, we employed the Coordinate Ascent algorithm [17] with random restart of 3, optimized for NDCG@100. All experiments were performed using 5-fold cross-validation, where the folds were obtained from the collection (DBpedia-Entity V2). This makes our results comparable to the DBpedia-Entity V2 baseline runs, as the same folds are used for all the methods. Entity re-ranking was performed on top 1000 entities ranked by two state-of-the-art term-based entity retrieval models: FSDM and BM25F-CA [12] . For all experiments, we used the embedding vectors of 100 dimensions, which were trained using the settings described in Sect. 4.2.",
            "cite_spans": [
                {
                    "start": 166,
                    "end": 170,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 614,
                    "end": 618,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Parameter Setting"
        },
        {
            "text": "To answer our first research question, whether embeddings improve the score of entity retrieval, we compare our entity re-ranking approach with a number of baseline entity retrieval models. Table 2 shows the results for different models with respect to NDCG@10 and NDCG@100, the default evaluation measures for DBpedia-entity V2. In this table, the embedding-based similarity component (Eq. (5)) is denoted by ESim, where c and cg subscripts refer to the two versions of our entity embeddings: without and with link graph. Table 2 . Results of embedding-based entity re-ranking approach on different query subsets of DBpedia-Entity V2 collection. Significance of results is explained in running text.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 190,
                    "end": 197,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 523,
                    "end": 530,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Overall Performance"
        },
        {
            "text": "SemSearch INEX-LD ListSearch QALD-2 Total NDCG @10 @100 @10 @100 @10 @100 @10 @100 @10 @100",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "Reranking the FSDM top 1000 entities The results of our method are presented for components ESim c and ESim cg by themselves (i.e., \u03bb = 1 in Eq. (6)), and also in combination with FSDM and BM25F-CA. The mean and standard deviation of \u03bb found by the Coordinate Ascent algorithm over all folds are: 0.34 \u00b1 0.02 for FSDM+ESim c , 0.61 \u00b1 0.01 for FSDM+ESim cg , 0.81 \u00b1 0.03 for BM25F-CA+ESim c , and 0.88 \u00b1 0.00 for BM25F-CA+ESim cg . The results show that the embedding-based scores alone do not perform very well, however, when combining them with other scores, the performance improves by a large margin. We determine the statistical significance of the difference in effectiveness for both the NDCG@10 and the NDCG@100 values, using the two-tailed paired t-test with \u03b1 < 0.05. The results show that both versions of FSDM+ESim and BM25-CA+ESim models yield significant improvements over FSDM and BM25-CA models (with respect to all metrics), respectively. Also, FSDM+ESim cg improves significantly over FSDM+ELR with respect to NDCG@100, showing that our embedding based method captures entity similarities better than the strong entity ID matching approach used in the ELR method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "When considering the query subsets, we observe that FSDM+ESim cg significantly outperforms FSDM for SemSearch and QALD queries with respect to NDCG@10, and for INEX-LD queries with respect to NDCG@100. Improvements over BM25F-CA were more substantial: BM25F-CA+ESim cg brings significant improvements for all categories (with respect to all metrics) except for SemSearch queries for NDCG@100. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model"
        },
        {
            "text": "The results of Table 2 suggest that graph-based entity embeddings yield better performance compared to context only entity embeddings. To analyze why graph-based entity embeddings are beneficial for entity retrieval models, we conduct a set of experiments and investigate properties of embeddings with and without the graph structure.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 15,
                    "end": 22,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Entity Embeddings Analysis"
        },
        {
            "text": "According to the cluster hypothesis [14] , documents relevant to the same query should cluster together. We consider the embeddings as data-points to be clustered and compare the resulting clusters in several ways. First, we compute the Davies Bouldin index [5] and the Silhouette index [24] , which are: 3.16 and 0.08 for the embeddings with link graph, and 3.98 and \u22120.05 for the embeddings without link graph, respectively. Both measures indicate that better clusters arise for the embeddings that capture graph structure.",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 258,
                    "end": 261,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 287,
                    "end": 291,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Entity Embeddings Analysis"
        },
        {
            "text": "To get an indication of how coherent the clusters are, we compute for each query the coherence score defined in [13] . This score measures the similarity between item pairs of a cluster and returns the percentage of items with similarity score higher than a threshold, thereby assigning high scores to the clusters that are coherent. Formally, given a document set D, the coherence score is computed as:",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 116,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Entity Embeddings Analysis"
        },
        {
            "text": "where M is total number of documents and the \u03b4 function for each document pair d i and d j is defined as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Entity Embeddings Analysis"
        },
        {
            "text": "We compute the coherence score with thresholds 0.8, 0.9, using cosine for similarity function sim(d i , d j ), where d i and d j correspond to entities. Figure 1 shows the results of coherence score for all queries in our collection. Each point represents the coherence score of all relevant entities (according to the qrels) for a query. We considered only queries with more than 10 relevant entities, for clusters large enough to compute a meaningful score. Queries are sorted on the xaxis by the number of relevant entities. The plots clearly show that the coherence score for graph-based entity embeddings is higher than for context only ones. Based on these performance improvements we conclude that adding the graph structure results in embeddings that are more suitable for entity-oriented tasks. Figures 2 helps to visually understand how clusters of entities differ for the two methods (a subset of all entities is shown for clarity). The data points correspond to the entities with a relevance grade higher than 0, for 12 queries with 100-200 relevant entities in the ground truth data. We use Uniform Manifold Approximation and Projection (UMAP) [16] to reduce the embeddings dimensions from 100 to two and plot the projected entities for each query. In Fig. 2b most of the clusters are overlapping in a star-like shape, while in Fig. 2a entities of the query and are also placed in the vicinity of them, although they do not address the information needs of the query. This is consistent with the plots of Fig. 2 and in line with our conclusion on the effect of graph embeddings for entity-oriented search.",
            "cite_spans": [
                {
                    "start": 1157,
                    "end": 1161,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 153,
                    "end": 161,
                    "text": "Figure 1",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1265,
                    "end": 1272,
                    "text": "Fig. 2b",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1341,
                    "end": 1348,
                    "text": "Fig. 2a",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1518,
                    "end": 1524,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Entity Embeddings Analysis"
        },
        {
            "text": "Next, we investigate our second research question and analyse queries that are helped and hurt the most by our embedding-based method. Table 3 shows six queries that are affected the most by BM25F-CA+ESim cg compared to BM25F-CA (on NDCG@100). Each of the three queries with highest gains are linked to at least one relevant entity (according to the assessments). The losses can be attributed to various sources of errors. For the query \"spring shoe canada\", the only relevant entity belongs to the 2.4% of entities that have no embedding (cf. \u00a74.2). Query \"vietnam war movie\" is linked to entities Vietnam War and War film, with confidence scores of 0.7 and 0.2, respectively. This emphasizes Vietnam war facts instead of its movies, and could be resolved by improving the accuracy of the entity linker and/or employing a re-ranking approach that is more robust to linking errors. The query \"mr rourke fantasy island\" is linked to a wrong entity due to a spelling mistake. To conclude, errors in entity linking form one of the main reasons of performance loss in our approach.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 135,
                    "end": 142,
                    "text": "Table 3",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Query Analysis"
        },
        {
            "text": "To further understand the difference between the two versions of the embeddings at the query-level, we selected the queries with the highest and lowest gain in NDCG@100 (i.e., comparing BM25F+ESim cg and BM25F+ESim c ). For the query \"Which instruments did John Lennon play?\", the two linked entities (with the highest confidence score) are John Lennon and Musical Instruments. Their closest entity in graph embedding space is John Lennon's musical instruments, relevant to the query. This entity, however, is not among the most similar entities when we consider the context-only case.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Query Analysis"
        },
        {
            "text": "For the other queries in Table 4 , the effect is similar but less large than in the BM25F and BM25F + ESim cg case, probably due to the lower value of \u03bb.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 25,
                    "end": 32,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Query Analysis"
        },
        {
            "text": "We investigated the use of entity embeddings for entity retrieval. We trained entity embeddings with Wikipedia2Vec, combined these with state-of-the-art entity ranking models, and find empirically that using graph embeddings leads to increased effectiveness of query results on DBpedia-Entity V2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The empirical findings can be interpreted as evidence for the cluster hypothesis. Including a representation of the graph structure in the entity embeddings leads to better clusters and higher effectiveness of retrieval results. We further see that queries which get linked to relevant entities or pages neighboring to relevant entities get helped the most, while queries with wrongly linked entities are helped the least.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "We conclude that enriching entity retrieval methods with entity embeddings leads to improved effectiveness, but acknowledge the following limitations of this study. Not all query categories lead to improvements on NDCG. While the state-of-the-art in entity-linking has made significant progress in recent years, we applied Tagme to identify the entities in queries. As we observed that lower performance of queries can often be attributed to erroneously linked entities, we expect better results by replacing this component for a state-of-the-art approach. Finally, we have only experimented using the embeddings constructed by Wikipedia2Vec, and plan to continue our experiments using alternative entity embedding methods like TransE.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Machine learning & embeddings for large knowledge graphs",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Paulheim",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Entity-Oriented Search",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-93935-3"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Fast and space-efficient entity linking in queries",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Blanco",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Ottaviano",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Meij",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "179--188",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Translating embeddings for modeling multi-relational data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bordes",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Usunier",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garcia-Duran",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Yakhnenko",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2787--2795",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A cluster separation measure",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "L"
                    ],
                    "last": "Davies",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "W"
                    ],
                    "last": "Bouldin",
                    "suffix": ""
                }
            ],
            "year": 1979,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "1",
            "issn": "2",
            "pages": "224--227",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Neural ranking models with weak supervision",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dehghani",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zamani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Severyn",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kamps",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "B"
                    ],
                    "last": "Croft",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "65--74",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Query expansion with locally-trained word embeddings",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Diaz",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Mitra",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Craswell",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
            "volume": "",
            "issn": "",
            "pages": "367--377",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "TAGME: on-the-fly annotation of short text fragments (by wikipedia entities)",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ferragina",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Scaiella",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 19th ACM International IW3C2 on Information and Knowledge Management",
            "volume": "",
            "issn": "",
            "pages": "1625--1628",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Entity linking in queries: tasks and evaluation",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hasibi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Bratsberg",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 International Conference on The Theory of Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "171--180",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Exploiting entity linking in queries for entity retrieval",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hasibi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Bratsberg",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "209--218",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Nordlys: a toolkit for entityoriented and semantic search",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hasibi",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Balog",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Garigliotti",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "1289--1292",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "DBpedia-Entity V2: a test collection for entity search",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Hasibi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "1265--1268",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Exploring topic structure: coherence, diversity and relatedness. SIKS",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "The use of hierarchic clustering in information retrieval",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Jardine",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "J"
                    ],
                    "last": "Van Rijsbergen",
                    "suffix": ""
                }
            ],
            "year": 1971,
            "venue": "Inf. Storage Retrieval",
            "volume": "7",
            "issn": "5",
            "pages": "217--240",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Explore entity embedding effectiveness in entity retrieval",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1908.10554"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "UMAP: uniform manifold approximation and projection",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mcinnes",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Healy",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Saul",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Grossberger",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "J. Open Source Softw",
            "volume": "3",
            "issn": "29",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Linear feature-based models for information retrieval",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Metzler",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Bruce Croft",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Inf. Retrieval",
            "volume": "10",
            "issn": "3",
            "pages": "257--274",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Efficient estimation of word representations in vector space",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "1st International Conference on Learning Representations, ICLR",
            "volume": "",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Distributed representations of words and phrases and their compositionality",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "S"
                    ],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in Neural Information Processing Systems (NIPS)",
            "volume": "",
            "issn": "",
            "pages": "3111--3119",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Industry-scale knowledge graphs: lessons and challenges",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Noy",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Narayanan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Patterson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Taylor",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Commun. ACM",
            "volume": "62",
            "issn": "8",
            "pages": "36--43",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Glove: global vectors for word representation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "DeepWalk: online learning of social representations",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Perozzi",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Al-Rfou",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Skiena",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "701--710",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "The probabilistic relevance framework: BM25 and beyond",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Robertson",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zaragoza",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Found. Trends R Inf. Retrieval",
            "volume": "3",
            "issn": "4",
            "pages": "333--389",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Rousseeuw",
                    "suffix": ""
                }
            ],
            "year": 1987,
            "venue": "J. Comput. Appl. Math",
            "volume": "20",
            "issn": "",
            "pages": "53--65",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Wikipedia2Vec: an optimized tool for learning embeddings of words and entities from Wikipedia",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Yamada",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Asai",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shindo",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Takeda",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Takefuji",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Joint learning of the embedding of words and entities for named entity disambiguation",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Yamada",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shindo",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Takeda",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Takefuji",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The SIGNLL Conference on Computational Natural Language Learning",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Fielded sequential dependence model for adhoc entity retrieval in the web of data",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Zhiltsov",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kotov",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Nikolaev",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "volume": "",
            "issn": "",
            "pages": "253--262",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "0.660 0.736 0.466 0.552 0.452 0.535 0.390 0.483 0.487 0.572",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Coherence score of all relevant entities per query, computed for the versions of entity embeddings (without and with link graph). The queries are ordered by the number of their relevant entities in x-axis.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "UMAP visualization of entity embeddings for a subset of queries. Color-codes correspond to the relevant entities per query. Queries per code are listed inTable 5of the Appendix. Default settings of UMAP in python were used. (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "the clusters are more separated and the ones with similar search intents are close to each other; e.g., queries QALD2 te-39 and QALD2 tr-64 (which are both about companies), or INEX LD-20120112 and INEX LD-2009063 (which are both about war) are situated next to each other. To observe how false positive entities are placed in the embedding space, we added the 10 highest ranked false positives to the data and created new UMAP plots. In the obtained plots, false positive entities that are semantically similar to the true positive entities are close to each other. For example, two false positive entities for the query \"South Korean girl groups\" are: SHINee (a South Korean boy band) and Hyuna (a South Korean female singer). Both of these entities are semantically similar to the relevant",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Missing entities with different settings",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Top queries with the highest gains and losses in NDCG at cut-offs 10 and 100, BM25F + ESimcg vs. BM25F.Table 4. Top queries with the highest gains and losses in NDCG at cut-offs 10 and 100, BM25F + ESimcg vs. BM25F + ESimc. Which other weapons did the designer of the Uzi develop? Companies that John Hennessey serves on the board of \u22120.173 \u22120.173",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}