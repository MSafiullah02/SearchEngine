{
    "paper_id": "6fe4a3d22b8b12039a6321db033035b944778697",
    "metadata": {
        "title": "The 1st Challenge on Remote Physiological Signal Sensing (RePSS)",
        "authors": [
            {
                "first": "Xiaobai",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Oulu",
                    "location": {
                        "country": "Finland"
                    }
                },
                "email": ""
            },
            {
                "first": "Hu",
                "middle": [],
                "last": "Han",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Hao",
                "middle": [],
                "last": "Lu",
                "suffix": "",
                "affiliation": {},
                "email": "hao.lu@miracle.ict.ac.cn"
            },
            {
                "first": "Xuesong",
                "middle": [],
                "last": "Niu",
                "suffix": "",
                "affiliation": {},
                "email": "xuesong.niu@vipl.ict.ac.cn"
            },
            {
                "first": "Zitong",
                "middle": [],
                "last": "Yu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Oulu",
                    "location": {
                        "country": "Finland"
                    }
                },
                "email": ""
            },
            {
                "first": "Antitza",
                "middle": [],
                "last": "Dantcheva",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "INRIA",
                    "location": {
                        "country": "France"
                    }
                },
                "email": "antitza.dantcheva@inria.fr"
            },
            {
                "first": "Guoying",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Oulu",
                    "location": {
                        "country": "Finland"
                    }
                },
                "email": "guoying.zhao@oulu.fi"
            },
            {
                "first": "Shiguang",
                "middle": [],
                "last": "Shan",
                "suffix": "",
                "affiliation": {},
                "email": "sgshan@ict.ac.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "Remote measurement of physiological signals from videos is an emerging topic. The topic draws great interests, but the lack of publicly available benchmark databases and a fair validation platform are hindering its further development. For this concern, we organize the first challenge on Remote Physiological Signal Sensing (RePSS), in which two databases of VIPL and OBF are provided as the benchmark for kin researchers to evaluate their approaches. The 1st challenge of RePSS focuses on measuring the average heart rate from facial videos, which is the basic problem of remote physiological measurement. This paper presents an overview of the challenge, including data, protocol, analysis of results and discussion. The top ranked solutions are highlighted to provide insights for researchers, and future directions are outlined for this topic and this challenge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Physiological signals such as the heart rate (HR), respiration rate (RR), and heart rate variability (HRV) are important indicators of human physical conditions. Until today most main-stream approaches for measuring physiological signals still reply on contact sensors, including special medical instruments like the electrocardiography (ECG), and some commercial products like sport watches or smart bracelets. To pursue convenient and comfort way for physiological signal measurement, efforts have been made during the last decade for remote measurement from facial videos recorded with commonly accessible cameras. Compared with contact measures, the advantages that remote measures could bring are that, firstly, breaking the constrain of physical distance that people can be measured at different locations; secondly, allowing more comfortable monitoring, especially for patients with special conditions that might be irritated by contact means; and thirdly, integrating the measurement with camera systems which are available almost everywhere in the world. If feasible, remote physiological signal measures would facilitate applications in many fields, e.g., pushing the Telemedicine to another level, which stands high value in the background of global COVID-19 outbreak while the paper was written.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Many methods for remote physiological signal measurement have been proposed ever since 2008, when Verkruysse et al. [21] first reported that plethysmography (PPG) signals can be captured from human faces under ambient light. Of all the physiological signals, the HR was the main focus of most studies, while other signals (e.g., the RR and HRV) were explored in a small number of studies. From the feature point of view, remote HR measurement methods can be divided as color-based approaches and motion-based approaches. Color-based approaches such as [16] , [17] , [4] , [8] , [10] and [22] rely on the subtle color changes of facial skin pixels to measure HRs, while motionbased approaches such as [1] , [11] and [23] track the motion trajectories of facial pixels to measure HRs. From the learning point of view, remote HR measurement methods can be divided as training-free approaches and learning-based approaches. Most earlier approaches are training-free, including [16, 17] , [4] , [1] and [10] , which dont involve any training process, and rely on signal filtering methods such as blind source separation and others to refine the HR signals. Later studies started to exploit the strength of machine learning or deep learning to further tackle the problem, such studies include [6] , [14] , [12] , [3] , [24] and [25] . More details of the development of remote HR measurement are referred to survey papers [18] and [19] .",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "text": "[21]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 552,
                    "end": 556,
                    "text": "[16]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 559,
                    "end": 563,
                    "text": "[17]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 566,
                    "end": 569,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 572,
                    "end": 575,
                    "text": "[8]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 578,
                    "end": 582,
                    "text": "[10]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 587,
                    "end": 591,
                    "text": "[22]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 700,
                    "end": 703,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 706,
                    "end": 710,
                    "text": "[11]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 715,
                    "end": 719,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 973,
                    "end": 977,
                    "text": "[16,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 978,
                    "end": 981,
                    "text": "17]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 984,
                    "end": 987,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 990,
                    "end": 993,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "text": "[10]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1287,
                    "end": 1290,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1293,
                    "end": 1297,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 1300,
                    "end": 1304,
                    "text": "[12]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1307,
                    "end": 1310,
                    "text": "[3]",
                    "ref_id": null
                },
                {
                    "start": 1313,
                    "end": 1317,
                    "text": "[24]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "text": "[25]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "text": "[18]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "text": "[19]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Despite the thriving research interests, the lack of publicly available benchmark databases and a fair validation platform are the major issues that hinder its further development. Kin researchers have to make repetitive efforts on self-collecting small datasets to test proposed methods, which makes it difficult to fairly evaluate and compare the actual strength and weakness of each proposed method, as self-collected data are of different recording conditions and qualities. For this concern, we organize the first challenge on Remote Physiological Signal Sensing (RePSS) in conjunction with the CVPM workshop 1 in CVPR 2020 at Seattle, USA. As the first open challenge on remote physiological signal sensing, we will be focusing on measuring the average HR from color facial videos, which is the most fundamental problem in this field.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest part of the paper are organized as follows: Section 2 gives the overview of the RePSS challenge, including the data, challenge protocol and evaluation metrics; Section 3 briefly introduces some proposed approaches that achieved leading performance in the challenge, Section 4 reports challenge results and discussions, and at last in Section 5 we discuss future directions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The data used for the RePSS challenge come from two databases: the VIPL-HR-V2 and the OBF.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "VIPL-HR-V2 is provided by the Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS), China. VIPL-HR-V2 is the second version of VIPL-HR [13, 14] , and the construction of VIPL-HR-V2 started from 2018. One important motivation for building VIPL-HR-V2 is to provide large scale data which could meet the need of deep learning methods for the purpose of remote Physiological signal sensing. So far the data of more than 3000 persons were collected in VIPL-HR-V2. The statistical information of the all subjects is listed in Table 1 . VIPL-HR-V2 contains facial videos recorded with color cameras under relatively natural ambient light. The subjects were in sitting position in front of recording cameras (Realsense F200) on a table for capturing videos. Ground truth physiological signals of HR, SpO2 and blood volumn pulse (BVP) signals are synchronously recorded with facial videos using a CONTEC CMS60C BVP sensor. Besides, subjects were asked to look as natural as possible during video recording, i.e., allowing head movement and talking.",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 161,
                    "text": "[13,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 162,
                    "end": 165,
                    "text": "14]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [
                {
                    "start": 542,
                    "end": 549,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Data"
        },
        {
            "text": "OBF is provided by the Center for Machine Vision and Signal Analysis (CMVS), University of Oulu, Finland. The data was collected from 2017 to 2018, which contains data from more than 200 subjects. OBF has subjects of both healthy person and patients with atrial fibrillation (AF), as a main motivation for OBF is to promote remote sensing in medical oriented applications. OBF subjects are from various ethnics including typical eastern Asians (Chinese, Japanese, etc.), Caucasians (Finnish, Russian, Spanish, etc), and others (Indian, Pakistanis, etc.), which means the OBF 1 http://www.es.ele.tue.nl/cvpm20/ data covers wide range of skin tones. More statistical information of the 100 healthy subjects are listed in Table 1 . Facial videos were recorded with one RGB camera (Blackmagic URFA mini) at 60 fps with resolution of 1920 by 1080, and one NIR camera at 30 fps with resolution of 640 by 480. Three channels of physiological signals (ECG at 256Hz, BVP at 128 Hz, and respiration at 32Hz) were synchronized and recorded with a NeXus-10 MKII platform. Subjects were recorded firstly at resting state and then after five-minutes of intense exercise in order to cover a wider range of HR variance. Heart rate value corresponding to each video is provided, which is the average of all heart rates in the corresponding time period of the video. RePSS challenge training data The training data of RePSS are randomly selected from VIPL-HR-V2 database. RGB videos of 500 subjects recorded with Realsense F200 camera at the average speed of 25 fps with resolution of 960 by 720 are used. For each subject we randomly cut five clips of ten-second long videos, so that the training set contains 2500 samples. The ground truth of HR (in beat-per-minute bpm) is the average of HRs of corresponding clip and provided to challenge participants for their training of models.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 719,
                    "end": 726,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Data"
        },
        {
            "text": "RePSS challenge testing data The testing data of RePSS challenge consists of two parts, that 100 subjects (no overlap with the training set) from the VIPL-HR-V2 database and 100 subjects (all from the healthy group) from the OBF databases are used. For each subject from both databases, we randomly cut five clips of ten-second long videos, so that the testing set contains 1000 samples. For the VIPL-HR-V2 part, all videos were recorded with the color camera at the average speed of 25 fps with resolution of 960 by 720, and for the OBF part, all videos were recorded with the RGB camera at frame rate of 30 fps (down sampled from the original 60fps to match with the VIPL-HR-V2 data) with resolution of 1920 by 1080. Even though all the participating subjects have signed consent forms and given the permission to use all the recorded data for scientific research and demonstrations in e.g., publications and presentations, the OBF videos were anonymized by adding mosaic blocks covering important facial features to better protect the personal identification while data is being used for research. Face positions and facial landmark locations were detected using face-alignment 2 and were provided for challenge participants to facilitate the testing process if needed. The testing data from VIPL-HR-V2 were processed in the same way to unify the format. Sample images of the anonymized testing videos are shown in Figure 1. Ground truth average HRs were computed from corresponding BVP signals of both databases, which were not provided to challenge participants and only be used for the evaluation carried out the challenge organizers based on the results submitted from the participants.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "The RePSS challenge is operated on the CodaLab platform, and consists of two stages as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Challenge protocol"
        },
        {
            "text": "Training phase (15.01.2020 20.02.2020) The training data was released on 15th. Jan. 2020. During the training phase, registered participants get access to the labelled training data and establish their machine learning models. There was no specific limitation of using outer source data, i.e., if some participants want, they can also use their own data. No result submission could be made to the challenge website during the training phase.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Challenge protocol"
        },
        {
            "text": "Testing phase (23.02.2020 06.03.2020) The testing data was released on 23rd. Feb. 2020. During the testing phase, challenge participants were asked to adjust their models using the testing data and submit testing results to the challenge website to check the performance. Test results were asked to be submitted in the form of an excel table which contains estimated average HR for each test sample. The ground truth HRs were embedded in the CodaLab platform to automatically produce final performance when a new result was submitted. Executable codes were not asked for this challenge, but may be considered in future. Each registered participant (or team) can submit results up to five times before the submission deadline, and the best performance (of the participant or the team) will be ranked and 2 https://github.com/1adrianb/face-alignment shown in the final result leading board. It is possible that one participant could register under multiple names though.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Challenge protocol"
        },
        {
            "text": "Three evaluation metrics were used for the RePSS challenge, including the mean average error (MAE), the root mean square error (RMSE) and the Pearsons correlation efficient r (R). All three metrics are widely used in related papers. The MAE and RMSE can evaluate the approaches by showing the difference of estimated HRs compared to the actual HRs on an average level, thus smaller value indicates better performance; while the correlation R shows how strongly the relationship is on scale of [- 1 1] , of the estimated HR and the corresponding GT HRs, thus larger R indicates better performance.",
            "cite_spans": [
                {
                    "start": 496,
                    "end": 500,
                    "text": "1 1]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Evaluation /metrics"
        },
        {
            "text": "Altogether 129 teams (registered CodaLab names) from 36 organizations all over world participated the first RePSS challenge, and all participants signed license agreements for data access. No constraint was put on which category of method to be preferred or forbidden as long as they can work for remote HR measure. Three approaches from the top three ranked teams are introduced in the following.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed approaches"
        },
        {
            "text": "The overview of Mixanik method is shown in Figure 2 . Data augmentation: speed-up and slow-down augmentation (or frequency morphing) [15] is used to increase training dataset size as well as variance of the reference pulse rate distribution. This should improve the performance of the algorithm especially for subjects with very low or very high pulse rate. Horizontal flip augmentation is used as well.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 137,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [
                {
                    "start": 43,
                    "end": 51,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Mixanik (Neurodata Lab)"
        },
        {
            "text": "Video preprocessing: First, the method detects faces using a RetinaNet network [9] with MobileNet backbone [7] trained with focal loss. Affine face alignment based on facial landmarks detection [5] is performed for each face. ROI average pooling is used to resize facial areas to the size of W\u00d7H for the heart rate estimation neural network, where W =H=36. After that, resampling to 25 fps by cubic interpolation is performed. Bandpath filter for [45 bpm, 180 bpm] frequencies is applied for each (pixel, channel) pair independently in order to filter out signals not related to pulse cycles.",
            "cite_spans": [
                {
                    "start": 79,
                    "end": 82,
                    "text": "[9]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 107,
                    "end": 110,
                    "text": "[7]",
                    "ref_id": null
                },
                {
                    "start": 194,
                    "end": 197,
                    "text": "[5]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Mixanik (Neurodata Lab)"
        },
        {
            "text": "Pulse rate estimation neural network: A convolutional neural network is trained for pulse rate estimation. It has 3 inputs: 1). diff input is a discrete time derivative of the preprocessed frames sequence described above. 2). frames input consists of the preprocessed frames themselves. 3). masks consists of frame-wise masks. These masks are based on the facial landmarks. Each mask pixel equals 0 if the corresponding pixel does not belong to face or belongs to mouth or eyes area, and 1 otherwise. A 3D spatiotemporal attention CNN is used followed by global spatial average pooling for PPG features extraction. Diff input is processed with the 3D CNN with separable spatio-temporal 3D convolutions and spatial pooling layers. Frames and masks are used for attention weights evaluating to select most relevant face areas for pulse rate estimation. 3D CNN outputs 32 time series, one for each channel of the last convolutional layer. Each time series is processed with a 1D CNN, which was pre-trained to evaluate pulse rate on synthetic PPG-like curves. 32 pulse rate estimations are achieved, which are combined to a single output with a 2layer perceptron. The whole 3D+1D CNN was trained endto-end with MAE loss function on our MoLi-ppg dataset [in press] ( 11.5 hours, 40 subjects) and then or the VIPL-HR V2 training data. Adam optimizer is used during training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mixanik (Neurodata Lab)"
        },
        {
            "text": "Predictions post-processing: There are 5 video fragments featuring each subject in the competition dataset. According to the training set, these 5 fragments had nearly the same reference pulse rate for most of the subjects. Let (p 1 , p 2 , p 3 , p 4 , p 5 ) be the neural network outputs on these fragments for some subject. Then the final pulse rate estimation on a video fragment is f i = 0.01 \u00d7 p i + 0.99 \u00d7 median (p 1 , p 2 , p 3 , p 4 , p 5 ). The fragments are not grouped by subjects in the test set. To evaluate median value each video was matched with other videos of the same subject. For this purpose the researchers first evaluate a simple embedding of the first frame for each video. This embedding for VIPL dataset videos consists of RGB colors of pixels of two 100 \u00d7 150 rectangles (top-left and top-right), each resized to 10 \u00d7 15. So, VIPL videos embedding have length 2 \u00d7 10 \u00d7 15 \u00d7 3 = 900 and represent background color information. All OBF videos have the same background, so for OBF videos chest area (bottom 420 \u00d7 1080 pixels rectangle resized to 8 \u00d7 20) is used as a color embedding. The researchers use 1 \u2212 \u03c1(a, b) as a distance metric on the embeddings described above, where \u03c1 is a Pearson correlation coefficient. Videos are grouped by subjects by an iterative DBSCAN procedure. First the researchers set =0.01 in DBSCAN, and then gradually increase it up to 0.4. If there are any clusters of size 5 on each step, it is assumed that each of these clusters corresponds to videos of one subject. These videos are not considered on the subsequent clustering iterations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mixanik (Neurodata Lab)"
        },
        {
            "text": "The overview of AWoyczyk method is shown in Figure 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 44,
                    "end": 52,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "AWoyczyk (Fachhochschele Dortmund)"
        },
        {
            "text": "The examination of vital parameters is an essential element of modern medicine. The heart rate is one of the most important parameters. It is typically recorded via electrocardiography or by photoplethysmography (PPG) using sensors attached to the patient. Current research focuses on non-contact alternatives to capture physiological signals. One promising approach uses videos to derive a pulse signal (imaging PPG, iPPG). A common approach to derive the heart rate by iPPG first defines a region of interest (ROI) and secondly combines the colour information from that ROI to yield a pulse signal. To define the ROI, simple face detectors producing a facial bounding box, more complex variants yielding facial landmarks and skin classifiers, respectively, are in use. However, previous research has shown that homogeneous skin areas contribute to a better signal. Trumpp et al. therefore presented a level set segmentation to identify a homogeneous skin region [20] . Since the level set segmentation described by Chan et al. [2] does not guarantee to segment skin from non-skin but merely fore-from background, this contribution adopts the method to the special case of skin segmentation. The researchers propose an approach using a Gaussian mixture model (GMM) based level set formulation to yield a time-varying and homogeneous ROI on which further iPPG processing steps can build up.",
            "cite_spans": [
                {
                    "start": 964,
                    "end": 968,
                    "text": "[20]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1029,
                    "end": 1032,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "AWoyczyk (Fachhochschele Dortmund)"
        },
        {
            "text": "The researchers model the probability distributions for the pixel skin and no-skin class by two separate GMM. They are trained on the first frame using the expectation maximation algorithm. The skin GMM originates from the ROI of a face detection algorithm (OpenCVs Viola & Jones Face detection) while the surrounding pixels define the nonskin GMM. The proportion of the posterior probabilities are than included in the energy term minimized by the level set function, i.e. the inclusion of non-skin pixels according to the individualized model is penalized, as well as skin pixels outside the ROI. In order to keep track of movements and facial expressions, the level set function is updated for each frame. This procedure yields a time-varying ROI on which the further processing builds up. To derive the pulse signal from the ROI, we use CHROM [4] . CHROM uses a combination of normalized chrominance signals, derived from the red, green and blue channel to make the signal more robust to intensity changes originating from motion or reflectance. The CHROM signal is further processed by a bandpass filter. Finally, the heart rate is determined as a frequency belonging to the highest amplitude in the frequency spectrum of the extracted pulse signal.",
            "cite_spans": [
                {
                    "start": 847,
                    "end": 850,
                    "text": "[4]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "AWoyczyk (Fachhochschele Dortmund)"
        },
        {
            "text": "The method was proposed originally for the purpose of remote monitoring of driving scenarios, which was then adapted to the task of remote HR measurement for attending the RePSS challenge. Based on statistical signal processing (SSS) and Monte Carlo simulations, the researchers propose a new algorithm, ADaptive spectral filter banks (AD), which provides better balance to robustness and sensibility of remote monitoring for driving scenarios. HR estimation with rPPG can be approximately modeled as single-tone frequency estimation with additive white noise. This estimation problem has been discussed thoroughly in the SSS and the probability of outliers can be derived from corresponding signal-to-noise ratio (SNR). Based on the proba-bility of outliers, the method provides a viable spectral filter option to balance the robustness and sensibility. In the design of AD filter banks, the exponential smoothers are selected due to the simple relationship between time constant and design of parameters. If the SNR is high enough and the probability of outlier is tolerable, the time constant of AD is small to enhance tracking sensibility; by contrast, if the SNR is low, large time constant is applied for stability. In addition, because the design is based on SSS and Monte Carlo simulation, the method has a potential advantage over applications with different band-width or applications with different requirement between sensibility and stability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PoWeiHuang (National Chiao Tung University)"
        },
        {
            "text": "The researchers built a driving database to verify the proposed algorithm and analyzed the influence on rPPG from drivers habits (amateur and professional), vehicle types (compact cars and buses), and routes. In total, a driving database with over 23 hours of data and 104 trials has been built. Moreover, the researchers also adapt their method to the RePSS challenge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "PoWeiHuang (National Chiao Tung University)"
        },
        {
            "text": "In this section we report the results obtained by participating teams. First, the main results are reported and shown in the ranked leaderboard. Then we compare results achieved on the two databases of VIPL-HR-V2 and OBF. At last we analyze the performance on different HR ranges. The results from the top six groups are shown for the last two analysis due to limited space.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Challenge results and discussion"
        },
        {
            "text": "The main results are ranked with the metric of MAE, and we also calculated two other metrics of RMSE and correlation R in order to evaluate the methods on a fuller scope. The ranking leaderboard is shown in Table 2 . For each registered name, up to five submissions can be made and the system chooses the submission and ranked the highest among the five. The best results were achieved by Mixanik with an MAE of 6.94 bpm. Mixanik also leads on the other two metrics, with the RMSE of 10.68 bpm and R of 0.75. To further evaluate the performance under different conditions, we choose the results from the top three ranked teams (named as T1, T2 T6 accordingly) to carry out two comparison analysis in the following two subsections. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 207,
                    "end": 214,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "The main results and ranking"
        },
        {
            "text": "The testing data includes two halves, i.e., 500 samples from the VIPL-HR-V2, and the rest 500 samples from the OBF. We would like to compare the performance on the two parts of data. The metric of MAE was calculated separately on VIPL-HR-V2 and OBF for each team, and the results of T1 to T6 are shown as a bar chart in Figure 4 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 320,
                    "end": 328,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Performance on the two databases"
        },
        {
            "text": "The results show that for the top three teams, their methods performed significantly better on the OBF than on the VIPL-HR-V2 data. The best MAE on OBF was 2.56 bpm achieved by T2. The differences are much smaller for T4, T5 and T6. One reason for the difference might be that the OBF videos have higher resolution than the VIPL-HR-V2 videos, which may indicate that the top three approaches are more sensitive to the face size or input resolution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance on the two databases"
        },
        {
            "text": "Healthy adults HRs distribute in the range of [50, 130] bpm in most daily life scenarios when middle to low level intensity of activities are involved. The distribution of HRs of our training and testing samples are shown in Figure 5 . The distribution of the testing data match similar pattern of the training data. Our test data covers the range of [49, 134] bmp, which makes a good representation of ordinary HR cases. We divided the testing samples into three groups (of similar number of samples) of low (less than 77 bpm), middle (77 to 90 bpm) and high (more than 90 bpm) HR levels according to the GT HR values, so that we can examine how well the approaches performed on different HR levels. The MAE values of the three HR groups are calculated for each of the top six teams, and the results are shown in Figure 6 . It can be seen that all teams performed the best on the middlelevel group of data, i.e., ranged in [77, 90] bpm, while the MAE values are significantly larger when tested on either high-level or low-level groups of data. The challenge of measuring high or low level of HRs needs to be addressed in future works. Figure 6 . Comparison of the performance on low vs. middle vs. high HR levels of the top six teams.",
            "cite_spans": [
                {
                    "start": 46,
                    "end": 50,
                    "text": "[50,",
                    "ref_id": null
                },
                {
                    "start": 51,
                    "end": 55,
                    "text": "130]",
                    "ref_id": null
                },
                {
                    "start": 351,
                    "end": 355,
                    "text": "[49,",
                    "ref_id": null
                },
                {
                    "start": 356,
                    "end": 360,
                    "text": "134]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 225,
                    "end": 233,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 814,
                    "end": 822,
                    "text": "Figure 6",
                    "ref_id": null
                },
                {
                    "start": 1137,
                    "end": 1145,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Performance on different HR ranges"
        },
        {
            "text": "As the very first challenge held on the topic of remote physiological signal sensing, the RePSS attracted great interests within short time. Registered teams come from various countries and regions (e.g., China, Russia, Germany, USA, Australia, and etc.), and more than one hundred results were submitted at the end, which shows that it is a widely concerned topic. As the first trial, we started with the basic task of measuring average HR from color facial videos. Meanwhile, we provided large amount of training and testing data with non-overlap subjects, and concerned different recording scenarios (e.g., talking, moving, and different lighting). By these means we increase the challenge difficulty level, and make the task more resembling to applications in real world. Very good performance were achieved thanks to the efforts of all participating teams, especially of the top three groups. The MAE values of about 7 bpm is a good starting point, considering that the testing data include masked faces, and are even from different recording sources.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future directions"
        },
        {
            "text": "We expect to continue with the RePSS challenge in the following years. We expect that more advanced approaches could be developed to further improve the HR measurement accuracy, i.e., achieve smaller MAEs and higher Rs. Moreover, we will also consider other aspects to make the challenge better, which include: 1). Increase the data size.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future directions"
        },
        {
            "text": "2). Enrich data for special concerns, e.g., data with higher or lower HR levels, data from darker skin tones, etc.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future directions"
        },
        {
            "text": "3). To have more than one test channels focusing on different tasks, so that teams can join and choose their favorite. 4). Invite leading teams from institutions or companies in this domain to increase the visibility of RePSS challenge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future directions"
        },
        {
            "text": "In terms of future directions for the RePSS challenge, the analysis of the current results also gave us good hints. First, the measurement of average HR will continue to be a major focus, as the accuracy can be further improved. We may include the measurement of HRV features to elevate the difficulty level. Second, we might set test specially focused on facial resolutions to explore the impact of face size, and hopefully some approaches that could counter for the disadvantage of low resolution would appear. Third, we consider adding the task of measuring respiration rate as it is also an important vital sign in many application scenes. We would also like to hear ideas and concerns from participants, and hope the RePSS challenge could develop and thrive to be a better platform supporting this topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future directions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Detecting pulse from head motions in video",
            "authors": [
                {
                    "first": "Guha",
                    "middle": [],
                    "last": "Balakrishnan",
                    "suffix": ""
                },
                {
                    "first": "Fredo",
                    "middle": [],
                    "last": "Durand",
                    "suffix": ""
                },
                {
                    "first": "John",
                    "middle": [],
                    "last": "Guttag",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proc. IEEE CVPR",
            "volume": "",
            "issn": "",
            "pages": "3430--3437",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Active contours without edges",
            "authors": [
                {
                    "first": "L",
                    "middle": [
                        "A"
                    ],
                    "last": "Vese",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "F"
                    ],
                    "last": "Chan",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "IEEE Trans. Image Processing",
            "volume": "10",
            "issn": "2",
            "pages": "266--277",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "RealSense = real heart rate: Illumination invariant heart rate estimation from videos",
            "authors": [],
            "year": 2016,
            "venue": "Proc. IEEE IPTA",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Robust pulse rate from chrominance-based rppg",
            "authors": [
                {
                    "first": "Gerard",
                    "middle": [],
                    "last": "De Haan",
                    "suffix": ""
                },
                {
                    "first": "Vincent",
                    "middle": [],
                    "last": "Jeanne",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Biomed. Eng",
            "volume": "60",
            "issn": "10",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "I"
                    ],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Weng",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "E"
                    ],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sheikh",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. IEEE CVPR",
            "volume": "",
            "issn": "",
            "pages": "360--368",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A machine learning approach to improve contactless heart rate monitoring using a webcam",
            "authors": [
                {
                    "first": "Monkaresi",
                    "middle": [],
                    "last": "Hamed",
                    "suffix": ""
                },
                {
                    "first": "Rafael",
                    "middle": [
                        "A"
                    ],
                    "last": "Calvo",
                    "suffix": ""
                },
                {
                    "first": "Yan",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE JOURNAL OF BIOMED-ICAL AND HEALTH INFORMATICS",
            "volume": "18",
            "issn": "4",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [
                        "H"
                    ],
                    "last": "Howard",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "G"
                    ],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Remote heart rate measurement from face videos under realistic situations",
            "authors": [
                {
                    "first": "Xiaobai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Jie",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Guoying",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Matti",
                    "middle": [],
                    "last": "Pietikainen",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proc. IEEE CVPR",
            "volume": "",
            "issn": "",
            "pages": "4264--4271",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Focal loss for dense object detection",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Goyal",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Dollr",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. IEEE CVPR",
            "volume": "",
            "issn": "",
            "pages": "2980--2988",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Improvements in remote cardiopulmonary measurement using a five band digital camera",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Mcduff",
                    "suffix": ""
                },
                {
                    "first": "Sarah",
                    "middle": [],
                    "last": "Gontarek",
                    "suffix": ""
                },
                {
                    "first": "Rosalind",
                    "middle": [
                        "W"
                    ],
                    "last": "Picard",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Biomed. Eng",
            "volume": "61",
            "issn": "10",
            "pages": "2593--2601",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Ballistocardiographic artifacts in ppg imaging",
            "authors": [
                {
                    "first": "Andreia",
                    "middle": [],
                    "last": "Vieira Moco",
                    "suffix": ""
                },
                {
                    "first": "Stuijk",
                    "middle": [],
                    "last": "Sander",
                    "suffix": ""
                },
                {
                    "first": "Gerard",
                    "middle": [],
                    "last": "De Haan",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "IEEE Trans. Biomed. Eng",
            "volume": "63",
            "issn": "9",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Synrhythm: Learning a deep heart rate estimator from general to specific",
            "authors": [
                {
                    "first": "Xuesong",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Hu",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Shiguang",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "Xilin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. IAPR ICPR",
            "volume": "",
            "issn": "",
            "pages": "3580--3585",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "VIPL-HR: A multi-modal database for pulse estimation from less-constrained face video",
            "authors": [
                {
                    "first": "Xuesong",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Hu",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Shiguang",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "Xilin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proc. ACCV",
            "volume": "",
            "issn": "",
            "pages": "562--576",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Rhythmnet: End-to-end heart rate estimation from face via spatial-temporal representation",
            "authors": [
                {
                    "first": "Xuesong",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Shiguang",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "Hu",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Xilin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Image Processing",
            "volume": "1",
            "issn": "2",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Robust remote heart rate estimation from face utilizing spatial-temporal attention",
            "authors": [
                {
                    "first": "Xuesong",
                    "middle": [],
                    "last": "Niu",
                    "suffix": ""
                },
                {
                    "first": "Xingyuan",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Hu",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "Abhijit",
                    "middle": [],
                    "last": "Das",
                    "suffix": ""
                },
                {
                    "first": "Antitza",
                    "middle": [],
                    "last": "Dantcheva",
                    "suffix": ""
                },
                {
                    "first": "Shiguang",
                    "middle": [],
                    "last": "Shan",
                    "suffix": ""
                },
                {
                    "first": "Xilin",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proc. IEEE FG",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Non-contact, automated cardiac pulse measurements using video imaging and blind source separation",
            "authors": [
                {
                    "first": "Ming-Zher",
                    "middle": [],
                    "last": "Poh",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Daniel",
                    "suffix": ""
                },
                {
                    "first": "Rosalind",
                    "middle": [
                        "W"
                    ],
                    "last": "Mcduff",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Picard",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Opt. Express",
            "volume": "18",
            "issn": "10",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Advancements in noncontact, multiparameter physiological measurements using a webcam",
            "authors": [
                {
                    "first": "Ming-Zher",
                    "middle": [],
                    "last": "Poh",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Daniel",
                    "suffix": ""
                },
                {
                    "first": "Rosalind",
                    "middle": [
                        "W"
                    ],
                    "last": "Mcduff",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Picard",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Trans. Biomed. Eng",
            "volume": "58",
            "issn": "1",
            "pages": "7--11",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Remote heart rate measurement using low-cost rgb face video: a technical literature review",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Philipp",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Rouast",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "P"
                    ],
                    "last": "Marc",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Adam",
                    "suffix": ""
                },
                {
                    "first": "Chiong",
                    "middle": [],
                    "last": "Raymond",
                    "suffix": ""
                },
                {
                    "first": "Cornforth",
                    "middle": [],
                    "last": "David",
                    "suffix": ""
                },
                {
                    "first": "Ewa",
                    "middle": [],
                    "last": "Lux",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Frontiers of Computer Science (electronic)",
            "volume": "",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Cardiovascular assessment by imaging photoplethysmography -a review",
            "authors": [
                {
                    "first": "Alexander Trumpp Daniel Wedekind",
                    "middle": [],
                    "last": "Sebastian",
                    "suffix": ""
                },
                {
                    "first": "Zaunseder",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Malberg",
                    "middle": [],
                    "last": "Hagen",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "In Biomed. Eng. -Biomed. Tech",
            "volume": "",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Skin detection and tracking for camera-based photoplethysmography using a bayesian classifier and level set segmentation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Trumpp",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Rasche",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wedekind",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schmidt",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Waldow",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Gaetjen",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zaunseder",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Bildverarbeitung fr die Medizin",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Remote plethysmographic imaging using ambient light",
            "authors": [
                {
                    "first": "Wim",
                    "middle": [],
                    "last": "Verkruysse",
                    "suffix": ""
                },
                {
                    "first": "Lars",
                    "middle": [
                        "O"
                    ],
                    "last": "Svaasand",
                    "suffix": ""
                },
                {
                    "first": "J Stuart",
                    "middle": [],
                    "last": "Nelson",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Opt. Express",
            "volume": "16",
            "issn": "26",
            "pages": "21434--21445",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Algorithmic principles of remote ppg",
            "authors": [
                {
                    "first": "Wenjin",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Sander",
                    "middle": [],
                    "last": "Brinker",
                    "suffix": ""
                },
                {
                    "first": "Gerard",
                    "middle": [],
                    "last": "Stuijk",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "De Haan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Biomed. Eng",
            "volume": "64",
            "issn": "7",
            "pages": "1479--1491",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Estimating heart rate and rhythm via 3D motion tracking in depth video",
            "authors": [
                {
                    "first": "Cheng",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Gene",
                    "middle": [],
                    "last": "Cheung",
                    "suffix": ""
                },
                {
                    "first": "Vladimir",
                    "middle": [],
                    "last": "Stankovic",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Multimedia",
            "volume": "19",
            "issn": "7",
            "pages": "1625--1636",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks",
            "authors": [
                {
                    "first": "Zitong",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Xiaobai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Guoying",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proc. BMVC",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Remote heart rate measurement from highly compressed facial videos: An end-to-end deep learning solution with video enhancement",
            "authors": [
                {
                    "first": "Zitong",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "Wei",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Xiaobai",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Xiaopeng",
                    "middle": [],
                    "last": "Hong",
                    "suffix": ""
                },
                {
                    "first": "Guoying",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Proc. IEEE ICCV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Sample images of anonymized testing videos. The left one from VIPL-HR, and the right one from OBF.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Overview of Mixanik method.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Overview of AWoyczyk method. Skin GMM (red) and non-skin GMM (blue), segmented ROI after applying level set and extracted pulse wave via CHROM.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Comparison of the performance on VIPL-HR-V2 and OBF of the top six teams.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Distribution of HR levels of the RePSS training and testing data.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Statistical information of VIPL-HR-V2 and OBF subjects.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "The final result leaderboard of the 1st challenge of RePSS.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The first challenge of RePSS was funded by Academy of Finland and University of Oulu, and Chinese National Natural Science Foundation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}