{
    "paper_id": "c8b45c6052cbc388de67682ac02a185fc62ba815",
    "metadata": {
        "title": "A Machine Learning Application for Raising WASH Awareness in the Times of Covid-19 Pandemic",
        "authors": [
            {
                "first": "Rohan",
                "middle": [],
                "last": "Pandey",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shiv Nadar University",
                    "location": {
                        "settlement": "Noida",
                        "region": "UP",
                        "country": "India"
                    }
                },
                "email": ""
            },
            {
                "first": "Vaibhav",
                "middle": [],
                "last": "Gautam",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Shiv Nadar University",
                    "location": {
                        "settlement": "Noida",
                        "region": "UP",
                        "country": "India"
                    }
                },
                "email": ""
            },
            {
                "first": "Kanav",
                "middle": [],
                "last": "Bhagat",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Indraprastha Institute of Information Technology",
                    "location": {
                        "settlement": "Delhi",
                        "country": "India"
                    }
                },
                "email": ""
            },
            {
                "first": "Tavpritesh",
                "middle": [],
                "last": "Sethi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Indraprastha Institute of Information Technology",
                    "location": {
                        "settlement": "Delhi",
                        "country": "India"
                    }
                },
                "email": "*tavpriteshsethi@iiitd.ac.in"
            }
        ]
    },
    "abstract": [
        {
            "text": "A proactive approach to raise awareness while preventing misinformation is a modern-day challenge in all domains including healthcare. Such awareness and sensitization approaches to prevention and containment are important components of a strong healthcare system, especially in the times of outbreaks such as the ongoing Covid-19 pandemic. However, there is a fine balance between continuous awareness-raising by providing new information and the risk of misinformation. In this work, we address this gap by creating a life-long learning application that delivers authentic information to users in Hindi, the most widely used local language in India. It does this by matching sources of verified and authentic information such as the WHO reports against daily news by using machine learning and natural language processing. It delivers the narrated content in Hindi by using stateof-the-art text to speech engines. Finally, the approach allows user input for continuous improvement of news feed relevance on a daily basis. We demonstrate a focused application of this approach for Water, Sanitation, Hygiene as it is critical in the containment of the currently raging Covid-19 pandemic through the WashKaro android application. Thirteen combinations of pre-processing strategies, word-embeddings, and similarity metrics were evaluated by eight human users via calculation of agreement statistics. The best performing combination achieved a Cohen's Kappa of 0.54 and was deployed in the WashKaro application backend. Interventional studies for evaluating the effectiveness of the WashKaro application for preventing WASH-related diseases are planned to be carried out in the Mohalla clinics that provided 3.5 Million consults in 2019 in Delhi, India. Additionally, the application also features human-curated and vetted information to reach out to the community as audio-visual content in local languages.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Raising healthcare awareness for primary prevention of diseases is a challenge all across the globe. Hygiene promotion is the most cost-effective health intervention if accurate content is delivered effectively. A majority of preventable diseases result from unhygienic practices. Water, Sanitation and Hygiene (WASH) measures such as hand-washing are also important in limiting the spread of pandemics such as the currently raging Covid-19. Further, the awareness raising content is often not available to those who need it the most and in a format that they easily understand leading to profoundly wide socio-economic impacts of this lack. In 2017, around 55% of the global population did not make use of a safely managed sanitation service effected in part due to lack of awareness in addition to the lack of facilities at home [2] . Around 827,000 people in low and middle-income countries die as a result of inadequate water, sanitation, and hygiene each year. A significant proportion of these deaths can be averted through dissemination of information about WASH practices and their critical role in preventing diseases by delivering authentic information content in local languages. This cuts across the Sustainable Development Goal 3 (Good Health and Well Being for All) and 6 (Adequate Sanitation and Hygiene for All).",
            "cite_spans": [
                {
                    "start": 831,
                    "end": 834,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "India is the second-most populous country in the world, with more than 1 billion citizens where a staggering 344 million lack hygienic defecation facilities [1] . The World Health Organisation states that more than 500 children under the age of five die each day from diarrhoea in India alone [1] and estimates that 21 per cent of communicable diseases in India are linked to unsafe water and the lack of hygiene practices [1] .",
            "cite_spans": [
                {
                    "start": 157,
                    "end": 160,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 293,
                    "end": 296,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 423,
                    "end": 426,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Ironically, India is also one of the largest and fastestgrowing markets for digital consumers, with 560 million internet subscribers in 2018 [4] and about 60% of Indian users anticipate that the m-Health technologies will improve healthcare within the next three years [5] . This offers a unique opportunity to bridge the gap in information availability through m-Health technologies to reach out to those who need it the most, and in a medium that they understand the most, e.g. audios delivered in local languages, thus narrowing the divide between these resources and the masses.",
            "cite_spans": [
                {
                    "start": 141,
                    "end": 144,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 269,
                    "end": 272,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The recent pandemic outbreak of Coronavirus (Covid-19) has demonstrated the need for proactive containment and prevention measures including repeated hand-washing. Every single day lost of proactive interventions has an exponential impact and countries that acted early were able to contain the disease effectively, thus saving thousands of lives and dollars [9] . Therefore, there exists a dire need for proactive information in addition to proactive testing while preventing the spread of misinformation.",
            "cite_spans": [
                {
                    "start": 359,
                    "end": 362,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we demonstrate an awareness raising solution WashKaro that uses NLP approaches, machine learning and m-Health to combine authentic sources of information with daily news and delivers these in Hindi, the most widely understood local language across India. The application also hosts human-curated and vetted information to reach out to the community as audio-visual content in local languages.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We have validated our approach using the following datasets:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset"
        },
        {
            "text": "This dataset comprises of WHO guidelines obtained from publically available WHO reports with special emphasis of Water, Sanitation and Hygiene (WASH) from various WHO reports published. The dataset comprises of more than 400 WHO articles manually scraped from individual reports owing to the varied format of each report. The dataset comprises of the title of the guideline, the guideline, a category in which it belongs, and the URL of the WHO published report.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WHO Guidelines"
        },
        {
            "text": "Some of the broad categories of these reports are ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WHO Guidelines"
        },
        {
            "text": "This dataset comprises of news articles scraped from publically available news articles. The dataset comprises of the article headline, article text, URL of the article and the date of publishing. We have maintained the following news article datasets:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "News Article Dataset"
        },
        {
            "text": "The English News Article dataset consists of news articles extracted from 'The Hindu'. The news articles are filtered using the following keywords: 'Handwash', 'Hygiene', 'sanitation', and 'health'.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "English"
        },
        {
            "text": "The Hindi News Article dataset consists of news articles extracted from 'jagran'. The news articles are filtered using the following keywords: 'svachta' , 'safai', 'haath dhona', ' saaf', 'haath ragad' which are Hindi translations of 'Cleanliness', 'handwash', 'hygiene' and 'sanitation'.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hindi"
        },
        {
            "text": "In this section, we present our proposed public healthcare intervention workflow, designed and centred around imparting healthcare information effectively. Our methodology is represented in figure2 and further explained in the following sections.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methodology"
        },
        {
            "text": "The dataset as described in section 2 needs to be preprocessed in order to transfer text from human language to machinereadable format for further processing. The following preprocessing is done: ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preprocessing"
        },
        {
            "text": "The entire text is converted into lowercase. Lowercasing significantly helps with consistency of expected output.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conversion to lowercase"
        },
        {
            "text": "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tokenization"
        },
        {
            "text": "Stop words are the most common words in a language like \"the\", \"a\", \"on\", \"is\", \"all\". These words do not carry important meaning and are usually removed from texts.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Stop word removal"
        },
        {
            "text": "Stemming is a process of reducing words to their word stem, base or root form (for example, books -book, lookedlook). The main two algorithms are the Porter stemming algorithm (removes common morphological and inflexional endings from words [14] ) and Lancaster stemming algorithm (a more aggressive stemming algorithm). We have deployed the Porter Stemming algorithm. The pipeline takes in news articles and WHO reports and constructs two-level sentence similarity between titles and the full-text to construct a relevance score. The relevance score thresholds are continuously tuned as more user data are collected. Finally the relevant texts are subject to text to speech translation for consumption in local language (Hindi). ",
            "cite_spans": [
                {
                    "start": 241,
                    "end": 245,
                    "text": "[14]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Stemming"
        },
        {
            "text": "Embedding is a technique to transform text and convert them into a form, such that a machine can process it. It is one of the most popular representations of document vocabulary. The transformation is done in such a way that machine level analysis can be carried out on them. It is capable of capturing the context of a word in a document, semantic and syntactic similarity, relation with other words, etc. They are basically a form of word representation that bridges the human understanding of language to that of a machine. An embedding is a learned representation for text where words that have the same meaning have a similar representation. Our methodology employs the following embeddings: Word2Vec [7] : is a statistical method for efficiently learning a standalone word embedding from a text corpus.Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.",
            "cite_spans": [
                {
                    "start": 706,
                    "end": 709,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Text Similarity Model Embeddings"
        },
        {
            "text": "Two different learning models were introduced that can be used as part of the word2vec approach to learning the word embedding: Continous Bag of Words (CBOW) and skipgram model. The CBOW model learns the embedding by predicting the current word based on its context. The continuous skip-gram model learns by predicting the surrounding words given a current word. The key benefit of the approach is that high-quality word embeddings can be learned efficiently (low space and time complexity), allowing larger embeddings to be learned (more dimensions) from much larger corpora of text (billions of words).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Text Similarity Model Embeddings"
        },
        {
            "text": "GloVe (Global Vectors for Word Representation) [8] : is an extension to the word2vec method for efficiently learning word vectors. Classical vector space model representations of words were developed using matrix factorization techniques such as Latent Semantic Analysis (LSA) that do a good job of using global text statistics but are not as good as the learned methods like word2vec at capturing meaning and demonstrating it on tasks like calculating analogies. GloVe is an approach to marry both the global statistics of matrix factorization techniques like LSA with the local context-based learning in word2vec. Rather than using a window to define local context, GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may result in generally better word embeddings.",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 50,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Text Similarity Model Embeddings"
        },
        {
            "text": "Google Sentence Encoder [3] : encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.The model is trained and optimized for greater-thanword length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. The universal-sentence-encoder-large model is trained with a Transformer encoder.",
            "cite_spans": [
                {
                    "start": 24,
                    "end": 27,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Text Similarity Model Embeddings"
        },
        {
            "text": "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. This is composed of 2 parts: TF, which measures how frequently a term occurs in a document, and IDF, which measures how important a term is by giving higher weight to words occurring only in a few documents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "TF-IDF (Term frequency -inverse document frequency)"
        },
        {
            "text": "Where N i is the number of times i appears in a document and N is the total number of terms in the document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "TF-IDF (Term frequency -inverse document frequency)"
        },
        {
            "text": "Where N is the total number of documents and df i is the number of documents in which word i occurs.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "TF-IDF (Term frequency -inverse document frequency)"
        },
        {
            "text": "In order to generate a similarity score, between the news article and the WHo guideline, the following similarity metrics have been used:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Similarity Metric"
        },
        {
            "text": "Cosine Similarity: is a metric used to measure the similarity between the two documents.It is independent of the size of the documents. Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. This is calculated as:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Similarity Metric"
        },
        {
            "text": "Mathematically speaking, Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0 \u2022 is 1, and it is less than 1 for any angle in the interval (0, \u03c0] radians. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors oriented at 90 \u2022 relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Similarity Metric"
        },
        {
            "text": "Word Mover Distance: suggests that distances and between embedded word vectors are to some degree semantically meaningful. It utilizes the property of word vector embeddings and treats text documents as a weighted point cloud of embedded words. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \"travel\" to reach the embedded words of another document. WMD shows that this distance metric can be cast as an instance of the Earth Mover's Distance (a wellstudied transportation problem for which several highly efficient solvers have been developed).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Similarity Metric"
        },
        {
            "text": "WMD enables us to assess the \"distance\" between two documents in a meaningful way, even when they have no words in common. (dis)similarity between the two sentences. The method also uses the bag-of-words representation of the documents (simply put, the word's frequencies in the documents). The intuition behind the method is that we find the minimum \"traveling distance\" between documents, in other words the most efficient way to \"move\" the distribution of document 1 to the distribution of document 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Similarity Metric"
        },
        {
            "text": "A baseline cut-off similarity score is maintained. All news articles are mapped against the WHO guideliness which results in the generation of a similarity score. All the pairs above this cut-off score are accepted and published to the user ensuring than only relevant pairs are delivered. User feedback is obtained at the end of each pairing, the current cut-off score increases by a small margin when a user marks a pair irrelevant and decreases when a user marks a pair relevant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cut-off Score"
        },
        {
            "text": "The news article and matched WHO guideline with relevance score greater than the cut-off, are converted into the local language(Hindi) using Google Could Platform. Google's pretrained neural machine translation delivers fast and dynamic translation results and Google Cloud's Text-to-Speech converts text into human-like speech.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Translation and Text2Speech"
        },
        {
            "text": "As our entire methodology is aimed at efficiently providing healthcare information to the masses, our success needs to be a measure of the acceptability by the masses. Inter-rater reliability is the extent to which two or more raters (or observers, coders, examiners) agree. It addresses the issue of consistency of the implementation of a rating system. Inter-rater reliability can be evaluated by using a number of different statistics. High inter-rater reliability values refer to a high degree of agreement between two examiners. Low inter-rater reliability values refer to a low degree of agreement between two examiners. We have evaluated our performance using the following commonly accepted statistics:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Metrics"
        },
        {
            "text": "Percentage Agreement amongst raters is a statistic calculated as the number of agreement scores divided by the total number of scores. In the case of multiple users, this technique gets complex. If total raters are n then it has to check for n(n-1)/2 combinations. Also, it does not take the chance of agreement into account and overestimate the level of agreement. Hence it is not reliable alone.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Percentage agreement"
        },
        {
            "text": "Cohen's kappa coefficient [6] is a statistic that is used to measure inter-rater reliability for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since k takes into account the agreement occurring by chance. Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories. To find the coefficient, we use the following formula:",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 29,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "where p o is the relative observed agreement among raters (identical to accuracy), and p e is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly seeing each category.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "The first step of our experimentation involved the creation of the datasets mentioned in section 2. Manual scraping of the WHO Guidelines dataset was done and a total of nearly 400 WHO articles were stored in a local CSV file. English News Articles and Hindi News Articles of the current day were also scraped and stored in their respective datasets.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "In the first set of experimentations, WHO Guidelines and English News articles dataset was used. These datasets were preprocessed by the steps removing unwanted characters as mentioned in the section 3.1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "After the preprocessing was done, pairs were generated where the current days news articles are mapped against the entire WHO guidelines database. Pair generation is followed by evaluating the sentence similarity amongst the News Article Text vs WHO article Text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "After the generation of pairs, the set of models mentioned in Table 1 are employed on each of these sets of pairs. Table  1 specifies the following technical details:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 62,
                    "end": 69,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 115,
                    "end": 123,
                    "text": "Table  1",
                    "ref_id": null
                }
            ],
            "section": "Cohen Kappa"
        },
        {
            "text": "\u2022 Embedding used for each model",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "\u2022 If preprocessing was done",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "\u2022 If TF-IDF weighting was done",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Cohen Kappa"
        },
        {
            "text": "After running these models on each of the four sets of pairs generated, the relevance scores generated are obtained. The text similarity models work as shown in figure 3 . These scores differ from one model to another.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 161,
                    "end": 169,
                    "text": "figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "\u2022 Similarity Metric used"
        },
        {
            "text": "The obtained relevance scores are sorted and the top 5 pairs obtained from each similarity model on a particular set of pair are filtered out. These top pairs were presented to reviewers to classify as relevant and irrelevant (binary classification). The reviewer will classify the pair relevant if the pair of news article and WHO guideline has some relation between them and can be clubbed while if they don't have anything in common, a reviewer will mark that pair as irrelevant. Since the idea of relevance and irrelevance is subjective, therefore a total of 8 reviewers were used. The reviewers were given a set of total 65 stories(top 5 stories of each model) and were asked to press 1 if they think the news article and the WHO report are relevant and 0 otherwise. To find the best model, percentage agreement and Kappa score are calculated for each of the Models and the particular model which has the highest Kappa score among all these models is selected. The best model selected on the basis of these scores is used for future tasks also.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022 Similarity Metric used"
        },
        {
            "text": "After the model is selected, we translate both the WHO dataset and the News articles into Hindi. The translated text is converted into speech for ease of access. The methodology incorporates a feedback system wherein for each Newsarticle and WHO report presented to the user, they can classify the matching as relevant or irrelevant. After learning from the feedback, only the stories that have a similarity score above the learned baseline(cut-off score) are provided to the end-user. This decision line improves with each feedback and ensures delivery of effective content. With the feedback given by the user as relevant/ irrelevant, the value of threshold changes thus pushing only the relevant stories.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022 Similarity Metric used"
        },
        {
            "text": "The same workflow is followed by scraping Hindi news articles, which are converted into English for running sentence similarity models. As done above, the resultant pairs are provided to the users in the form of Hindi text and speech.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u2022 Similarity Metric used"
        },
        {
            "text": "To evaluate the performance of our healthcare intervention methodology. We relied on on various inter-rater reliability evaluation metrics as mentioned in section 4. After calculating the similarity scores for a set of News articles and WHO guidelines using models defined in table 1, 8 different users were asked to classify the matched pairs as relevant or irrelevant. At this time, the cut-off was set to 0.5, and all pairs with relevance score greater than the cut-off score were provided to these user's for rating.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Observations and Results"
        },
        {
            "text": "We calculated percentage agreement and Kappa Score for these 8 raters, on the pairs of News articles and WHO Guidelines and following results are shown in Table2 and Table3 for the Hindi and the English News article dataset as mentioned in section 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Observations and Results"
        },
        {
            "text": "Preprocessing TF-IDF Similarity Metric 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Embedding"
        },
        {
            "text": "Word2Vec",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Embedding"
        },
        {
            "text": "Word2Vec",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Embedding"
        },
        {
            "text": "Glove X Cosine 10",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Embedding"
        },
        {
            "text": "Glove Cosine 11",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Embedding"
        },
        {
            "text": "Glove X X Word Mover Distance 12",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Embedding"
        },
        {
            "text": "Glove X Word Mover Distance 13",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model Embedding"
        },
        {
            "text": "Google Sentence Encoder X X Cosine Table 1 : Combinations of approaches tested. These included pre-processing word-embedding models and similarity metrics for evaluation by eight human users. Table 1 . It is seen that Cohen's Kappa provided a better discrimination among models and was further used for model selection.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 35,
                    "end": 42,
                    "text": "Table 1",
                    "ref_id": null
                },
                {
                    "start": 192,
                    "end": 199,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "Model Embedding"
        },
        {
            "text": "In both the cases, we can see that the model 2 ( Preprocessing + Word2Vec Embedding + Cosine Similarity) gave the best results with 0.54912 Kappa Score and 79.2420 Percentage Agreement. As seen in Fig.5 and Fig.4 , Hindi News Article dataset provided better results in terms of both metrics.Hence for the purpose of this app, model 2 was chosen.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 197,
                    "end": 202,
                    "text": "Fig.5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 207,
                    "end": 212,
                    "text": "Fig.4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Model Embedding"
        },
        {
            "text": "Our android application WashKaro is available for free download on Google Play Store at https://play.google.com/store/apps/details?id=inspire2connect.inspire2connect. The application feed displays various news articles related to sanitation and hygiene. Every news article has a corresponding WHO guideline matched with it. The entire news article is provided in the form of text and an audio file in the local language. User can switch between the news article and the matched WHO guideline. After going through the article, user can review the corresponding matching as relevant or irrelevant. The feedback from the user is used to improve the cut-off score.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WashKaro Application"
        },
        {
            "text": "As this application is targeted towards the lesser educated Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 60,
                    "end": 67,
                    "text": "Table 1",
                    "ref_id": null
                }
            ],
            "section": "WashKaro Application"
        },
        {
            "text": "The model yielding highest agreement among humans was selected for deployment.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WashKaro Application"
        },
        {
            "text": "section of the society, and onboarding section is added to help the time users. This is followed by a an optional questionnaire comprising of a few basic questions on patient demographics to help us understand the prevalence and impact of interventions planned.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "WashKaro Application"
        },
        {
            "text": "To the best of our knowledge, this is the first application that demonstrates the use of state-of-the-art machine learning and m-Health technologies to specifically address the issue of ongoing WASH awareness in a local language in India. This is a daily-learning platform that allows user feedback on the relevance of content. The results of the technical approach taken in this work were evaluated by a panel of eight human users to choose the most appropriate model. However, this study has several limitations. The models have been trained on a relatively small corpus and we have only implemented the approach in Hindi, which is the most widely understood language in India. We do plan to incorporate more languages and local context to the application. All the humans evaluating the models were from a similar educational background. We hope to overcome this limitation through the feedback obtained from users of the WashKaro app. We also plan to devise a ranking score for the feedback providers based upon their reputation score for Public Health published via an accompanying website. Finally, the most important limitation is the lack of assessment of the interventional impact of this application. We plan to address this through a phased roll out with the primary health clinics in Delhi and appropriate partnerships delivering digital health interventions onground. Regardless, our current work highlights the potential of machine learning, m-Health and natural language processing in addressing primary health challenges and provides a framework for replicating such studies in a variety of public health challenges including the Covid-19 pandemic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "India's water and sanitation crisis",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Progress on household drinking water, sanitation and hygiene 2000-2017. special focus on inequalities. new york: United nations children's fund (unicef) and world health organization",
            "authors": [],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Universal sentence encoder",
            "authors": [
                {
                    "first": "Daniel",
                    "middle": [],
                    "last": "Cer",
                    "suffix": ""
                },
                {
                    "first": "Yinfei",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Sheng-Yi",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "Nan",
                    "middle": [],
                    "last": "Hua",
                    "suffix": ""
                },
                {
                    "first": "Nicole",
                    "middle": [],
                    "last": "Limtiaco",
                    "suffix": ""
                },
                {
                    "first": "Rhomni",
                    "middle": [],
                    "last": "St John",
                    "suffix": ""
                },
                {
                    "first": "Noah",
                    "middle": [],
                    "last": "Constant",
                    "suffix": ""
                },
                {
                    "first": "Mario",
                    "middle": [],
                    "last": "Guajardo-Cespedes",
                    "suffix": ""
                },
                {
                    "first": "Steve",
                    "middle": [],
                    "last": "Yuan",
                    "suffix": ""
                },
                {
                    "first": "Chris",
                    "middle": [],
                    "last": "Tar",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1803.11175"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Digital india: Technology to transform a connected nation",
            "authors": [
                {
                    "first": "Noshir",
                    "middle": [],
                    "last": "Kaka",
                    "suffix": ""
                },
                {
                    "first": "Alok",
                    "middle": [],
                    "last": "Madgavkar",
                    "suffix": ""
                },
                {
                    "first": "Rajat",
                    "middle": [],
                    "last": "Kshirsagar",
                    "suffix": ""
                },
                {
                    "first": "James",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Manyika",
                    "suffix": ""
                },
                {
                    "first": "Shishir",
                    "middle": [],
                    "last": "Bahl",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Gupta",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Emerging mhealth: paths for growth",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Levy",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wasden",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Difilippo",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Sur",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "PwC M-Health",
            "volume": "",
            "issn": "",
            "pages": "1--44",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Interrater reliability: the kappa statistic",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Mary",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Mchugh",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Biochemia medica: Biochemia medica",
            "volume": "22",
            "issn": "3",
            "pages": "276--282",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Distributed representations of words and phrases and their compositionality",
            "authors": [
                {
                    "first": "Tomas",
                    "middle": [],
                    "last": "Mikolov",
                    "suffix": ""
                },
                {
                    "first": "Ilya",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                },
                {
                    "first": "Kai",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Greg",
                    "middle": [
                        "S"
                    ],
                    "last": "Corrado",
                    "suffix": ""
                },
                {
                    "first": "Jeff",
                    "middle": [],
                    "last": "Dean",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "3111--3119",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Glove: Global vectors for word representation",
            "authors": [
                {
                    "first": "Jeffrey",
                    "middle": [],
                    "last": "Pennington",
                    "suffix": ""
                },
                {
                    "first": "Richard",
                    "middle": [],
                    "last": "Socher",
                    "suffix": ""
                },
                {
                    "first": "Christopher D",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
            "volume": "",
            "issn": "",
            "pages": "1532--1543",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Response to covid-19 in taiwan: Big data analytics, new technology, and proactive testing",
            "authors": [
                {
                    "first": "Jason",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chun",
                    "suffix": ""
                },
                {
                    "first": "Robert H",
                    "middle": [],
                    "last": "Ng",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Brook",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "JAMA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Stages in preprocessing Removal of Unwanted Characters All characters except A-Z and ['.', ',' ,':'] are removed.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Methodology.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Text Similarity Model",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Percentage Agreement Score for models enumerated in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Cohen's Kappa Score for models enumerated in",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "\u2022 Corona virus Prevention\u2022 Guidelines for safe recreational water environments \u2022 Water, sanitation and hygiene in health care facilities \u2022 Progress on Sanitation and Drinking Water \u2022 A practical guide to Auditing water safety plans \u2022 Progress on Drinking Water, Sanitation and Hygiene",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Results of inter-rater agreement between eight users on English News Articles. Cohen's Kappa and Percentage agreement were the two metrics used to evaluate the models.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Results of inter-rater agreement between eight users on Hindi News Articles. Cohen's Kappa and Percentage agreement were the two metrics used to evaluate the models.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}