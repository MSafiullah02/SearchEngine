{
    "paper_id": "bd92c7de94c57302e5628fc37465be30d875bc64",
    "metadata": {
        "title": "Towards Understanding Transfer Learning Algorithms Using Meta Transfer Features",
        "authors": [
            {
                "first": "Xin-Chun",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210046",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "De-Chuan",
                "middle": [],
                "last": "Zhan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210046",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": "zhandc@lamda.nju.edu.cn"
            },
            {
                "first": "Jia-Qi",
                "middle": [],
                "last": "Yang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210046",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": "yangjq@lamda.nju.edu.cn"
            },
            {
                "first": "Yi",
                "middle": [],
                "last": "Shi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210046",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": "shiy@lamda.nju.edu.cn"
            },
            {
                "first": "Cheng",
                "middle": [],
                "last": "Hang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "National Key Laboratory for Novel Software Technology",
                    "institution": "Nanjing University",
                    "location": {
                        "postCode": "210046",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": "hangc@lamda.nju.edu.cn"
            },
            {
                "first": "Yi",
                "middle": [],
                "last": "Lu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Huawei Technologies Co., Ltd",
                    "location": {
                        "postCode": "210012",
                        "settlement": "Nanjing",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Transfer learning, which aims to reuse knowledge in different domains, has achieved great success in many scenarios via minimizing domain discrepancy and enhancing feature discriminability. However, there are seldom practical determination methods for measuring the transferability among domains. In this paper, we bring forward a novel meta-transfer feature method (MetaTrans) for this problem. MetaTrans is used to train a model to predict performance improvement ratio from historical transfer learning experiences, and can consider both the Transferability between tasks and the Discriminability emphasized on targets. We apply this method to both shallow and deep transfer learning algorithms, providing a detail explanation for the success of specific transfer learning algorithms. From experimental studies, we find that different transfer learning algorithms have varying dominant factor deciding their success, so we propose a multi-task learning framework which can learn both common and specific experience from historical transfer learning results. The empirical investigations reveal that the knowledge obtained from historical experience can facilitate future transfer learning tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In real-world tasks, test data usually differs from training data in the aspects of distributions, features, class categories, etc. Even there are some cases that the real applied circumstances occur in different domains without sufficient labels, i.e., in these cases, we need to exploit the full usage of the original model for adapting to the target domain, thus transfer learning is proposed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Transfer learning algorithms can be grouped into two large categories according to using deep networks or not. The first category is shallow transfer learning, such as TCA [12] , GFK [6] , SA [4] , KMM [8] , ITL [15] and LSDT [22] . These algorithms can be further classified into instance-based and subspace-based ones according to what to transfer [13] . In the category of deep transfer learning, discrepancy-based, adversarial-based, and reconstruction-based algorithms are the three main approaches [19] , among which DAN [10] and RevGrad [5] are classical networks for transfer learning or domain adaptation 1 .",
            "cite_spans": [
                {
                    "start": 172,
                    "end": 176,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 183,
                    "end": 186,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 192,
                    "end": 195,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 202,
                    "end": 205,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 212,
                    "end": 216,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 226,
                    "end": 230,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 350,
                    "end": 354,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 504,
                    "end": 508,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 527,
                    "end": 531,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 544,
                    "end": 547,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 614,
                    "end": 615,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Although many transfer learning algorithms are proposed, there are still few researches devoted to the three key issues in transfer learning, that is, when to transfer, how to transfer and what to transfer [13] . In this paper, we consider the three issues as one problem, i.e., we need to answer whether tasks can be transferred (when), and moreover, how to measure the Transferability. The later one implies the methods to transfer (how) and the information that can be transferred (what). As proposed in [3] , we propose a novel MetaTrans method from both aspects of Transferability and Discriminability. Transferability means the similarity between the source and target domains, and Discriminability means how discriminative are the features extracted from a specific algorithm. In order to understand the internal mechanism of transfer learning algorithms and explain why they can improve the performance a lot, we extract some critical features according to these two dominant factors, which are called Meta Transfer Features.",
            "cite_spans": [
                {
                    "start": 206,
                    "end": 210,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 507,
                    "end": 510,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Inspired by meta-learning methods [21] and the recent work [20] , we build a model mapping Meta Transfer Features to the transfer performance improvement ratio using historical transfer learning experiences. Different from [20] , we propose a multi-task learning framework to use historical experiences, with the reason that experiences from different algorithms vary a lot.",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 38,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 59,
                    "end": 63,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 223,
                    "end": 227,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we make three contributions as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We propose a novel method MetaTrans to map Meta Transfer Features to the transfer performance improvement, from both aspects of Transferability and Discriminability. -With the built mapping, we provide a detailed analysis of the success of both shallow and deep transfer algorithms. -We propose a multi-task learning framework utilizing varying historical transfer experiences from different transfer learning algorithms as much as possible.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we introduce some related works, including basic notations, theoretical analysis in transfer learning, deep domain adaptation and some recent researches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Works"
        },
        {
            "text": "In this work, we focus on the homogeneous unsupervised domain adaptation problem. The labeled source domain is denoted by D S = {X S , Y S }, and similarly, D T = {X T } for the unlabeled target domain. In order to evaluate a specific transfer learning algorithm, the real labels of target domain are denoted by Y T . We denote by h \u2208 H the hypothesis (a.k.a. classifier in classification tasks) mapping from sample space X to label space Y.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Notations"
        },
        {
            "text": "From the previous theoretical result for domain adaptation [1] , we have the generalization error bound on the target domain of a classifier trained in the source domain as follows:",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 62,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Theoretical Bound for Transfer Learning"
        },
        {
            "text": "Let H be a hypothesis space, and \u03bb = min h\u2208H ( S (h) + T (h)) be the most ideal error of the hypothesis space on the source and target jointly, then for any h \u2208 H,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Bound for Transfer Learning"
        },
        {
            "text": "This bound contains three terms. The first one refers to the Discriminability of the features, being smaller if the learned features become more discriminative. The second one determines how similar are the source and target domains, the smaller the better, referred to as Transferability.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theoretical Bound for Transfer Learning"
        },
        {
            "text": "Deep domain adaptation contains adversarial-based and discrepancy-based methods. The framework of adversarial domain adaptation, such as RevGrad [5] and ADDA [18] , utilizes the domain discriminator to separate the source and target domain as much as possible, that is, maximize the Transferability between domains. In addition, the task classifier component is used to maximize the performance of the source domain using the extracted features, in order to preserve the Discriminability. Similarly, discrepancy-based frameworks, such as DDC [17] and DAN [10] , considering both the discrepancy loss (e.g. MMD loss) between two domains (Transferability) and the task specific loss (Discriminability).",
            "cite_spans": [
                {
                    "start": 145,
                    "end": 148,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 158,
                    "end": 162,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 542,
                    "end": 546,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 555,
                    "end": 559,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Deep Domain Adaptation"
        },
        {
            "text": "Recently, [3] analyzes the relation between Transferability and Discriminability in adversarial domain adaptation via the spectral analysis of feature representations, and proposed a batch spectral penalization algorithm to penalize the largest singular values to boost the feature discriminability. [20] proposes to use transfer learning experiences to automatically infer what and how to transfer in future tasks. [23] first addresses the gap between theories and algorithms, and then proposes new generalization bounds and a novel adversarial domain adaptation framework via the introduced margin disparity discrepancy.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 13,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 300,
                    "end": 304,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 416,
                    "end": 420,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Recent Researches"
        },
        {
            "text": "In this section, we introduce the proposed MetaTrans, including Meta Transfer Features and the multi-task learning framework.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MetaTrans Method"
        },
        {
            "text": "The Transferability refers to the discrepancy between two domains, and we can approximate it using different distance metrics. In this paper, we select the proxy A-distance and the MMD distance as two approximations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximate Transferability"
        },
        {
            "text": "Proxy A Distance. The second term in the generalization bound in Eq. 1 is called the H-divergence [9] between two domains. In order to approximate the H-divergence with finite samples from source and target, the empirical H-divergence is defined as",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 101,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Approximate Transferability"
        },
        {
            "text": "where D S and D T are sets sampled from the corresponding marginal distribution with the size being n S and n T . I[\u00b7] is the identity function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximate Transferability"
        },
        {
            "text": "The empirical H-divergence is also called proxy A distance. We can train a binary classifier h to discriminate the source and target domain, and the classification error can be used as an approximation of the proxy A distance,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximate Transferability"
        },
        {
            "text": "where the err(h) is the classification error of the specific classifier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximate Transferability"
        },
        {
            "text": "Another distance commonly used to measure the difference of two domains is MMD distance [7] , a method to match higherorder moments of the domain distributions. The MMD distance is defined as",
            "cite_spans": [
                {
                    "start": 88,
                    "end": 91,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Maximum Mean Discrepancy."
        },
        {
            "text": "where \u03c6 is a function maps the sample to the reproducing kernel Hilbert space H. In order to approximate the MMD distance from finite samples, the empirical MMD distance is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maximum Mean Discrepancy."
        },
        {
            "text": "In order to get the empirical MMD distance, a kernel function is needed, and the commonly used kernel is the RBF kernel defined as k(",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maximum Mean Discrepancy."
        },
        {
            "text": "To avoid the trouble of selecting the best kernel bandwidth \u03c3, we use multi-kernel MMD (MK-MMD), and the multi-kernel is defined as a linear combination of N RBF kernels with the form K = N k=1 K k .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Maximum Mean Discrepancy."
        },
        {
            "text": "The Discriminability measures the discriminative ability of feature representations. We propose three approximate features including the empirical source error, the supervised discriminant criterion and the unsupervised discriminant criterion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Approximate Discriminability"
        },
        {
            "text": "In the generalization bound for domain adaptation (Eq. 1), the source error is an important factor determining the target generalization error. The empirical source error is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Source Domain Error."
        },
        {
            "text": "where y i is the real label for the i-th sample and l is the loss function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Source Domain Error."
        },
        {
            "text": "According to the supervised dimension reduction methods (such as LDA), the ratio of between-class scatter and innerclass scatter implies the discriminative level of the features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Discriminant Criterion."
        },
        {
            "text": "Supposing there are C classes in the source domain, and the mean vector for these classes are {\u03bc c } C c=1 accordingly, then we have the inner-class scatter as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Discriminant Criterion."
        },
        {
            "text": "where the c-th class has n c samples and x cj is the j-th sample of the c-th class. Meanwhile, the between-class scatter is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supervised Discriminant Criterion."
        },
        {
            "text": "where \u03bc 0 is the mean center of all samples in the source domain. We approximate the source discriminability with the formulation c sdc = d between d inner + d between (9) where c sdc is the notation of supervised discriminant criterion.",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 171,
                    "text": "(9)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Supervised Discriminant Criterion."
        },
        {
            "text": "If no labeled data can be obtained, the supervised discriminant criterion can not be used. Towards measuring the discriminant ability of the feature representations in the target domain with no label, the unsupervised discriminant criterion can be applied. Similarly, there are two types of scatter in unsupervised discriminant criterion called the localscatter and global-scatter. The local-scatter is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised Discriminant Criterion."
        },
        {
            "text": "where H is defined as neighbor affinity matrix, being K ij when x i and x j are neighbors to each other, and being 0 otherwise. K ij is the kernel matrix item using the multi-kernel proposed as before. And similarly, the global scatter is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised Discriminant Criterion."
        },
        {
            "text": "Therefore, we use the ratio of the global scatter in the total scatter as an approximation to the discriminability of the feature representations in the target domain, which is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised Discriminant Criterion."
        },
        {
            "text": "and the c udc is the abbreviation of unsupervised discriminant criterion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Unsupervised Discriminant Criterion."
        },
        {
            "text": "With the above approximations, the Meta Transfer Features are denoted as a five-tuple (d A , d mmd , S , c sdc , c udc ). In transfer learning, we always focus on the performance improvement ratio brought by using a specific transfer learning algorithm compared to the case without using it. We build a machine learning model in source domain D S = {X S , Y S }, and we denote it as h S . Without using any transfer learning algorithms, the target domain error is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statements"
        },
        {
            "text": "where l is the loss function and X T i is the ith sample in target domain. A specific transfer learning algorithm g, with the input as X S , X T , could output the aligned data samples asX S ,X T 2 . The aligned source and target domains become {X S , Y S } and {X T }, and then similarly, we can get the new target domain error w = 1 nT nT i=1 l(\u0125 S (X T i ), Y T i ), where\u0125 S is the model learned from new source domain samples. If w is smaller than wo , we believe that g has made an improvement, and the ratio is defined as r imp :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statements"
        },
        {
            "text": "Given the source and target domains D S = {X S , Y S } and D T = {X T }, using a transfer learning algorithm g, we can get representationsD S = {X S , Y S } and D T = {X T }. From D S and D T , we can get a five tuple Meta Transfer Features denoted as (d A , d mmd , S , c sdc , c udc ), and similarly, fromD S andD T , we can get another five tuple denoted as d A ,d mmd ,\u02c6 S ,\u0109 sdc ,\u0109 udc . We combine this two tuples together, and get the features denoted as x meta . Using these features, we want to regress the transfer improvement ratio r imp denoted as y meta .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statements"
        },
        {
            "text": "From historical transfer learning experiences, we can get pairs of (x meta , y meta ), and then we can build a model maps Meta Transfer Features to the transfer improvement ratio. With this obtained model, we can have a better understanding of the internal mechanism of transfer learning algorithms and provide some prior knowledge for future transfer learning tasks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Statements"
        },
        {
            "text": "Considering transfer learning algorithms are designed with different mechanisms, it is not wise to build a single mapping from their experiences, losing the specialities. Additionally, we want to learn something common which can be applied to new transfer learning algorithms so that we can not train models individually. Therefore, we propose a multi-task learning framework to learn common and specific knowledge from varying transfer learning experiences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-task Learning Framework"
        },
        {
            "text": "To be specific, given the transfer learning experiences of T different algorithms denoted as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-task Learning Framework"
        },
        {
            "text": ". For simplicity, we use linear regression with regularization as our mapping function. We divide mapping functions into two parts, the common and specific ones, denoted by (w, b) and {(w t , b t )} T t=1 correspondingly. Then our optimization target is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-task Learning Framework"
        },
        {
            "text": "is the regularization term, such as the L2-norm regularization and \u03b8 = {w, b, {w} T t=1 , {b} T t=1 } denotes the parameters to be learned. In order to solve this problem, we use the alternative optimization strategy. First, we fix the global parameters (w, b) and optimize (w t , b t ) for each task, and then we fix local parameters (w t , b t ) T t=1 and optimize the (w, b) alternatively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-task Learning Framework"
        },
        {
            "text": "In this section, we display some experiments with both synthetic and public data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Studies"
        },
        {
            "text": "One of the contributions of this work is the proposed Meta Transfer Features, so we will provide some experimental results on synthetic data to understand why these features matter so much. In order to understand the Transferability, we sample data from two 2-dim gaussian distributions as the source and target domain, which is shown in the top row of Fig. 1 . From the figure, the proxy A distance (HDIV in figure) and MMD distance become larger when two domains become further. As to the Discriminability, we sample data from five gaussian distributions as five classes. From the bottom row in Fig. 1 , it is shown that both the supervised and unsupervised discriminative criterion become larger with the overlap among classes becomes smaller, which means the features are more discriminative for classification. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 353,
                    "end": 359,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 409,
                    "end": 416,
                    "text": "figure)",
                    "ref_id": null
                },
                {
                    "start": 597,
                    "end": 603,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Understanding Meta Transfer Features"
        },
        {
            "text": "As proposed further, different transfer learning algorithms have their individual mechanisms, so we will provide experimental results for this finding.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Understanding Transfer Learning Methods"
        },
        {
            "text": "In this section, we implement TCA [12] , SA [4] and ITL [15] as examples, showing the different mechanisms among them. In order to visualize the learned representations, we use synthetic data constructed as follows: we sample data from two 2-dim gaussian distributions as two classes in source domain (S0, S1 in Fig. 2 (a) ), and then we rotate the guassian means with a definite angle, and the new means are used to sample target data (T0, T1 in Fig. 2 (a) ) with the same covariance. Then we use TCA, SA and ITL to get aligned distributions in 1-dim space, and for every algorithm, we select the best parameters to get almost the same 10% improvement in classification accuracy compared to the case without using this algorithm. Considering the overlap between two classes in two domains in 1-dim space, we plot them separately with different y-axis values as in Fig. 2 . From this visualization result, it is obvious that ITL can get a more discriminative representation then TCA and SA, for the appearance that the samples in different classes are largely separated as shown in Fig. 2 (d) . The result fits well with the information-theoretic factors considered in the designation process of ITL, and we refer readers to [15] for more details. In addition, TCA can get a better alignment between source and target domains as shown in Fig. 2 (b) .",
            "cite_spans": [
                {
                    "start": 34,
                    "end": 38,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 44,
                    "end": 47,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 56,
                    "end": 60,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 312,
                    "end": 322,
                    "text": "Fig. 2 (a)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 447,
                    "end": 457,
                    "text": "Fig. 2 (a)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 865,
                    "end": 871,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1082,
                    "end": 1092,
                    "text": "Fig. 2 (d)",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1338,
                    "end": 1348,
                    "text": "Fig. 2 (b)",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Shallow Transfer Methods."
        },
        {
            "text": "Aside from the shallow transfer learning algorithms, we explore the change of Meta Transfer Features in the learning process of deep transfer learning algorithms. We take DAN [10] as an example. We use the Amazon (A) and DSLR (D) in Office [14] dataset as source and target domains. For each training epoch, we extract Meta Transfer Features from the hidden representations learned from DAN network, and we plot the change of these features as shown in Fig. 3 (a) (the plot is normalized with min-max normalizetion). It is obvious that MMD distance (MMD in Figure) becomes smaller and smaller with the optimization process of domain alignment mechanism in DAN, while proxy A distance (HDIV in Figure) oscillates a lot. In addition, the sdc becomes smaller, showing that features could be more confusing with the overlap between two domains becoming larger.",
            "cite_spans": [
                {
                    "start": 175,
                    "end": 179,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 240,
                    "end": 244,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 453,
                    "end": 459,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 557,
                    "end": 564,
                    "text": "Figure)",
                    "ref_id": null
                },
                {
                    "start": 693,
                    "end": 700,
                    "text": "Figure)",
                    "ref_id": null
                }
            ],
            "section": "Deep Transfer Methods."
        },
        {
            "text": "Transfer learning experiences are constructed from sub-tasks sampled from the classical datasets: Office [14] , Caltech [6] , MNIST (M) and USPS (U). The Office and Caltech datasets have four domains in total: Amazon (A), Caltech (C), DSLR (D) and Webcam (W). For a specific source and target combination such as A \u2192 C, we sample tasks with a subset classes in the total 10 classes. For example, we can sample a 4-classes classification task, and there are will be 210 unique tasks in total can be sampled. For the prediction experiments, we only focus on shallow transfer learning algorithms, including RPROJ 3 , PCA, TCA [12] , MSDA [2] , CORAL [16] , GFK [6] , ITL [15] , LSDT [22] , GTL [11] and KMM [8] . These algorithms contain almost all kinds of shallow transfer learning algorithms, such as instance-based, subspace-based, manifold-based, information-based and reconstruction-based. For each sampled task, we apply all of these algorithms with random selected hyperparameters and get the (x meta , y meta ) pairs. We compare our proposed multi-task learning framework (Meta-MTL) with two baselines: the first one is training a single model together (Meta-Sin), and the second one is training a model for each transfer algorithm individually (Meta-Ind). We use both MSE and MAE as the evaluation criterions. The prediction results can be found in Table 1 , which verifies the validity of our MTL framework. Our MTL framework can predict the transfer improvement ratio more accurate for unseen transfer tasks. It also explains that experiences from different transfer learning algorithms should not be utilized equally. The first column displays the source and domain pairs we use to obtain transfer learning experiences, and we find the ignored dataset information also matters a lot, which will be the future work to research. In addition, in order to visualize the difference among transfer learning algorithms, we use MDS to get the lower representations in 2-dim space keeping the euclidean distances among their specific weights unchanged as much as possible. We plot the relationships in Fig. 3 (b) . From this figure, we can find and search some similar transfer learning methods for alternative algorithms, and meanwhile, some diverse algorithms can be used for ensemble learning. To be specific, we find MSDA and TCA may be alternative transfer learning methods in this experiment.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 109,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 120,
                    "end": 123,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 623,
                    "end": 627,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 635,
                    "end": 638,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 647,
                    "end": 651,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 658,
                    "end": 661,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 668,
                    "end": 672,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 680,
                    "end": 684,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 691,
                    "end": 695,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 704,
                    "end": 707,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [
                {
                    "start": 1356,
                    "end": 1363,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 2103,
                    "end": 2113,
                    "text": "Fig. 3 (b)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Prediction Results"
        },
        {
            "text": "In this paper, we propose MetaTrans from both Transferability and Discriminability aspects and give a comprehensive understanding of both shallow and deep transfer learning algorithms. As to the use of historical transfer learning experiences, we propose a multi-task learning framework, and the experimental results show that it could utilize experiences better and predict future transfer performance improvement more accurate. Considering more meta-features, taking the dataset information into consideration or learning task embeddings are future works.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Analysis of representations for domain adaptation",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ben-David",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Blitzer",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Crammer",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pereira",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "137--144",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Marginalized denoising autoencoders for domain adaptation",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "Q"
                    ],
                    "last": "Weinberger",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sha",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 29th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1627--1634",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Transferability vs. discriminability: batch spectral penalization for adversarial domain adaptation",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 36th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1081--1090",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Unsupervised visual domain adaptation using subspace alignment",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Fernando",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Habrard",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sebban",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Tuytelaars",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "volume": "",
            "issn": "",
            "pages": "2960--2967",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Unsupervised domain adaptation by backpropagation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ganin",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lempitsky",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 32nd International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1180--1189",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Geodesic flow kernel for unsupervised domain adaptation",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sha",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Grauman",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "2066--2073",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "A kernel method for the two-sample-problem",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gretton",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Rasch",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "513--520",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Correcting sample selection bias by unlabeled data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gretton",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "J"
                    ],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "601--608",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Detecting change in data streams",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kifer",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ben-David",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gehrke",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proceedings of the 30th International Conference on Very Large Data Bases",
            "volume": "",
            "issn": "",
            "pages": "180--191",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Learning transferable features with deep adaptation networks",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 32nd International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "97--105",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Transfer learning with graph co-regularization",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "26",
            "issn": "",
            "pages": "1805--1818",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Domain adaptation via transfer component analysis",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "W"
                    ],
                    "last": "Tsang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "T"
                    ],
                    "last": "Kwok",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEEE Trans. Neural Netw",
            "volume": "22",
            "issn": "",
            "pages": "199--210",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "A survey on transfer learning",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Pan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "22",
            "issn": "",
            "pages": "1345--1359",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Adapting visual category models to new domains",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Saenko",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kulis",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Fritz",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "ECCV 2010",
            "volume": "6314",
            "issn": "",
            "pages": "213--226",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-15561-1_16"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Information-theoretical learning of discriminative clusters for unsupervised domain adaptation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sha",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 29th International Conference on International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "1275--1282",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Return of frustratingly easy domain adaptation",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Saenko",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Deep domain confusion: maximizing for domain invariance",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Tzeng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hoffman",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Saenko",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1412.3474"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Adversarial discriminative domain adaptation",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Tzeng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hoffman",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Saenko",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "volume": "",
            "issn": "",
            "pages": "7167--7176",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Deep visual domain adaptation: a survey",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Neurocomputing",
            "volume": "312",
            "issn": "",
            "pages": "135--153",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Transfer learning via learning to transfer",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 35th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "5085--5094",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Learning embedding adaptation for few-shot learning",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "J"
                    ],
                    "last": "Ye",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "C"
                    ],
                    "last": "Zhan",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sha",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1812.03664"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "LSDT: latent sparse domain transfer learning for visual adaptation",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zuo",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Image Process",
            "volume": "25",
            "issn": "",
            "pages": "1177--1191",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Bridging theory and algorithm for domain adaptation",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of the 36th International Conference on Machine Learning",
            "volume": "",
            "issn": "",
            "pages": "7404--7413",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Understanding Meta Transfer Features. The first row illustrates the Transferability between source and target domains, while the second row shows the Discriminability of features with five classes.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Understanding shallow transfer learning algorithms. From left to right: (a) The synthetic data. (b) The 1-dim features obtained from TCA. (c) The 1-dim features obtained from SA. (d) The 1-dim features obtained from ITL.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "(a) Understanding deep transfer learning algorithms: the change of Meta Transfer Features in the training process. (b) Task visualization using MDS, mapping the learned weights into the 2-dim space.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Prediction results of different methods of utilizing the transfer learning experiences. Train: A \u2192 C, A \u2192 D, \u00b7 \u00b7 \u00b7 , W \u2192 D Meta-Sin 0.0339 0.1573 Test: U \u2192 M, M \u2192 U Meta-Inv 0.0418 0.1724 Meta-MTL 0.0314 0.1507",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}