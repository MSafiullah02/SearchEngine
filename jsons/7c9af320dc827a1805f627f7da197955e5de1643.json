{
    "paper_id": "7c9af320dc827a1805f627f7da197955e5de1643",
    "metadata": {
        "title": "Attention U-Net Based Adversarial Architectures for Chest X-ray Lung Segmentation",
        "authors": [
            {
                "first": "Guszt\u00e1v",
                "middle": [],
                "last": "Ga\u00e1l",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AI Research Group",
                    "institution": "E\u00f6tv\u00f6s Lor\u00e1nd University",
                    "location": {
                        "settlement": "Budapest",
                        "country": "Hungary"
                    }
                },
                "email": ""
            },
            {
                "first": "Bal\u00e1zs",
                "middle": [],
                "last": "Maga",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AI Research Group",
                    "institution": "E\u00f6tv\u00f6s Lor\u00e1nd University",
                    "location": {
                        "settlement": "Budapest",
                        "country": "Hungary"
                    }
                },
                "email": ""
            },
            {
                "first": "Andr\u00e1s",
                "middle": [],
                "last": "Luk\u00e1cs",
                "suffix": "",
                "affiliation": {
                    "laboratory": "AI Research Group",
                    "institution": "E\u00f6tv\u00f6s Lor\u00e1nd University",
                    "location": {
                        "settlement": "Budapest",
                        "country": "Hungary"
                    }
                },
                "email": "lukacs@cs.elte.hu"
            }
        ]
    },
    "abstract": [
        {
            "text": "Chest X-ray (CXR) is the most common test among medical imaging modalities. It is applied for detection and differentiation of, among others, lung cancer, tuberculosis, and pneumonia, the last with importance due to the COVID-19 disease. Integrating computer-aided detection methods into the radiologist diagnostic pipeline, greatly reduces the doctors' workload, increasing reliability and quantitative analysis. Here we present a novel deep learning approach for lung segmentation, a basic, but arduous task in the diagnostic pipeline. Our method uses state-of-the-art fully convolutional neural networks in conjunction with an adversarial critic model. It generalized well to CXR images of unseen datasets with different patient profiles, achieving a final DSC of 97.5% on the JSRT dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "X-ray is the most commonly performed radiographic examination, being significantly easier to access, cheaper and faster to carry out than computed tomography (CT), diagnostic ultrasound and magnetic resonance imaging (MRI), as well as having lower dose of radiation compared to a CT scan. According to the publicly available, official data of the National Health Service ( [1] ), in the period from February 2017 to February 2018, the count of imaging activity was about 41 million in England, out of which almost 22 million was plain X-ray. Many of these imaging tests might contribute to early diagnosis of cancer, amongst which chest X-ray is the most commonly requested one by general practitioners. In order to identify lung nodules, lung segmentation of chest X-rays is essential, and this step is vital in other diagnostic pipelines as well, such as calculating the cardiothoracic ratio, which is the primary indicator of cardiomegaly. For this reason, a robust algorithm to perform this otherwise arduous segmentation task is much desired in the field of medical imaging.",
            "cite_spans": [
                {
                    "start": 373,
                    "end": 376,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Semantic segmentation aims to solve the challenging problem of assigning a pre-defined class to each pixel of the image. This task requires a high level of visual understanding, in which state-ofthe-art performance is attained by methods utilizing Fully Convolutional Networks (FCN) [2] . In [3] , adversarial training is used to enhance segmentation of colored images. This idea was incorporated to [4] in order to segment chest X-rays with a fully convolutional, residual neural network. Recently, Mask R-CNN [5] is utilized to realize instance segmentation on chest X-rays and obtained state-ofthe-art results [6, 7] .",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 286,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 292,
                    "end": 295,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 400,
                    "end": 403,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 511,
                    "end": 514,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 613,
                    "end": 616,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 617,
                    "end": 619,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our goal is to produce accurate organ segmentation masks on chest X-rays, meaning for input images we want pixel-wise dense predictions regarding if the given pixel is either part of the left lung, the right lung, the heart, or none of the above. For this purpose Fully Convolutional Networks (FCNs) are known to significantly outperform other widely used registration-based methods. Specifically we applied a U-Net architecture, thus enabling us to efficiently compute the segmentation mask in the same resolution as the input images. The fully convolutional architecture also enables the use images of different resolutions, since unlike standard convolutional networks, FCNs don't contain input-size dependent layers. In [8] it has been shown that for medical image analysis tasks the integration of the proposed Attention Gates (AGs) improved the accuracy of the segmentation models, while preserving computational efficiency. The architecture of the proposed Attention U-Net is described by Figure 1 . Without the use of AGs, it's common practice to use cascade CNNs, selecting a Region Of Interest (ROI) with another CNN where the target organ is likely contained. With the use of AGs we eliminate the need for such a preselecting network, instead the Attention U-Net learns to focus on most important local features, and dulls down the less relevant ones. We note that the dulling of less relevant local features also result in decreased false positive rates. In order to enhance the performance of Attention U-Net, we further experimented with adversarial techniques, motivated by [4] . In that work, the authors first designed a Fully Convolutional Network (FCN) for the lung segmentation task, and noted that in certain cases the network tends to segment abnormal and incorrect organ shapes. For example, the apex of the ribcage might be mistaken as an internal rib bone, resulting in the mask bleeding out to the background, which has similar intensity as the lung field. To address this issue, they developed an adversarial scheme, leading to a model which they call Structure Correcting Adversarial Network (SCAN). This architecture is based on the idea of the General Adversarial Networks [9] . They use the pretrained Fully Convolutional Network as a generator of a General Adversarial Network, and they also train a critic network which is fed the ground truth mask, the predicted mask and optionally the original image. The critic network has roughly the same architecture, resulting in similar capacity. This approach forces the generator to segment more realistic masks, eventually removing obviously wrong shapes.",
            "cite_spans": [
                {
                    "start": 724,
                    "end": 727,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1589,
                    "end": 1592,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 2203,
                    "end": 2206,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 996,
                    "end": 1004,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Network Architecture"
        },
        {
            "text": "In our work, besides the standard Attention U-Net, we also created a network of analogous structure, in which the FCN used in [4] is replaced by the Attention U-Net. We did not introduce any modification in the critic model design, such experiments are left to future work.",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 129,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Network Architecture"
        },
        {
            "text": "In the field of medical imaging, Dice Score Coefficient (DSC) is probably the most widespread and simple way to measure the overlap ratio of the masks and the ground truth, and hence to compare and evaluate segmentations. Given two sets of pixels X, Y , their DSC is If Y is in fact the result of a test about which pixels are in X, we can rewrite it with the usual notation true/false positive (TP/FP), false negative (FN) to be",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tversky Loss"
        },
        {
            "text": "We would like to use this concept in our setup. The class c we would like to segment corresponds to a set, but it is more appropriate to consider its indicator function g, that is g i,c \u2208 {0, 1} equals 1 if and only if the ith pixel belongs to the object. On the other hand, our prediction is a probability for each pixel denoted by p i,c \u2208 [0, 1]. Then the Dice Score of the prediction in the spirit of the above description is defined to be",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tversky Loss"
        },
        {
            "text": "where N is the total number of pixels, and \u03b5 is introduced for the sake of numerical stability and to avoid divison by 0. The linear Dice Loss (DL) of the multiclass prediction is then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tversky Loss"
        },
        {
            "text": "A deficiency of Dice Loss is that it penalizes false negative and false positive predictions equally, which results in high precision but low recall. For example practice shows that if the region of interests (ROI) are small, false negative pixels need to have a higher weight than false positive ones. Mathematically this obstacle is easily overcome by introducing weights \u03b1, \u03b2 as tuneable parameters, resulting in the definition of Tversky similarity index [10] :",
            "cite_spans": [
                {
                    "start": 459,
                    "end": 463,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Tversky Loss"
        },
        {
            "text": "where p i,c = 1 \u2212 p i,c and g i,c = 1 \u2212 g i,c , that is the overline simply stands for describing the complement of the class. Tversky Loss is obtained from Tversky index as Dice Loss was obtained from Dice Score Coefficient:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tversky Loss"
        },
        {
            "text": "Another issue with the Dice Loss is that it struggles to segment small ROIs as they do not contribute to the loss significantly. This difficulty was addressed in [11] , where the authors introduced the quantity Focal Tversky Loss in order to improve the performance of their lesion segmentation model:",
            "cite_spans": [
                {
                    "start": 162,
                    "end": 166,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Tversky Loss"
        },
        {
            "text": "where \u03b3 \u2208 [1, 3] . In practice, if a pixel with is misclassified with a high Tversky index, the Focal Tversky Loss is unaffected. However, if the Tversky index is small and the pixel is misclassified, the Focal Tversky Loss will decrease significantly.",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 13,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 14,
                    "end": 16,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Tversky Loss"
        },
        {
            "text": "The explanation of the training of our structure correcting network is a bit longer to explain, we directly follow the footsteps of [4] . Let S, D be the segmentation network and the critic network, respectively. The data consist of the input images x i and the associated mask labels y i , where x i is of shape [H, W, 1] for a single-channel gray-scale image with height H and width W , and y i is of shape [H, W, C] where C is the number of classes including the background. Note that for each pixel location (j, k), y jkc i = 1 for the labeled class channel c while the rest of the channels are zero (y jkc i = 0 for c = c). We use S(x) \u2208 [0, 1] [H,W,C] to denote the class probabilities predicted by S at each pixel location such that the class probabilities normalize to 1 at each pixel. Let D(x i , y) be the scalar probability estimate of y coming from the training data. They defined the optimization problem as",
            "cite_spans": [
                {
                    "start": 132,
                    "end": 135,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "where J s (\u0177, y) := 1 HW j,k C c=1 \u2212y jkc ln y jkc is the multiclass cross-entropy loss for predicted mask\u0177 averaged over all pixels. is the binary logistic loss for the critic's prediction. \u03bb is a tuning parameter balancing pixel-wise loss and the adversarial loss. We can solve equation (1) by alternate between optimizing S and optimizing D using their respective loss functions. This is a point where we introduced a modification: instead of using the multiclass cross-entropy loss J s (\u0177, y) in the first term, we applied the Focal Tversky Loss F T L(\u0177, y). (1) does not depend on D, we can train our critic network by minimizing the following objective with respect to D for a fixed S:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Training"
        },
        {
            "text": "Moreover, given a fixed D, we train the segmentation network by minimizing the following objective with respect to S:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Now since the first term in equation"
        },
        {
            "text": "Following the recommendation in [9] , we use J d (D(x i , S(x i )), 1) in place of \u2212J d (D(x i , S(x)), 0), as it leads to stronger gradient signals. After tests on the value of \u03bb we decided to use \u03bb = 0.1.",
            "cite_spans": [
                {
                    "start": 32,
                    "end": 35,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Now since the first term in equation"
        },
        {
            "text": "Concerning training schedule, we found that following pretraining the generator for 50 epochs, we can train the adversarial network for 50 epochs, in which we perform 1 optimization step on the critic network after each 5 optimization step on the generator. This choice of balance is also borrowed from [4] , however, we note that the training of our network is much faster.",
            "cite_spans": [
                {
                    "start": 303,
                    "end": 306,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Now since the first term in equation"
        },
        {
            "text": "For training-and validation data, we used the Japanese Society of Radiological Technology (JSRT) dataset [12] , as well as the Montgomery-and Shenzhen dataset [13] , all of which are public datasets of chest X-rays with available organ segmentation masks reviewed by expert radiologists. The JSRT dataset contains a total of 247 images, of which 154 contains lung nodules. The X-rays are all in 2048\u00d72048 resolution, and have 12-bit grayscale levels. Both lung and heart segmentation masks are available for this dataset. The Montgomery dataset contains 138 chest X-rays, of which 80 X-rays are from healthy patients, and 58 are from patients with tuberculosis. The X-rays have either a resolution of 4020 \u00d7 4892 or 4892 \u00d7 4020, and have 12-bit grayscale levels as well. In the case of this dataset, only lung segmentation masks are publicly available. The Shenzhen dataset contains a total of 662 chest X-rays, of which 326 are of healthy patients, and in a similar fashion, 336 are of patients with tuberculosis. The images vary in sizes, but all are of high resolution, with 8-bit grayscale levels. Only lung segmentation masks are publicly available for the dataset.",
            "cite_spans": [
                {
                    "start": 105,
                    "end": 109,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 159,
                    "end": 163,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Datasets"
        },
        {
            "text": "X-rays are grayscale images with typically low contrast, which makes their analysis a difficult task. This obstacle might be overcome by using some sort of histogram equalization technique. The idea of standard histogram equalization is spreading out the the most frequent intensity values to a higher range of the intensity domain [0, 255] by modifying the intensities so that their cumulative distribution function (CDF) on the complete modified image is as close to the CDF of the uniform distribution as possible. Improvements might be made by using adaptive histogram equalization, in which the above method is not utilized globally, but separately on pieces of the image, in order to enhance local contrasts. However, this technique might overamplify noise in near-constant regions, hence our choice was to use Contrast Limited Adaptive Histogram Equalization (CLAHE), which counteracts this effect by clipping the histogram at a predefined value before calculating the CDF, and redistribute this part of the image equally among all the histogram bins. Applying CLAHE to an X-ray image has visually appealing results, as displayed in Figure 3 . As our experiments displayed, it does not merely help human vision, but also neural networks. Figure 3 : Example of chest X-ray images before and after CLAHE",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1140,
                    "end": 1148,
                    "text": "Figure 3",
                    "ref_id": null
                },
                {
                    "start": 1245,
                    "end": 1253,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "Preprocessing Data"
        },
        {
            "text": "The images were then resized to 512x512 resolution and mapped to [\u22121, 1] before being fed to our network.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preprocessing Data"
        },
        {
            "text": "The aforementioned Attention U-Net architecture was implemented using Keras and TensorFlow Python neural-network libraries, to which we have fed our dataset and trained for 40 epochs with 8 X-ray scans in each batch. Our optimizer of choice was Stochastic Gradient Descent, having found that Adam failed to converge in many cases. As loss function, we applied Focal Tversky Loss.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "We have found that applying various data augmentation techniques such as flipping, rotating, shearing the image as well as increasing or decreasing the brightness of the image were of no help and just resulted in slower convergence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "Using the Attention U-Net infrastructure, we managed to reach a dice score of 0.962 for the segmentation of the lung. Unlike in [4] , where no major preprocessing was done, with our preprocessing method, the network performed very well even if the test-and the validation sets were of different datasets. This is extremely important for real world applications, as X-ray images of different machines are significantly different, largely dependent on the specific calibration of each machine, thus it is no trivial task to have X-rays accurately evaluated that are from machines from which no images were in the training set. We note that even though introducing the adversarial scheme in our setting increased the dice scores, the improvement was not as drastic as in the case of the FCN and SCAN. By checking the masks generated by the vanilla Attention U-Net, we found that this phenomenon can be attributed to the fact that while the FCN occasionally produces abnormally shaped masks, due to our preprocessing steps the Attention U-Net does not commit this mistake. Consequently, the adversarial scheme is responsible for subtle shape improvements only, which is indicated by the Dice Score less spectacularly.",
            "cite_spans": [
                {
                    "start": 128,
                    "end": 131,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Experiments and Results"
        },
        {
            "text": "So far we have not experimented with the architecture of the critic network, we found the performance of the architecture in [4] completely satisfying. However, it would be desirable to carry out further tests in this direction in order to achieve better understanding of the role of adversarial scheme.",
            "cite_spans": [
                {
                    "start": 125,
                    "end": 128,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Future Work"
        },
        {
            "text": "2018-1.2.1-NKP-00008 and the grant EFOP-3.6.3-VEKOP-16-2017-00002. The last author was supported by project no. ED 18-1-2019-0030 (Application domain specific highly reliable IT solutions) funded from the NRDI Fund of Hungary, under the Thematic Excellence Programme scheme.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Future Work"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Diagnostic imaging dataset statistical release",
            "authors": [
                {
                    "first": "Nhs",
                    "middle": [],
                    "last": "England",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Improvement",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Fully convolutional networks for semantic segmentation",
            "authors": [
                {
                    "first": "Jonathan",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "Evan",
                    "middle": [],
                    "last": "Shelhamer",
                    "suffix": ""
                },
                {
                    "first": "Trevor",
                    "middle": [],
                    "last": "Darrell",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "volume": "",
            "issn": "",
            "pages": "3431--3440",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Semantic segmentation using adversarial networks",
            "authors": [
                {
                    "first": "Pauline",
                    "middle": [],
                    "last": "Luc",
                    "suffix": ""
                },
                {
                    "first": "Camille",
                    "middle": [],
                    "last": "Couprie",
                    "suffix": ""
                },
                {
                    "first": "Soumith",
                    "middle": [],
                    "last": "Chintala",
                    "suffix": ""
                },
                {
                    "first": "Jakob",
                    "middle": [],
                    "last": "Verbeek",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1611.08408"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "SCAN: Structure correcting adversarial network for organ segmentation in chest X-rays",
            "authors": [
                {
                    "first": "Nanqing Dong Wei",
                    "middle": [],
                    "last": "Dai",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "Zeya",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Xiaodan",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "Hao",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Eric",
                    "middle": [
                        "P"
                    ],
                    "last": "Xing",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop",
            "volume": "11045",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Mask R-CNN",
            "authors": [
                {
                    "first": "Kaiming",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Georgia",
                    "middle": [],
                    "last": "Gkioxari",
                    "suffix": ""
                },
                {
                    "first": "Piotr",
                    "middle": [],
                    "last": "Doll\u00e1r",
                    "suffix": ""
                },
                {
                    "first": "Ross",
                    "middle": [],
                    "last": "Girshick",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the IEEE international conference on computer vision",
            "volume": "",
            "issn": "",
            "pages": "2961--2969",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Instance segmentation of anatomical structures in chest radiographs",
            "authors": [
                {
                    "first": "Jie",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Zhigang",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Rui",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "Zhen",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)",
            "volume": "",
            "issn": "",
            "pages": "441--446",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "An effective approach for CT lung segmentation using mask region-based convolutional neural networks",
            "authors": [
                {
                    "first": "Qinhua",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "Lu\u00eds",
                    "middle": [],
                    "last": "Fabr\u00edcio",
                    "suffix": ""
                },
                {
                    "first": "Gabriel",
                    "middle": [],
                    "last": "Souza",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Bandeira Holanda",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Shara",
                    "suffix": ""
                },
                {
                    "first": "Francisco",
                    "middle": [],
                    "last": "Alves",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "H\u00e9rcules",
                    "suffix": ""
                },
                {
                    "first": "Tao",
                    "middle": [],
                    "last": "Silva",
                    "suffix": ""
                },
                {
                    "first": "Pedro P Rebou\u00e7as",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Filho",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Artificial Intelligence in Medicine",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Attention U-Net: Learning where to look for the pancreas",
            "authors": [
                {
                    "first": "Ozan",
                    "middle": [],
                    "last": "Oktay",
                    "suffix": ""
                },
                {
                    "first": "Jo",
                    "middle": [],
                    "last": "Schlemper",
                    "suffix": ""
                },
                {
                    "first": "Loic",
                    "middle": [
                        "Le"
                    ],
                    "last": "Folgoc",
                    "suffix": ""
                },
                {
                    "first": "Matthew",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "Mattias",
                    "middle": [],
                    "last": "Heinrich",
                    "suffix": ""
                },
                {
                    "first": "Kazunari",
                    "middle": [],
                    "last": "Misawa",
                    "suffix": ""
                },
                {
                    "first": "Kensaku",
                    "middle": [],
                    "last": "Mori",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Mcdonagh",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Nils",
                    "suffix": ""
                },
                {
                    "first": "Bernhard",
                    "middle": [],
                    "last": "Hammerla",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Kainz",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1804.03999"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Generative adversarial nets",
            "authors": [
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Goodfellow",
                    "suffix": ""
                },
                {
                    "first": "Jean",
                    "middle": [],
                    "last": "Pouget-Abadie",
                    "suffix": ""
                },
                {
                    "first": "Mehdi",
                    "middle": [],
                    "last": "Mirza",
                    "suffix": ""
                },
                {
                    "first": "Bing",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "David",
                    "middle": [],
                    "last": "Warde-Farley",
                    "suffix": ""
                },
                {
                    "first": "Sherjil",
                    "middle": [],
                    "last": "Ozair",
                    "suffix": ""
                },
                {
                    "first": "Aaron",
                    "middle": [],
                    "last": "Courville",
                    "suffix": ""
                },
                {
                    "first": "Yoshua",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Advances in neural information processing systems",
            "volume": "",
            "issn": "",
            "pages": "2672--2680",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Features of similarity",
            "authors": [
                {
                    "first": "Amos",
                    "middle": [],
                    "last": "Tversky",
                    "suffix": ""
                }
            ],
            "year": 1977,
            "venue": "Psychological review",
            "volume": "84",
            "issn": "4",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A novel focal Tversky loss function with improved attention U-Net for lesion segmentation",
            "authors": [
                {
                    "first": "Nabila",
                    "middle": [],
                    "last": "Abraham",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Naimul Mefraz Khan",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)",
            "volume": "",
            "issn": "",
            "pages": "683--687",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists' detection of pulmonary nodules",
            "authors": [
                {
                    "first": "Junji",
                    "middle": [],
                    "last": "Shiraishi",
                    "suffix": ""
                },
                {
                    "first": "Shigehiko",
                    "middle": [],
                    "last": "Katsuragawa",
                    "suffix": ""
                },
                {
                    "first": "Junpei",
                    "middle": [],
                    "last": "Ikezoe",
                    "suffix": ""
                },
                {
                    "first": "Tsuneo",
                    "middle": [],
                    "last": "Matsumoto",
                    "suffix": ""
                },
                {
                    "first": "Takeshi",
                    "middle": [],
                    "last": "Kobayashi",
                    "suffix": ""
                },
                {
                    "first": "Ken-Ichi",
                    "middle": [],
                    "last": "Komatsu",
                    "suffix": ""
                },
                {
                    "first": "Mitate",
                    "middle": [],
                    "last": "Matsui",
                    "suffix": ""
                },
                {
                    "first": "Hiroshi",
                    "middle": [],
                    "last": "Fujita",
                    "suffix": ""
                },
                {
                    "first": "Yoshie",
                    "middle": [],
                    "last": "Kodera",
                    "suffix": ""
                },
                {
                    "first": "Kunio",
                    "middle": [],
                    "last": "Doi",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "American Journal of Roentgenology",
            "volume": "174",
            "issn": "1",
            "pages": "71--74",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery",
            "authors": [
                {
                    "first": "Stefan",
                    "middle": [],
                    "last": "Jaeger",
                    "suffix": ""
                },
                {
                    "first": "Sema",
                    "middle": [],
                    "last": "Candemir",
                    "suffix": ""
                },
                {
                    "first": "Sameer",
                    "middle": [],
                    "last": "Antani",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Y\u00ec-Xi\u00e1ng",
                    "suffix": ""
                },
                {
                    "first": "Pu-Xuan",
                    "middle": [],
                    "last": "W\u00e1ng",
                    "suffix": ""
                },
                {
                    "first": "George",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Thoma",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "4",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Schematic architecture of the Attention U-Net[8]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Schematic architecture of the Structure Correcting Adversarial Networks[4]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "d (t, t) := \u2212t lnt + (1 \u2212 t) ln(1 \u2212t)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Epoch-wise dice score coefficient",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Dice scores of different architectures over different datasets.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The project was partially supported by the AI4EU project, funded by EU H2020 programme (contract no. 825619). Furthermore, it was supported by the Hungarian National Excellence Grant",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments and Disclosure of Funding"
        }
    ]
}