{
    "paper_id": "f21707bfddf373f108637c11b489aeac057f29ab",
    "metadata": {
        "title": "Group Based Unsupervised Feature Selection",
        "authors": [
            {
                "first": "Kushani",
                "middle": [],
                "last": "Perera",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Melbourne",
                    "location": {
                        "postCode": "3010",
                        "settlement": "Melbourne",
                        "region": "VIC",
                        "country": "Australia"
                    }
                },
                "email": "bperera@student.unimelb.edu.au"
            },
            {
                "first": "Jeffrey",
                "middle": [],
                "last": "Chan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "RMIT University",
                    "location": {
                        "postCode": "3000",
                        "settlement": "Melbourne",
                        "region": "VIC",
                        "country": "Australia"
                    }
                },
                "email": "jeffrey.chan@rmit.edu.au"
            },
            {
                "first": "Shanika",
                "middle": [],
                "last": "Karunasekera",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Melbourne",
                    "location": {
                        "postCode": "3010",
                        "settlement": "Melbourne",
                        "region": "VIC",
                        "country": "Australia"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Unsupervised feature selection is an important task in machine learning applications, yet challenging due to the unavailability of class labels. Although a few unsupervised methods take advantage of external sources of correlations within feature groups in feature selection, they are limited to genomic data, and suffer poor accuracy because they ignore input data or encourage features from the same group. We propose a framework which facilitates unsupervised filter feature selection methods to exploit input data and feature group information simultaneously, encouraging features from different groups. We use this framework to incorporate feature group information into Laplace Score algorithm. Our method achieves high accuracy compared to other popular unsupervised feature selection methods (\u223c30% maximum improvement of Normalized Mutual Information (NMI)) with low computational costs (\u223c50 times lower than embedded methods on average). It has many real world applications, particularly the ones that use image, text and genomic data, whose features demonstrate strong group structures.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Feature selection is an important task in preparing high dimensional data for machine learning tasks. It improves the prediction accuracy and simplicity of the learning models and reduces the computational costs. Unlike deep learning methods, feature selection identifies the important features that can be interpreted by the humans when explaining AI decisions (E.g.: genes related to certain diseases [12] ). Feature selection methods are of two types, supervised and unsupervised, based on the availability of class labels in data. Among them, unsupervised feature selection has wide applicability because data in most real world scenarios are unlabelled. For example, there is a vast amount of text and image data in the web, yet the label information, such as the subject of a tweet, the topic of an image is only rarely available. Due to the unavailability of labels, unsupervised approach is more challenging than the supervised approach and achieving good accuracy remains a challenge.",
            "cite_spans": [
                {
                    "start": 403,
                    "end": 407,
                    "text": "[12]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Many unsupervised feature selection methods evaluate features using instance-feature data alone, which is available in the form of the data matrix [9, 14] . In contrast, recent work shows that features can be grouped according to various criteria and this group information can improve the usefulness of the feature selection [17] . For example, the nearby pixels in images can be grouped together considering the spatial locality to improve selection of pixels for image analysis. The words in document datasets can be grouped according to their semantics [13] to improve selection of words for document analysis. Genes in genomic data can be grouped using Gene Ontology information [3] to improve bio-marker identification for disease prediction and drug discovery. We show that considering this group structure can enable selection of a better feature subset in real world applications. In Sect. 4, we illustrate this using a concrete text data example.",
            "cite_spans": [
                {
                    "start": 147,
                    "end": 150,
                    "text": "[9,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 151,
                    "end": 154,
                    "text": "14]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 326,
                    "end": 330,
                    "text": "[17]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 557,
                    "end": 561,
                    "text": "[13]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 684,
                    "end": 687,
                    "text": "[3]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In contrast to supervised feature selection [11] , little work exist in unsupervised feature selection which exploits feature group information. The existing ones are limited to genomic data in which feature selection is limited to simple methods such as selecting the centroids of feature groups [3] . They do not use group information in combination with instance-feature data, which is also useful for feature selection. Hierarchical Unsupervised Feature Selection (HUFS) [17] uses feature group information together with instance-feature data to improve feature selection accuracy and is applicable for different data types. Like many state of the art feature selection methods, HUFS is also an embedded approach, yet embedded methods do not have a significant advantage in unsupervised feature selection due to the unavailability of class labels. Compared to embedded methods, filter methods are fast and produce more generic solutions [15] . Consequently, they are still popular in applications such as bio-marker identification [12] and have growing interest in big data applications [7, 16, 20] .",
            "cite_spans": [
                {
                    "start": 44,
                    "end": 48,
                    "text": "[11]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 297,
                    "end": 300,
                    "text": "[3]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 475,
                    "end": 479,
                    "text": "[17]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 941,
                    "end": 945,
                    "text": "[15]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "text": "[12]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1091,
                    "end": 1094,
                    "text": "[7,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1095,
                    "end": 1098,
                    "text": "16,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1099,
                    "end": 1102,
                    "text": "20]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We propose a framework which helps incorporating feature group information into unsupervised filter feature selection methods. To demonstrate the usefulness of our approach, we incorporate feature group information into Laplace Score (LS) algorithm [9] , a well established feature selection method which achieves good accuracy with very low computational costs. We mathematically show that the proposed feature selection objective can be represented as a standard quadratic optimisation problem, such that standard optimisation algorithms can be used to solve the optimisation problem. However, quadratic programming optimisation algorithms are slow and cannot scale to larger problems which are typically encountered, hence we also propose a greedy optimisation method, Group Laplace Score (GLS ), which is faster than quadratic optimisation algorithms, yet show comparable performance. Through extensive experiments we show that GLS achieves high clustering performance with low computational costs, compared to existing feature selection methods. Our main contributions are as follows.",
            "cite_spans": [
                {
                    "start": 249,
                    "end": 252,
                    "text": "[9]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "-We propose a framework which facilitates unsupervised filter feature selection methods to exploit the knowledge about feature groups to achieve higher clustering performance. -We use the proposed framework to incorporate feature group information into LS algorithm and propose a new feature selection algorithm, GLS. -We experimentally show that GLS obtains significantly higher clustering performance than the existing feature selection algorithms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Many unsupervised feature selection methods, both similarity preserving (filter) [9, 19] and embedded [6, 8, 10, 14] methods, are based on input data alone and rarely take the advantage of the external sources of knowledge about feature group structures. The feature groups used by some feature selection methods are also formed with input data [15, 18] . Some domain specific unsupervised methods [3] are proposed for selecting genes from different gene groups, yet they do not combine group based feature selection with instance-feature data which is also useful for feature selection. In contrast, HUFS uses feature group information to improve the instance-feature data based feature selection and is applicable for different data types. However, HUFS encourages features from the same group which is not effective in most real world applications [11] . In contrast, our method encourages features from different groups and we experimentally show that our method outperforms HUFS in terms of accuracy and efficiency. Compared to HUFS, our method requires less parameter tuning too.",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 84,
                    "text": "[9,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 85,
                    "end": 88,
                    "text": "19]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 102,
                    "end": 105,
                    "text": "[6,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 106,
                    "end": 108,
                    "text": "8,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 109,
                    "end": 112,
                    "text": "10,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 113,
                    "end": 116,
                    "text": "14]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 345,
                    "end": 349,
                    "text": "[15,",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 350,
                    "end": 353,
                    "text": "18]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 398,
                    "end": 401,
                    "text": "[3]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 851,
                    "end": 855,
                    "text": "[11]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "This section discusses some frequently used definitions and terms in the paper. X \u2208 R n\u00d7m is the input data matrix, where n is the number of instances and m is the number of features in X. F is the set of all features in X, S \u2286 F is the selected feature subset, f i \u2208 F the i th feature in X and k is the number of features to be selected. G i is the set of features in i th feature group and r is number of groups. Given a matrix A \u2208 R n\u00d7m , a i,j , is its element in i th row and j th column. L 1,1 norm of A, ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Preliminaries"
        },
        {
            "text": "In this section, we demonstrate the importance of external feature group information for feature selection accuracy, using Reuters (RT) text dataset [1] as a concrete example. As the complete dataset is too large, we select only some instances and feature values which are helpful for the discussion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motivation and Background"
        },
        {
            "text": "Example 1: Figure 1a shows a part of the RT dataset in which the words are the features and documents (d i ) are the instances. Feature values represent the occurrence frequency of each word in each document. Each document is one of the three types: Business, Health, Technical, but in the unsupervised feature selection, the algorithm is not provided this. The feature selection problem is to select three features which achieves the best clustering performance.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 11,
                    "end": 20,
                    "text": "Figure 1a",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Motivation and Background"
        },
        {
            "text": "The features which result in small distances between the same class instances and large distances between different class instances help the same class instances to get clustered together. For example, with respect to \"Bank\", business documents have lower distances between each other and large distances with the rest (Manhattan distance of 3 between d 1 and d 2 and 13 between d 1 and d 5 ). Therefore, \"Bank\" discriminates business documents from the rest. Similarly, \"Google\" and \"Patient\" discriminate some technical (d 5 ) and health (d 3 ) documents. {Bank, Patient, Google} collectively discriminate between different class instances from one another. Figure 1b shows the k-means (k = 3) cluster assignments for this feature subset. Only d 4 is assigned to a wrong cluster and cluster purities are 1,1, and 0.67. Clustering performance in terms of NMI [9] is 0.74.",
            "cite_spans": [
                {
                    "start": 860,
                    "end": 863,
                    "text": "[9]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 660,
                    "end": 669,
                    "text": "Figure 1b",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Motivation and Background"
        },
        {
            "text": "In contrast, no feature in {Bank, Patient, Cell} discriminates between business and technical documents and \"Patient\" and \"Cell\" cause large distances between the health documents, the same class instances, leading to poor clustering performance. Figure 1c shows that d 4 , d 5 , d 6 are assigned to wrong clusters, resulting in impure clusters (cluster purities of 1, 1, and 0.5) compared to the previous case. Clustering performance in terms of NMI is 0.65. Therefore, {Bank, Patient, Google} is better compared to {Bank, Patient, Cell}. However, \"Cell\" and \"Google\" have very similar feature value distributions, and class labels are not available for feature selection. Therefore, \"Cell\" and \"Google\" cannot be differentiated from one another using instance feature data alone. We show this using LS algorithm, which selects the features which best preserve the locality structure of the instances, as a concrete example. LS Algorithm: Given that A is the adjacency matrix between the instances, D is the degree matrix and L is the Laplace matrix such that",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 247,
                    "end": 256,
                    "text": "Figure 1c",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Motivation and Background"
        },
        {
            "text": "(1). LS algorithm achieves this by selecting the features with k minimum Laplace scores. Figure 2a shows L for RT dataset, assuming a 1-Nearest Neighbour A. Laplace scores for \"Bank\", \"Cell\", \"Patient\" and \"Google\" are 0.39, 1.06, 1.06 and 1.1, respectively. The selected feature subset is therefore {Bank, Cell, Patient}, which is not optimal.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 89,
                    "end": 98,
                    "text": "Figure 2a",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Motivation and Background"
        },
        {
            "text": "Using Feature Group Information: Consider using Wordnet [13] as an external source of knowledge for Example 1. Wordnet shows a high semantic similarity (0.7) between \"Cell\" and \"Patient\", and low similarity between other feature pairs (0.1 between \"Google\" and \"Bank\"). Three feature groups can be created based on semantic similarity. Group 1: {Bank}, Group 2: {Patient, Cell}, Group 3: {Google}. Encouraging features from different groups results in {Bank, Patient, Google}, which is optimal. This is because semantically similar words tend to occur in similar types of documents. Consequently, words from different groups discriminate different types of documents from one another and result in lower distances between the same type of documents. For example, given \"Patient\", selecting \"Google\" (from a different group), results in a lower distance between d 3 and d 4 than selecting \"Cell\" (from the same group). Opposed to \"Cell\", \"Google\" also discriminates between business and technical documents.",
            "cite_spans": [
                {
                    "start": 56,
                    "end": 60,
                    "text": "[13]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Motivation and Background"
        },
        {
            "text": "We propose a framework which facilitates the unsupervised filter feature selection methods to encourage features from different groups and use this framework to incorporate feature group information into LS algorithm. When the feature groups have different importance levels based on factors such as group size and group quality, more features are encouraged from the groups with higher importance. Proposed feature selection objective can be solved using quadratic optimisation methods, but we also propose a greedy approach, GLS, which achieves the same performance faster. In this paper, we focus on non-overlapped groups, yet the proposed method can easily be extended to overlapped groups as well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Method"
        },
        {
            "text": "Modelling Feature Group Information: We define G \u2208 R m\u00d7m , the feature group matrix. If f i , f j \u2208 F are in the same group, g i,j = g j,i = 1. Otherwise g i,j = g j,i = 0. \u2200i = 1, . . . , m, g i,i = 0. G for Example 1 is shown in Fig. 2b . Multiplying G by U twice makes the rows and columns of G corresponding to the unselected features all zeros. This results in G = UGU \u2208 R m\u00d7m , feature group matrix of the features in S. The number of zeros in G increases when the features in S are from different feature groups and all the elements in G \u2265 0. Therefore, given that k features are to be selected, to encourage features from different feature groups, our objective is to select U to minimise UGU 1,1 subject to U 1,1 = k. Figure 2c and d show U and G when S = {Bank, Patient, Cell}, for which G 1,1 = 2. When S = {Bank, Patient, Google} U is a diagonal matrix where diag(U ) = [1, 1, 0, 1], G \u2208 R 4\u00d74 is a matrix of all zeros and G 1,1 = 0. This shows that UGU 1,1 is minimal when the features are selected from different groups. When the feature groups have different importance levels, to encourage more features from the groups with higher importance, we set g i,j = g j,i = 1 \u03b1i (instead of 1), where \u03b1 i is the weight of G i .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 231,
                    "end": 238,
                    "text": "Fig. 2b",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 727,
                    "end": 736,
                    "text": "Figure 2c",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Proposed Method"
        },
        {
            "text": "Input Data Based Feature Selection: We next propose a common framework to combine group based feature selection with any unsupervised filter feature ranking method. Let Q be a diagonal matrix, where, q i,i = l i , where l i is the feature score of f i , in terms of its capability to preserve the sample similarity. Q = UQU is the feature score matrix for selected features in S. Q is a diagonal matrix in which q i,i = l i if f i \u2208 S and q i,i = 0 otherwise. Given that l i \u2265 0, \u2200 i, the feature selection objective is to select U to minimise or maximise UQU 1,1 subject to U 1,1 = k. Minimisation or maximisation is decided based on the algorithm used to compute l i .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Method"
        },
        {
            "text": "Theorem 1 shows that Laplace score is always non-negative and eligible for Q. and Q 1,1 = 2.55. Therefore, minimal UQU 1,1 is achieved for {Bank, Patient, Cell}, the same feature subset selected by LS algorithm. For the rest of the paper, we assume l i is computed using Laplace score, therefore minimise UQU 1,1 . Maximisation is equivalent to minimising -UQU 1,1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Method"
        },
        {
            "text": "Proof. Because L and D are positive definite. Refer to this link 1 for the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 1. Given that l i is the Laplace score of"
        },
        {
            "text": "The feature selection objective which combines both group based feature selection and input data based feature selection is shown in Eq. (2). \u03bb is a user defined parameter. In this paper, we assign a fixed value for \u03bb. In future, we plan to iteratively decide \u03bb value for each feature selected. Based on Theorem 2, we reformulate Eq. (2) into Eq. (3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Selection Objective:"
        },
        {
            "text": "Refer to this link (See footnote 1) for the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Selection Objective:"
        },
        {
            "text": "Given u = [u 1 , \u00b7 \u00b7 \u00b7 , u m ] T , where u i is the i th diagonal element of U , Theorem 3 shows that U (Q + \u03bbG)U 1,1 can be reformulated as a quadratic function of u. Therefore, to solve Eq. (3), we use two approaches: (1) Standard Quadratic Programming (QP) methods (2) Greedy method (GLS algorithm). As the QP method, we use the MATLAB inbuilt \"fmincon\" function with \"interior point\" method, but omitted the details due to space limitations. Please refer to this link (See footnote 1) for details. The greedy method showed comparable accuracy to QP method, yet faster. Therefore, in this paper, we focus on the greedy method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Feature Selection Objective:"
        },
        {
            "text": "Given that H = Q + \u03bbG, and u as defined above, UHU 1,1 = u T Hu = h(u), that is UHU 1,1 is a quadratic function of u.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 3."
        },
        {
            "text": "Proof. Please refer to this link (See footnote 1) for the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 3."
        },
        {
            "text": ", where u t\u22121 and u t are the selected feature indicator vectors (u) after Iteration (t \u2212 1) and t, respectively and S t\u22121 is the unselected feature subset after Iteration t \u2212 1. According to Theorem 4, this is equivalent to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 3."
        },
        {
            "text": "St\u22121 and S t\u22121 is the selected feature subset after Iteration t \u2212 1. Therefore, as shown in Algorithm 1, GLS selects f x to minimise this quantity (Line 5), which avoids complex matrix multiplication operations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Theorem 3."
        },
        {
            "text": "input : Dataset (X), Requested feature count (k), Group weights (\u03b11 \u00b7 \u00b7 \u00b7 \u03b1r) output: Selected feature subset (S) ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1: GLS algorithm"
        },
        {
            "text": "Proof. Refer to this link (See footnote 1) for the proof.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1: GLS algorithm"
        },
        {
            "text": "Example 1 Revisited: We apply GLS for Example 1, given the feature groups created in Sect. 4. \u03bb = 1, \u03b1 i = 1 \u2200 = i. GLS first selects \"Bank\" which has the minimum Laplace score (0.39). In Iteration 2, for all remaining features, w i = 0. Therefore, GLS selects \"Patient\" or \"Cell\", which has next minimum Laplace score (1.06). Assume it selects \"Patient\". In Iteration 3, for \"Cell\" and \"Google\", w i = 0.5 and 0, respectively and l i + \u03bb wi \u03b1i = 1.56 and 1.1, respectively. GLS selects \"Google\" which has minimal feature score. Therefore, the selected feature subset is {Bank, Patient, Google}, which is optimal according to Sect. 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1: GLS algorithm"
        },
        {
            "text": "Given F and S are as defined in Sect. 3, time complexity for computing the Laplace score is O(|F |). The complexity of the iterative group based feature selection (Line 2-11 in Algorithm 1), is O(|S||F |). As |S| << |F |, the time complexity of GLS is linear to |F |.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Computation Complexity Analysis:"
        },
        {
            "text": "In this section, we discuss the experimental results obtained by GLS algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Evaluation"
        },
        {
            "text": "We evaluate GLS, using real datasets, which are benchmark datasets used to test group based feature selection. Table 1 shows a summary of them. Yale, ORL and COIL20 have a 32 \u00d7 32 pixel map and USPS a 16 \u00d7 16 pixel map.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 111,
                    "end": 118,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Datasets:"
        },
        {
            "text": "Feature Grouping: To introduce spatial locality information, which is not available from the input data matrix alone, we partition the pixel map of an image into p\u00d7p non overlapping squares. Each square is a feature group. Default p for USPS is 2 and 4 for other image datasets. In text data, pairwise semantic similarities between the words are found using WordNet [13] and words are clustered based on the similarity values, using spectral clustering. We use only 2,468 words, available in WordNet. Genes in genomic data are clustered based on Gene Ontology information as discussed in [3] . Number of groups is set to 0.04 of the original feature set based on the previous findings for MT dataset [3] . Baselines: As baselines, we use LS algorithm and Spectral Feature Selection SPEC [19] as similarity preserving methods and Multi Cluster Feature Selection (MCFS) [6] , Robust Unsupervised Feature Selection (RUFS) [14] and HUFS as embedded methods. RUFS has proven high performance compared to many existing embedded methods and HUFS uses feature group information similar to our method. RUFS and MCFS use two different approaches to control feature redundancy (L 2,1 norm vs. L 1 norm). k-medoid (KM) [3] is specific for genomic datasets, therefore, we use it with genomic data only. For HUFS, we consider the complete pixel hierarchy as described in [17] .",
            "cite_spans": [
                {
                    "start": 366,
                    "end": 370,
                    "text": "[13]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 588,
                    "end": 591,
                    "text": "[3]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 700,
                    "end": 703,
                    "text": "[3]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 787,
                    "end": 791,
                    "text": "[19]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 868,
                    "end": 871,
                    "text": "[6]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 919,
                    "end": 923,
                    "text": "[14]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "text": "[3]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "text": "[17]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Datasets:"
        },
        {
            "text": "We consider the clustering performance as the measure of feature selection accuracy and evaluate it in terms of NMI [9] . k-means is the cluster method used. It is run 20 times and we report the average NMI. SD is the standard deviation of NMI obtained for the 20 iterations. Average accuracy of an algorithm in a dataset is the average of the NMIs obtained for all the selected feature numbers in that dataset. We select features up to the point all algorithm accuracies converge. Algorithm run times are measured in seconds.",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 119,
                    "text": "[9]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Criteria:"
        },
        {
            "text": "We split each dataset, 60% instances for training set and 40% for test test, using stratified random sampling method and remove the class labels from both. We perform feature selection on the training dataset and evaluate the clustering performance of the test set, using only the selected feature subset. By default, \u03b1 i = 1 for all feature groups and \u03bb = 1. Experimental Results: Table 2 shows that GLS achieves the highest NMI over baselines in 7 out of 9 datasets. In ORL and COIL20, GLS achieves the highest NMI with a smaller number of features than baselines. In all datasets, GLS has the highest average accuracy (rank 1), yet the rankings of baselines vary across the datasets. GLS 's average NMI gain over SPEC in Multi-B dataset is \u223c30%, which is its maximum NMI gain over baselines. Maximum NMI gain of GLS over the NMI obtained by the complete feature set is 3%, 1%, 1%, 2%, 10%, 11%, 4%, 12% and 24% for Yale, ORL, COIL20, USPS, RT, MT, CNS, DLBCL-B and Multi-B respectively. GLS 's average accuracy gains for \u03b1 i = |Gi| |F | over \u03b1 i = 1 are 0.3% and 3% in RT and DLBCL-B datasets, respectively. Due to space limitations, we omit the results graphs for Experiment 1 and 2. Please refer to this link (See footnote 1) to see all the results graphs. GLS also has the lowest SD for clustering performance for 7 out of 9 datasets. Figure 3a shows that GLS has only little increase of run time than LS, which is significantly low compared to embedded methods. For COIL20 dataset, the run time of GLS is \u223c50, \u223c20 and \u223c70 times lower than the run time of MCFS, RUFS and HUFS. Figure 3b shows that compared to large and small feature groups (p = 2, 16), GLS performance for medium sized groups (p = 4, 8) is high. According to Fig. 3c , clustering performance is less sensitive to \u03bb for \u03bb > 0, yet significantly low for \u03bb \u2264 0.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 382,
                    "end": 389,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1341,
                    "end": 1350,
                    "text": "Figure 3a",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1583,
                    "end": 1592,
                    "text": "Figure 3b",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 1733,
                    "end": 1740,
                    "text": "Fig. 3c",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Experimental Setup:"
        },
        {
            "text": "Evaluation Insights: Compared to baselines, GLS consistently shows high clustering performance for all the datasets (highest average accuracy in all datasets and maximum accuracy in 7 out of 9 datasets), with low computational costs (\u223c50 times lower run time than embedded methods on average). In all datasets, GLS achieves higher accuracy than using the complete feature set, with a comparatively smaller number of features. Higher accuracy obtained by weighted feature groups show that in some cases, knowledge about the importance level of different feature groups improves the accuracy of GLS. Low SD values for NMI show that GLS produces more stable clusters and more precise performance results than the baselines. Medium sized groups achieve higher accuracy because large and small groups more resemble the case of no groupings. This demonstrates the contribution of feature group information to achieve high accuracy. Low accuracy for \u03bb \u2264 0 supports our hypothesis that selecting features from the same group is less effective than selecting from different groups. Less parameter tuning is required for GLS as its accuracy is less sensitive to \u03bb (> 0).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Setup:"
        },
        {
            "text": "We propose a framework which facilitates exploiting feature group information by unsupervised feature selection methods and use this framework to incorporate feature group information into LS algorithm. We show that compared to baselines, the proposed method achieves high clustering performance for the datasets with feature group structures with low computational costs and requires less parameter tuning. Our future work includes using the proposed framework for unsupervised feature selection methods other than the LS algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Unsupervised gene selection using biological knowledge: application in sample clustering",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Acharya",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saha",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Nikhil",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "BMC Bioinform",
            "volume": "18",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Speed up kernel discriminant analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "VLDB J",
            "volume": "20",
            "issn": "1",
            "pages": "21--33",
            "other_ids": {
                "DOI": [
                    "10.1007/s00778-010-0189-3"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Learning a spatially smooth subspace for face recognition",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of IEEE CVPR",
            "volume": "",
            "issn": "",
            "pages": "1--7",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Unsupervised feature selection for multi-cluster data",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Proceedings of the 16th ACM SIGKDD",
            "volume": "",
            "issn": "",
            "pages": "333--342",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Feature selection in machine learning: a new perspective",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Neurocomputing",
            "volume": "300",
            "issn": "",
            "pages": "70--79",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Dependence guided unsupervised feature selection",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Laplacian score for feature selection",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Niyogi",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "NIPS",
            "volume": "",
            "issn": "",
            "pages": "507--514",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Joint embedding learning and sparse regression: a framework for unsupervised feature selection",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Nie",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "IEEE Trans. Cybern",
            "volume": "44",
            "issn": "6",
            "pages": "793--804",
            "other_ids": {
                "DOI": [
                    "10.1109/TCYB.2013.2272642"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Exclusive feature learning on arbitrary structures via l1,2-norm",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kong",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fujimaki",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "NIPS",
            "volume": "",
            "issn": "",
            "pages": "1655--1663",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A survey on filter techniques for feature selection in gene expression microarray analysis",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lazar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Taminau",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Meganck",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "IEEE/ACM TCBB",
            "volume": "9",
            "issn": "4",
            "pages": "1106--1119",
            "other_ids": {
                "DOI": [
                    "10.1109/TCBB.2012.33"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "WordNet: a lexical database for English",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "A"
                    ],
                    "last": "Miller",
                    "suffix": ""
                }
            ],
            "year": 1995,
            "venue": "Commun. ACM",
            "volume": "38",
            "issn": "11",
            "pages": "39--41",
            "other_ids": {
                "DOI": [
                    "10.1145/219717.219748"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Robust unsupervised feature selection",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IJCAI",
            "volume": "",
            "issn": "",
            "pages": "1621--1627",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Feature selection model based on clustering and ranking in pipeline for microarray data",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sahu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Dehuri",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Jagadev",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Inform. Med. Unlocked IMU",
            "volume": "9",
            "issn": "",
            "pages": "107--122",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Feature selection methods for big data bioinformatics: a survey from the search perspective",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Chang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Methods",
            "volume": "111",
            "issn": "",
            "pages": "21--31",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Exploiting hierarchical structures for unsupervised feature selection",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 SDM",
            "volume": "",
            "issn": "",
            "pages": "507--515",
            "other_ids": {
                "DOI": [
                    "10.1137/1.9781611974973.57"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Unsupervised group feature selection for media classification",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zaharieva",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Breiteneder",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hudec",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int. J. Multimed. Inf. Retr",
            "volume": "6",
            "issn": "3",
            "pages": "233--249",
            "other_ids": {
                "DOI": [
                    "10.1007/s13735-017-0126-y"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Spectral feature selection for supervised and unsupervised learning",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 24th ICML",
            "volume": "",
            "issn": "",
            "pages": "1151--1157",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A novel features ranking metric with application to scalable visual and bioinformatics data classification",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zeng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Neurocomputing",
            "volume": "173",
            "issn": "",
            "pages": "346--354",
            "other_ids": {
                "DOI": [
                    "10.1016/j.neucom.2014.12.123"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Given that S is the selected feature subset and G i is the set of features in i th feature group, w i = No. of features in S and Gi No. of features in S = |S\u2229Gi| |S| . results for {Bank, Patient, Google} (c) Cluster results for {Bank, Patient, Cell}",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Feature selection in the text dataset in Example 1",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Matrices for the dataset in Example 1. b: Bank, p: Patient, c: Cell, g: Google",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Consequently, Eq. (1) can be reformulated as minimising UQU 1,1 subject to U 1,1 = k, where l i = Laplace score of f i . For example, in Example 1, diag(Q) = [0.39, 1.06, 1.06, 1.1]. When S = {Bank, Patient, Cell}, diag(Q ) = [0.39, 1.06, 1.06, 0] and Q 1,1 = 2.51. When S = {Bank, Patient, Google}, diag(Q ) = [0.39, 1.06, 0, 1.1]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "\u2190 argmin x\u2208S scorex; 8 S \u2190 S + fmin; S \u2190 S \u2212 fmin; 9 j \u2190 Group index of Gj where fmin \u2208 Gj; 10 nj++; wj \u2190 n j |S| ; f Count++; 11 end 12 return S;",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "GLS execution time and accuracy variation for different settings for COIL20",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Dataset description. m: # features, n: # instances, c: # classes",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Experiment 1 evaluates the clustering performance of different algorithms for different numbers of selected features. Experiment 2 evaluates the clustering performance of GLS in text and genomic data, for \u03b1 i = |Gi| |F | and \u03b1 i = 1 \u2200 i. This tests the effect of group weights on clustering performance. Experiment 3 executes each feature selection algorithm 100 times and reports the log value of the average run time to evaluate the algorithm efficiency. Experiment 4 performs feature selection in image datasets for p = 2,4,8,16. This tests the effect of the group size on the clustering performance. Experiment 5 runs GLS for \u03bb \u2208 [-1, 3]. This tests the effect of \u03bb on the clustering performance.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This work is supported by the Australian Government.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}