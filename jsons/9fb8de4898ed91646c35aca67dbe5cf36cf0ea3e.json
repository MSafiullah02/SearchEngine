{
    "paper_id": "9fb8de4898ed91646c35aca67dbe5cf36cf0ea3e",
    "metadata": {
        "title": "Statistical Disclosure Control via Information Theory",
        "authors": [
            {
                "first": "Genqiang",
                "middle": [],
                "last": "Wu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "SIE",
                    "institution": "Lanzhou University of Finance and Economics",
                    "location": {
                        "postCode": "730020",
                        "settlement": "Lanzhou",
                        "country": "China"
                    }
                },
                "email": "genqiang80@gmail.com"
            },
            {
                "first": "Xianyao",
                "middle": [],
                "last": "Xia",
                "suffix": "",
                "affiliation": {},
                "email": "xianyao@nfs.iscas.ac.cn"
            },
            {
                "first": "Yeping",
                "middle": [],
                "last": "He",
                "suffix": "",
                "affiliation": {
                    "laboratory": "SKLCS",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "postCode": "100190",
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Differential privacy is a strong privacy model based on indistinguishability of statistical outputs of two neighboring datasets, which represent two states that an individual's information is within or without a dataset. However, when the informations of different individuals are dependent, the representation would lose its foundation. In order to remedy the drawback of differential privacy, we revisit the Dalenius' paper [1] that motivates differential privacy, and introduce a new privacy model to control individual's information disclosure. The rationality of the privacy model is based on the information theory, i.e., the mutual information of one individual's information and the statistical outputs is upper bounded by a small value. The new privacy model accurately captures the weakness of differential privacy when dealing with dependent informations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Furthermore, the new privacy model gets on well with differential privacy. When the informations of individuals are independent, we prove that a mechanism that satisfies \u01eb-differential privacy would ensure to satisfy the new privacy model. When the informations of individuals are dependent, we prove that the group privacy method to achieve differential privacy in dependent case can be used to achieve the new privacy model. When the dependence extents of the informations of individuals are weak, we find differentially private mechanism which can satisfy the new privacy model with noise magnitude far less than the mechanism based on the group privacy based method. These results imply that the new model inherits (almost) all of the mechanisms of differential privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Keywords-differential privacy, mutual information, dependent information X = {t 1 , t 2 }. Assume that every datasets have two records, each of which is generated by one of the individuals X 1 , X 2 , respectively. Then, the set of assignments of X = (X 1 , X 2 ) is Z = {z 1 , z 2 , z 3 , z 4 }, where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "The query function f is to count the number of infected individuals, i.e., the number of t 1 , in a dataset. We have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "We now measure the amount of information of X i contained in Y , which is measured by the mutual information I(X i ; Y ). Intuitively, if I(X i ; Y ) is small, the privacy of X i would be preserved. Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "For simplicity, we analyze the quantity Pr[Y =r,Xi=xi] Pr[Y =r] Pr[Xi=xi] . Note that if each quantity Pr[Y =r,Xi=xi] Pr[Y =r] Pr[Xi=xi] \u2264 e \u01eb , then there is Pr[Xi=xi,Y =r] Pr[Xi=xi] Pr[Y =r]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "and all x i \u2208 X i , which ensure the corresponding mechanism satisfies \u01eb-information privacy with respect to X. However, Pr[X1=t1,X2=t2,Y =r] Pr[X1=t1,X2=t2][Y =r] = \u221e. The claim is proved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "However, when the sources are independent, satisfying \u01ebdifferential privacy would imply \u01eb-group information privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Corollary 1. Let the sources X = (X 1 , . . . , X n ) be independent. Assume M satisfies \u01eb-differential privacy. Then it satisfies \u01eb-group information privacy with respect to X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Proof. This is an immediate corollary of Lemma 4 and Proposition 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Achieving group information privacy for the dependent sources by differentially private mechanisms is presented in Corollary 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "In this section, we consider how to set the parameters of information privacy model, i.e., how to set the dataset universe D (or the record sequence universe Z) and the probability distribution F (x), x \u2208 D of the dataset random variable",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The governments and companies in the world are trying the best to collect our data [2] , [3] . Our income data, medical data, online shopping data, online searching history, even the amount of water or power we used every day [4] , are all collected by all sorts of these entities. To these entities, our data is their gold ore in which they can mine a lot of valuable information. To ours, however, all of our life is monitored: When we use the Google search engine, we are monitored by Google; When we use Google Maps, we are monitored by Google; When we walk around, we are monitored by the monitors in every street corners. It seems that there are limited ways to prevent these collections [3] . Therefore, our only choice is just hoping these entities may not abuse our data. However, how to correctly use these data without leaking personal private information is not a simple problem since it is not previously clear whether the statistical outputs contain sensitive information traceable back to particular individuals [5] , [6] . How to use data while preserving the privacy of individuals contained in dataset is then a vital problem, which spurs the research field of the privacy-preserving data mining or privacy-preserving data processing [7] . There are currently many privacy protection models [8] , [9] . The current hotspot about privacy research is around differential privacy [9] , [10] . The model is popular, since it offers both provable privacy guarantee and practical algorithms to achieve it. Furthermore, the application of differential privacy in the iOS 10 by the Apple company makes differential privacy enter into the public's field of vision 1 .",
            "cite_spans": [
                {
                    "start": 83,
                    "end": 86,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 89,
                    "end": 92,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 226,
                    "end": 229,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 694,
                    "end": 697,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1027,
                    "end": 1030,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1033,
                    "end": 1036,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1252,
                    "end": 1255,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 1309,
                    "end": 1312,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1315,
                    "end": 1318,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1395,
                    "end": 1398,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 1401,
                    "end": 1405,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1673,
                    "end": 1674,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "In this paper, we propose a new privacy model to treat the problems of differential privacy appeared in related works [11] , [12] , [13] . Our focus is the ability of differential privacy to treat the dependent data [14] , [15] , [13] . The classic differential privacy model, in general, assumes that each individual's data is represented as a record and the privacy is captured by pairs of datasets (x, x \u2032 ), called neighboring datasets, which represent two states that an individual's information is within or without a dataset [16] , [17] . The above representation implies the assumption that records of different individuals are independently generated respectively [11] . However, the assumption is too strong to be holding in reality. For example, the search histories of individuals within a (small) social network would be highly dependent. Due to this reason, differential privacy faces many criticisms [11] , [12] , [13] .",
            "cite_spans": [
                {
                    "start": 118,
                    "end": 122,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 125,
                    "end": 129,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 132,
                    "end": 136,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 216,
                    "end": 220,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 223,
                    "end": 227,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 230,
                    "end": 234,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 532,
                    "end": 536,
                    "text": "[16]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 539,
                    "end": 543,
                    "text": "[17]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 673,
                    "end": 677,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 915,
                    "end": 919,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 922,
                    "end": 926,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 929,
                    "end": 933,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "There are currently many works to extend the classic differential privacy model to treat dependent information. These works are mainly based on either the Bayesian inference method [18] , [12] , [19] , [20] or the group privacy method [14] , [15] , [21] . The Bayesian inference method tries to model the background knowledge of adversaries, such as the knowledge of dependent information, and then designs some policies to defend corresponding attacks. One major drawback of the Bayesian inference method is that the models based on which do not inherit most of the results of differential privacy and therefore are hard to be implemented [18] , [12] , [19] , [20] .",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 185,
                    "text": "[18]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 188,
                    "end": 192,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 195,
                    "end": 199,
                    "text": "[19]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 202,
                    "end": 206,
                    "text": "[20]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 235,
                    "end": 239,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 242,
                    "end": 246,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 249,
                    "end": 253,
                    "text": "[21]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 640,
                    "end": 644,
                    "text": "[18]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 647,
                    "end": 651,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 654,
                    "end": 658,
                    "text": "[19]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 661,
                    "end": 665,
                    "text": "[20]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The group privacy method combines the dependent records as a new record and then the newly allocated datasets would satisfy the independent assumption [21] . The differential pri-vacy mechanisms are then applicable for the new datasets. One drawback of the group privacy method is that it needs the number of dependent records within a dataset to be small. Or else, the method would add very much noise to statistical outputs. In order to reduce the noise needed in the group privacy method, the papers [15] , [14] introduce two variants of the group privacy-based model. However, the later two models fall short of theoretical foundation of privacy guarantee, which will be discussed in detail in Section VII.",
            "cite_spans": [
                {
                    "start": 151,
                    "end": 155,
                    "text": "[21]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 510,
                    "end": 514,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Therefore, a new treatment about dependent records problem is needed. The new privacy model should both solves the dependent records problem in differential privacy and inherits most of the results of differential privacy. We start our discussion by analyzing an example. There are currently many examples about the drawback of differential privacy in treating dependent records [11] , [12] , [14] , [15] , [21] . The most famous and mostly cited one is the \"contagious disease example\" which is firstly used in [11, Example 2.1] . We now revisit the example and rewrite it as the following example. Example 1. Bob or one of his 9 immediate family members may have contracted a highly contagious disease, in which case the entire family would have been infected. An attacker who knows all 9 family members have this disease already has strong evidence that Bob is infected. An attacker who knows nothing about the family's health can ask the query \"how many in Bob's family have this disease\". The true answer is almost certainly going to be either 0 or 10. Suppose Laplace(1/\u01eb) noise [9] is added to the true answer and that the resulting differentially private query answer was 12. The answer 12 is e 10\u01eb times more likely when the true answer is 10 (and hence Bob is sick) than when the true answer is 0 (and Bob is healthy). Thus data dependence and a differentially private answer produced a result where an attacker's probability estimate (of Bob being sick) can change by a (large) factor of e 10\u01eb , which is greatly larger than the expected e \u01eb .",
            "cite_spans": [
                {
                    "start": 379,
                    "end": 383,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 386,
                    "end": 390,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 393,
                    "end": 397,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 400,
                    "end": 404,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 407,
                    "end": 411,
                    "text": "[21]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 512,
                    "end": 516,
                    "text": "[11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 517,
                    "end": 529,
                    "text": "Example 2.1]",
                    "ref_id": null
                },
                {
                    "start": 1085,
                    "end": 1088,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Almost every papers about dependent records analyze Example 1 in the manner similar as above. In this paper, we treat it in a different and formal way. Since the notion of neighboring datasets is not a good notion to capture features of privacy if the individuals' informations are dependent as discussed in this section, the heart of the treatment is to find a new notion to capture the features of privacy. Therefore, we revisit some seminal papers of differential privacy [16] , [22] and the Dalenius' paper [1] that motivates differential privacy. We find that the proposing of differential privacy is based on the observation that the privacy model based on the semantic security in cryptography, which says that \"anything that can be learned about an individual from a dataset should be learnable without access to the dataset\", can not be achieved [16] , [1] , [17] , and on the observation that one can define privacy model based on the increased risk to an individual's privacy incurred by participating in a dataset [16] , [22] , [17] . However, the drawback of differential privacy makes us to rethink the Dalenius' goal: control the statistical disclosure of individuals' informations. In this paper, we employ some tools in information theory, such as the mutual information, to control the disclosure of individuals' informations. Intuitively, if a privacy mechanism's outputs contain very little information about each individual, then the individuals' privacies would be preserved. The other motivation of employing tools from information theory is that there are many mature tools to measure the dependence of individuals' informations. The details are as follows.",
            "cite_spans": [
                {
                    "start": 475,
                    "end": 479,
                    "text": "[16]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 482,
                    "end": 486,
                    "text": "[22]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 855,
                    "end": 859,
                    "text": "[16]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 862,
                    "end": 865,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 868,
                    "end": 872,
                    "text": "[17]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "text": "[16]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "text": "[22]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1040,
                    "end": 1044,
                    "text": "[17]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "We can model each individual as a random variable X i . A dataset can be modeled as an instantiation of all the individuals X = (X 1 , . . . , X n ). A mechanism M takes a dataset x as input and outputs a random variable Y valued on a domain R. The mechanism M satisfies \u01eb-differential privacy if, for any two datasets x, x \u2032 differing on at most one record and any r \u2208 R, there is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "where Pr[M(x) = r] := Pr[Y = r|X = x]. The above representations are in line with the papers [23] , [24] . We formalize Example 1 as following. For notational simplicity, we assume Bob has only one immediate family member.",
            "cite_spans": [
                {
                    "start": 93,
                    "end": 97,
                    "text": "[23]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 100,
                    "end": 104,
                    "text": "[24]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Example 2. Let t 1 denote the state of infecting the disease and let t 2 denote the state of not infecting the disease. Assume there are two individuals X 1 , X 2 , each of which is represented as a random variable following the Bernoulli distribution which generates one of t 1 , t 2 with probability p(t 1 ) = p 1 , p(t 2 ) = 1 \u2212 p 1 . Set the domain of each X i be I(X i ; Y ) \u2264 \u01eb. For any r \u2208 R and any t \u2208 X , there is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "(1) If X 1 , X 2 are independent random variables, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Therefore, there is I(X 1 , Y ) \u2264 \u01eb. Similarly, we can prove I(X 2 , Y ) \u2264 \u01eb. These results can be explained as, if X 1 , X 2 are independent, then the output Y would contain information of X i no more than \u01eb. This means that the differential privacy model effectively controls the information disclosure of X i when X 1 , X 2 are independent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "We next consider the equation (1) when X 1 , X 2 are dependent. We assume that X 1 , X 2 can only have the same value, as they can only infect disease simultaneously or none. Then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Assume that M is the Laplace mechanism and r = 2, we obtain",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "The above equation can be explained that, if X 1 , X 2 are dependent, the influence of Y = 2 to X 1 would possibly be e 2\u01eb , which is significantly larger than the expected e \u01eb . This may result in I(X 1 , Y ) > \u01eb and the information disclosure of X 1 may be out of control. Therefore, in the dependent case, the ability of differential privacy to protect privacy is weakened. From the above analyses we can see that, in the dependent case, in order to improve the privacy guarantee of differential privacy, one should set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "\u2264 e \u01eb for all r \u2208 R, t \u2208 X , which ensure I(X i , Y ) \u2264 \u01eb. In section III, we will formalize the above observations and present our new privacy model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "I. INTRODUCTION"
        },
        {
            "text": "Our contributions are as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Contribution"
        },
        {
            "text": "\u2022 We present a new privacy model: information privacy model. This model captures the weakness of differential privacy in dealing with dependent records. The new privacy model is based on the information theory. We show that the mutual information of the outputs of the privacy model and each individual is controlled to be a small quantity. We compare the new privacy model with other privacy models based on information theory and show the advantage of our model among these models. \u2022 We prove the following results, which show that the information privacy model inherits most of mechanisms of differential privacy model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Contribution"
        },
        {
            "text": "-If the individuals are independent, satisfying \u01ebdifferential privacy would ensure \u01eb-information privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Contribution"
        },
        {
            "text": "-If the (k) individuals are dependent, satisfying \u01eb/kdifferential privacy would ensure \u01eb-information privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Contribution"
        },
        {
            "text": "-If the dependence extents of individuals are weak, we find differentially private mechanism which can achieve information privacy with noise magnitude far less than the mechanism based on the group privacy based method as shown in Theorem 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Contribution"
        },
        {
            "text": "Let [n] = {1, 2, . . . , n}. Let \u01eb > 0. Let z 1 represent the \u2113 1 -norm of the real vector z. In this paper, unless noted otherwise, any set is not a multiset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Notations"
        },
        {
            "text": "Similar with the model of [1] , our model comprises n individuals. Each individual is modeled as an information source and therefore is modeled as a random variable as in information theory [25] , [26] . Each individual randomly generates records according to a probability distribution. Our model tries to model the identities of individuals and the dependence relation among individuals that the differential privacy model neglects [11] , [12] , [19] , [13] . The details are as follows.",
            "cite_spans": [
                {
                    "start": 26,
                    "end": 29,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 190,
                    "end": 194,
                    "text": "[25]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 197,
                    "end": 201,
                    "text": "[26]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 434,
                    "end": 438,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 441,
                    "end": 445,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 448,
                    "end": 452,
                    "text": "[19]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 455,
                    "end": 459,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "B. The Generation of Datasets"
        },
        {
            "text": "Let the random variables X 1 , . . . , X n denote n individuals. Let X i denote the record universe of X i and let F i be the probability distribution of X i . The probability distribution F i is called the profile of X i . A dataset is a collection (a multiset) of n records r 1 , . . . , r n , where r i \u2208 X i denotes the one generated by X i . We differentiate a record sequence (z 1 , . . . , z n ) from a dataset {z 1 , . . . , z n } the record sequence corresponds to: the former has order among the records but the later does not. The universe of record sequences Z is defined as Z = {(z 1 , . . . , z n ) : z i \u2208 X i , i \u2208 [n]}. The universe of datasets D is defined as D = {{z 1 , . . . , z n } : z i \u2208 X i , i \u2208 [n]}. We remark that D is not a multiset, in which the same elements are merged as one element. There may be several record sequences which correspond to a same dataset. We call the dataset {z 1 , . . . , z n } as the dataset of the record sequence (z 1 , . . . , z n ). For a dataset y \u2208 D, let D y := {x \u2208 Z : x \u2212 y 1 = 0} (2) denote the set of record sequences corresponding to the same dataset y and call D y as the sequences set of y.",
            "cite_spans": [
                {
                    "start": 1047,
                    "end": 1050,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "B. The Generation of Datasets"
        },
        {
            "text": "Definition 1 (Dataset Random Variable). Let X 1 , . . . , X n , Z and D be denoted as above. Then the Z-valued random vector X := (X 1 , . . . , X n ) is called the dataset random variable of the sources X 1 , . . . , X n . Let F (x), x \u2208 Z denote the probability distribution of X. For any y \u2208 D, set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. The Generation of Datasets"
        },
        {
            "text": "In this manner, X can also be seen as a D-valued random variable with the probability distribution F (y), y \u2208 D.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. The Generation of Datasets"
        },
        {
            "text": "The dataset random variable X is used to model the randomness of generating datasets. Note that the dependence of records can be explained as the dependence of the corresponding random variables (information sources).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. The Generation of Datasets"
        },
        {
            "text": "For notational simplicity, in the following of this paper, we assume Z and D are both discrete. There is no essential difference for other settings, which will be presented in the full paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. The Generation of Datasets"
        },
        {
            "text": "For a dataset x \u2208 D, we use the histogram representation x \u2208N |X | to denote the dataset x, where the ith entry x i of x represents the number of elements in x of type i \u2208 X [27] , [9] , [28] . Two datasets x, x \u2032 \u2208 D are said to be neighbors (or neighboring datasets) if x \u2212 x \u2032 1 = 2. Two record sequences x, x \u2032 \u2208 Z are said to be neighbors if their corresponding datasets are neighbors. As noted in Section I, a mechanism M takes a record sequence x \u2208 Z as input and outputs a random variable Y valued in a domain R. In this paper, for any mechanism M and any x \u2208 D, set M(y) = M(z) for any two y, z \u2208 D x . Therefore, for a dataset x \u2208 D,",
            "cite_spans": [
                {
                    "start": 174,
                    "end": 178,
                    "text": "[27]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 181,
                    "end": 184,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 187,
                    "end": 191,
                    "text": "[28]",
                    "ref_id": "BIBREF31"
                }
            ],
            "ref_spans": [],
            "section": "C. Differential Privacy"
        },
        {
            "text": "Differential privacy characterizes the changes of outputs when one's record in a dataset is changed. These changes are captured by the notion of the neighboring datasets (or the neighboring record sequences). For the dataset universe D and a query function f , let R = {f (x) : x \u2208 D}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Differential Privacy"
        },
        {
            "text": "Definition 2 (\u01eb-Differential Privacy [22] , [16] , [9] ). Let X, Y, D, Z, R be denoted as above. Let P(R) denote the set of all the probability measures on R. A mechanism M : D \u2192 P(R) gives \u01eb-differential privacy if for all neighbors x, x \u2032 \u2208 Z (or neighbors x, x \u2032 \u2208 D), and all r \u2208 R, there is",
            "cite_spans": [
                {
                    "start": 37,
                    "end": 41,
                    "text": "[22]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 44,
                    "end": 48,
                    "text": "[16]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 51,
                    "end": 54,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "C. Differential Privacy"
        },
        {
            "text": "where Pr[M(x) = r] := Pr[Y = r|X = x], and where we abuse the notation M(x) as either denoting a probability distribution in P(R) or denoting a random variable following the probability distribution.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C. Differential Privacy"
        },
        {
            "text": "Note that Definition 2 is the same as those in [23] , [24] , and is also equivalent to the definition of differential privacy in [22] , [16] , [9] .",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 51,
                    "text": "[23]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 54,
                    "end": 58,
                    "text": "[24]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 129,
                    "end": 133,
                    "text": "[22]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 136,
                    "end": 140,
                    "text": "[16]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 143,
                    "end": 146,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "C. Differential Privacy"
        },
        {
            "text": "The following two lemmas will be used frequently in this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Two Lemmas"
        },
        {
            "text": "(\u03b20x+\u03b21) 2 , by which the claims are immediate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Two Lemmas"
        },
        {
            "text": "Proof. We only prove the equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Two Lemmas"
        },
        {
            "text": "The other equation can be proved similarly. Since ai bi \u2264 a k b k , we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Two Lemmas"
        },
        {
            "text": "On the other hand, the equation (6) is equivalent to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Two Lemmas"
        },
        {
            "text": "Therefore, the equation (6) holds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Two Lemmas"
        },
        {
            "text": "The proof is complete.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Two Lemmas"
        },
        {
            "text": "In this section, we introduce a new privacy model: information privacy. As discussed in Section I, the new model tries to capture the weakness of differential privacy model when treating dependent data. We formalize the discussions in Section I as the following model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Definition 3 (Information Privacy). Let X = (X 1 , . . . , X n ) be the dataset random variable. Set X (i) = (X 1 , . . . , X i\u22121 , X i+1 , . . . , X n ), X (i) = X 1 \u00d7 \u00b7 \u00b7 \u00b7\u00d7 X i\u22121 \u00d7 X i+1 \u00d7 \u00b7 \u00b7 \u00b7\u00d7 X n and x (i) = (x 1 , . . . , x i\u22121 , x i+1 , . . . , x n ). Let M be a mechanism and let Y be its output random variable valued in a domain R. The mechanism M satisfies \u01eb-information privacy with respect to X if for each X i , any record x i \u2208 X i and any r \u2208 R, there is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Note that the inequality (8) is equivalent to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "since",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "In information theory, the relative entropy is used to measure the distance between two probability distributions and the mutual information is used to measure the amount of information that one random variable contains about another random variable [26] . The relative entropy of (X i |Y = r) and X i , denoted as D((X i |Y = r) X i ), and the mutual information of X i and Y , denoted as I(X i ; Y ), have the following results. Theorem 1. Let the mechanism M satisfies \u01eb-information privacy and let Y be the output random variable of the mechanism. We have",
            "cite_spans": [
                {
                    "start": 250,
                    "end": 254,
                    "text": "[26]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Proof. The claims are the immediate results of the fact that the inequality (8) is equivalent to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "\u2264 e \u01eb and the fact that D((X i |Y = r) X i ) and [26] .",
            "cite_spans": [
                {
                    "start": 49,
                    "end": 53,
                    "text": "[26]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Note that, as the inequality (8), we can also define privacy metrics by the notions of the \u01eb-relative entropy privacy, i.e., max i,r D((X i |Y = r) X i ) \u2264 \u01eb, and the \u01eb-mutual information privacy, i.e., max i I(X i ; Y ) \u2264 \u01eb. Furthermore, the paper [29] proposes a privacy notion called \u01eb-inferential privacy, i.e.,",
            "cite_spans": [
                {
                    "start": 249,
                    "end": 253,
                    "text": "[29]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "We now discuss the relation among the above three privacy notions and the \u01eb-information privacy. There are the following results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Proposition 1. We have the following relation among the privacy metrics: \u01eb-inferential privacy \u21d2 a \u01eb-information privacy \u21d2 b \u01eb-relative entropy privacy \u21d2 c \u01eb-mutual information privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Proof. The claim \u21d2 a is due to the inequality",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "The claims are proved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Proposition 1 shows that the four privacy metrics, \u01ebinferential privacy, \u01eb-information privacy, \u01eb-relative entropy privacy and \u01eb-mutual information privacy, are in decreasing order in terms of their strength to protect privacy. On the other hand, by the analysis of Example 2 in Section I, all of the four privacy metrics can capture the characteristics of the dependent sources problem in differential privacy. Therefore, one can choose any one of the four metrics as the privacy metric, which depends on the privacy level of demand.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "The paper [24] discusses the relation between the conditional mutual information I(X i ; Y |X (i) ) and differential privacy. We have the following result, which is the same as in [24] . Proposition 2. Let the mechanism M satisfies \u01eb-differential privacy and let Y be the output random variable of the mechanism. We have",
            "cite_spans": [
                {
                    "start": 10,
                    "end": 14,
                    "text": "[24]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 180,
                    "end": 184,
                    "text": "[24]",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Proof. Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "The proof is complete.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "One should be emphasised is that Proposition 2 is irrelevant to the dependence of the sources. This implies that the quantity",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "and then the conditional mutual information I(X i ; Y |X (i) ) could not capture the characteristics of the problem of dependent sources in differential privacy. In this respect, Definition 3 has dominated advantage to the the above two quantities in treating dependent sources.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "The papers [30] , [31] , [23] employ either the mutual information I(X; Y ) or the related quantity Pr[Y =r,X=x]",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[30]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 18,
                    "end": 22,
                    "text": "[31]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 25,
                    "end": 29,
                    "text": "[23]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "to define privacy notions. We remark that both of the two",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "Pr [Y =r] Pr[X=x] \u2264 e \u01eb are too strong to be used to define privacy notions. The reasons are as follows. Note that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "By the theory of differential privacy, the last inequality implies the outputs of two datasets x, x \u2032\u2032 of distance n (therefore they are very different) should be indistinguishable for x \u2208 n i=1 X i , which will lead to useless outputs. The inequality I(X; Y ) \u2264 \u01eb has the similar problem since",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "The general cases can be treated similarly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "III. THE MODEL OF INFORMATION PRIVACY"
        },
        {
            "text": "The group privacy is a very important property to protect the privacy of a group of individuals. Differential privacy has group privacy property, which ensures that the strength of the privacy guarantee drops linearly with the size of the group.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "Lemma 4 (Group Privacy [9] ). Let M be an \u01eb-differentially private mechanism. Then",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 26,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "for any two datasets x, y \u2208 D and any r \u2208 R.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "We now give a formal definition of the group privacy of information privacy model. . . , i n }. Set X I = (X i1 , . . . , X i \u2113 ), X (I) = (X i \u2113+1 , . . . , X in ). The quantities X I , X (I) , x I , x (I) are set accordingly. A mechanism M satisfies \u01eb-group information privacy with respect to X if for any non-empty set I \u2286 [n], any X I , any record x I \u2208 X I and any r \u2208 R, there is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "We remark that there is no group privacy property for the information privacy model, in general. There is the following result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "Proposition 3. Let X = (X 1 , . . . , X n ) be the sources. Let \u2126 be the set of mechanisms satisfying \u01eb-information privacy with respect to X. There then exists mechanism M \u2208 \u2126, which satisfies that there exist non-empty set I \u2286 [n], x I , x \u2032 I \u2208 X I and r \u2208 R such that the quantity",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "goes to \u221e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "Proof. Since ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "= 0 for all other x 1 \u2208 X 1 and x 2 \u2208 X 2 . Then X = (X 1 , . . . , X n ) in the equation (8). This is challenging since the probability distribution F (x) of the dataset random variable X is unknown in reality. This problem is common for almost all of the models about the dependent sources problem in differential privacy [21] , [15] , [14] , [18] , [12] , [20] , [32] , [13] . A probabilistic model is needed to estimate these probabilities. As stated in [14] , the techniques in [33] would be one possible way to set these probabilities. In this paper, we do not want to give a detailed discussion about how to set F (x) but to give a discussion about how to set the dataset universe D (or the record sequence universe Z).",
            "cite_spans": [
                {
                    "start": 324,
                    "end": 328,
                    "text": "[21]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 331,
                    "end": 335,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 338,
                    "end": 342,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 345,
                    "end": 349,
                    "text": "[18]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 352,
                    "end": 356,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 359,
                    "end": 363,
                    "text": "[20]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 366,
                    "end": 370,
                    "text": "[32]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 373,
                    "end": 377,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 458,
                    "end": 462,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 483,
                    "end": 487,
                    "text": "[33]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "One needs to be emphasized is that the dataset universe D (or the record sequence universe Z) should be set carefully since D itself may leak individual's privacy. In order to see the above result clearly, we consider a query function f (x) = x, x \u2208 D as an example, which can be considered as the abstraction of data publishing function [34] , [35] , [36] . Note that the codomain of f is R = {f (x) : x \u2208 D} = D. Both of the differential privacy model and the information privacy model employ randomized techniques to protect privacy: When the real dataset is x, in order to preserve privacy, a privacy mechanism first samples a dataset y \u2208 D (according to a probability distribution) and then outputs f (y) \u2208 R as the final query result of f . Or equivalently, the privacy mechanism directly samples a value r from the codomain R of f as the final query result. The major difference of the two models is that the probability distributions used to sample y or r are different. Assume that the individual X i 's record universe X i is different from all other individuals' record universe. Then, finding a record r i \u2208 X i within an outputted dataset x would strongly conclude the participation of the individual X i , which obviously discloses privacy. Therefore, we should set appropriate D and therefore appropriate X i for i \u2208 [n] such that the set D does not leak the participation of an individual. The perfect setting is to set X i = X for all i \u2208 [n] as in [9, p 227] .",
            "cite_spans": [
                {
                    "start": 338,
                    "end": 342,
                    "text": "[34]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 345,
                    "end": 349,
                    "text": "[35]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 352,
                    "end": 356,
                    "text": "[36]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1466,
                    "end": 1476,
                    "text": "[9, p 227]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "Furthermore, as shown in Section II, we state that each dataset has n records and that two datasets x, x \u2032 are said to be neighbors if changing one record to another record in x generates x \u2032 , which is captured by x \u2212 x \u2032 1 = 2. The above definition of neighboring datasets is called bounded neighboring datasets [11] . The other definition of neighboring datasets, which is called unbounded neighboring datasets [11] , defines that two datasets x, x \u2032 are said to be neighbors if deleting one record or appending one record to x generates x \u2032 , which is captured by x \u2212 x \u2032 1 = 1. In order to be consistent with the unbounded neighboring datasets, we need to append an empty record, denoted as \u22a5, to each X i . In this setting, if x i = \u22a5, it means that the individual X i does not generate record in the dataset x.",
            "cite_spans": [
                {
                    "start": 314,
                    "end": 318,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 414,
                    "end": 418,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "A. Remark on Group Privacy"
        },
        {
            "text": "Differential privacy has the following relation with information privacy. Proposition 4. Assume the sources X 1 , . . . , X n are independent. If the mechanism M is \u01eb-differentially private, then it satisfies \u01eb-information privacy with respect to X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "Proof. Since the sources are independent, the equation (8) is equivalent to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "On the other hand, since M is \u01eb-differentially private, we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "for all x \u2032 i \u2208 X i . Therefore, the left side of the equation (12) is less than",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "which obviously equals e \u01eb .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "The claim is proved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "By combining Theorem 1 with Proposition 4, we have the following result.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "Corollary 2. Let the mechanism M satisfies \u01eb-differential privacy and let Y be the output random variable of the mechanism. Let the sources X 1 , . . . , X n be independent. We have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "The next proposition says that, if the sources are dependent, we can use the group privacy property of differential privacy to achieve \u01eb-information privacy. The group privacy method is used extensively to improve the ability of differential privacy to protect privacy for dependent data [14] , [15] , [21] as noted in Section I. Proposition 5. Set k \u2264 n. Assume that any one source is independent to at least other n \u2212 k sources. Assume also that M satisfies \u01eb/k-differential privacy. Then M satisfies \u01ebinformation privacy with respect to X = (X 1 , . . . , X n ).",
            "cite_spans": [
                {
                    "start": 288,
                    "end": 292,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 295,
                    "end": 299,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 302,
                    "end": 306,
                    "text": "[21]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "Proof. LetX (i) ,X (i) denote the sources which are independent to and dependent to the source X i , respectively. Letx (i) ,x (i) andX (i) ,X (i) denote one assignment and the universe ofX (i) ,X (i) , respectively. For onex (i) , set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "IV. RELATION TO DIFFERENTIAL PRIVACY"
        },
        {
            "text": "x",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "The last inequality is due to that M satisfies \u01eb/k-differential privacy and the group privacy property, which ensure e \u2212\u01eb Mx (i) = e \u2212k\u00d7\u01eb/k Mx (i) \u2264 mx (i) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "The proof is complete.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "One needs to be reminded is that the results of Proposition 4 and Proposition 5 are not reversible. When the sources are independent, there may exist mechanism which satisfies \u01ebinformation privacy but does not satisfy \u01eb-differential privacy. This can be known from the equation (12) The results in the last paragraph would make us to take a sceptical attitude to the ability of \u01eb-information privacy to protect privacy. However, This is overwrought since Theorem 1 ensures that outputs of an \u01eb-information private mechanism contain little information about each individual. On the contrary, the non-equivalence may not be the weakness of information privacy but be its merit: more flexible than differential privacy model. This is because of letting",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "i ,x (i) )=r] be larger than e \u01eb when Pr[X (i) = x (i) ] being very small may not have significant difference for the privacy protection level from the case of the ratio less than e \u01eb . After all, these events happen with very small probabilities. Therefore, the flexibility would give the information privacy model more opportunities to improve the utility of outputs than differential privacy model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "Similarly, when the sources are dependent, \u01eb-differential privacy and \u01eb-information privacy are not equivalent, which may also not be the drawback of the later compared to the former but the merit of the former. For example, the Black Death in the 14th century and the SARS coronavirus in [2002] [2003] show that most people of the world are dependent. If using the group privacy method, differential privacy model will add very large noise. However, since these events are rare events, the information privacy can add relatively smaller noise by the probabilities of these events, which shows the flexibility of information privacy.",
            "cite_spans": [
                {
                    "start": 289,
                    "end": 295,
                    "text": "[2002]",
                    "ref_id": null
                },
                {
                    "start": 296,
                    "end": 302,
                    "text": "[2003]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "We now explain how the ability of differential privacy to protect privacy is weakened when the sources are dependent for general cases. We first give a lemma.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "and min i,j\u2208{0,...,k}",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "Moreover, the bounds of the above inequalities are reachable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "Proof. The equation (15) is an immediate corollary of Lemma 3. The proof of the equation (16) is immediate and is omitted.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "When applying Lemma 5 to the left side of the equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "and (17) is determined by the probabilities of two neighboring datasets (or neighboring sequences). However, the extreme values in the equation (18) is determined by the probabilities of two datasets (or record sequences) not limited to be neighbors, which would result in individuals' information disclosure since the mutual information I(X i , Y ), an expected value of the left side of the equation (8), would be out of control. These would be the immediate cause of weakening the privacy guarantee of differential privacy when the sources are dependent.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Pr[M(xi,x(i),x(i))=r] Pr[X(i)=x(i),X(i)=x(i)|Xi=xi]"
        },
        {
            "text": "In this section, we discuss how to achieve information privacy. Proposition 5 implies that one can use the group privacy property of differential privacy to achieve information privacy. However, as discussed in Section I, this will add to much noise to outputs. We now explore new methods to achieve information privacy with less noise. First, we present a lemma. Lemma 6. Let X = (X 1 , . . . , X k ) be the sources. For any i \u2208 [k] and any x (i) \u2208 X (i) , set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. ACHIEVING INFORMATION PRIVACY"
        },
        {
            "text": "Let the mechanism M satisfy \u01eb/kdifferential privacy and let \u01eb(",
            "cite_spans": [],
            "ref_spans": [],
            "section": "V. ACHIEVING INFORMATION PRIVACY"
        },
        {
            "text": "We have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "where the inequality \u2264 a is due to the fact that M satisfies \u01eb/k-differential privacy and the group privacy property of M, the inequality \u2264 b is due to Lemma 2, the inequality \u2264 c is due to the inequalities x (i) \u2208X (i) a x (i) \u2264 1, and the inequality \u2264 d is due to the inequalities \u01eb(1 \u2212 1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "The claim is proved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "We then have the following theorem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "Theorem 2. Let X = (X 1 , . . . , X n ) be the sources. Assume that, for any one source X i , there are at least n \u2212 k sources in X which are independent to X i . For any i \u2208 [n] and any",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "Let the mechanism M satisfy \u01eb/kdifferential privacy and let \u01eb(",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "Proof. The proof of the theorem is the combination of Lemma 6 and the proof techniques of Proposition 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "Note that the weak dependence of the sources X implies",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": ". Therefore, Theorem 2 implies that, even if the maximum number of dependent sources k is large, there are differentially private mechanisms which can achieve information privacy with noise magnitude far less than the group privacy based method so long as the dependent extent of the sources are low, i.e., \u03b7 is small or b is large.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "By combining Theorem 2 and Lemma 4, we have the following result to achieve \u01eb-group information privacy. . Assume that, for any x I , x \u2032 I \u2208 X I and any I, there is 1 \u2212 x (I) \u2208X (I) a x (I) \u2264 \u03b7. Let the mechanism M satisfy \u01eb/kdifferential privacy and let \u01eb(1 \u2212 1 k ) \u2265 b, where \u03b7 = exp(\u2212b). Then, M satisfies (\u01eb \u2212 b + log 2)-group information privacy with respect to X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proof. For any i \u2208 [k] and any x"
        },
        {
            "text": "The composition privacy property of differential privacy in Lemma 1 is a very power tool to construct differentially private mechanisms [34] , [36] , [37] . However, we remark that, in general, there is no composition privacy property for the information privacy model. Proposition 6. Let X = (X 1 , . . . , X n ) be the sources. Let \u2126 j be the set of mechanisms satisfying \u01eb j -information privacy with respect to X for j \u2208 [\u2113]. Assume that, for any mechanisms",
            "cite_spans": [
                {
                    "start": 136,
                    "end": 140,
                    "text": "[34]",
                    "ref_id": "BIBREF39"
                },
                {
                    "start": 143,
                    "end": 147,
                    "text": "[36]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 150,
                    "end": 154,
                    "text": "[37]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": ", there are",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "for x \u2208 Z, which imply that M \u2032 1 , . . . , M \u2032 \u2113 are independent when the input dataset is fixed. There then exist mechanisms M 1 , . . . , M \u2113 with M j \u2208 \u2126 j for j \u2208 [\u2113], which satisfy that there exist x i , x \u2032 i \u2208 X i and r = (r 1 , . . . , r \u2113 ) such that the quantity",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "Proof. The proof is sketched as follows. By the equation (19), there is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "We construct M j 's as follows. For M 1 , choosex (i) \u2208 X (i) and set Pr[",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "Then the denominator of (20) goes to 0. However, we are relatively easy to choose other parameters to let the nominator of (20) be positive and to let each M j satisfy differential privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "The claim is proved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "Although Proposition 6 shows that, in general, there is no composition privacy property of information privacy, we can modularly construct information private mechanisms through the composition privacy property of differential privacy. By combining Theorem 2 with Lemma 1, we have the following composition privacy of information privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "Corollary 4. Let X = (X 1 , . . . , X n ) be the sources. Assume that, for any one source X i , there are at least n \u2212 k sources in X which are independent to X i . For any i \u2208 [n] and any",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "Assume that, for any x i , x \u2032 i \u2208 X i and any i \u2208 [n], there is 1 \u2212 x (i) \u2208X (i) a x (i) \u2264 \u03b7. Let the mechanism M j satisfy \u01eb j /kdifferential privacy for j \u2208 [\u2113] and let",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": ". Then the compositional mechanism M, defined as M(x) = (M 1 (x), . . . , M \u2113 (x)), x \u2208 Z, satisfies ( \u2113 j=1 \u01eb i \u2212 b + log 2)-information privacy with respect to X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Remark on Composition Privacy"
        },
        {
            "text": "In this section, we generalize the information privacy model to deal with the case where multiple organizations independently output privacy-preserving data about overlapping individuals [5] , [6] . For example, each of the Netflix and the IMDb may publish a different dataset, where the two datasets are generated by a same group of individuals [6] . We remark that this problem is different from the composition privacy problem in Section V-A, where the former is to output different privacy-preserving datas of the different datasets generated by a same group of individuals but the later is to output different privacy-preserving datas of the same dataset. In this scenario, an individual X i should be represented by a stochastic process X i := {X t i , t \u2208 T } (but not a random variable). The output Y also should be represented by a stochastic process Y := {Y t , t \u2208 T }. In this setting, we can define the mutual information privacy of the above problem as I({X t i , t \u2208 T }; {Y t , t \u2208 T }) \u2264 \u01eb. That is, the outputs {Y t , t \u2208 T } should contain little information of each individual {X t i , t \u2208 T }. For the information privacy, we have the following definition.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 190,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 193,
                    "end": 196,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 346,
                    "end": 349,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "Definition 5 (Information Privacy for Stochastic Processes). Let X = (X 1 , . . . , X n ) be the sources, where each",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "Let M be a mechanism and let Y := {Y t , t \u2208 T } be its output stochastic process valued in a domain R :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "The mechanism M satisfies \u01eb-information privacy with respect to X if for each X i , any record x i \u2208 X i and any r \u2208 R, there is",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "In this paper, we set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "(23) That is, the sub-mechanisms M 1 , . . . , M T of M are independent mechanisms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "We now show that the composition privacy problem in Section V-A is a special case of the generalized composition privacy problem in Definition 5 where, for each stochastic process X i := {X t i , t \u2208 T }, the random variables X t i , t \u2208 T are all equal 2 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "Proposition 7. Let X, Y be as shown in Definition 5. Assume, for each i \u2208 [n], the random variables in the stochastic process X i := {X t i , t \u2208 T } are all equal. Then, I(X i ; Y ) = I(X 1 i ; Y ) and, for any x 1 i and any r, there is ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "The equation (24) follows by the above two results. The equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "is an immediate corollary of the equation (24) . Most of results of the information privacy model also hold for the stochastic processes. We have the following results whose proofs are similar to those proofs of those original results and are omitted. ",
            "cite_spans": [
                {
                    "start": 42,
                    "end": 46,
                    "text": "(24)",
                    "ref_id": "BIBREF27"
                }
            ],
            "ref_spans": [],
            "section": "VI. GENERALIZED COMPOSITION PRIVACY"
        },
        {
            "text": "We now consider a simple scenario where the generated datas of each stage are independent. For example, for a group of individuals, their shopping data in Amazon would be independent (or less dependent) to their research data in DBLP. This setting can be modeled as that the T random vectors (X 1 , Y 1 ), . . . , (X T , Y T ) are independent. We have the following result about the setting. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Independent Stochastic Processes"
        },
        {
            "text": "Proof. The equation (25) is due to the independence of the T random vectors (X 1 , Y 1 ), . . . , (X T , Y T ). The equation",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Independent Stochastic Processes"
        },
        {
            "text": "follows by the definition of mutual information and the equation (25) .",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 69,
                    "text": "(25)",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "A. Independent Stochastic Processes"
        },
        {
            "text": "Note that, in Proposition 8, for each t, the random variables X t 1 , . . . , X t n , Y t do not need to be independent. Proposition 8 shows that, if the datasets are independently generated, the information privacy would degrade linearly.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A. Independent Stochastic Processes"
        },
        {
            "text": "The information privacy model for stochastic processes in Definition 5 is powerful to model many complicated application scenarios. For example, the independent data publications of datas of the Netflix and the IMDb in Section VI, the independent data publications of the online and offline data in [38] , and the independent data publications of the voter registration data and the medical data in [39] . For each of the above scenarios, the composition attack [5] , [6] technique may employ the relation between/among different datasets to infer the privacy of individuals whose datas are contained in these datasets. We remark that differential privacy model does not work well to resist the composition attack, such as on the above application scenarios. The reason is the same as the discussion in the last part of Section IV. That is, due to the dependence of individuals, their generated datas are dependent even if the datas are from different applications, which will result in the maximal value of the left side of the equation (22) is not determined by the ratio of the probabilities of two neighboring datasets and then will result in the mutual information I(X i , Y ) be out of control.",
            "cite_spans": [
                {
                    "start": 299,
                    "end": 303,
                    "text": "[38]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 399,
                    "end": 403,
                    "text": "[39]",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 462,
                    "end": 465,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 468,
                    "end": 471,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "text": "(22)",
                    "ref_id": "BIBREF24"
                }
            ],
            "ref_spans": [],
            "section": "B. Remark on Composition Attack"
        },
        {
            "text": "The information privacy model accurately models the dependence among the datasets generated by the individuals. The effect of the dependence on the privacy is reflected on the mutual information I(X i , Y ). By limiting the value I(X i , Y ) we can control the privacy disclosure of the individuals effectively. In this manner, the information privacy model would be immune to the composition attack.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B. Remark on Composition Attack"
        },
        {
            "text": "In differential privacy, the paper [11] is the first to point out that dependent sources will weaken the ability of differential privacy to protect privacy. As noted in Section I, there are mainly two directions to treat the dependent sources problem: the group privacy method and the bayesian inference method. The group privacy method is firstly used in the paper [21] , where the dependent sources problem is treated by the group privacy property of differential privacy and by magnifying the global sensitivity of query function, i.e., if there are at most k sources are dependent, one can alleviate the influence of dependent sources to decrease the privacy guarantee of differential privacy by achieving \u01eb/k-differential privacy or by multiplying k to the global sensitivity of the query function. The group privacy method is consistent with the result of Proposition 5. However, the shortcoming of the group privacy is also obvious: the method assumes there must be limited numbers of dependent sources, while the assumption is too strong to be holding in reality. For example, the spreading of the Black Death in the 14th century 3 and the spreading of the SARS coronavirus in 2002-2003 4 (if without effective controlling) show that people all over the world are dependent. Moreover, the small world phenomenon [43] , [44] also shows the dependence among people. These facts show that, if using group privacy method, there will add too much noise to the outputs of privacy mechanism. In order to alleviate the shortcoming of the group privacy method, the paper [15] and the paper [14] introduce the notions of \"correlated sensitivity\" and \"dependence coefficient\", respectively. The two notions can be explained as introducing a dependent coefficient (which is much less than 1 in general) between two sources to decrease the increasing speed of the global sensitivity of the query function. Although the two notions can add less noise than the group privacy method, the privacy guarantee of the two models have less theoretical foundation. On the contrary, the information privacy model is based on the information theory as shown in Theorem 1 and therefore has strong theoretical foundation. Furthermore, the information privacy model can accurately represent the dependent sources according to the conditional probabilities. It seems be more flexible than differential privacy model. For example, since the Black Death and the SARS coronavirus events are rare events, they will have very small effect to the inequality (8) since they have very small probabilities. This makes our model to allow the ratio of the probabilities of the outputs of two neighbors be much larger than e \u01eb , but still satisfies \u01eb-information privacy. The model in [21] has less such flexibility.",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 39,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 366,
                    "end": 370,
                    "text": "[21]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1320,
                    "end": 1324,
                    "text": "[43]",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "text": "[44]",
                    "ref_id": "BIBREF50"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "text": "[15]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1589,
                    "end": 1593,
                    "text": "[14]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 2752,
                    "end": 2756,
                    "text": "[21]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "VII. RELATED WORK"
        },
        {
            "text": "As noted in Section I, another method to treat the dependent sources problem is the Bayesian inference method [18] , [12] , [20] , [32] . Since an adversary's background knowledge can be model as a probability distribution over the dataset universe D [16] , [12] , each Bayesian inference model specifies a set of probability distributions to model adversaries' background knowledge. The intention of the method is to limit the ability of adversaries with some special background knowledge, such as the knowledge about the dependent sources, to infer the private data of individuals. We emphasize that differential privacy is strong enough to preserve privacy when the individuals are independent as shown in Corollary 2 of this paper and in Proposition 1.6 of [10] , and that the weakness of differential privacy is its limited ability to treat the dependent sources. Therefore, modeling too much adversaries' background knowledge which have limited relation to the dependence of the sources is not necessary but increasing the hardness of achieving the corresponding models. That is, we only need to model the dependent relations among the sources, but not to model any background knowledge of any adversaries. Due to this reason, our model employs information-theoretic tools to model the dependent sources and tries to root the privacy protection into the information theory. The results of this paper show that the combination of differential privacy and information theory greatly strengthens the ability of differential privacy to treat the dependent sources problem, but not sacrifices efficient mechanisms to achieve the corresponding model.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 114,
                    "text": "[18]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 117,
                    "end": 121,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 124,
                    "end": 128,
                    "text": "[20]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 131,
                    "end": 135,
                    "text": "[32]",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 251,
                    "end": 255,
                    "text": "[16]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 258,
                    "end": 262,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 761,
                    "end": 765,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "VII. RELATED WORK"
        },
        {
            "text": "The information privacy can be traced back to Shannon's perfect secrecy in [40, page 680] and is related to the semantic security in Cryptography. The perfect secrecy ensures the output Y contains no information about input X, i.e.,I(X, Y ) = 0. The semantic security is a computational complexity relaxation to the perfect secrecy, which implies that any information revealed cannot be extracted by PPT adversaries 5 [42] . However, the information privacy implies that any information revealed about X i is no more than \u01eb, i.e.,I(X i , Y ) \u2264 \u01eb, which is another relaxation to the perfect secrecy. We emphasize that any good privacy notion should ensure I(X i , Y ) to be controllable. Otherwise, the privacy notion would be contrary to information theory, which is 5 https://en.wikipedia.org/wiki/Semantic security unacceptable: The privacy theory and the information theory are both to deal with data, even if they may be different and less related, but at least should not be contrary. Unfortunately, differential privacy, whose output may contain too much information about each individual when individuals are dependent as shown in Section I, Section IV and Section VI, is contrary to information theory.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 89,
                    "text": "[40, page 680]",
                    "ref_id": null
                },
                {
                    "start": 418,
                    "end": 422,
                    "text": "[42]",
                    "ref_id": "BIBREF48"
                }
            ],
            "ref_spans": [],
            "section": "VII. RELATED WORK"
        },
        {
            "text": "There are many other works to construct privacy model by information theory. As shown in Section III, the papers [30] , [31] , [23] \u2264 e \u01eb are too strong to be used to define privacy metrics, which will lead to useless outputs. The paper [24] relates differential privacy with the conditional mutual information I(X i ; Y |X (i) ), where I(X i ; Y |X (i) ) \u2264 \u01eb is proved to be weaker than \u01eb-differential privacy but stronger than (\u01eb, \u03b4)-differential privacy. As stated in Proposition 2, I(X i ; Y |X (i) ) \u2264 \u01eb can't capture the features of dependent sources problem. By Proposition 1, the \u01eb-inferential privacy model in [29] can be considered as a special case of our model. Furthermore, the paper [29] does not relate \u01eb-inferential privacy model to information theory. Moreover, Theorem 2 gives mechanism which can add much less noise than the group privacy method while not lowering the privacy level as long as the dependence levels among the sources are low, which is different from the one in [29] . The adversarial privacy model [45] can be considered as using Pr \u2264 \u01eb to define privacy metric, which is somewhat equivalent to Definition 3. However, the paper [45] does not discuss the informationtheoretic implication of the adversarial privacy model as in Section III. Furthermore, Proposition 4, Proposition 5 and Theorem 2 in this paper show that the information privacy model inherits almost all of the mechanisms of differential privacy model, which are more general than Algorithm 2.1 in [45] and more applicable than Theorem 2 in [45] . The Bayesian differential privacy model [13] uses the inequality \u2264 e \u01eb , max i,I,r D((X i |Y = r, X I ) (X i |X I )) \u2264 \u01eb and max i,I I(X i ; Y |X I ) \u2264 \u01eb [24] . Obviously, the four privacy metrics in Proposition 1 are all implied by the Bayesian differential privacy metric (when I = \u2205). One drawback of the Bayesian differential privacy model is that it is too strong to inherit most mechanisms of differential privacy model [13] . The paper [46] relates the utility function in Exponential mechanism to the rate distortion function and then discusses the relation between information leakage and privacy. Another kind of work for treating differential privacy via information theory is to measure the bound of noise complexity of differential privacy [47] , [48] . Our model uses the relative entropy D(X i (X i |Y = r)) and the mutual information I(X i ; Y ) to treat the dependent sources problem of differential privacy. The results in Theorem 1 show that the information privacy model can ensure individual information disclosure to be upper bounded by a small value \u01eb.",
            "cite_spans": [
                {
                    "start": 113,
                    "end": 117,
                    "text": "[30]",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 120,
                    "end": 124,
                    "text": "[31]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 127,
                    "end": 131,
                    "text": "[23]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 237,
                    "end": 241,
                    "text": "[24]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 619,
                    "end": 623,
                    "text": "[29]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 697,
                    "end": 701,
                    "text": "[29]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "text": "[29]",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "text": "[45]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1066,
                    "end": 1068,
                    "text": "Pr",
                    "ref_id": null
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "text": "[45]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1499,
                    "end": 1503,
                    "text": "[45]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1542,
                    "end": 1546,
                    "text": "[45]",
                    "ref_id": "BIBREF52"
                },
                {
                    "start": 1589,
                    "end": 1593,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "text": "[24]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 1975,
                    "end": 1979,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1992,
                    "end": 1996,
                    "text": "[46]",
                    "ref_id": "BIBREF54"
                },
                {
                    "start": 2302,
                    "end": 2306,
                    "text": "[47]",
                    "ref_id": "BIBREF55"
                },
                {
                    "start": 2309,
                    "end": 2313,
                    "text": "[48]",
                    "ref_id": "BIBREF56"
                }
            ],
            "ref_spans": [],
            "section": "VII. RELATED WORK"
        },
        {
            "text": "The privacy model in [49] uses the notion of zeroknowledge proof to define privacy model. However, there is seldom way to achieve the model for general query functions. The outlier privacy model [50] and the crowd-blending privacy [51] are both different from the information privacy model.",
            "cite_spans": [
                {
                    "start": 21,
                    "end": 25,
                    "text": "[49]",
                    "ref_id": "BIBREF57"
                },
                {
                    "start": 195,
                    "end": 199,
                    "text": "[50]",
                    "ref_id": "BIBREF59"
                },
                {
                    "start": 231,
                    "end": 235,
                    "text": "[51]",
                    "ref_id": "BIBREF61"
                }
            ],
            "ref_spans": [],
            "section": "VII. RELATED WORK"
        },
        {
            "text": "From the above discussions, we can conclude that the information privacy model is the best to be consistent with all of differential privacy, the dependent sources problem and information theory.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VII. RELATED WORK"
        },
        {
            "text": "The results of this paper show that the information theory is very suitable to model the dependent sources problem in differential privacy. This should be natural since information theory has very mature tools to capture the meaning of dependence of one thing to others. The results also shows that the mutual information inequality I(X i , Y ) \u2264 \u01eb and the related inequalities in Proposition 1 are very suitable to define privacy models.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. CONCLUSION"
        },
        {
            "text": "The results in Section IV and in Section V show that the information privacy model inherits almost all of the mechanisms of differential privacy model. This implies that there are efficient and (relatively) high-utility algorithms to achieve the model. The information privacy model has its own drawbacks: does not satisfy the composition privacy property and the group privacy property in general as shown in Proposition 6 and Proposition 3. However, differentially private mechanisms would ensure the two properties of information privacy model in some extent as shown in Corollary 4 and Corollary 3. This implies that the composition of differential privacy and information privacy would be helpful to absorb each other's advantages and to eliminate each other's disadvantages.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. CONCLUSION"
        },
        {
            "text": "Furthermore, it is currently unknown whether the information privacy model resists other kinds of attacks and whether it introduces other privacy leakage. In any case, we believe that the dependent sources problem will greatly deepen the connection between information theory and differential privacy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIII. CONCLUSION"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Towards a methodology for statistical disclosure control",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dalenius",
                    "suffix": ""
                }
            ],
            "year": 1977,
            "venue": "Statistik Tidskrift",
            "volume": "5",
            "issn": "2-1",
            "pages": "429--444",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Big data: The next frontier for innovation, competition, and productivity",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Manyika",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Chui",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bughin",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dobbs",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Roxburgh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "H"
                    ],
                    "last": "Byers",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Big Data: Seizing Opportunities, Preserving Values",
            "authors": [],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "I have a dream! (differentially private smart metering)",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "\u00c1cs",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Castelluccia",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Information Hiding -13th International Conference",
            "volume": "",
            "issn": "",
            "pages": "118--132",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-24178-9_9"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Composition attacks and auxiliary information in data privacy",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Ganta",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Kasiviswanathan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "265--273",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Robust de-anonymization of large sparse datasets",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Narayanan",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Shmatikov",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "2008 IEEE Symposium on Security and Privacy (S&P 2008)",
            "volume": "",
            "issn": "",
            "pages": "111--125",
            "other_ids": {
                "DOI": [
                    "10.1109/SP.2008.33"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Privacy-Preserving Data Mining -Models and Algorithms, ser. Advances in Database Systems",
            "authors": [],
            "year": 2008,
            "venue": "",
            "volume": "34",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/978-0-387-70992-5"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Privacypreserving data publishing: A survey of recent developments",
            "authors": [
                {
                    "first": "B",
                    "middle": [
                        "C M"
                    ],
                    "last": "Fung",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "ACM Comput. Surv",
            "volume": "42",
            "issn": "4",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/1749603.1749605"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "The algorithmic foundations of differential privacy",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dwork",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Roth",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Foundations and Trends in Theoretical Computer Science",
            "volume": "9",
            "issn": "3-4",
            "pages": "211--407",
            "other_ids": {
                "DOI": [
                    "10.1561/0400000042"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "The complexity of differential privacy",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vadhan",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "No free lunch in data privacy",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kifer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Machanavajjhala",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the ACM SIGMOD International Conference on Management of Data",
            "volume": "",
            "issn": "",
            "pages": "193--204",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/1989323.1989345"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Membership privacy: a unifying framework for privacy definitions",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "H"
                    ],
                    "last": "Qardaji",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "ACM SIGSAC Conference on Computer and Communications Security, CCS'13",
            "authors": [],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "889--900",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2508859.2516686"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Bayesian differential privacy on correlated data",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sato",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Nakagawa",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data",
            "volume": "",
            "issn": "",
            "pages": "747--762",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Dependence makes you vulnerable: Differential privacy under dependent tuples",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Mittal",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chakraborty",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "23nd Annual Network and Distributed System Security Symposium, NDSS 2016",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Correlated differential privacy: Hiding information in non-iid data set",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Information Forensics and Security",
            "volume": "10",
            "issn": "2",
            "pages": "229--242",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Differential privacy",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dwork",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "ICALP",
            "volume": "",
            "issn": "2",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "A firm foundation for private data analysis",
            "authors": [],
            "year": 2011,
            "venue": "Commun. ACM",
            "volume": "54",
            "issn": "1",
            "pages": "86--95",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Pufferfish: A framework for mathematical privacy definitions",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kifer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Machanavajjhala",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ACM Trans. Database Syst",
            "volume": "39",
            "issn": "1",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2514689"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Differential identifiability",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Clifton",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12",
            "volume": "",
            "issn": "",
            "pages": "1041--1049",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2339530.2339695"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "A rigorous and customizable framework for privacy",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kifer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Machanavajjhala",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proceedings of the 31st ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS 2012",
            "volume": "",
            "issn": "",
            "pages": "77--88",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2213556.2213571"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Correlated network data publication via differential privacy",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "C M"
                    ],
                    "last": "Fung",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Yu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "C"
                    ],
                    "last": "Desai",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "VLDB J",
            "volume": "23",
            "issn": "4",
            "pages": "653--676",
            "other_ids": {
                "DOI": [
                    "10.1007/s00778-013-0344-8"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Calibrating noise to sensitivity in private data analysis",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dwork",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Mcsherry",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nissim",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "D"
                    ],
                    "last": "Smith",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Theory of Cryptography, Third Theory of Cryptography Conference",
            "volume": "",
            "issn": "",
            "pages": "265--284",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "On the relation between identifiability, differential privacy, and mutual-information privacy",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Ying",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Information Theory",
            "volume": "62",
            "issn": "9",
            "pages": "5018--5029",
            "other_ids": {
                "DOI": [
                    "10.1109/TIT.2016.2584610"
                ]
            }
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Differential privacy as a mutual information constraint",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cuff",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security",
            "volume": "",
            "issn": "",
            "pages": "43--54",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2976749.2978308"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "A Mathematical Theory of Communication",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "E"
                    ],
                    "last": "Shannon",
                    "suffix": ""
                }
            ],
            "year": 1948,
            "venue": "The Bell System Technical Journal",
            "volume": "27",
            "issn": "3",
            "pages": "379--423",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Elements of information theory",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Cover",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Thomas",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "The geometry of differential privacy: the sparse and approximate cases",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Nikolov",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Talwar",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Symposium on Theory of Computing Conference, STOC'13",
            "volume": "",
            "issn": "",
            "pages": "351--360",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2488608.2488652"
                ]
            }
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "The matrix mechanism: optimizing linear counting queries under differential privacy",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Miklau",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hay",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mcgregor",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Rastogi",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "VLDB J",
            "volume": "24",
            "issn": "6",
            "pages": "757--781",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Inferential privacy guarantees for differentially private mechanisms",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of the 2017 ACM Conference on Innovations in Theoretical Computer Science",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Privacy against statistical inference",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Du Pin Calmon",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Fawaz",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "50th Annual Allerton Conference on Communication, Control, and Computing",
            "volume": "",
            "issn": "",
            "pages": "1401--1408",
            "other_ids": {
                "DOI": [
                    "10.1109/Allerton.2012.6483382"
                ]
            }
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Privacy-utility tradeoff under statistical uncertainty",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Makhdoumi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Fawaz",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "51st Annual Allerton Conference on Communication, Control, and Computing",
            "volume": "",
            "issn": "",
            "pages": "1627--1634",
            "other_ids": {
                "DOI": [
                    "10.1109/Allerton.2013.6736724"
                ]
            }
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "Blowfish privacy: tuning privacy-utility trade-offs using policies",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Machanavajjhala",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Conference on Management of Data",
            "volume": "",
            "issn": "",
            "pages": "1447--1458",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2588555.2588581"
                ]
            }
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Representing and querying correlated tuples in probabilistic databases",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Deshpande",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proceedings of the 23rd International Conference on Data Engineering",
            "volume": "",
            "issn": "",
            "pages": "596--605",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Privtree: A differentially private algorithm for hierarchical decompositions",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference",
            "volume": "",
            "issn": "",
            "pages": "155--170",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2882903.2882928"
                ]
            }
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Differentially private sequential data publication via variable-length n-grams",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "\u00c1cs",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Castelluccia",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ACM Conference on Computer and Communications Security",
            "volume": "",
            "issn": "",
            "pages": "638--649",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Publishing set-valued data via differential privacy",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Mohammed",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "C M"
                    ],
                    "last": "Fung",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "C"
                    ],
                    "last": "Desai",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Xiong",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Differentially private data release for data mining",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Mohammed",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "C M"
                    ],
                    "last": "Fung",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "KDD",
            "volume": "",
            "issn": "",
            "pages": "493--501",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "From online behaviors to offline retailing",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Luo",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Shen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "175--184",
            "other_ids": {
                "DOI": [
                    "http:/doi.acm.org/10.1145/2939672.2939683"
                ]
            }
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "k-anonymity: A model for protecting privacy",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Sweeney",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems",
            "volume": "10",
            "issn": "5",
            "pages": "557--570",
            "other_ids": {
                "DOI": [
                    "10.1142/S0218488502001648"
                ]
            }
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Communication theory of secrecy systems",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "E"
                    ],
                    "last": "Shannon",
                    "suffix": ""
                }
            ],
            "year": 1949,
            "venue": "The Bell System Technical Journal",
            "volume": "28",
            "issn": "4",
            "pages": "656--715",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "How to fool an unbounded adversary with a short key",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Russell",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Advances in Cryptology -EUROCRYPT 2002, International Conference on the Theory and Applications of Cryptographic Techniques",
            "volume": "",
            "issn": "",
            "pages": "133--148",
            "other_ids": {
                "DOI": [
                    "10.1007/3-540-46035-7_9"
                ]
            }
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "The Foundations of Cryptography",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Goldreich",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "2",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF49": {
            "ref_id": "b49",
            "title": "Networks, Crowds, and Markets -Reasoning About a Highly Connected World",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "A"
                    ],
                    "last": "Easley",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "M"
                    ],
                    "last": "Kleinberg",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF50": {
            "ref_id": "b50",
            "title": "Complex Graphs and Networks (Cbms Regional Conference Series in Mathematics)",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chung",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF52": {
            "ref_id": "b52",
            "title": "Relationship privacy: output perturbation for queries with joins",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Rastogi",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hay",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Miklau",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Suciu",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the Twenty-Eigth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems",
            "volume": "",
            "issn": "",
            "pages": "107--116",
            "other_ids": {}
        },
        "BIBREF54": {
            "ref_id": "b54",
            "title": "Information-theoretic foundations of differential privacy",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Mir",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Foundations and Practice of Security -5th International Symposium, FPS 2012",
            "volume": "",
            "issn": "",
            "pages": "374--381",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-37119-6_25"
                ]
            }
        },
        "BIBREF55": {
            "ref_id": "b55",
            "title": "Information-theoretic bounds for differentially private mechanisms",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Barthe",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "K\u00f6pf",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of the 24th IEEE Computer Security Foundations Symposium",
            "volume": "",
            "issn": "",
            "pages": "191--204",
            "other_ids": {
                "DOI": [
                    "10.1109/CSF.2011.20"
                ]
            }
        },
        "BIBREF56": {
            "ref_id": "b56",
            "title": "The limits of two-party differential privacy",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mcgregor",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Mironov",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Pitassi",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Reingold",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Talwar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Vadhan",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "FOCS",
            "volume": "",
            "issn": "",
            "pages": "81--90",
            "other_ids": {}
        },
        "BIBREF57": {
            "ref_id": "b57",
            "title": "Towards privacy for social networks: A zero-knowledge based definition of privacy",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gehrke",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Lui",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pass",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Theory of Cryptography -8th Theory of Cryptography Conference",
            "volume": "",
            "issn": "",
            "pages": "432--449",
            "other_ids": {}
        },
        "BIBREF59": {
            "ref_id": "b59",
            "title": "Outlier privacy",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Lui",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pass",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Theory of Cryptography -12th Theory of Cryptography Conference",
            "volume": "",
            "issn": "",
            "pages": "277--305",
            "other_ids": {}
        },
        "BIBREF61": {
            "ref_id": "b61",
            "title": "Crowd-blending privacy",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gehrke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hay",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Lui",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pass",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Advances in Cryptology -CRYPTO 2012 -32nd",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Group information privacy). Let X = (X 1 , . . . , X n ) be the sources. For non-empty set I = {i 1 , . . . , i \u2113 } \u2286 [n], let [n] \u2212 I = {i \u2113+1 , .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": ", in which one can let thosePr[M(xi,x (i) )=r] Pr[M(x \u2032 i ,x (i) )=r] be larger than e \u01eb if their corresponding Pr[X (i) = x (i) ] are very small, and let those Pr[M(xi,x (i) )=r] Pr[M(x \u2032 i ,x (i) )=r] be smaller than e \u01eb if the corresponding Pr[X (i) = x (i) ] are large, but the equation (12) still holds. Proposition 5 has the similar problem.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "The equation (17) denotes the extreme values of the left side of the equation (8) when the sources are independent. The equation (18) denotes the extreme values of the left side of the equation (8) when the sources are dependent. Note that the bounds of the above two inequalities are reachable by Lemma 5. We can find that the extreme values in the equation",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Let X = (X 1 , . . . , X n ) be the sources. Assume that, for any one source X i , there are at least n \u2212 k sources in X which are independent to X i . Let I, X I , X (I) , X I , X(I) , x I , x (I) be as shown in Definition 4. For any I and any x (I) \u2208 X (I) , set a x (I) = min Pr[X (I) = x (I) |X I = x I ], Pr[X (I) = x (I) |X i = x \u2032 I ]",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "All of the claims in theorem 1, Proposition 4, Proposition 5 and Theorem 2 hold for the stochastic processes in Definition 5.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Let the notations be as shown in Definition 5. Assume the T random vectors (X 1 , Y 1 ), . . . , (X T , Y T ) are independent. Then, for each i \u2208 [n], I(X i , Y ) = t\u2208T I(X t i , Y t ) and, for any x i and any r, there is Pr[",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "(i,I) \u2208X (i,I) Pr[M(xi,I ,x (i,I) )=r] Pr[X (i,I) =x (i,I) |Xi,I =xi,I ] x (i,I) \u2208X (i,I) Pr[M(x \u2032 i,I ,x (i,I) )=r] Pr[X (i,I) =x (i,I) |Xi,I =x \u2032 i,I ] \u2264 e \u01eb to define privacy metric, where I \u2286 [n] \\ {i},X i,I = (X i , X I ), x i,I = (x i , x I ), x \u2032 i,I = (x \u2032 i , x I",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "we prove the claim by showing Pr[XI =xI ,Y =r] Pr[XI =xI ] Pr[Y =r] \u2192 \u221e. For simplicity, we only prove the case of n = \u2113 = 2. Other cases can be treated similarly. Choosing t 1",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "employ either the mutual information I(X; Y ) or the related quantity Pr[Y =r,X=x] Pr [Y =r] Pr[X=x] to define privacy notions. However, as discussed in Section III, both of the two inequalities I(X; Y ) \u2264 \u01eb, Pr[Y =r,X=x] Pr [Y =r] Pr[X=x]",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "[Xi=xi|Y =r] Pr[Xi=xi]",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Note that we can define information-theoretic variants about the Bayesian differential privacy model, i.e., max i,I,xi,xI ,r Pr[Y =r,Xi=xi|XI =xI ] Pr[Y =r|XI =xI ] Pr[Xi=xi|XI =xI ]",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}