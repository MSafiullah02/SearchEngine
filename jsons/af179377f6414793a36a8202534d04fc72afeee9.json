{
    "paper_id": "af179377f6414793a36a8202534d04fc72afeee9",
    "metadata": {
        "title": "Tracking COVID-19 using online search",
        "authors": [
            {
                "first": "Vasileios",
                "middle": [],
                "last": "Lampos",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University College London",
                    "location": {}
                },
                "email": "v.lampos@ucl.ac.uk"
            },
            {
                "first": "Simon",
                "middle": [],
                "last": "Moura",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University College London",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Elad",
                "middle": [],
                "last": "Yom-Tov",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Microsoft Research",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Michael",
                "middle": [],
                "last": "Edelstein",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Public Health England",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Maimuna",
                "middle": [],
                "last": "Majumder",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Harvard Medical School",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Yohhei",
                "middle": [],
                "last": "Hamada",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Institute for Global Health",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Molebogeng",
                "middle": [
                    "X"
                ],
                "last": "Rangaka",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Institute for Global Health",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Rachel",
                "middle": [
                    "A"
                ],
                "last": "Mckendry",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University College London",
                    "location": {}
                },
                "email": ""
            },
            {
                "first": "Ingemar",
                "middle": [
                    "J"
                ],
                "last": "Cox",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University College London",
                    "location": {}
                },
                "email": ""
            }
        ]
    },
    "abstract": [],
    "body_text": [
        {
            "text": "Online search data is routinely used to monitor the nationwide prevalence of infectious diseases, such as influenza [1] [2] [3] [4] [5] . Previous work has focused on supervised learning solutions, where ground truth information, in the form of historical syndromic surveillance reports, can be used to train machine learning models. However, no sufficient data -in terms of validity, representativeness, and time span-exist to apply such approaches for monitoring the emerging COVID-19 infectious disease pandemic caused by a novel coronavirus (SARS-CoV-2). Therefore, unsupervised, or semi-supervised solutions should be sought. Recent outcomes have shown that it is possible to transfer an online search based model for influenza-like illness (ILI) from a source to a target country without using ground truth data for the target location 6 . The transferred model's accuracy depends on identifying the correct search queries and corresponding weights via a transfer learning methodology. In this work, we draw a parallel to previous findings and attempt to develop an unsupervised model for COVID-19 by: (i) carefully choosing search queries that refer to related symptoms as identified by a survey from the National Health Service (NHS) in the United Kingdom (UK), and (ii) weighting them based on their reported ratio of occurrence in people infected by COVID-19. Furthermore, understanding that online searches can also be driven by concern rather than infections, we attempt to minimise this part of the signal by incorporating a basic news media coverage metric. In addition, we propose a transfer learning method for mapping supervised COVID-19 models from a country to another, in an effort to transfer noisy knowledge from areas that are ahead in the epidemic curve. Finally, we conduct a correlation and regression analysis to uncover potentially useful online search queries in relation to confirmed COVID-19 cases. Results are presented for the UK, United States of America (US), Australia, Canada, France, Italy, Greece, and South Africa.",
            "cite_spans": [
                {
                    "start": 116,
                    "end": 119,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 120,
                    "end": 123,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 124,
                    "end": 127,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 128,
                    "end": 131,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 132,
                    "end": 135,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 842,
                    "end": 843,
                    "text": "6",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Google search. Google search data is obtained from the Google Health Trends API, a non public API created by Google for research on health-related topics. Data represent daily online search query frequencies for specific areas of interest. Query frequencies are defined as the sum of search sessions that include a target search term divided by the total number of search sessions (for a day and area of interest). 1 We have obtained data from September 30, 2011 to April 9, 2020 for the UK, US, Australia, Canada, France, Italy, Greece, and South Africa. The list of search terms is determined by COVID-related symptoms Figure 1 . Average daily news articles ratio about COVID-19 across all countries in our analysis and corresponding confidence intervals (two standard deviations above and below the mean). and keywords. For each country, we mainly used queries in its native language(s). 2 News media volume. We are using an extensive global news corpus to extract news media coverage trends for COVID-19 in all the countries of our study. This is estimated by counting the proportion of articles mentioning a COVID-19 related term. In particular, daily counts of total news media articles, and the subset that included at least one relevant keyword anywhere in the body of the text were collected from the MediaCloud database 3 via national corpora for the UK (93), US (225), Australia (61), Canada (79), France (360), Italy (178), Greece (75), and South Africa (135), where in the parentheses we state number of media sources considered per country. These counts were collected from September 30, 2019 through April 9, 2020, based on the following keywords:",
            "cite_spans": [
                {
                    "start": 415,
                    "end": 416,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 621,
                    "end": 629,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Data"
        },
        {
            "text": "\u2022 \u1fef\u03ba\u03bf\u03c1\u03bf\u03bd\u03bf\u03ca\u03cc\u03c2\u0384, \u1fef\u03ba\u03bf\u03c1\u03bf\u03bd\u03bf\u03ca\u03bf\u03cd\u0384, \u1fef\u03ba\u03bf\u03c1\u03c9\u03bd\u03bf\u03ca\u03cc\u03c2\u0384, \u1fef\u03ba\u03bf\u03c1\u03c9\u03bd\u03bf\u03ca\u03bf\u03cd\u0384, \u1fef\u03ba\u03bf\u03c1\u03c9\u03bd\u03bf\u03ca\u03bf\u03af\u0384, \u1fef\u03ba\u03bf\u03c1\u03bf\u03bd\u03bf\u03ca\u03bf\u03af\u0384, 'covid', 'covid-19', 'covid 19', 'covid19', 'coronavirus', and 'ncov' for Greece, and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "\u2022 'covid', 'covid-19', 'covid 19', 'covid19', 'coronavirus', 'ncov' for the rest of the countries. Figure 1 depicts the average daily ratio across all countries, as soon as it started being above zero (beginning of week 3, 2020), with two standard deviations as confidence intervals. There exists a distinctive pattern of (exponential) increase and, more recently, of a slowly decreasing trend. In addition, we observe a certain variability across locations and/or time periods, that adds to the potential value of this signal.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 99,
                    "end": 107,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "Data"
        },
        {
            "text": "COVID-19 symptoms. We used data from the NHS first few hundred (FF100) symptom questionnaire based on people who have contracted SARS-CoV-2. FF100 provides a probability for each identified symptom.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "Aggregated confirmed COVID-19 cases. The numbers of confirmed COVID-19 cases on a daily basis for the UK and England are obtained from PHE. 4 For the rest of the locations we obtain daily confirmed cases data from the European Centre for Disease Prevention and Control (ECDC). 5",
            "cite_spans": [
                {
                    "start": 140,
                    "end": 141,
                    "text": "4",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Data"
        },
        {
            "text": "Unsupervised symptom-based online search model for COVID-19. We generate k symptom-based search query groups using the k identified symptoms from the FF100 NHS questionnaire for COVID-19 (k = 18). In a separate model, we also consider two additional groups one referring to the symptom of anosmia, 6 and another that includes specific COVID-19 terminology, i.e. the \"covid-19\" keyword itself among others. Query groups may include different wordings for the same symptom or queries with minor grammatical differences (especially for queries in Greek and French). If a symptom is represented by more than one search query, then we obtain the total frequency (sum) across these queries. Query group time series are smoothed using a harmonic mean over the past 14 days (see Eq. 11 for a definition of the harmonic mean), and any trends across the entire period of the analysis are removed using linear detrending. We then apply a min-max normalisation to the frequency time series of each query group to obtain a balanced representation between more and less frequent searches. We divide our data into two periods of interest, the current one (from September 30, 2019 until April 9, 2020) and a historical one (from September 30, 2011 to September 29, 2019). The corresponding data sets are denoted by X \u2208 R N 1 \u00d7k \u22650 and H \u2208 R N 2 \u00d7k \u22650 , where N 1 , N 2 represent the different numbers of days in the current and historical data, respectively. We use the symptom conditional probability distribution from the FF100 to assign weights (w \u2208 R k \u22650 ) to each query category, and compute weighted time series (x = Xw, h = Hw), which are subsequently divided by the sum of w (weighted average). For the historical data, we divide their time span into yearly periods, and compute an average time series trend, h \u00b5 , using two standard deviations as upper and lower confidence intervals.",
            "cite_spans": [
                {
                    "start": 298,
                    "end": 299,
                    "text": "6",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Minimising the effect of news media using confirmed COVID-19 cases. On any given day the proportion of news articles about the COVID-19 pandemic is m \u2208 [0, 1], and the weighted score of symptom-related online searches (see previous paragraph) is equal to g; we can apply a min-max normalisation so that g \u2208 [0, 1] as well. We hypothesise that g incorporates two signals based on infected (g p ) and concerned (g c ) users, respectively, i.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Then, there exists a constant \u03b3 \u2208 [0, 1] such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "We apply ordinary least squares (OLS) regression to learn a mapping from g and m to the actual number of confirmed infections, d, per day. For a meaningful interpretation of the regression's weights, d is also min-max normalised, i.e. such that d \u2208 [0, 1].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "In particular, at each day, we use the previous N days (including the current one) to optimise arg min",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "where a 1 and a 2 \u2208 R denote the weights of the online search and news signals, respectively. If a 1 > 0 and a 2 < 0, we can then hypothesise that the negative component coming from the media (a 2 m) is approximately equal to the unwanted component of the online search signal that is related to concern, i.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Solving this for \u03b3, we get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Now, if a 1 > 0 and a 2 > 0, we can adjust for the relative contribution from the media by directly solving the equation d = a 1 g + a 2 m = \u03b3g, which results to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "In the rare case that a 2 is set to a positive number that is close to zero (i.e. a 2 \u2264 .01), we set \u03b3 = 1, as the impact coming from the news media signal is negligible. If a 1 \u2264 0, our current approach does not attempt to interpret this further, and therefore we also set \u03b3 = 1, meaning that we consider the signal from the online search data in its entirety. Valid values for \u03b3 are thresholded so that \u03b3 is always in [0, 1].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Using the above approach, we can learn a different \u03b3 per day, and use g p = \u03b3g as our unsupervised (or semi-supervised in this case) online search signal, attempting to minimise the impact of news in a dynamic fashion.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Methods"
        },
        {
            "text": "Minimising the effect of news media using autoregression. The previously described method requires a form of ground truth in order to be applicable, i.e. confirmed COVID-19 cases that are represented by variable d. However, confirmed cases may not be a population representative statistic given that in most countries tests are not yet conducted at the community level. To alleviate the effect of using potentially inaccurate information, we also obtain an estimate for \u03b3 using only the time series of g (online search score) and m (news media COVID-19 ratio). 7 The rationale of this approach is similar to the logic behind a Granger causality test 7 . First, we train a linear autoregressive (AR) model for forecasting the online search score at a time point (day) t, g t , using its previous values; this is denoted by AR(g). We also train a linear AR model with the same forecasting target, but an expanded space of observations that includes current (m t ) and previous (e.g. m t\u22121 ) values of the news articles ratio; this is denoted by AR(g, m). We then use the relative error difference of the two models in forecasting g t , as our \u03b3 for time point t. In particular, we first solve arg min",
            "cite_spans": [
                {
                    "start": 561,
                    "end": 562,
                    "text": "7",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "3/10"
        },
        {
            "text": "to obtain a pair of weights (w) and an intercept term (b 1 ) for AR(g). We use 2 lags (past values) to keep the complexity of the task tractable given the small amount of samples at our disposal (N). We then solve arg min",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/10"
        },
        {
            "text": "to obtain the weights ([w; v]) and an intercept term (b 2 ) for AR(g, m). Using both models, we forecast the next (unseen) value of g, which following the notation in the equations above is\u011d t+1 , and compute the absolute error from its known true value, g t+1 . This yields errors, \u03b5 1 and \u03b5 2 for AR(g) and AR(g, m), respectively. If \u03b5 1 < \u03b5 2 , then the news media signal does not help to improve the accuracy of AR(g), and hence we assume that it does not affect the online searches. Otherwise, we estimate its effect to be represented by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/10"
        },
        {
            "text": "After obtaining a time series of \u03b3's for the all days in our analysis, we smooth each one of them using a harmonic mean over the values of the previous 6 days (or 7 days including the day of focus).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/10"
        },
        {
            "text": "Transferring supervised COVID-19 models to different countries. Previous work has shown that it is possible to transfer a model for seasonal flu, based on online search query frequency time series, from one country that has access to historical syndromic surveillance data to another that has not 6 . Here, we adapt this method to transfer a model for COVID-19 from a source country where the disease spread has progressed significantly to a target country that is still in earlier stages of the epidemic curve. The rationale for this is that a supervised model based on data from the source country might be able to capture the disease dynamics better. The steps and data transformations that are required to apply this technique are detailed below. Search query frequency time series are denoted by S \u2208 R",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/10"
        },
        {
            "text": "and T \u2208 R M\u00d7n T",
            "cite_spans": [],
            "ref_spans": [],
            "section": "M\u00d7n S \u22650"
        },
        {
            "text": ", for the source and target countries respectively; M denotes the number of days considered, and n S , n T the number of queries for the two locations. As these time series are quite volatile for some locations in our study, something that does not help in cross-location mapping of the data, we have smoothed them using a harmonic query frequency mean based on a window of the D past days. More specifically, a smoothed search query frequency s i for a day i is equal to:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u22650"
        },
        {
            "text": "where x (\u00b7) denotes the raw (non smoothed) search query frequency. We train an elastic net model on data from the source location 8 , similarly to previous work on ILI 2, 4, 9 . In particular, we solve the following optimisation task arg min",
            "cite_spans": [
                {
                    "start": 168,
                    "end": 177,
                    "text": "2, 4, 9 .",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "\u22650"
        },
        {
            "text": "where y \u2208 R M denotes the daily number of confirmed COVID-19 cases in the source location, \u03bb 1 , \u03bb 2 \u2208 R >0 are the 1 -and 2 -norm regularisation parameters, and w \u2208 R n S , b \u2208 R denote the query weights and regression intercept, respectively. Prior to deploying elastic net, we apply a min-max normalisation on both S and y. We fix the ratio of \u03bb 's, and then train q models for different values of \u03bb 1 . All different regression models represented by the columns of W \u2208 R n S \u00d7q , and the elements of b \u2208 R q , are used as an ensemble for a more inclusive transfer that combines various source models with different sparsity levels.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u22650"
        },
        {
            "text": "To generate an equivalent feature space for the target location (same dimensionality, similar feature attributes), we first identify query group pairs between the source and the target location using the symptom categories in the NHS FF100 questionnaire. We map a source query to the target query from the same symptom category that maximises their Pearson correlation based on their frequency time series. To do this more effectively, prior to computing correlations, we shift the data by z days (looking at a maximum window of 60 days backwards or forwards) so that the average correlation between search query frequencies in S and T are maximised; S here denotes a subset of S that includes only the search queries that have been assigned a non zero weight by the elastic net (Eq. 12). If no target search query exists for a certain symptom category, we use the best correlated one from all target queries available (irrespectively of the symptom category) as its mapping. After this process, we end up with a subset Z \u2208 R M\u00d7n S of the target feature space T. Notably, Z does not necessarily hold data for n S distinct queries as different source queries may have been mapped to the same target query. Z is subsequently normalised using min-max. To make both feature spaces (S, Z) numerically compatible we scale the latter based on their mean, column-wise (per search query) ratio r \u2208 R n S \u22650 , i.e. Z S = Z r. Now, we can deploy the ensemble source models to the target space, making multiple inferences (for different \u03bb 1 values) held in Y \u2208 R n S \u00d7q :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u22650"
        },
        {
            "text": "We then reverse the min-max normalisation for each one of the inferred time series (columns of Y) using values from the source model's ground truth y (prior to its normalisation). Finally, we compute the mean of the ensemble (across the rows of Y) as our target estimate, and also use two standard deviations to form a 95% confidence interval.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u22650"
        },
        {
            "text": "Correlation and regression analysis. The relationship of search frequency time series and confirmed cases can uncover symptoms or behaviours related to COVID-19. However, since confirmed cases data may not be representative of community level disease rates, looking at this relationship separately for each country might produce misleading outcomes. To mitigate this to the extent possible, we combine the data from C countries and produce an aggregate set of query frequencies, Z \u03b1 \u2208 R CM\u00d7n , where M, n denote the considered days and search queries, respectively. We denote the aggregated daily confirmed COVID-19 cases for these countries with y \u03b1 \u2208 R CM . Prior to the aggregation, we apply min-max normalisation on the query frequency, and confirmed cases time series separately for each country. Initially, we compute the Pearson correlation between the columns of Z \u03b1 and y \u03b1 . Correlation is an informative metric, but considers each search query in isolation. Therefore, we also perform a multivariate regression analysis to more rigorously estimate the impact of each search query in estimating confirmed cases. To do this, we apply elastic net regularised regression (see Eq. 12), training and testing models for the past K days. During each of the training phases, we use data up to and including the past k \u2212 1 days, and test only the K-th day (unseen); this results into daily test sets of size C (one value for each country). We explore elastic net's regularisation path to consider L models that maintain (by assigning a nonzero weight) up to a reasonable percentage of the features (e.g. 50%), so as a solution is not overfitting. We do this gradually, selecting first 1% of the features and moving towards the maximum considered percentage. In this experiment, we use the test set to identify the most accurate (in terms of mean squared error) model at each density level. For this model, we determine the impact of each one of the features (search queries) by considering both its frequency and allocated weight. The impact \u0398(\u00b7) of a query q is equal to",
            "cite_spans": [],
            "ref_spans": [],
            "section": "\u22650"
        },
        {
            "text": "where f t, j denotes the query frequency at time point (day) t and for country j, w ,t the corresponding weight at sparsity level , and\u0177 ,t, j the respective estimated confirmed cases. Impacts are summed across all the considered days, and model densities, and normalised at the end by the sum of all the corresponding COVID-19 case estimates. 8 These normalised impacts are used to inform our regression analysis.",
            "cite_spans": [
                {
                    "start": 344,
                    "end": 345,
                    "text": "8",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "\u22650"
        },
        {
            "text": "The current online search based scores for COVID-19 in 8 nations are depicted in Figures 2 and 3 (data up to April 9, 2020). For a better visualisation, all time series are smoothed using a 7-point moving average, 3 days prior and after each point.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 81,
                    "end": 96,
                    "text": "Figures 2 and 3",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Results"
        },
        {
            "text": "Online search based scores for COVID-19 related symptoms as identified by the NHS FF100 survey for 8 nations up to and including April 9, 2020. Query frequencies are weighted by symptom frequency as described in Methods (blue line).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Figure 2."
        },
        {
            "text": "We have also included estimates after minimising news media effects using data from a global news media corpus (black line). These scores are compared with an average 8-year trend of the weighted model (dashed line) and its corresponding confidence intervals (shaded area). For a better visualisation all time series are smoothed using a 7-point moving average. Figure 2 shows scores based on symptom-related query frequencies that are weighted by the actual symptom probability as reported in the NHS FF100 survey for COVID-19. Expanding on this, Figure 3 shows scores when search queries that are about the symptom of anosmia as well as strictly about COVID-19 are added as additional query groups. We set the weight of the anosmia symptom category to 0.4 (2 in 5 cases), as we wait for confirmation from an expert analysis. The weight of the strictly COVID-19 related queries is set equal to 1. The rationale behind including the latter category is that by now (and added query categories (Fig. 3) increase the maximum scores per country, but do not affect the overall trend.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 362,
                    "end": 370,
                    "text": "Figure 2",
                    "ref_id": null
                },
                {
                    "start": 548,
                    "end": 556,
                    "text": "Figure 3",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 992,
                    "end": 1000,
                    "text": "(Fig. 3)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Figure 2."
        },
        {
            "text": "Looking at the scores where we have attempted to minimise the effect of news (black lines) using the autoregressive approach (note that we use the previous N = 56 days to determine this; see Eqs. 8, 9, and 10), we observe more conservative estimates in all locations, including a recent drop or an altered trend (e.g. increasing vs. decreasing) in some of them (e.g. the US). A detailed analysis of the observed trends is left for a later version of this manuscript as conclusions cannot be drawn before the (approximate) end of the first wave of the pandemic. Figure 4 showcases the outcome of an experiment where we trained a model for Italy and then transferred it to the rest of countries in our analysis. Italy was chosen as the source country because it is considered to be in front of the rest in terms of epidemic progression. During this experiment search query frequency time series were smoothed (as explained in Methods) using a harmonic mean of the past 14 days. We train 100,000 elastic net models for the source location, exploring the entire 1 -norm regularisation path. For all target countries, we transfer source models that activate (non zero weight) from 1% to 99% of the search queries (88,138 models in total). Interestingly, the mapped trends correspond sufficiently well to confirmed cases data in most countries taking into account the fact that they lack supervision at the target locations. Notably though, here the goal is not necessarily for the two trends (estimated and confirmed cases) to match, as the transferred models may be capable of capturing signals that are missed out by the current surveillance systems. The caveat of this approach is that it relies on the existence of a representative population sample of confirmed COVID-19 cases at the source country (in this case Italy). Otherwise, the transferred model will inherit the biases of the source model. Furthermore, the transfer learning approach itself requires a significant level of similarity in user search behaviour about COVID-19 between different countries, as explained in a similar attempt to transfer models for influenza-like illness 6 .",
            "cite_spans": [
                {
                    "start": 2142,
                    "end": 2143,
                    "text": "6",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 561,
                    "end": 569,
                    "text": "Figure 4",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Figure 2."
        },
        {
            "text": "Finally, we performed a correlation and regression analysis on online search frequency and COVID-19 confirmed cases time series as described in Methods. We aggregated data from 4 English speaking countries, namely the UK, US, Australia, and Canada, as their share the exact same search queries. We first estimated the Pearson correlation between queries and confirmed cases at all locations (in an aggregate fashion), from December 31, 2019 up to and including April 9, 2020. Results are depicted in Figure 5 (A). It is quite striking that the search query \"stay home\" comes up on top, which is not a symptom, but a behaviour associated with reducing the spread of the virus. Focusing on symptoms, we can also see that anosmia, loss of taste (ageusia), chest pain, headache show quite strong positive correlations. On the other hand symptoms such as migraine and vomiting show strong anticorrelation. For the regression analysis, we focused on the past 4 weeks, meaning that we trained models for and tested them on each day of that period. We considered models up to a 50% feature density (nonzero weights for half of the considered search queries), at which point we saw signs of overfitting. The outcomes of this analysis are depicted in Figure 5 (B). We see that the most impactful feature is search queries that include the term \"covid\", which is something expected given the magnitude of this pandemic. Anosmia, loss of taste, high temperature, headache, nasal congestion, breathing difficulty, chest tightness, and nausea are symptoms (in that order) that are also impactful from the perspective of search predictiveness. In addition, behaviours such as isolation, staying at home are also relevant.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 500,
                    "end": 508,
                    "text": "Figure 5",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 1241,
                    "end": 1249,
                    "text": "Figure 5",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Figure 2."
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Using Internet Searches for Influenza Surveillance",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "M"
                    ],
                    "last": "Polgreen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "M"
                    ],
                    "last": "Pennock",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "D"
                    ],
                    "last": "Nelson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Weinstein",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Clin. Infect. Dis",
            "volume": "47",
            "issn": "",
            "pages": "1443--1448",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Advances in nowcasting influenza-like illness rates using search query logs",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lampos",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Miller",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Crossan",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Stefansen",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Sci. Rep",
            "volume": "5",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Accurate Estimation of Influenza Epidemics using Google Search Data via ARGO",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Santillana",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Kou",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "PNAS",
            "volume": "112",
            "issn": "",
            "pages": "14473--14478",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Enhancing Feature Selection Using Word Embeddings: The Case of Flu Surveillance",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lampos",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Cox",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proc. of the 26th International Conference on World Wide Web",
            "volume": "",
            "issn": "",
            "pages": "695--704",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "The added value of online user-generated content in traditional methods for influenza surveillance",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lampos",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Cox",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pebody",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Sci. Rep",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Transfer Learning for Unsupervised Influenza-like Illness Models from Online Search Data",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lampos",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Cox",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proc. of the 28th International Conference on World Wide Web",
            "volume": "",
            "issn": "",
            "pages": "2505--2516",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Investigating causal relations by econometric models and cross-spectral methods",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "W J"
                    ],
                    "last": "Granger",
                    "suffix": ""
                }
            ],
            "year": 1969,
            "venue": "Econometrica",
            "volume": "37",
            "issn": "",
            "pages": "424--438",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Regularization and variable selection via the elastic net",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zou",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Hastie",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "J. Royal Stat. Soc.: Ser. B",
            "volume": "67",
            "issn": "",
            "pages": "301--320",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Assessing the impact of a health intervention via user-generated internet content",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lampos",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Yom-Tov",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Pebody",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "J"
                    ],
                    "last": "Cox",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Data Min. Knowl. Discov",
            "volume": "29",
            "issn": "",
            "pages": "1434--1457",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Online search based scores for COVID-19 related symptoms as identified by the NHS FF100 survey, in addition to queries about the symptom of anosmia, and a group of coronavirus-related terms, for 8 nations up to and including April 9, 2020. Query frequencies are weighted by symptom frequency as described in Methods (blue line). We have also included estimates after minimising news media effects using data from a global news media corpus (black line). These scores are compared with an average 8-year trend of the weighted model (dashed line) and its corresponding confidence intervals (shaded area). For a better visualisation all time series are smoothed using a 7-point moving average. perhaps at an earlier time point) people who experience COVID-19 related symptoms might search about the disease directly as its name(s) and associated symptoms are broadly known.Focusing on the weighted signal (blue lines), we observe exponentially increasing rates that exceed the estimated confidence intervals in most investigated countries. At the same time, we are also observing a recent drop of the score in all countries. The",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Transferring a supervised model for Italy to other countries in our analysis. The figures show an estimated confirmed cases trend (with confidence intervals) for all locations in our analysis (minus Italy) compared to the recorded confirmed cases as reported by PHE and the ECDC. Plot lines have been standardised, and then smoothed using a 3-point moving average.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Correlation and regression analysis of search query frequencies against confirmed COVID-19 cases in four countries (US, UK, Australia, and Canada). (A) Top-20 positively and top-10 negatively correlated search queries with COVID-19 confirmed cases; (B) Top-20 positively and top-10 negatively impactful queries in nowcasting COVID-19 confirmed cases.",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "V.L, S.M., I.J.C, and R.M. would like to acknowledge all levels of support from the EPSRC projects EP/K031953/1 (\"EPSRC IRC in Early-Warning Sensing Systems for Infectious Diseases\") and EP/R00529X/1 (\"i-sense: EPSRC IRC in Agile Early Warning Sensing Systems for Infectious Diseases and Antimicrobial Resistance\"). The authors would like to thank the NHS FF100 team for their effort in collecting symptom information for COVID-19. We would also like to thank Ettore Severi, Anna Odone, and Daniela Paolotti for assisting in the translation of search queries from English to Italian. Finally, V.L. would like to thank Sam J. Gilbert for interesting discussions and pointers during the development of this work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgements"
        }
    ]
}