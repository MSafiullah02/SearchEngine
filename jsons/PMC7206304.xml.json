{
    "paper_id": "PMC7206304",
    "metadata": {
        "title": "Attention-Based Graph Evolution",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Shuangfei",
                "middle": [],
                "last": "Fan",
                "suffix": "",
                "email": "sophia23@vt.edu",
                "affiliation": {}
            },
            {
                "first": "Bert",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "email": "bhuang@vt.edu",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "As a fundamental topic in graph modeling, graph generation has a long history that began as early as the 1950s [6]. However, most traditional methods rely on prior knowledge of the graph topology and are limited in capability of learning generative properties from observations. To solve this problem, researchers have recently been exploring trainable deep models for graph generation based on the effectiveness of graph neural networks\u2014e.g., graph convolutional networks [14]\u2014which have been applied to various kinds of data describing, for example, molecular chemicals for drug design and scientific publications for predicting citations [25, 31]. However, these approaches are unconditional generative models, which limits their control over the generating procedure and makes them unable to produce graphs in context. These limitations restrict the applicability of these approaches to real world settings where graphs transform from one state to another and evolve in dynamic network settings.",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 113,
                    "mention": "6",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 474,
                    "end": 476,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 642,
                    "end": 644,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 646,
                    "end": 648,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Modeling graph evolution is an important task that can be applied to various practical applications. A model of graph evolution would be a powerful tool for both predicting the future and the transformation of networks. For example, a marketer aiming to post an advertisement on an online social network may only have access to short-hop ego networks around users, but they need to know how the information would spread into the extended network beyond these ego networks. In disease control and prevention, when an infectious disease emerges and starts to spread, it is important to understand how it may spread beyond the visible network. Because graph data represents real-world phenomena that is changing or incompletely observed, there are many other examples of problems that could benefit from new tools for modeling graph evolution. Yet existing methods lack the flexibility of deep generative models or the ability to condition on previous graph states.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "To provide this missing capability, we introduce an attention-based graph evolution model (AGE). AGE is a model for conditional graph generation based on the attention mechanism that allows consideration of global information with parallel computation across all graph nodes. AGE adopts the encoder-decoder structure, where the encoder tries to learn the representation of conditioned graphs using a self-attention mechanism, and the decoder tries to generate the representation of the target graphs using the correlation with the conditioned graphs and also with itself. The decoder can thus capture both global and local information. This graph-conditioned generation framework greatly enriches the potential applications for graph generation. AGE can be used to model not only graph evolution in space and in time, but also the transformation between graphs from one state to another. To evaluate how AGE performs on this problem setting, we perform experiments on datasets in various areas. The experiment results in terms of both the evaluation metrics, show that AGE can not only generate extremely realistic graphs, but also has the strong ability to model the evolution of graphs as a powerful conditioned graph generative model.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Graph generation is one of the core topics in graph analysis. Many methods have been proposed to solve this problem, which can be traced back to at least 1959 when Erd\u00f6s and R\u00e9nyi [6] first introduced the Erd\u00f6s-R\u00e9nyi (E-R) model for generating random graphs. The model is based on the assumption that each pair of nodes are connected with a fixed pre-defined probability. However, this assumption is not realistic in most real world networks. To mimic the structure of real graphs, Albert and Barab\u00e1si [2] proposed the preferential attachment model by further customizing the probability of each possible edge to be conditioned on current degrees of nodes. Separately, Airoldi et al. [1] proposed the mixed-membership stochastic block model (MMSB) to generate graphs that have a fixed number of communities based on a probability matrix to determine the possibility of a node pair from two communities been connected. This model is able to learn distributions from observed data, which makes it generate more useful random graphs based on basic assumptions. Other classical graph generative models include exponential random graph models (ERGMs) [21, 26], the stochastic block model (SBM) [9], the Watts-Strogatz model [29], the Kronecker graph model [16], and many more. These older approaches have limited ability to learn about graph distributions from collections of graphs.",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 182,
                    "mention": "6",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 503,
                    "end": 504,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 685,
                    "end": 686,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1147,
                    "end": 1149,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1151,
                    "end": 1153,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1190,
                    "end": 1191,
                    "mention": "9",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1220,
                    "end": 1222,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1252,
                    "end": 1254,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "Recently, researchers also have proposed to use deep models to learn distributions for graph generation. These methods can be divided into two categories. Some of them are auto-regressive models, which generate the graph in a sequential manner. Examples of these are the DeepGMG model [18] and the GraphRNN model You et al. [31]. While some other methods are non-auto regressive models [22, 25]. Among them, many models are based on generative adversarial networks (GANs) [10], which learn data distributions without explicitly defining a density function [3, 5, 7]. However, these deep models are either limited to generating small graphs with less than thirty nodes [18, 25], or to generating specific types of graphs such as molecular graphs [5, 30]. More importantly, the overarching drawback of all these deep generative models is that they are unconditioned, which severely limits their applicability to real-world tasks.",
            "cite_spans": [
                {
                    "start": 286,
                    "end": 288,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 325,
                    "end": 327,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 387,
                    "end": 389,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 391,
                    "end": 393,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 473,
                    "end": 475,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 557,
                    "end": 558,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 560,
                    "end": 561,
                    "mention": "5",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 563,
                    "end": 564,
                    "mention": "7",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 669,
                    "end": 671,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 673,
                    "end": 675,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 746,
                    "end": 747,
                    "mention": "5",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 749,
                    "end": 751,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "To further strengthen the power of graph generative models, Fan and Huang [7] proposed a conditioned model, which can generate graphs conditioned on discrete labels based on the conditional GAN frameworks [19, 20]. However, this approach cannot be applied to circumstances where we want to generate graphs conditioned on another graph, which the motivating case for graph evolution and graph transformation. Also Jin et al. [12] proposed a model with the junction tree encoder-decoder framework for graph to graph transformation. However, they only target the task of molecular optimization. We largely expand the applications of conditioned graph generative model to various interesting problems for graph transformation, such as predicting the graph evolution in space (e.g., how ego-networks would look if expanded to a larger radius) and predicting graph evolution in time (forecasting the changes of dynamic graphs).\n",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 76,
                    "mention": "7",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 206,
                    "end": 208,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 210,
                    "end": 212,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 425,
                    "end": 427,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Related Work",
            "ref_spans": []
        },
        {
            "text": "In AGE, the encoder and the decoder each have their own self-attention block, which is designed to learn high-level node representations based on other nodes within the same graph. In the encoder, the representation of node i in source graphs is updated based on the following rule:1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{h}}_i^{t+1} = {\\varvec{h}}_i^t + \\sigma (\\sum _{j=1}^{N_{s}}a_{i,j}^t\\times {\\varvec{W}_s^{s}} {\\varvec{h}}_j^t), \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_{i,j}$$\\end{document} is the normalized weights the model learns between node \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_j$$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{h}_i$$\\end{document} is the hidden node feature of node i, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_s$$\\end{document} is the number of nodes in the source graph, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma $$\\end{document} is a nonlinear activation and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}_s^{s}}$$\\end{document} is the linear transformation where the weights are learnable parameters separately instantiated for each attention step in the model. The edge weights between two nodes are computed based on the attention mechanism:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} e_{i,j}^{t+1} = \\text {Attention} ({\\varvec{W}}_s{\\varvec{h}}_j^{t+1}, {\\varvec{W}_{s}^\\prime }{\\varvec{h}}_i^{t+1}), \\quad a_{i,j}^{t+1} = \\frac{\\text {exp}(e_{i,j}^{t+1})}{\\sum \\nolimits _{k=1}^{N_s}\\text {exp}(e_{k,j}^{t+1})}, \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_{i,j}$$\\end{document} is the normalized attention weight of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_{i,j}$$\\end{document}, which is the attention weight of edge from node i to node j, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}}_s$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{W}}_s^\\prime $$\\end{document} are linear transformations.",
            "cite_spans": [],
            "section": "Self Attention ::: Attention-Based Graph Evolution Model",
            "ref_spans": []
        },
        {
            "text": "To learn the correlations between the nodes in source graph and the ones to be generated by decoder, we apply a source-target (S-T) attention block after the self-attention operations. The representation of a predicted node j in generated graph is updated based on the learned embeddings of all nodes in the source graph using the following rule:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\varvec{h}}_j= {\\varvec{h}}_j + \\sigma \\left( \\sum _{i=1}^{N_s}a_{i,j}\\times {\\varvec{W}_{s}^{t}}{\\varvec{h}}_i\\right) , \\end{aligned}$$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma $$\\end{document} is a nonlinear activation function, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{W}_s^{t}$$\\end{document} is a learnable linear transformation and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$a_{i,j}$$\\end{document} is the normalized weights the model learned between node \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_i$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$v_j$$\\end{document}. The edge weights between two nodes in different graphs are typically calculated in the same way as shown in Eq. 2.",
            "cite_spans": [],
            "section": "Source-Target Attention ::: Attention-Based Graph Evolution Model",
            "ref_spans": []
        },
        {
            "text": "In AGE, the encoder takes in a source graph \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {sou}}$$\\end{document} represented by its initial representations \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {sou}} = [\\varvec{A}_s ; \\varvec{F}_s]$$\\end{document} (we can leave out \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{F}_s$$\\end{document} if it is not given) and maps it to a high-level embedding. Here \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{A}_s$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{F}_s$$\\end{document} are the adjacency matrix and the feature matrix of the source graph, where the nodes are arranged in a breadth-first-search (BFS) ordering. We concatenate the feature matrix if we have one. When available, we can use features to generate new node features in addition to the graph structure. In our experiments, we focus on undirected, unweighted graphs where the adjacency matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{A}$$\\end{document} is a symmetric binary matrix with each element represents the connectivity of a pair of nodes, but our approach can be easily extended to both directed and weighted graphs.",
            "cite_spans": [],
            "section": "Encoder ::: Attention-Based Graph Evolution Model",
            "ref_spans": []
        },
        {
            "text": "We use a fixed maximum number of nodes, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_s$$\\end{document} for the source graph and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N_t$$\\end{document} for the target graph. AGE can learn about and generate structures with various sizes smaller than these maximums by ignoring isolated nodes in the generated graph. We also define a fixed minimum number of nodes for both source and target graphs to ensure that the input graph is not empty, and to ensure that there are some differences between the source and target graphs.\n",
            "cite_spans": [],
            "section": "Encoder ::: Attention-Based Graph Evolution Model",
            "ref_spans": []
        },
        {
            "text": "The decoder is composed of several stacked attention modules that alternate self-attention and source-target attention layers. The input for the decoder includes two parts: the target graphs \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {tar}}$$\\end{document} and the learned embeddings \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}_s$$\\end{document} of the source graph (provided by the encoder). The target graph \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {tar}}$$\\end{document} is represented by the shifted node representations (shifted to the right by one position): \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ G_{\\text {tar}} = [\\varvec{A}_t ; \\varvec{F}_t]$$\\end{document} (leaving out \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{F}_t$$\\end{document} if it is not given), with a start token and an end token filled at the beginning and appended to the end to ensure that the decoder predicts the next node based on the previously generated set. Like the source graph, the nodes in the target graph are also arranged in a breadth-first-search (BFS) ordering at training time, and the model is expected to learn to generate BFS orders.",
            "cite_spans": [],
            "section": "Decoder ::: Attention-Based Graph Evolution Model",
            "ref_spans": []
        },
        {
            "text": "Given \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}_s$$\\end{document}, the autoregressive decoder generates an output sequence of nodes one at a time, where each step is also conditioned on the previously generated nodes. The decoder maps the embedding to the space of adjacency matrices and space of label matrices (if the data has label information) to reconstruct the generated graphs. We use a generator which is a combination of a linear transformation and the sigmoid activation function to map \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}_t$$\\end{document} to the adjacency matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\hat{\\varvec{A}}}_t$$\\end{document} and we use a classifier which is a combination of a linear transformation and the softmax activation function to map \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\varvec{H}}_t$$\\end{document} to the label vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\hat{\\varvec{L}}}_t$$\\end{document}:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} {\\hat{\\varvec{A}}}_t = \\text {sigmoid}({\\varvec{W}_g}{\\varvec{H}}_t); \\quad {\\hat{\\varvec{L}}}_t = \\text {softmax}({\\varvec{W}_c}{\\varvec{H}}_t). \\end{aligned}$$\\end{document}For the predicted adjacency matrix, we use the binary cross-entropy loss function to measure the differences:5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{\\text {adj}} = -\\sum _{i=1}^{N}\\sum _{j=1}^{N} {\\varvec{A}}_{ij}\\log (\\hat{\\varvec{A}}_{ij}) + (1 - {\\varvec{A}}_{ij})\\log (1 - (\\hat{\\varvec{A}}_{ij})). \\end{aligned}$$\\end{document}Moreover, if the data has the label information, we also added the loss on labels based on label smoothing using the KL divergence loss.",
            "cite_spans": [],
            "section": "Decoder ::: Attention-Based Graph Evolution Model",
            "ref_spans": []
        },
        {
            "text": "Our first evaluation setting considers the graph evolution problem in space. In real-world networks, graph data is collected by subsampling from larger graphs. Due to resource constraints, data collection may not gather as large subsamples as needed. A generative model that can conditionally add nodes in a manner consistent with how graphs grow as one expands the subsample could enable larger analyses of semi-synthetic networks.",
            "cite_spans": [],
            "section": "Graph Evolution in Space ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Datasets. We test this problem setting on citation networks. The problem is to predict the expansion of ego networks with farther-hop neighbors. We used the Cora and Citeseer datasets [23]. We evaluated our models with different graph sizes. For small datasets (Cora_small and Citeseer_small), we extract one-hop (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {sou}} = G_1 = \\{V_1, E_1\\}$$\\end{document}) and two-hop (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {tar}} = G_2 = \\{V_2, E_2\\}$$\\end{document}) ego networks with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$5 \\le |V_1| \\le 20$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$30 \\le |V_2| \\le 50$$\\end{document} as the source and target graphs. For the large datasets (Cora and Citeseer), we extract two-hop (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {sou}} = G_2 = \\{V_2, E_2\\}$$\\end{document}) and three-hop (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {tar}} = G_3 = \\{V_3, E_3\\}$$\\end{document}) ego networks with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$10 \\le |V_2| \\le 50$$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$40 \\le |V_3| \\le 170$$\\end{document} as the source and target graphs. Data construction for this problem is illustrated in Fig. 2. The training data consists of graph pairs extracted from the datasets. The source graph \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {sou}}$$\\end{document} is the i-hop ego network where the initial embeddings is constructed by concatenating the adjacency matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{A}_s$$\\end{document} and the feature matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{F}_s$$\\end{document} (if \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{F}_s$$\\end{document} is given). The target graphs \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{\\text {tar}}$$\\end{document} are the (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i+1$$\\end{document})-hop ego networks of the same node v where the initial embeddings is constructed by concatenating the adjacency matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{A}_t$$\\end{document} and the feature matrix \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\varvec{F}_t$$\\end{document}.",
            "cite_spans": [
                {
                    "start": 185,
                    "end": 187,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Graph Evolution in Space ::: Experiments",
            "ref_spans": [
                {
                    "start": 3086,
                    "end": 3087,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Results are listed in Table 1. (In all tables, values are rounded to two decimal places.) The metrics indicate that AGE is a strong graph generator in both its ability to mimic graph distributions and match the target graphs. Considering the evaluation of the distance between the distributions of generated graphs and target graphs, AGE achieves the best scores. AGE scores less than 0.1 MMD on all cases, with at least a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$30\\%$$\\end{document} decrease compared to the second best method, GraphRNN on two datasets with different graph sizes. This result corroborates that, as a graph generative model, AGE can generate realistic graphs that appear to be from the same distribution as the true target graphs. Moreover, considering how well generated graphs match the specific target graphs, we also calculate the graph similarities between the generated graphs and the target graphs. The kernel similarity scores are normalized, so they range from 0 to 1. The graphs AGE generates consistently have the best similarity scores.\n",
            "cite_spans": [],
            "section": "Graph Evolution in Space ::: Experiments",
            "ref_spans": [
                {
                    "start": 28,
                    "end": 29,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Many graph generation methods are designed for static graphs. However in practice, many networks are not static. Instead, they change and evolve over time, with the addition of new nodes and edges, such as in citation networks and collaboration networks, and also with the deletion of existing nodes and edges, such as in computer networks and social networks.\n",
            "cite_spans": [],
            "section": "Graph Evolution in Time ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Datasets. For this task, we evaluate AGE on three datasets: the Facebook Friendship Networks [28], the Bitcoin Networks [15], and two citation networks in Physics: cit-HepPh and cit-HepTh [8]. We extract two-hop (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_2 = \\{V_2, E_2\\}$$\\end{document}) ego networks with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$30 \\le |V_2| \\le 120$$\\end{document} (or \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$20 \\le |V_2| \\le 50$$\\end{document} for _small data) at time t as the source graphs and the two-hop ego networks of the same node at time \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t+1$$\\end{document} as the target graphs. Here, we have \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_2^t \\in G_2^{t+1}$$\\end{document}, and the problem is to model how networks evolve (or grow) with actual time.",
            "cite_spans": [
                {
                    "start": 94,
                    "end": 96,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 121,
                    "end": 123,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 189,
                    "end": 190,
                    "mention": "8",
                    "ref_id": "BIBREF29"
                }
            ],
            "section": "Graph Evolution in Time ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We compare AGE with other graph generative models and the results are shown in Table 2. The evaluation results show that AGE can accurately model the graph evolution or growth over time. We compute the distance between the distributions of generated graphs and target graphs, and, as before, AGE achieves the best scores among all the generative models regarding the realism of the generated graphs. Again, this is strong evidence that AGE can generate realistic graphs that appear to be from the same distribution of the target graphs. Considering the graph similarities between the generated graphs by all models and the target graphs, Table 2 shows that among all models, AGE is the only one that can reach similarity 0.9 for all three graph kernels, while the other methods cannot consistently score high across different kernels. This suggests some aspect of graph similarity is not satisfied by these other generation procedures. These results again demonstrate that AGE represents a significant step in our ability to model the evolution of graphs in time.",
            "cite_spans": [],
            "section": "Graph Evolution in Time ::: Experiments",
            "ref_spans": [
                {
                    "start": 85,
                    "end": 86,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 644,
                    "end": 645,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "To evaluate the performance of AGE on modeling the evolution of graphs with deletion, study cases where the source graphs evolves with not only addition of new nodes and edges, but also allows the deletion of existing nodes and edges.\n",
            "cite_spans": [],
            "section": "Graph Evolution in Time with Deletion ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "Datasets. We use the Computer Network dataset [17], which is a network describing peering information inferred from Oregon route-views with nine different timestamps in total. We extract two-hop (\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_2 = \\{V_2, E_2\\}$$\\end{document}) ego networks with \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$30 \\le |V_2| \\le 120$$\\end{document} at the first and last timestamp, respectively, as the source and target graphs. In this experiment, we focus on the more difficult problem of modeling the evolution of graphs with deletion. The difference with the second experiment is that in this case, the condition \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G^t_2 \\subseteq G_2^{t+1}$$\\end{document} does not hold anymore.",
            "cite_spans": [
                {
                    "start": 47,
                    "end": 49,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                }
            ],
            "section": "Graph Evolution in Time with Deletion ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We compare AGE with other graph generative models, listing results in Table 3. The evaluation results show that, even for this more complex problem, AGE still maintains a high-level performance compared to the other generative models in terms of both the realism of generated graphs and the similarity to the target ones. Therefore, together with the second experiment, we find that AGE is not only able to learn graph evolution through growth, but also the more complex setting of volatile evolution.",
            "cite_spans": [],
            "section": "Graph Evolution in Time with Deletion ::: Experiments",
            "ref_spans": [
                {
                    "start": 76,
                    "end": 77,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "In this work, we proposed attention-based graph evolution (AGE), a conditioned generative model for graphs based on the attention mechanism, which can model graph evolution in both space and time. AGE is capable of generating graphs conditioned on existing graphs. Our model can be useful for many applications in various domains, such as for predicting information propagation in social networks, disease control for healthcare, and traffic prediction in road networks. We model graph generation as a sequential problem, yet we are able to train AGE models in parallel by adopting the transformer framework. Our experimental results demonstrate that AGE is a powerful and efficient conditioned graph generative model, which outperforms all the other state-of-the-art deep generative models for graphs. In our several experiments on various datasets, AGE is to be able to adapt to various kinds of evolution or transformations between graphs, and it performs consistently well in terms of both the realism of its generated graphs and the similarity to ground-truth target graphs. Finally, AGE has a flexible structure that can be used to generate graphs with or without features and labels. This flexibility thus enables a wider range of applications by allowing it to model many forms of graph evolution.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1.: Comparison of AGE and other generative models on graph evolution in space using MMD evaluation metrics and graph kernel similarities.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2.: Comparison of AGE and other generative models on graph evolution in time using MMD evaluation metrics and graph kernel similarities.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3.: Comparison of AGE and other generative models on graph evolution in time with deletion using MMD evaluation metrics and graph kernel similarities.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig. 1.: The model architecture of AGE.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig. 2.: Data construction for graph evolution in space. The graph and matrix on the left represents the input source graph, which contains a portion of the full target graph on the right. The full target graph contains the adjacency and feature matrices of the source graph in this setting.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "Mixed membership stochastic blockmodels",
            "authors": [
                {
                    "first": "EM",
                    "middle": [],
                    "last": "Airoldi",
                    "suffix": ""
                },
                {
                    "first": "DM",
                    "middle": [],
                    "last": "Blei",
                    "suffix": ""
                },
                {
                    "first": "SE",
                    "middle": [],
                    "last": "Fienberg",
                    "suffix": ""
                },
                {
                    "first": "EP",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "J. Mach. Learn. Res.",
            "volume": "9",
            "issn": "Sep",
            "pages": "1981-2014",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "A kernel two-sample test",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gretton",
                    "suffix": ""
                },
                {
                    "first": "KM",
                    "middle": [],
                    "last": "Borgwardt",
                    "suffix": ""
                },
                {
                    "first": "MJ",
                    "middle": [],
                    "last": "Rasch",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Sch\u00f6lkopf",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Smola",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "J. Mach. Learn. Res.",
            "volume": "13",
            "issn": "Mar",
            "pages": "723-773",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Kronecker graphs: an approach to modeling networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Chakrabarti",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Faloutsos",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ghahramani",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J. Mach. Learn. Res.",
            "volume": "11",
            "issn": "Feb",
            "pages": "985-1042",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Statistical mechanics of complex networks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Albert",
                    "suffix": ""
                },
                {
                    "first": "AL",
                    "middle": [],
                    "last": "Barab\u00e1si",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "Rev. Mod. Phys.",
            "volume": "74",
            "issn": "1",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1103/RevModPhys.74.47"
                ]
            }
        },
        "BIBREF12": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "An introduction to exponential random graph (p*) models for social networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Robins",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Pattison",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Kalish",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lusher",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Soc. Netw.",
            "volume": "29",
            "issn": "2",
            "pages": "173-191",
            "other_ids": {
                "DOI": [
                    "10.1016/j.socnet.2006.08.002"
                ]
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Collective classification in network data",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Sen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Namata",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bilgic",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Getoor",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Galligher",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Eliassi-Rad",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "AI Mag.",
            "volume": "29",
            "issn": "3",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1609/aimag.v29i3.2157"
                ]
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "New specifications for exponential random graph models",
            "authors": [
                {
                    "first": "TA",
                    "middle": [],
                    "last": "Snijders",
                    "suffix": ""
                },
                {
                    "first": "PE",
                    "middle": [],
                    "last": "Pattison",
                    "suffix": ""
                },
                {
                    "first": "GL",
                    "middle": [],
                    "last": "Robins",
                    "suffix": ""
                },
                {
                    "first": "MS",
                    "middle": [],
                    "last": "Handcock",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Sociol. Methodol.",
            "volume": "36",
            "issn": "1",
            "pages": "99-153",
            "other_ids": {
                "DOI": [
                    "10.1111/j.1467-9531.2006.00176.x"
                ]
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Collective dynamics of small-world networks",
            "authors": [
                {
                    "first": "DJ",
                    "middle": [],
                    "last": "Watts",
                    "suffix": ""
                },
                {
                    "first": "SH",
                    "middle": [],
                    "last": "Strogatz",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Nature",
            "volume": "393",
            "issn": "6684",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1038/30918"
                ]
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "On random graphs I",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Erd\u00f6s",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "R\u00e9nyi",
                    "suffix": ""
                }
            ],
            "year": 1959,
            "venue": "Publicationes Math. Debrecen",
            "volume": "6",
            "issn": "",
            "pages": "290-297",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "Overview of the 2003 KDD cup",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Gehrke",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Ginsparg",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Kleinberg",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "ACM SIGKDD Explor. Newsl.",
            "volume": "5",
            "issn": "2",
            "pages": "149-151",
            "other_ids": {
                "DOI": [
                    "10.1145/980972.980992"
                ]
            }
        },
        "BIBREF30": {
            "title": "A survey of statistical network models",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Goldenberg",
                    "suffix": ""
                },
                {
                    "first": "AX",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "SE",
                    "middle": [],
                    "last": "Fienberg",
                    "suffix": ""
                },
                {
                    "first": "EM",
                    "middle": [],
                    "last": "Airoldi",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Found. Trends Mach. Learn.",
            "volume": "2",
            "issn": "2",
            "pages": "129-233",
            "other_ids": {
                "DOI": [
                    "10.1561/2200000005"
                ]
            }
        }
    }
}