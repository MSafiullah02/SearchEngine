{
    "paper_id": "d908792846a558e785b0ecbeafbaca02ed5ffa38",
    "metadata": {
        "title": "Towards a Better Contextualization of Web Contents via Entity-Level Analytics",
        "authors": [
            {
                "first": "Amit",
                "middle": [],
                "last": "Kumar",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Universit\u00e9 de Caen Normandie",
                    "location": {
                        "addrLine": "Campus C\u00f4te de Nacre",
                        "postCode": "14032",
                        "settlement": "Caen Cedex",
                        "country": "France"
                    }
                },
                "email": "amit.kumar@unicaen.fr"
            }
        ]
    },
    "abstract": [
        {
            "text": "With the abundance of data and wide access to the internet, a user can be overwhelmed with information. For an average Web user, it is very difficult to identify which information is relevant or irrelevant. Hence, in the era of continuously enhancing Web, organization and interpretation of Web contents are very important in order to easily access the relevant information. Many recent advancements in the area of Web content management such as classification of Web contents, information diffusion, credibility of information, etc. have been explored based on text and semantic of the document. In this paper, we propose a purely semantic contextualization of Web contents. We hypothesize that named entities and their types present in a Web document convey substantial semantic information. By extraction of this information, we aim to study the reasoning and explanation behind the Web contents or patterns. Furthermore, we also plan to exploit LOD (Linked Open Data) to get a deeper insight of Web contents.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Even in the 30 th year of World Wide Web, we can still observe the enormous amount of growth in the Web contents being created and subsequently available to any internet user. Because of the inconsistency of the data being generated, it is very hard for an ordinary user to distinguish the Web contents according to their societal relevance. With the availability of NER techniques [5] and LOD [1, 11, 13] , we have access to a lot of information about the named entities described in a Web content. This contextualization of the entities contained in a text can help us to deal with Web contents. We observe that, for a text describing an event, there are specific recurring patterns of entity types appearing together. For instance, in the case of 'natural disasters', entities like organizations, countries, presidents appear together whereas in the case of 'political events', entities like parties, leaders, business-persons appear together. The availability of tools like AIDA [16] or DBPedia Spotlight [12] , which can interlink text documents to LOD has provided us efficient means to capture the semantics of a plain text using the entity-level.",
            "cite_spans": [
                {
                    "start": 382,
                    "end": 385,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 394,
                    "end": 397,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 398,
                    "end": 401,
                    "text": "11,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 402,
                    "end": 405,
                    "text": "13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 983,
                    "end": 987,
                    "text": "[16]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "The purpose of this study is to analyze those large amounts of data and to help the user in getting a better semantic understanding of a Web document. To this end, we aim to study a Web document semantically using entity-level analytics. Ultimately, we plan to exploit and aggregate external knowledge using LOD for the proper contextualization of a Web content.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction and Motivation"
        },
        {
            "text": "Knowledge bases (KBs) are an effective way to store Web documents semantically in a structured format. Because of easy accessibility, these KBs are fruitful resources for many tasks in information retrieval [4] and natural language processing [9] . Recently, researchers from different domains have developed different knowledge acquisition approaches for the creation of knowledge graphs. This results in an emanation of large publicly accessible KBs such as Freebase [1] , DBPedia [11] , YAGO [13] , which accommodate spatial and temporal information in addition to structural knowledge. Many applications such as complex event detection [17] , named entities disambiguation [14] or social media topic classification [3] from various domains have acquired the benefit by integrating knowledge from LOD.",
            "cite_spans": [
                {
                    "start": 207,
                    "end": 210,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 243,
                    "end": 246,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 469,
                    "end": 472,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 483,
                    "end": 487,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 495,
                    "end": 499,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 640,
                    "end": 644,
                    "text": "[17]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 677,
                    "end": 681,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 719,
                    "end": 722,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Background and Related Work"
        },
        {
            "text": "Entity-level analytics aggregates semantic information by incorporating knowledge about an entity or its types. The problem of event diffusion prediction into foreign language communities [8] has shown encouraging results with the assimilation of knowledge about the entities contained in a document. Here the introduced framework ELEVATE only utilizes the information about the entities in the document and resources from YAGO [13] . In [7] , the authors address the task of Web content fine-grained hierarchical classification. They hypothesize that a document is symbolized by the named entities it comprised. They propose the idea of the 'semantic fingerprinting' method that expresses the overall semantics of a Web document by a compact vector. Entity-level analytics is also effective in computational fact checking of information [2] . The authors claim that human fact checking can be achieved by finding the shortest path on a conceptually or semantically defined network such as knowledge graphs (KGs).",
            "cite_spans": [
                {
                    "start": 188,
                    "end": 191,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 428,
                    "end": 432,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 438,
                    "end": 441,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 838,
                    "end": 841,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Background and Related Work"
        },
        {
            "text": "Entity-level analytics provide a depth insight into Web contents. KGs carry a lot of information about entities, but all the information is not equally important for a given text. The novelty of this thesis is to discriminate between interesting and uninteresting semantic information about entities w.r.t. the context of a text.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Background and Related Work"
        },
        {
            "text": "The idea of semantic fingerprinting [6] -as an approach towards Web analytics was well acknowledged by researchers in the semantic Web community. Thus, we presented the CALVADOS system [7] as an extension of semantic fingerprinting. At first, this system filters all the named entities present in a given text. By utilization of type information for all these entities from YAGO, it creates a representative vector (called semantic fingerprint) for the text. In last, it predicts the fine-grained type of the content using machine learning techniques. Moreover, it reports the semantic building block of the text. Figure 1 outlines the conceptual pipeline. The notable contributions of the mentioned scientific article are:",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 39,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 185,
                    "end": 188,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 614,
                    "end": 622,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "CALVADOS : For Entity-Level Content Analysis"
        },
        {
            "text": "\u2022 employ semantic fingerprint to represent document's semantics \u2022 exploration and visualization of dependencies among entities comprised \u2022 data digestion supported by providing contextual KB data (e.g., types) ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CALVADOS : For Entity-Level Content Analysis"
        },
        {
            "text": "Exploitation of named entities and their types are always valuable in getting a better contextualization of a Web document [7] . But, sometimes we can be easily drowned by too much information. For example, recent articles involving Donald Trump deal with his position of president. But Trump has 76 facets (considering Wordnet types) like communicator, president, business-person, etc., which are not equally relevant. For some entities, it is even more complicated. For instance, Arnold Schwarzenegger is famous to be an actor, a politician and a bodybuilder. When an article deals with him, the context of the article makes us understand which facet is relevant, e.g., 'actor' if the article deals with a film release. Hence, it arises two research questions:",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 126,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Concise Entity-Type Extraction"
        },
        {
            "text": "\u2022 RQ1: What are the most relevant type(s) for an entity in general (i.e., without context)? \u2022 RQ2: What are the most relevant type(s) for an entity in a given context? Computational Model: Let d i D represents a document. The named entities associated with a document d i is given by N (d i ) . T (n j ) represents all the k types associate with any entity n j . Entity-level document type is represented by d t i as shown in Eq. 4. Our task is to select m number of types from T (n j ), where m << k. We define two types of models -First, for the calculation of T Gen (i.e. RQ1, Eq. 5) and second, for the calculation of T Con (i.e. RQ2, Eq. 6).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 281,
                    "end": 292,
                    "text": "by N (d i )",
                    "ref_id": null
                }
            ],
            "section": "Concise Entity-Type Extraction"
        },
        {
            "text": "T Gen = f gen (T (n j )) (5)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concise Entity-Type Extraction"
        },
        {
            "text": "T Con = f con (T (n j ), text)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concise Entity-Type Extraction"
        },
        {
            "text": "Currently, we have focus on our first research question (RQ1). Our first challenge is to find or develop the appropriate data set for the aforementioned task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concise Entity-Type Extraction"
        },
        {
            "text": "It is not easy to find the most relevant type(s) for an entity in general. We create the gold standard based on the Wordnet hierarchy mentioned in YAGO (1981 types). We consider that the most relevant type(s) for any entity is mentioned in its Wikipedia page. Precisely, we extract the types that are mentioned in the first or second sentence of entity's Wikipedia page and map them to the mentioned hierarchy. Example -Extracted Wikipedia labels for 'Arnold Schwarzenegger' are actor, filmmaker, businessman, author, bodybuilder and politician. After mapping of these labels to Schwarzenegger's Wordnet hierarchy in YAGO, the ground truths are actor, film-maker, businessman, bodybuilder and politician.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gold Standard Creation:"
        },
        {
            "text": "Experimental Pipeline: Our next challenge is to find the suitable mechanism for concise entity-type prediction in general. We rely only on the structural information, which we get by exploring knowledge graph of entity in YAGO. We implemented several techniques as baselines but none of these techniques show promising results. These models are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gold Standard Creation:"
        },
        {
            "text": "\u2022 Based on Leaf Node: We had the intuition that the most specific or relevant type of an entity should be at the deepest in the YAGO Wordnet entity's hierarchy. So, we picked the type that is at the deepest in the hierarchy. \u2022 Based on Branching Factor: While implementing the model based on leaf node, we observe that sometimes, we were selecting a too specific type, e.g., forward (child of football-player in the hierarchy) instead of football-player. So, we decided to pick the node that has the highest branching factor (number of direct children) and at the deepest in the hierarchy. \u2022 Based on ML Classifier: We developed a model based on random forest.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gold Standard Creation:"
        },
        {
            "text": "We used all the Wordnet types present in the entity's hierarchy as features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Gold Standard Creation:"
        },
        {
            "text": "We aim to develop a relevant types prediction model based on Graph Neural Network [15] . More specifically, we utilize the concept of Graph Convolutional Network (GCN) [10] . While implementation, we faced the following challenges:",
            "cite_spans": [
                {
                    "start": 82,
                    "end": 86,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 168,
                    "end": 172,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Gold Standard Creation:"
        },
        {
            "text": "\u2022 In GCN, Readout function [15] is used to get embedding for the graph based on an aggregation of node features from the final iteration. Finding the suitable Readout function for our task is one of the main challenges. \u2022 Entity's graph is a sub-graph of YAGO Wordnet hierarchy. Only delivering the structure of the sub-graph is not sufficient. It needs label information along with the structure of sub-graph. Encoding node label is our next challenge. One hot encoding is one of the solutions for giving the label information.",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Gold Standard Creation:"
        },
        {
            "text": "Based on our proposed research and current work progress, we find the following challenges to handle in the near future:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Challenges and Next Steps"
        },
        {
            "text": "\u2022 In the early stage of our experiments, we realise that some of the types within a category are very hard to predict, e.g., in person category -there are entities with ground truth types 'intellectual' or 'military officer' where the model fails to predict it correctly. Our challenge is to find the common patterns among these sub-categories and to propose the solution for this failure. \u2022 Our next challenge is to develop a gold standard for task 2 (RQ2). \u2022 Our last challenge is to develop method for types prediction in a given context.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Challenges and Next Steps"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Bollacker",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of SIGMOD 2008",
            "volume": "",
            "issn": "",
            "pages": "1247--1250",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Computational fact checking from knowledge networks",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "L"
                    ],
                    "last": "Ciampaglia",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "PLoS ONE",
            "volume": "10",
            "issn": "6",
            "pages": "1--13",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Harnessing linked knowledge sources for topic classification in social media",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "E"
                    ],
                    "last": "Cano",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the 24th ACM Conference on Hypertext and Social Media, HT 2013",
            "volume": "",
            "issn": "",
            "pages": "41--50",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Entity query feature expansion using knowledge base links",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dalton",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Dietz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Allan",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of SIGIR 2014",
            "volume": "",
            "issn": "",
            "pages": "365--374",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Incorporating non-local information into information extraction systems by Gibbs sampling",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Finkel",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Grenager",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "Proceedings of ACL 2005",
            "volume": "",
            "issn": "",
            "pages": "363--370",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Semantic fingerprinting: a novel method for entitylevel content classification",
            "authors": [
                {
                    "first": "Alec",
                    "middle": [],
                    "last": "Govind",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Spaniol",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ICWE 2018",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-91662-0_21"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "CALVADOS: a tool for the semantic analysis and digestion of web contents",
            "authors": [
                {
                    "first": "",
                    "middle": [],
                    "last": "Govind",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Alec",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Spaniol",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ESWC 2019",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-32327-1_17"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "ELEVATE: a framework for entity-level event diffusion prediction into foreign language communities",
            "authors": [
                {
                    "first": "Spaniol",
                    "middle": [],
                    "last": "Govind",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of WebSci 2017",
            "volume": "",
            "issn": "",
            "pages": "111--120",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Pattern-revising enhanced simple question answering over knowledge bases",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Hao",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of COLING 2018",
            "volume": "",
            "issn": "",
            "pages": "3272--3282",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Semi-supervised classification with graph convolutional networks",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "N"
                    ],
                    "last": "Kipf",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Welling",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "DBpedia-a large-scale, multilingual knowledge base extracted from Wikipedia",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lehmann",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Semant. Web J",
            "volume": "6",
            "issn": "",
            "pages": "167--195",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Dbpedia spotlight: shedding light on the web of documents",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "N"
                    ],
                    "last": "Mendes",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Jakob",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garc\u00eda-Silva",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bizer",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of I-Semantics",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "YAGO: a multilingual knowledge base from Wikipedia, Wordnet, and Geonames",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Rebele",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Suchanek",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hoffart",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Biega",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Kuzey",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Weikum",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "9982",
            "issn": "",
            "pages": "177--185",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-46547-0_19"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "AGDISTIS -graph-based disambiguation of named entities using linked data",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Usbeck",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "ISWC 2014",
            "volume": "8796",
            "issn": "",
            "pages": "457--471",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "How powerful are graph neural networks?",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Leskovec",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jegelka",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "AIDA: an online tool for accurate disambiguation of named entities in text and tables",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Yosef",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Proceedings of VLDB",
            "volume": "2011",
            "issn": "",
            "pages": "1450--1453",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Event oriented dictionary learning for complex event detection",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "IEEE Trans. Image Process",
            "volume": "24",
            "issn": "6",
            "pages": "1867--1878",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Conceptual overview of the CALVADOS pipeline",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}