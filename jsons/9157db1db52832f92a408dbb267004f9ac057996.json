{
    "paper_id": "9157db1db52832f92a408dbb267004f9ac057996",
    "metadata": {
        "title": "AutoSUM: Automating Feature Extraction and Multi-user Preference Simulation for Entity Summarization",
        "authors": [
            {
                "first": "Dongjun",
                "middle": [],
                "last": "Wei",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "weidongjun@iie.ac.cn"
            },
            {
                "first": "Yaxin",
                "middle": [],
                "last": "Liu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "liuyaxin@iie.ac.cn"
            },
            {
                "first": "Fuqing",
                "middle": [],
                "last": "Zhu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "zhufuqing@iie.ac.cn"
            },
            {
                "first": "Liangjun",
                "middle": [],
                "last": "Zang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "zangliangjun@iie.ac.cn"
            },
            {
                "first": "Wei",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "zhouwei@iie.ac.cn"
            },
            {
                "first": "Yijun",
                "middle": [],
                "last": "Lu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Alibaba Cloud Computing Co. Ltd",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "yijun.lyj@alibaba-inc.com"
            },
            {
                "first": "Songlin",
                "middle": [],
                "last": "Hu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Chinese Academy of Sciences",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": "husonglin@iie.ac.cn"
            }
        ]
    },
    "abstract": [
        {
            "text": "With the growth of knowledge graphs, entity descriptions are becoming extremely lengthy. Entity summarization task, aiming to generate diverse, comprehensive and representative summaries for entities, has received an increasing interest recently. In most previous methods, features are usually extracted by the hand-crafted templates. Then the feature selection and multi-user preference simulation take place, depending too much on human expertise. In this paper, a novel integration method called AutoSUM is proposed for automatic feature extraction and multi-user preference simulation to overcome the drawbacks of previous methods. There are two modules in AutoSUM: extractor and simulator. The extractor module operates automatic feature extraction based on a BiLSTM with a combined input representation including word embeddings and graph embeddings. Meanwhile, the simulator module automates multi-user preference simulation based on a well-designed twophase attention mechanism (i.e., entity-phase attention and user-phase attention). Experimental results demonstrate that AutoSUM produces the state-of-the-art performance on two widely used datasets (i.e., DBpedia and LinkedMDB) in both F-measure and MAP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Semantic data enables users or machines to comprehend and manipulate the conveyed information quickly [10] . In major knowledge graphs, semantic data describes entities by Resource Description Framework (RDF) triples, referred as triples [4] . With the growth of knowledge graphs, entity descriptions are becoming extremely lengthy [23] . Since Google first released the knowledge graph, \"get the best summary\" for entities has been one of the main contributions in Google Search 1 [25] . Specifically, Google Search returns a top-k subset of triples which can best describe the entity from a query on the right-hand side of the result pages [15] . Motivated by the success of Google Search, entity summarization task has received an increasing interest recently [7, 25] , it aims to generate diverse, comprehensive and representative summaries for entities. In addition, entity summarization has been integrated into various applications such as document browsing, Question Answering (QA), etc. [15] .",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 106,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 238,
                    "end": 241,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 332,
                    "end": 336,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 482,
                    "end": 486,
                    "text": "[25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 642,
                    "end": 646,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 763,
                    "end": 766,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 767,
                    "end": 770,
                    "text": "25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Most previous entity summarization methods are adopted from random surfer [4] , clustering [9, 10] and Latent Dirichlet Allocation (LDA) [19] models, depending too much on the hand-crafted templates for feature extraction as well as human expertise for feature selection. Meanwhile, entities are capable to represent diverse information (or multi-aspect information) in knowledge graphs [21] , resulting in different user preference (sometimes multi-user preference [27] ). Take entity Triathlon at the 2000 Summer Olympics Men's in DBpedia 2 for instance, different users may prefer to the medal, event or type of this entity, respectively. In order to generate more diverse summaries, the specific model needs to be selected for providing a more distinguishable multi-user preference simulation [9, 21] . However, due to the countless quantities and unpredictable types of entities in real large-scale knowledge graphs, extracting discriminative features or selecting suitable models based on human expertise could be arduous [15] .",
            "cite_spans": [
                {
                    "start": 74,
                    "end": 77,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 91,
                    "end": 94,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 95,
                    "end": 98,
                    "text": "10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 137,
                    "end": 141,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 387,
                    "end": 391,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 466,
                    "end": 470,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 797,
                    "end": 800,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 801,
                    "end": 804,
                    "text": "21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, a novel integration method called AutoSUM is proposed for automatic feature extraction and multi-user preference simulation to overcome the drawbacks of above previous models. There are two modules in Auto-SUM: extractor and simulator. The extractor module operates automatic feature extraction based on a BiLSTM with a combined input representation including word embeddings and graph embeddings. Meanwhile, the simulator module automates multi-user preference simulation based on a well-designed two-phase attention mechanism (i.e., entity-phase attention and user-phase attention). Experimental results demonstrate that AutoSUM produces the state-of-the-art performance on two widely used datasets (i.e., DBpedia and LinkedMDB 3 ) in both F-measure and MAP.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Previous entity summarization methods mainly rely on human expertise. To find the most central triples, RELIN [4] and SUMMARUM [24] compute the relatedness and informativeness based on the features extracted from hand-crafted templates. Meanwhile, FACES [9] and ES-LDA [19] introduce a clustering algorithm and LDA model for capturing multi-aspect information, respectively. In order to generate more diverse summaries, the specific models need to be selected for providing a more distinguishable multi-user preference simulation [9, 19] . However, due to the countless quantities and unpredictable types of entities in the real large-scale knowledge graphs, extracting discriminative features and selecting suitable models based on human expertise could be arduous.",
            "cite_spans": [
                {
                    "start": 110,
                    "end": 113,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 127,
                    "end": 131,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 254,
                    "end": 257,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 269,
                    "end": 273,
                    "text": "[19]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 530,
                    "end": 533,
                    "text": "[9,",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 534,
                    "end": 537,
                    "text": "19]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Recently, deep learning methods relieve the dependency on human expertise in Natural Language Processing (NLP) [17] community. To generate the summaries without human expertise, an entity summarization method with a single-layer attention (ESA) [29] is proposed to calculate the attention score for each triple. Then top-k triples which have the highest attention scores are selected as the final results. However, ESA cannot extract features and capture multi-aspect information with the single-layer attention mechanism. Following ESA work, our proposed AutoSUM automates feature extraction and multi-user preference based on a novel extractor-simulator structure. In extractor, a BiL-STM with a combined input representation is utilized for feature extraction. The word embeddings and graph embeddings are included. Meanwhile, in simulator, a two-phase attention mechanism is designed for multi-user preference simulation.",
            "cite_spans": [
                {
                    "start": 111,
                    "end": 115,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 245,
                    "end": 249,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "An RDF triple is composed of a subject, a predicate, and an object. In major knowledge graphs, an entity of which is then defined as a subject with all predicates and corresponding objects to those predicates. When a user queries an entity in a knowledge graph, a set of triples {t 1 , t 2 , \u00b7 \u00b7 \u00b7 , t n } related with the entity will be returned, referred as an entity description document d, where t i is the i-th triple in d. Following Google Search [7, 15] , given a positive integer k, the summary of an entity is a top-k subset of d which can best describe the entity.",
            "cite_spans": [
                {
                    "start": 453,
                    "end": 456,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 457,
                    "end": 460,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Problem Description"
        },
        {
            "text": "As shown in Fig. 1 , AutoSUM has a novel extractor-simulator structure. The extractor extracts the features of triples in d as h = {h 1 , h 2 , \u00b7 \u00b7 \u00b7 , h n }, where h i is the feature vector of t i . Given h, the simulator calculates the attention scores a = {a 1 , a 2 , \u00b7 \u00b7 \u00b7 , a n }, where a i is the attention score of t i . Then top-k triples with the highest attention scores will be selected as the summary of an entity.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 18,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Overview"
        },
        {
            "text": "The extractor module in AutoSUM aims at extracting features of triples automatically. In this section, we introduce the input representation and the automatic feature extraction in details. Input Representation: As discussed above, the triples related with an entity share the same subject with different predicates and corresponding objects to those predicates. In order to map predicates and objects into a continuous vector space for feature extraction, we apply a combined input representation method including word embeddings and graph embeddings. Then we concatenate the embeddings of the predicates and corresponding objects as the representation for each triple.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extractor"
        },
        {
            "text": "Word Embedding: Learning word embeddings has been an effective method to enhance the performance of entity summarizers. In ES-LDA ext [20] , Pouriyeh et al. stated the key point of learning word embeddings was the definition for \"words\". Following Pouriyeh's work, we extract predicates and objects of triples as our words. Take \"http://dbpedia.org/ontology/goldMedalist\" for instance, we extract \"goldMedalist\" as the word for the above predicate. Given the embeddings of words, we then initialize a word embedding (lookup) table for future training.",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 138,
                    "text": "[20]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Extractor"
        },
        {
            "text": "Graph Embedding: Obviously, simple word embeddings cannot represent triples with a graph structure. To fully encode the graph information, we utilize a graph embedding technique called TransE [3] to pretrain the whole knowledge graph in the dataset. Given the embeddings of tirples, we then initialize a graph embedding table for future training.",
            "cite_spans": [
                {
                    "start": 192,
                    "end": 195,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Extractor"
        },
        {
            "text": "Automatic Feature Extraction. In Named Entity Recognition (NER) task, the bidirectional LSTM (BiLSTM) has been widely used for automatic feature extraction [14] . For instance, in order to automatically extract features from a small and supervised training corpus, an LSTM-CRF model was proposed by Lample et al. [14] , utilizing a BiLSTM for feature extraction and conditional random fields [13] for entity recognition. The BiLSTM extracted representative and contextual features of a word, aligning with other words in the same sentence [8] .",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 160,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 313,
                    "end": 317,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 392,
                    "end": 396,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 539,
                    "end": 542,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Extractor"
        },
        {
            "text": "As for summarizing entities, we also apply a BiLSTM to extract features of a triple, aligning with other triples related with the same entity. Specifically, due to the uncertain timing sequence of triples, we first map (serialize) the triples into a sequence comes randomly. Then we feed the input representation of triples in the sequence to the BiLSTM, and take the outputs as the extracted features for those triples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extractor"
        },
        {
            "text": "The simulator in AutoSUM aims at simulating multi-user preference based on a well-designed two-phase attention mechanism (i.e., entity-phase attention and user-phase attention). Entity-phase attention captures multi-aspect information from an entity, user-phase attention then simulates multi-user preference based on the captured information. In this section, we present the details of entityphase attention and user-phase attention.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulator"
        },
        {
            "text": "Entity-Phase Attention. The intuition of entity-phase attention is straightforward. Since the single-layer attention mechanism in ESA [29] cannot capture multi-aspect information, we then design a multi-aspect attention mechanism with multiple (stacked) attention layers to overcome the drawback of ESA. One seminal work using stacked attention layers is neural machine translation (NMT) [17] , where the stacked attention layers (Transformer) [26] are utilized to capture the multi-aspect information from a sentence. To our knowledge, we are the first to utilize the stacked attention layers to capture the multi-aspect information from an entity. Specifically, different attention layers capture information from an entity in different aspects. In each attention layer, a general attention function [17] is utilized to calculate the relevance between each triple and the information captured from the attention layer, termed attention scores. Here, instead of combining all attention layers to generate overall attention scores of Transformer [26] , we directly output the attention scores from each attention layer for multi-user preference simulation in user-phase attention. Notice that the number of the attention layers is a hyper-parameter which can be tuned during training.",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 138,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 388,
                    "end": 392,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 444,
                    "end": 448,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 802,
                    "end": 806,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Simulator"
        },
        {
            "text": "User-Phase Attention. When users browse triples, they will allocate high preference values (more attention) to triples which are more related with the information they are interested in [9] . Meanwhile, as described above, entityphase attention consists of different attention layers for capturing information in different aspects. In each attention layer, a general attention function is utilized to allocate higher attention scores to the triples which are more relevant to the information captured from the attention layer. To simulate the preference of users who are interested in the information captured by the current attention layer, user-phase attention assigns the user preference values of each triple with the same attention scores from the attention layer. Then different distributions of attention scores in different attention layers simulate the different preference of different users (multi-user preference).",
            "cite_spans": [
                {
                    "start": 186,
                    "end": 189,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Simulator"
        },
        {
            "text": "After simulating the multi-user preference, we have to allocate different attention scores for different user preference rather than treating them equally. The main reason is that some user preference may represent the preference of most users for an entity, while others may represent the preference of few users for the same entity. Allocating proper attention scores for each user preference is critical to generate a more comprehensive entity summarization result. Therefore, we combine a BiLSTM with a general attention score function for allocation. In NER, a BiLSTM can maintain the independence and capture the intrinsic relationships among words [8] . Similarly, a BiLSTM is adopted in user-phase attention to preserve independence as well as capture the intrinsic relationships between different user preference. Then the outputs of the BiLSTM are taken as the inputs to a general attention score function, in order to allocate attention scores for each user preference. At last, we integrate all the user preference based on the allocated attention scores. In addition, due to the uncertain order in user preference like triples, we also randomly map the user preference into a sequence as our input of the BiLSTM.",
            "cite_spans": [
                {
                    "start": 655,
                    "end": 658,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Simulator"
        },
        {
            "text": "In this section, we demonstrate the complete pipeline of AutoSUM. As described in Sect. 3.1, the input of AutoSUM is an entity description document d = {t 1 , t 2 , \u00b7 \u00b7 \u00b7 , t n }. Here, t i is the i-th triple in d, which is composed of a same subject s, a predicate p i and an object o i . Given d, we first split d into a predicate set p = {p 1 , p 2 , \u00b7 \u00b7 \u00b7 , p n } and an object set o = {o 1 , o 2 , \u00b7 \u00b7 \u00b7 , o n }, respectively. Given p and o, we combine word embeddings and graph embeddings to map p i and o i into a continuous vector space and concatenate them as e i , recursively. Given e = {e 1 , e 2 , \u00b7 \u00b7 \u00b7 , e n }, we randomly map e into a sequence q = (q 1 , q 2 , \u00b7 \u00b7 \u00b7 , q n ). Then we apply a BiLSTM to extract the features vector h i of q i as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "where \u2212 \u2192 c and \u2190 \u2212 c are the final hidden states in forward and backward LSTM networks. Given h = {h 1 , h 2 , \u00b7 \u00b7 \u00b7 , h n } and c, we utilize the multi-aspect attention mechanism to capture multi-aspect information. Specifically, for the j-th attention layer in multi-aspect attention mechanism, we calculate the attention score s i j for triple t i with a general score attention function as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "where W j is a parameter matrix of the general attention score function in the j-th attention layer, and m is the number of attention layers in the multi-aspect attention mechanism. Given s = {s 1 , s 2 , \u00b7 \u00b7 \u00b7 , s m }, we then simulate the preference of the j-th user u j who is interested in the information of triple t i captured by the j-th attention layer as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "where u i j is the preference value allocated to triple t i by u j . Given u = { u 1 , u 2 , \u00b7 \u00b7 \u00b7 , u m }, we randomly map u into a sequence q * = (q * 1 , q * 2 , \u00b7 \u00b7 \u00b7 , q * m ) and utilize a BiLSTM to encode u j into u * j as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "where \u2212 \u2192 c * and \u2190 \u2212 c * are the final hidden states from forward and backward LSTM networks. Then we calculate the attention score for user preference as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "where W * is a parameter matrix of the general attention score function. Having obtained a * , we integrate different user preference to generate the final attention score for each triple t i in d as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "Finally, we employ cross-entropy loss and define the loss function L for Auto-SUM, L(a, a) = CrossEntropy(a, a).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "Here, a = {a 1 , a 2 , \u00b7 \u00b7 \u00b7 , a n } is a gold(real) attention score vector associated with above entity from ESBM dataset. Specifically, we count the frequency of the i-th triple t i selected by users in ESBM dataset following ESA work, denoted as c i . Then the gold attention score \u03b1 i of t i is formulated as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The Complete Pipeline"
        },
        {
            "text": "Dataset. In this paper, we utilize ESBM dataset v1.1, which consists of 6.8k triples related with 125 entities from DBpedia [2] and 2.6k triples related with 50 entities from LinkedMDB [5] . Given an entity, ESBM asks 5 different users to select top-5 and top-10 triples which can best describe the entity. In addition, ESBM provides an evaluator for the comparison of different entity summarization methods. Both datasets and evaluator can be accessed from the ESBM website 4 .",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 127,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 185,
                    "end": 188,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Experimental Setup"
        },
        {
            "text": "Our baselines consist of some existing state-of-the-art entity summarization methods, including RELIN [4] , DIVERSUM [21] , CD [30] , FACES [9] , LinkSUM [23] , MPSUM [28] and ESA [29] . MPSUM 5 is an open source implementation of ES-LDA. To provide ablation studies, we also modify the original AutoSUM into 5 different versions, denoted as AutoSUM 1\u223c5 , which will be futher illustrated in Sect. 4.3.",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 105,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 117,
                    "end": 121,
                    "text": "[21]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 127,
                    "end": 131,
                    "text": "[30]",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 140,
                    "end": 143,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 154,
                    "end": 158,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 167,
                    "end": 171,
                    "text": "[28]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 180,
                    "end": 184,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                }
            ],
            "ref_spans": [],
            "section": "Baselines."
        },
        {
            "text": "Evaluation Methodology. Summarization tasks can be mainly divided into extractive and non-extractive tasks [1, 16] , which orient to unstructured and structured data, respectively. Sydow et al. [22] stated that entity summarization task could be treated as an extractive task of information retrieval (IR). IR returns the most relevant documents for a query, while entity summarization selects the top-k triples related with an entity. Following previous work, we utilize F-measure and mean average precision (MAP) metrics for evaluation, which are two standard evaluation metrics in IR [12, 15] . F-measure is the harmonic mean of recall and precision, and MAP is the mean average of precision. Meanwhile, given the limited number of entities in ESBM, we conduct 5-fold cross-validation to reduce the risk of overfitting without losing the number of learning instances [11] . Specifically, the entities in ESBM are divided into 5 folds randomly. The parameters for each model are tuned on 4-of-5 folds. The final fold in each case is utilized to evaluate the optimal parameters. Since ESA has significantly better than all other state-of-the-art methods in our baselines, we then compare the statistical significance among ESA and AutoSUMs (i.e., the original AutoSUM and the modified AutoSUM 1\u223c5 , respectively) utilizing Student's paired t-test (p-value \u2264 0.05) [12] .",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 110,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 111,
                    "end": 114,
                    "text": "16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 194,
                    "end": 198,
                    "text": "[22]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 587,
                    "end": 591,
                    "text": "[12,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 592,
                    "end": 595,
                    "text": "15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 870,
                    "end": 874,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Baselines."
        },
        {
            "text": "Experimental Details. For experimental details, we tune the parameters on a validation set (i.e., a part of the training set). Specifically, to learn graph embeddings, we utilize TransE to pretrain the whole ESBM dataset. Here, the dimension of each triple is set to 100. As for word embeddings, we initialize the lookup table randomly, where the dimension of each word is set to 100. Then we apply a BiLSTM with a single layer in each LSTM cell for feature extraction, where the number of the layers in multi-aspect mechanism is set to 6. In addition, the graph embedding of each triple is fixed after pretraining, while all other parameters in AutoSUM are initialized randomly and tuned without weight sharing. We train the AutoSUM model for 200 epochs, and report the results of the best epoch under early stopping.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Baselines."
        },
        {
            "text": "As shown in Table 1 and 2, AutoSUM is significantly better than some existing state-of-art methods in our baselines. Comparison with Traditional Methods: Compared with traditional methods depending on manual feature extraction and multi-user preference simulation, AutoSUM automates the above processes without any human expertise effectively. The average improvement of AutoSUM over the best outperforming traditional methods is 38% and 36%, in terms of F-measure and MAP, respectively.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 19,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Comparison with Deep Learning Methods: Compared with ESA, which calculates attention scores without feature extraction and multi-user preference, AutoSUM achieves the state-of-the-art performance. The average improvement of AutoSUM over ESA is 26% and 23%, in terms of F-measure and MAP, respectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "In addition, we track the attention scores of entity Triathlon (Triathlon at the 2000 Summer Olympics Men's) in user-phase attention, as shown in Fig. 2 . We can observe that the user-phase attention simulates 3 groups of user preference of the entity, and the entity-phase attention allocates high attention scores to users who prefer medal as well as event than property, which is in accordance with the preference of most users in real world. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 146,
                    "end": 152,
                    "text": "Fig. 2",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "In this section, we provide ablation studies to demonstrate the effectiveness of the primary modules in AutoSUM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ablation Studies"
        },
        {
            "text": "AutoSUM 1 : To evaluate the features extracted by AutoSUM, AutoSUM 1 removes the BiLSTM in extractor and feeds the input representation of triples into simulator directly. Experimental results show the original AutoSUM is significantly better than AutoSUM 1 , proving that the BiLSTM extracts highquality features for user-preference simulation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ablation Studies"
        },
        {
            "text": "AutoSUM 2 and AutoSUM 4 : To explore whether the attention scores of different user preference are appropriate, AutoSUM 2 removes the BiLSTM in simulator and allocates equal attention scores for each user preference. Meanwhile, we also attempt to replace the BiLSTM with an FCN, referred as Auto-SUM 4 . As shown in Table 1 and 2, the original AutoSUM gains a significant improvement over AutoSUM 2 and AutoSUM 4 , indicating the BiLSTM with a general attention function allocates appropriate attention scores for each user preference. In addition, we can observe that the performance of FCN (AutoSUM 2 ) is even worse than allocating equal attention scores (AutoSUM 4 ) in our experiments.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 316,
                    "end": 323,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Ablation Studies"
        },
        {
            "text": "AutoSUM 3 : For comparison, AutoSUM 4 removes the BiLSTM in both extractor and simulator. Experimental results show that the performance of Auto-SUM 3 is worse than AutoSUM 1 and AutoSUM 2 , which remove the BiLSTM in extractor and simulator respectively, further proving the irreplaceable role of BiLSTM in AutoSUM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Ablation Studies"
        },
        {
            "text": "AutoSUM 5 To explore whether the multi-aspect mechanism captures the multiaspect information from an entity, we replace the multi-aspect mechanism with a single-aspect mechanism, i.e., setting the number of attention layers to 1. As shown in Table 1 and 2, we can observe that the original AutoSUM outperforms AutoSUM 5 in both F-measure and MAP. Experimental results indicate that the multi-aspect attention mechanism successfully captures the multi-aspect information. We also notice that AutoSUM 5 with a single-layer attention mechanism still outperforms all other methods in our baselines including ESA.",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 9,
                    "text": "5",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 242,
                    "end": 249,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Ablation Studies"
        },
        {
            "text": "In this paper, we propose a novel integration model called AutoSUM to automate feature extraction and multi-user preference simulation for entity summarization. The performance of our proposed AutoSUM is significantly better than other state-of-the-art methods in both F-measure and MAP. Meanwhile, sufficient ablation studies are provided to demonstrate the effectiveness of each module in AutoSUM. In the future, we expect to expand the ESBM dataset and introduce the notion of AutoSUM into other applications such as recommender systems [6, 18] .",
            "cite_spans": [
                {
                    "start": 540,
                    "end": 543,
                    "text": "[6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 544,
                    "end": 547,
                    "text": "18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Data summarization: a survey",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ahmed",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Knowl. Inf. Syst",
            "volume": "58",
            "issn": "2",
            "pages": "249--273",
            "other_ids": {
                "DOI": [
                    "10.1007/s10115-018-1183-0"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Dbpedia -a crystallization point for the web of data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Bizer",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "J. Web Semant",
            "volume": "7",
            "issn": "",
            "pages": "154--165",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Translating embeddings for modeling multi-relational data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bordes",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Usunier",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Garcia-Duran",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Weston",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Yakhnenko",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "2787--2795",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "RELIN: relatedness and informativeness-based centrality for entity summarization",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "ISWC 2011",
            "volume": "7031",
            "issn": "",
            "pages": "114--129",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-642-25073-6_8"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Managing linked data on the web: the LinkedMDB showcase",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "P"
                    ],
                    "last": "Consens",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Latin American Web Conference",
            "volume": "",
            "issn": "",
            "pages": "1--2",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "An attentive spatio-temporal neural model for successive point of interest recommendation",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "D"
                    ],
                    "last": "Doan",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K"
                    ],
                    "last": "Reddy",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "PAKDD 2019",
            "volume": "11441",
            "issn": "",
            "pages": "346--358",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-16142-2_27"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Official Google Blog: Introducing the knowledge graph: Things, not strings",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Generating sequences with recurrent neural networks",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Graves",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Faces: diversity-aware entity summarization using incremental hierarchical conceptual clustering",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Gunaratna",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Thirunarayan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Sheth",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Gleaning types for literals in RDF triples with application to entity summarization",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Gunaratna",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Thirunarayan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sheth",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sack",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Blomqvist",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Aquin",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ghidini",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "P"
                    ],
                    "last": "Ponzetto",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Lange",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ESWC 2016",
            "volume": "9678",
            "issn": "",
            "pages": "85--100",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-34129-3_6"
                ]
            }
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A deep relevance matching model for ad-hoc retrieval",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Fan",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ai",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "B"
                    ],
                    "last": "Croft",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Technical brief: agreement, the f-measure, and reliability in information retrieval",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Hripcsak",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "S"
                    ],
                    "last": "Rothschild",
                    "suffix": ""
                }
            ],
            "year": 2005,
            "venue": "J. Am. Med. Inform. Assoc.: JAMIA",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "D"
                    ],
                    "last": "Lafferty",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mccallum",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Pereira",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Neural architectures for named entity recognition",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lample",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ballesteros",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Subramanian",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Kawakami",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Dyer",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Entity summarization: State of the art and future challenges",
            "authors": [
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Gunaratna",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Graph summarization methods and applications: a survey",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Safavi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Dighe",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Koutra",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ACM Comput. Surv. (CSUR)",
            "volume": "51",
            "issn": "",
            "pages": "1--34",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Effective approaches to attention-based neural machine translation",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Luong",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A novel top-N recommendation approach based on conditional variational auto-encoder",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Pang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "PAKDD 2019",
            "volume": "11440",
            "issn": "",
            "pages": "357--368",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "ES-LDA: entity summarization using knowledge-based topic modeling",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Pouriyeh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Allahyari",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Kochut",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "R"
                    ],
                    "last": "Arabnia",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Combining word embedding and knowledge-based topic modeling for entity summarization",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Pouriyeh",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Allahyari",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Kochut",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "R"
                    ],
                    "last": "Arabnia",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "252--255",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "DIVERSUM: towards diversified summarisation of entities in knowledge graphs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sydow",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pikula",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schenkel",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "221--226",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "The notion of diversity in graphical entity summarisation on semantic knowledge graphs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sydow",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Pikula",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Schenkel",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "J. Intell. Inf. Syst",
            "volume": "41",
            "issn": "",
            "pages": "109--149",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "LinkSUM: using link analysis to summarize entity data",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Thalhammer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Lasierra",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rettinger",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "ICWE 2016",
            "volume": "9671",
            "issn": "",
            "pages": "244--261",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-38791-8_14"
                ]
            }
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Browsing DBpedia entities with summaries",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Thalhammer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rettinger",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Presutti",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Blomqvist",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Troncy",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sack",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ESWC 2014",
            "volume": "8798",
            "issn": "",
            "pages": "511--515",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-11955-7_76"
                ]
            }
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Fuse: entity-centric data fusion on linked data",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Thoma",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Thalhammer",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Harth",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Studer",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "ACM Trans. Web",
            "volume": "13",
            "issn": "2",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "User preference-aware review generation",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H.-T",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z.-H",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Gong",
                    "suffix": ""
                },
                {
                    "first": "M.-L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "PAKDD 2019",
            "volume": "11441",
            "issn": "",
            "pages": "225--236",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-030-16142-2_18"
                ]
            }
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "MPSUM: entity summarization with predicate-based matching",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "ESA: entity summarization with attention",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wei",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Generating characteristic and diverse entity summaries",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Qu",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The architecture of AutoSUM.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "MAP comparison for top-5 and top-10 entity summarization. \u2191 % is the relative improvement of AutoSUM, and (+/\u2212) is the indicator of significant improvement or degradation with respect to ESA (p-value \u2264 0.05).Fig. 2. The attention scores of Triathlon at the 2000 Summer Olympics Men's.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "This research is supported in part by the Beijing Municipal Science and Technology Project under Grant Z191100007119008.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgment."
        }
    ]
}