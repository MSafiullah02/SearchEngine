{
    "paper_id": "35757dfd5f2da4400a1f2069e531b342ceb1eb92",
    "metadata": {
        "title": "A cluster identification framework illustrated by a filtering model for earthquake occurrences",
        "authors": [
            {
                "first": "Zhengxiao",
                "middle": [],
                "last": "Wu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "National University of Singapore",
                    "location": {}
                },
                "email": "stawz@nus.edu.sg"
            }
        ]
    },
    "abstract": [
        {
            "text": "A general dynamical cluster identification framework including both modeling and computation is developed. The earthquake declustering problem is studied to demonstrate how this framework applies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "A stochastic model is proposed for earthquake occurrences that considers the sequence of occurrences as composed of two parts: earthquake clusters and single earthquakes. We suggest that earthquake clusters contain a \"mother quake\" and her \"offspring.\" Applying the filtering techniques, we use the solution of filtering equations as criteria for declustering. A procedure for calculating maximum likelihood estimations (MLE's) and the most likely cluster sequence is also presented.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "We borrow the Viterbi algorithm from HMM literature to compute the most likely cluster sequence in our setting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Let l \u22c6 j (d, i) be the maximum likelihood of all cluster sequences with D \u03c4j = d and y i as the latest mother quake. As in Section A.1, \u03c41] \u03b5(u, s)\u03bd(du) ds , l \u22c6 1 (1, 0) = l \u22c6 1 (0, 1) = 0, l \u22c6 1 (1, 1) = \u03b5 1 exp \u2212 E\u00d7[0,\u03c41] \u03b5(u, s)\u03bd(du) ds ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Suppose one observes a series of events X 1 , X 2 , . . . , X n occurring at times \u03c4 1 , \u03c4 2 , . . . , \u03c4 n . Each event is either \"normal\" or \"abnormal.\" The objective is to identify those \"abnormal\" events.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "One application of this problem is in epidemiology. For instance, the patients with Severe Acute Respiratory Syndrome (SARS) have symptoms similar to those of common flu patients. However since SARS is much more infectious than common flu, the SARS patients often appear in clusters. Such statistical evidence enables us to identify the SARS patients by mathematical tools. It provides a supplementary method to the costly medical test.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Another application is to collusion set detection. In a stock market, a group of traders forms a collusion set if they heavily trade among themselves in order to manipulate the stock price. It is of interest to catch this kind of malpractice as early as possible. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Considering each trade record as an event, it is intuitive that the malicious trading events tend to cluster. Assuming that a distance measuring the dissimilarity between any two records is available, Palshikar and Apte tackle the problem via graph clustering in [12] . They ignore the time stamp on the trade record so that a point process is reduced to a graph. But the temporal information is lost in their method.",
            "cite_spans": [
                {
                    "start": 263,
                    "end": 267,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "These examples motivated our filtering model. We model the observations as a mixture of two independent marked point processes representing the \"normal\" and the \"abnormal\" events, respectively. Each new \"abnormal\" event will change the intensity of the \"abnormal\" point process. Typically the \"abnormal\" event increases the intensity for additional \"abnormal\" events in its neighborhood. Our goal is to compute the conditional probability of each observed event being abnormal in real time. Employing filtering techniques, we derive versions of the Zakai and Kushner-Stratonovich equations. Under a Markov condition, a sequential algorithm is presented to calculate the exact conditional probability that we are interested in.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "Unfortunately, the data set for the two examples above is not available. We will present our methodology in the context of the \"earthquake declustering problem.\" Even though there is no agreement on the underlying mechanism of earthquake occurrence in the current seismology literature, we want to emphasize that this example is mainly for the purpose of illustration. Our framework is for general modeling and computation. It could be adapted for different data sets in various areas.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "It is well known that earthquakes often occur in clusters. The largest quake in a cluster is called the main shock, those before it are called foreshocks, and those after it are called aftershocks. The aftershocks in an earthquake swarm are relatively easy to predict. However, there are also many earthquakes that strike without any foreshocks or aftershocks. As the authors stated in [8] : \"To forecast the location of the large earthquakes, it is necessary to analyze the background seismicity, for which removal of temporal cluster members is considered to be of central importance.\"",
            "cite_spans": [
                {
                    "start": 386,
                    "end": 389,
                    "text": "[8]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "In this article, we propose a space-time point process model stemming from [14] . The observed earthquakes are considered as a mixture of earthquake swarms (a swarm contains at least two quakes) and single quakes. This could be considered as a special case of the \"cluster processes\" ([2], Section 6.3): a cluster process is composed of clusters that contain only a single point and clusters that have multiple points; in our model, we distinguish the single point events (single quakes) as the \"noise\" and the multiple point events (earthquake swarms) as the \"signal.\" The conditional probability that a quake is in a cluster becomes a natural criterion for declustering. The filtering theory hence can be applied. We assume that, at most, one cluster is active at a time. This assumption can be relaxed with increased computational complexity.",
            "cite_spans": [
                {
                    "start": 75,
                    "end": 79,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "In the literature, inference for partially observed stochastic processes is often obtained by using Markov chain Monte Carlo (MCMC) methods (see, e.g., [6] ). A particle algorithm is also proposed in [14] . Such approximation methods are more flexible, but they are time-consuming and the approximation error is usually difficult to estimate. This paper deals with finding analytic solutions for some cases.",
            "cite_spans": [
                {
                    "start": 152,
                    "end": 155,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 200,
                    "end": 204,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "The paper is organized as follows: Section 2 describes the generic model and the filtering equations; Section 3 presents the computational procedure for the conditional expectation of interest under the \"mother quake\" assumption (the first quake in a cluster triggers all the other quakes in that cluster); Section 4 illustrates the numerical results for earthquakes in central and western Japan; Section 5 summarizes the conclusions and describes future work; Appendix A gives the algorithm that calculates the maximum likelihood estimators of the parameters and the most likely cluster path; finally, the proofs are contained in Appendix B.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "2. The generic model",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "Suppose observed information about a quake is represented by a mark in a space E. For example, E could be R 3 , recording the earthquake's magnitude and the epicenter's location longitude and latitude. We model observations as a marked point process O with marks in E. O is the mixture of two independent point processes N and C, which stand for the single quakes and earthquake clusters, respectively. Hence letting O(A, t) denote the number of quakes characterized by values in A (A is a subset of E) observed up to time t, we can write",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "We assume that N is a Poisson process with intensity \u03b3 relative to a reference measure \u03bd, hence the single quake model is just a Poisson random measure on E \u00d7 [0, \u221e) with mean measure \u03bd 0 (du \u00d7 ds) = \u03b3(u, s)\u03bd(du) ds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "We model clusters to be randomly initiated and assume they eventually die out; we also assume that there is at most one active cluster at a time as mentioned in Section 1. Let D be the process that indicates whether a cluster is active or not. The process C adds a mark u at time s with non-negative predictable intensity \u03bb(u, s, D s\u2212 , \u03b7 s\u2212 ), where \u03b7 is the configuration of both the marks and occurrence times of all the previous cluster quakes. More precisely, if cluster quake c i occurs at t i , then \u03b7 t = {i: ti\u2264t} \u03b4 (ci,ti) , where \u03b4 (ci,ti) is the Dirac measure concentrated on the point (c i , t i ). Therefore, \u03b7 is a counting measure on E \u00d7 [0, \u221e).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "When D = 0, there is no active cluster and an intensity \u03bb(u, s, 0, \u03b7 s\u2212 ) gives the rate at which a new cluster is initiated by an event with mark u at time s. Once initiated, the cluster grows with intensity \u03bb(u, s, 1, \u03b7 s\u2212 ) until it dies out.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "Under very mild conditions on the intensities (see [5] ), the point processes can be written as solutions of stochastic differential equations. In particular, we can write",
            "cite_spans": [
                {
                    "start": 51,
                    "end": 54,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "where \u03be 1 and \u03be 2 are independent copies of a Poisson random measure on E \u00d7 [0, \u221e) \u00d7 [0, \u221e) with mean measure \u03bd \u00d7 \u2113 \u00d7 \u2113, denoting Lebesgue measure by \u2113.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "In this article, we define D as follows: D is equal to 1 once a cluster is initiated; D has a probability p to die out (i.e., D = 0) whenever a new observation is added to the cluster; D is independent of all previous history. Thus, for an arbitrary function f (D t , \u03b7 t ),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "where the {I k , k = 1, 2, . . .} are independent Bernoulli random variables with parameter p that are also independent of N and C. This follows by writing the right-hand side as a finite sum where most terms cancel out.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "In practice, f (D t , \u03b7 t ) contains information about D t and \u03b7 t . Statistical inferences can be drawn if we are able to compute the conditional expectation of f based on the observations O. The rest of the paper mostly deals with how to realize such a computation for arbitrary f .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "It is worth noting that our whole problem is essentially discrete and finite, hence the measurability of functions is (and should be) of minor concern. As D t is either 0 or 1, and \u03b7 t can only take finitely many values as well (2 n if there are n observations), thus the function domain of f is finite. Therefore, all the functions are measurable.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Formulation of the model"
        },
        {
            "text": "We derive the filtering equation for the conditional distribution of \u03b7 given observations of O using a reference measure approach. If (\u2126, F , Q) is a probability space and P is a second probability measure on F given by dP = L dQ, then for any sub-\u03c3-algebra D \u2282 F and L 1 -random variable Z,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "We are going to use a reference probability measure Q under which the observations have a relatively simple structure. In the following lemma, N and C are independent Poisson random measures under Q. We first introduce a definition that is used in the lemma.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "Lemma 2.2. On (\u2126, F , Q), let N and C be independent Poisson random measures with mean measures \u03bd 0 (du \u00d7 ds) = \u03b3(u, s)\u03bd(du) ds and \u03bd 1 (du \u00d7 ds) = \u03bb Q (u, s)\u03bd(du) ds, respectively; let D be a cadlag process independent of N . Assume all processes are compatible",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "and assuming that L is a {F t }-martingale. Let P satisfy dP |Ft = L(t) dQ |Ft . Then P is a probability measure and under P , for all A such that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "is a local martingale and N is independent of C and is a Poisson random measure with mean measure \u03bd 0 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "Thus under P both N and C have the intensity described in Section 2.1. Hence Lemma 2.2 gives the form of the Radon-Nikodym derivative (or the likelihood) of P with respect to Q. Our further computation then can be justified by the uniqueness of the martingale problem (see [9] or [4] , Chapter 4). The assumption that L is a {F t }-martingale is very mild.",
            "cite_spans": [
                {
                    "start": 273,
                    "end": 276,
                    "text": "[9]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 280,
                    "end": 283,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "Remark 2.3. The following condition is sufficient to ensure that (2.2) is a well-posed equation and that L is a martingale.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "The process D has finitely many jumps in bounded time intervals. Thus we can record the history of the process D by a counting measure h t = {i: ti\u2264t} \u03b4 (Dt i ,ti) ; the sum is over those t i when D takes jumps. Hence it represents a path that has value D ti in time interval [t i , t i+1 ). As in (2.1), let f be an arbitrary function on the two counting measures (h s , \u03b7 s ), and set",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "Since h t contains all the information on D t , we can write D t = D t (h t ). Further, we abuse the notation a little and write \u03bb(u, s,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "We need this expression to simplify the notation in the following theorem and in the application in Section 4. Let \u03b1 denote the indicator that specifies whether or not a cluster is currently active,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "The filtering equations"
        },
        {
            "text": "Theorem 2.4. For an arbitrary function f on (h s , \u03b7 s ), let \u03c6, \u03c0 and f new be defined as in equations (2.3)-(2.5). Then \u03c6 and \u03c0 satisfy the stochastic integral equations",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "In Section 4, we will take f as the indicator functions that indicate the status of \u03b7, so that \u03c0(f, t) gives us the conditional probability that an observation is in the cluster.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "Unlike the infinite-dimensional nonlinear filtering problem, the solution of which can only be approximated, the function space in our problem allows a natural finite decomposition since we have a finite function domain, that is, all the possible combinations of each observed event being in a quake swarm or not. Thus the exact solution could be computed theoretically, but generally the computational load increases exponentially as the number of observations increases. That is not feasible for online updating.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solutions of the filtering equation"
        },
        {
            "text": "In this section and also in Appendix A, we assume that when a cluster is active, the cluster is assumed to be triggered by the first quake (mother quake); when no cluster is active, a new cluster will be initiated randomly with an intensity \u03b5. To be precise, suppose one observes u i at time \u03c4 i . Let y i = (u i , \u03c4 i ) and the set of observations by time",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solutions of the filtering equation"
        },
        {
            "text": "where \u03b8 0 (y i ) = 1 {yi is the mother quake in the currently active cluster} and \u03b8 0 (y i ) is defined as 0 if there is no active cluster at that time. We suppose that the functional form of \u03bb(u, t, y i ) is known. For the application in Section 4, \u03bb(u, t, y i ) is a Gaussian kernel (4.1) that does not depend on t. Note that there is, at most, one \u03b8 0 (y i ) (i = 1, 2, . . . , k) non-zero at any moment t. The simple fact that \u03b8 0 (y i )\u03b8 0 (y j ) = 0 if i = j makes finding an analytic solution possible (see the proof of the following theorems). Formally, we can think of the intensity \u03bb as a vector with component \u03bb(u, t, y i ) at each \"orthogonal\" direction \u03b8 0 (y i ), i = 1, 2, . . . , k. With the help of this kind of \"orthogonal decomposition\" of the function space, the problem can be reduced to be of polynomial complexity. For simplicity, we also assume that there is no cluster active at time 0. Define a(y, t) = E \u03bb(u, t, y)\u03bd(du) for y \u2208 E and \u03b5(t) = E \u03b5(u, t)\u03bd(du). The following two theorems give the algorithm to compute \u03c0(f, t), f is an arbitrary function of D s and \u03b7 s . Recall that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solutions of the filtering equation"
        },
        {
            "text": "In Theorem 3.3, we can solve for \u03c0(\u03b8 0 (y i )\u03b1, t) as the first step in our algorithm. The task of computing \u03c0(f, t) for more general f is completed in the next theorem. Note that the solution \u03c0(\u03b8 0 (y i )\u03b1, t) is needed in (3.4) .",
            "cite_spans": [
                {
                    "start": 224,
                    "end": 229,
                    "text": "(3.4)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Solutions of the filtering equation"
        },
        {
            "text": "Combining Theorems 3.3 and 3.2, we can compute \u03c0(f, t) for an arbitrary f in real time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Solutions of the filtering equation"
        },
        {
            "text": "We use the same data set as in [8] : the earthquakes in the period of 1926-1995 in the rectangular area 34 \u2022 -39 \u2022 N and 131 \u2022 -140 \u2022 E with magnitudes greater than 4.0 and depths less than 100 km. We take \u03bd to be the uniform measure on the rectangular region, \u03b3(u) = \u03b3 and \u03bb(u, D t , \u03b7 t ) = 1 {Dt=1} k i=1 \u03bb(u, y i )\u03b8 0 (y i ) + 1 {Dt=0} \u03b5, where \u03bb(u, y i ) is proportional to a bivariate normal density: We compute \u03c0(\u03b8(y i )(\u00b7, \u00b7), T ) for all observed y i according to Theorem 3.2, where T is the last moment of the year 1995. The results are compared with [8] , where the authors declustered the observations by computing aftershock probabilities under an ETAS model. In Figure 1 , plot (a) is the histogram of the aftershock probabilities as presented in [8] . We denote their aftershock probabilities as p 1 . Plot (b) shows the distribution of the conditional probabilities p 2 in the mother quake model. We are pleased to see that our stochastic models give relatively deterministic answers. Around 95% of quakes have a probability of being in clusters that is either smaller than 0.1 or greater than 0.9, as can be seen in plot (b).",
            "cite_spans": [
                {
                    "start": 31,
                    "end": 34,
                    "text": "[8]",
                    "ref_id": null
                },
                {
                    "start": 561,
                    "end": 564,
                    "text": "[8]",
                    "ref_id": null
                },
                {
                    "start": 761,
                    "end": 764,
                    "text": "[8]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 676,
                    "end": 684,
                    "text": "Figure 1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Application to an earthquake data set"
        },
        {
            "text": "Although both results have a bimodal shape, the one in [8] disagrees with our models for many individual quakes. This can be seen from Figure 2 . The histogram presents the difference of these two probabilities. It seems that the data set supports our model more. We plot the earthquake clusters in each setting by removing quakes with a low probability of being in a cluster. The time-space plots in Figure 3 have 1500 quakes. The vertical axis represents time (unit in days). It is quite clear that the plots from our models have a stronger cluster pattern. The three-dimensional plots are available from http://www.stat.nus.edu.sg/~stawz/ and can be rotated and viewed in different perspectives.",
            "cite_spans": [
                {
                    "start": 55,
                    "end": 58,
                    "text": "[8]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 135,
                    "end": 143,
                    "text": "Figure 2",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 401,
                    "end": 409,
                    "text": "Figure 3",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Application to an earthquake data set"
        },
        {
            "text": "We also can compute \u03c0(D \u03c4i (\u00b7, \u00b7), T ) to see the status of the cluster at different times. Under the mother quake assumption, Figure 4 (a) gives us the conditional probability that the earthquake cluster is active. The answer is again quite distinct. Figure 4(b) shows that most conditional probabilities are either close to 0 or 1.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 127,
                    "end": 135,
                    "text": "Figure 4",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 252,
                    "end": 263,
                    "text": "Figure 4(b)",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Application to an earthquake data set"
        },
        {
            "text": "Assumption (3.1) is just an example. Another earthquake model called the \"domino\" model is given in [14] , where we assume that the last quake triggers the next quake in the cluster. It turns out that the mother quake model is more likely in our data set by comparing their likelihood. Roughly speaking, as long as the conditional intensity \u03bb modeling the cluster only depends on a \"small\" portion of the history (in (3.1), it only depends on the last mother quake), we can adopt an \"orthogonal decomposition\" and find an algorithm to find the analytic solution.",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 104,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Thus assuming that, at most, one cluster is active at a time is not essential for our method. This simplified assumption prevents the presentation from getting more messy. We can similarly work out a decomposition if we assume that, at most, say, three clusters are active at a time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Our filter separates the data set into the cluster quakes and the single quakes. Further data analysis in [14] shows geophysical differences. In particular, the magnitude of the cluster quakes is significantly different from the single quakes. The mother quakes are also significantly bigger than the offspring quakes. Note that we did not incorporate the magnitudes of the earthquakes into the model. This surprising finding further supports our model.",
            "cite_spans": [
                {
                    "start": 106,
                    "end": 110,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "The application to seismology is only a special case of the filtering approach to abnormal cluster identification proposed in [14] . Other possible applications include epidemiology, intrusion detection in network security, criminology and quality control. ",
            "cite_spans": [
                {
                    "start": 126,
                    "end": 130,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "and assume that they are {F t }-martingales. Let L = L N L C . L will also be an {F t }martingale. Let P satisfy dP |Ft = L(t) dQ |Ft . Then P is a probability measure and under P , for all A such that \u03bb(u, s, D s , \u03b7 s )\u03bd(du) ds is a local martingale and N is independent of C and is a Poisson random measure with mean measure \u03b3(u, s)\u03bd(du) ds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "Remark A.2. The L derived from the theorem is the likelihood of our observation, which is the mixture of two processes. It is necessary to have it for the estimation of parameters. While in Lemma 2.2, the simplified version (2.2) is sufficient for proving Theorem 2.4, since it only concerns f (D t , \u03b7 t ), which does not involve the process N . By applying L derived here, we can prove a more general form of Theorem 2.4 so that we can have filtering equations about f (N t , D t , \u03b7 t ). We omit it because the notation gets worse and we do not use it in our application.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "Our goal is to compute E Q [L|F O ], the likelihood in our model. We can first solve (A.1) and (A.2):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "where \u03c1 is the indicator of whether the observation is a cluster point. Under reference measure Q, \u03c1 1 , \u03c1 2 , . . . are i.i.d. Bernoulli(1/2), and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "(\u03bb(u, s, D s\u2212 , \u03b7 s\u2212 ) + \u03b3(u, s))\u03bd(du) ds .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "We use \"\u221d\" since we ignore a constant, which has no impact on likelihood inference. Recall \u03b3 j = \u03b3(y j , \u03c4 j ), \u03bb j,i = \u03bb(y j , \u03c4 j , y i ), \u03b5 j = \u03b5(y j , \u03c4 j ). Let \u03c4 0 = 0 and suppose one observes u i at time \u03c4 i , y i = (u i , \u03c4 i ). We recall the special form of \u03bb(u, t, D t , \u03b7 t ):",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "where \u03b8 0i (y j ) = 1 {yj is the latest mother quake in the cluster at time \u03c4i\u2212} and \u03bb i,j is defined as in Theorem 3.3. Let E i be the index of the latest mother quake at time \u03c4 i \u2212, then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "Assume no cluster is active at time 0, hence D 0 = 0. Therefore,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "[\u03b5(u, s) + \u03b3(u, s)]\u03bd(du) ds .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "Sum over two terms corresponding to \u03c1 1 = 0 and \u03c1 1 = 1, respectively, and we have",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "[\u03b5(u, s) + \u03b3(u, s)]\u03bd(du) ds .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Z. Wu"
        },
        {
            "text": "It is not practical to sum over all the terms by brute force since the number of terms increases exponentially with respect to the number of observations. However, we can reduce the complexity to O(n 2 ) by computing exp{",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "recursively. We recall that when a single quake is observed at time \u03c4 i it has no impact on the cluster quakes E i+1 = E i and D \u03c4i = D \u03c4i\u2212 , while when a cluster quake is observed at time \u03c4 i , there are two possible scenarios: The first case is D \u03c4i\u2212 = 0, so the observation at time \u03c4 i is a new mother quake D \u03c4i = 1 and E i+1 = i. The second case is D \u03c4i\u2212 = 1, so the observation is an offspring quake; hence, E i+1 = E i . This observation kills the cluster with probability p. In other words, we look at a Bernoulli(p) random variable I i , which is independent of everything else. If I i = 1, D \u03c4i = 1; otherwise D \u03c4i = 0. For 0 < i < j,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "(\u03bb(u, s, y i ) + \u03b3(u, s))\u03bd(du) ds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "(\u03b5(u, s) + \u03b3(u, s))\u03bd(du) ds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "The second equality utilizes the fact that the switch I j is triggered only when a cluster point is observed. The last equality uses the fact that I j is an independent Bernoulli variable with parameter p. Since \u03c1 j is an independent Bernoulli variable with parameter 1/2, we have:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "We ignore the constant factor 1/2 in the algorithm. This raises our forward algorithm. Let \u22121,\u03c4j] \u03bb(u, s, y i )\u03bd(du) ds l j\u22121 (1, i) ,",
            "cite_spans": [
                {
                    "start": 91,
                    "end": 97,
                    "text": "\u22121,\u03c4j]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 126,
                    "end": 132,
                    "text": "(1, i)",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "\u03bb(u, s, y i )\u03bd(du) ds l j\u22121 (1, i) From the discussion above, we can find the likelihood for any specific set of parameters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "Looking for the MLE hence is a standard optimization problem. It turns out that a nonderivative method works better in the example of this paper. In particular, we use the Nelder-Mead simplex method to search for the MLE (see [11] ). The asymptotic confidence intervals for the parameters can be constructed. Observe that we have a hidden Markov model (HMM), essentially. Hence the theorems of asymptotic normality of the MLE for a general HMM should apply (see [1, 3, 10] ). Consequently, there are corresponding likelihood-ratio tests for the HMM as established in [7] . The asymptotic confidence intervals can then be constructed by inverting the test statistics (see [14] ).",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 230,
                    "text": "[11]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 462,
                    "end": 465,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 466,
                    "end": 468,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 469,
                    "end": 472,
                    "text": "10]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 567,
                    "end": 570,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 671,
                    "end": 675,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "So the procedure to find the most likely cluster sequence starts from the calculation of l \u22c6 j (d, i), using recursion in (A.7) and (A.8) while always keeping a record of the \"winning sequence\" in the maximum finding operation. Finally the last state (d, i) \u22c6 is found where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "and, starting from this state, the sequence is recovered by backtracking. As before, normalization is necessary in each step of the recursion to prevent them from degenerating to 0 or infinity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "is a local martingale, and hence",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "is as well.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "Proof of Theorem 2.4. To simplify the notation, we use f (0, \u03b7 s\u2212 + \u03b4 (u,s) ) to denote f (\u00b7 + \u03b4 (0,s) , \u03b7 s\u2212 + \u03b4 (u,s) ) and f (1, \u03b7 s\u2212 + \u03b4 (u,s) ) to denote f (\u00b7 + \u03b4 (1,s) , \u03b7 s\u2212 + \u03b4 (u,s) ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "Noting that ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "Applying It\u00f4's formula,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "\u03c0(f new \u03bb(u, s, \u00b7, \u00b7), s\u2212) \u2212 \u03c0(\u03bb(u, s, \u00b7, \u00b7), s\u2212)\u03c0(f, s\u2212) \u03c0(\u03bb(u, s, \u00b7, \u00b7) + \u03b3(u, s), s\u2212)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "\u03c0(f (\u00b7, \u00b7)\u03bb(u, s, \u00b7, \u00b7), s) \u2212 \u03c0(f, s)\u03c0(\u03bb(u, s, \u00b7, \u00b7), s)\u03bd(du) ds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "Proof of Theorem 3.3. We apply Theorem 2.4. For \u03c4 k \u2264 t < \u03c4 k+1 ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "\u03c0(\u03b8 0 (y)\u03b1\u03bb(u, s, h, \u03b7), s)\u03bd(du) ds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "\u03c0(\u03b8 0 (y)\u03b1, s)\u03c0(\u03bb(u, s, h.\u03b7), s)\u03bd(du) ds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "We use the fact that \u03b1 = x\u2208O(s) \u03b8 0 (x)\u03b1 to get the second equality. Thus, for i = 1, 2, . . . , k,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "[a(y j , t) \u2212 \u03b5(s)]\u03c0(\u03b8 0 (y j ), t) \u03c0(\u03b8 0 (y i ), t), (B.1) (3.2 ) is the unique solution of this system of ordinary differential equations. At time \u03c4 k+1 , \u03c0(\u03b8 0 (y)\u03b1, \u03c4 k+1 ) = \u03c0(\u03b8 0 (y)\u03b1, \u03c4 k+1 \u2212) + \u03c0([1 \u2212 \u03b1(\u00b7, \u00b7)][\u03b8 0 (y)(1, \u00b7 + \u03b4 (u,s) )]\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212) d k+1 + \u03c0(\u03b1(\u00b7, \u00b7)q[(\u03b8 0 (y)\u03b1)(1, \u00b7 + \u03b4 (u,s) )]\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212) d k+1 \u2212 \u03c0(\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212)\u03c0(\u03b8 0 (y)\u03b1, \u03c4 k+1 \u2212) d k+1 = \u03c0(\u03b8 0 (y)\u03b1, \u03c4 k+1 \u2212)\u03b3 k+1 d k+1 + \u03c0([1 \u2212 \u03b1(\u00b7, \u00b7)][\u03b8 0 (y)(1, \u00b7 + \u03b4 (u,s) )]\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212) d k+1 + \u03c0(\u03b1(\u00b7, \u00b7)q[(\u03b8 0 (y)\u03b1)(1, \u00b7 + \u03b4 (u,s) )]\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212) d k+1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "For i < k + 1, \u03c0(\u03b8 0 (y i )\u03b1, \u03c4 k+1 ) = \u03c0(\u03b8 0 (y)\u03b1, \u03c4 k+1 \u2212)(\u03b3 k+1 + q\u03bb k+1,i ) d k+1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "For i = k + 1, \u03c0(\u03b8 0 (y k+1 )\u03b1, \u03c4 k+1 ) = k j=1 (q\u03bb k+1,j \u2212 \u03b5 k+1 )\u03c0(\u03b8 0 (y j )\u03b1, \u03c4 k+1 \u2212) + \u03b5 k+1 d k+1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "Proof of Theorem 3.2. For \u03c4 k \u2264 t < \u03c4 k+1 , \u03c0(\u03b8 0 (y)\u03b1f, t) = \u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k ) \u2212 E\u00d7[\u03c4 k ,t] \u03c0(\u03b8 0 (y)\u03b1f \u03bb(u, s, h, \u03b7), s)\u03bd(du) ds",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "\u03c0(\u03b8 0 (y)\u03b1f, s)\u03c0(\u03bb(u, s, h, \u03b7), s)\u03bd(du) ds = \u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k ) \u2212 \u03c0(\u03b8 0 (y)\u03b1f, s) = \u03c0(\u03b8 0 (y)\u03b1, s)\u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k ) \u03c0(\u03b8 0 (y)\u03b1, \u03c4 k ) and (3.3) follows easily.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "\u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k+1 ) = \u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k+1 \u2212) \u2212 \u03c0(\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212)\u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k+1 \u2212) d k+1 + \u03c0([1 \u2212 \u03b1(\u00b7, \u00b7)][(\u03b8 0 (y)f )(1, \u00b7 + \u03b4 (u,s) )]\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212) d k+1 + \u03c0(\u03b1(\u00b7, \u00b7)q[(\u03b8 0 (y)f )(1, \u00b7 + \u03b4 (u,s) )]\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212) d k+1 = [1 \u2212 \u03b4 y k+1 (y)]\u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k+1 \u2212)\u03b3 k+1 d k+1 + [1 \u2212 \u03b4 y k+1 (y)]q k j=1 \u03bb k+1,j \u03c0(f (1, \u00b7 + \u03b4 y k+1 )\u03b8 0 (y j )\u03b1, \u03c4 k+1 \u2212) d k+1 + \u03b4 y k+1 (y)\u03c0(f (1, \u00b7 + \u03b4 y k+1 )(1 \u2212 \u03b1), \u03c4 k+1 \u2212)\u03b5 k+1 d k+1 , \u03c0(f, \u03c4 k+1 \u2212)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "= \u03c0(f, \u03c4 k ) \u2212 + \u03c0(f, t)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "x\u2208Y (\u03c4 k ) \u03c0(\u03b8 0 (x)\u03b1, t)[a(x, t) \u2212 \u03b5(s)] , \u03c0(f, \u03c4 k+1 ) = \u03c0(f, \u03c4 k+1 \u2212) \u2212 \u03c0(\u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212)\u03c0(\u03b1f, \u03c4 k+1 \u2212) d k + 1 + \u03c0(f new \u03bb(y k+1 , \u03c4 k+1 , \u00b7, \u00b7), \u03c4 k+1 \u2212) d k+1",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "= \u03c0(f, \u03c4 k+1 \u2212)\u03b3 k+1 + \u03c0(f (1, \u00b7 + \u03b4 y k+1 )(1 \u2212 \u03b1)\u03b5 k+1 , \u03c4 k+1 \u2212) d k+1 + k j=1 \u03bb k+1,j \u03c0([pf (0, \u00b7 + \u03b4 y k+1 ) + qf (1, \u00b7 + \u03b4 y k+1 )]\u03b8 0 (y j )\u03b1, \u03c4 k+1 \u2212) d k+1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        },
        {
            "text": "Proof of Theorem A.1. Apply Lemma 2.2 and note the independence of C and N .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A.2. The forward algorithm and MLE"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Asymptotic normality of the maximumlikelihood estimator for general hidden markov models",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Bickel",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ritov",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ryden",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "Ann. Statist",
            "volume": "26",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "An Introduction to the Theory of Point Processes",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "J"
                    ],
                    "last": "Daley",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Vere-Jones",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Asymptotics of the maximum likelihood estimator for general hidden Markov models",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Douc",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Matias",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Bernoulli",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Markov Processes: Characterization and Convergence",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "N"
                    ],
                    "last": "Ethier",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "G"
                    ],
                    "last": "Kurtz",
                    "suffix": ""
                }
            ],
            "year": 1986,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Spatial birth and death processes as solutions of stochastic equations",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Garcia",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Kurtz",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "ALEA Lat. Am. J. Probab. Math. Stat",
            "volume": "1",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Markov Chain Monte Carlo in Practice",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Gilks",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Richardson",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Spiegelhalter",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Likelihood-ratio tests for hidden Markov models",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Giudici",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Ryden",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Vandekerkhove",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Biometrics",
            "volume": "56",
            "issn": "",
            "pages": "742--747",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Stochastic declustering of space-time earthquake occurrences",
            "authors": [],
            "year": null,
            "venue": "J. Amer. Statist. Assoc",
            "volume": "97",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Limit Theorems for Stochastic Processes",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jacod",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shiryaev",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Asymptotic normality of the maximum-likelihood estimator in state space models",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jensen",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Petersen",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Ann. Statist",
            "volume": "27",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Convergence properties of the nelder-mead simplex method in low dimensions",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lagarias",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Reeds",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "H"
                    ],
                    "last": "Wright",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "E"
                    ],
                    "last": "Wright",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "SIAM J. Optim",
            "volume": "9",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Collusion set detection using graph clustering",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Palshikar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M"
                    ],
                    "last": "Aple",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Data Mining and Knowledge Discovery",
            "volume": "16",
            "issn": "",
            "pages": "135--164",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Stochastic Integration and Differential Equations",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Protter",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A filtering approach to abnormal cluster identification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "This is an electronic reprint of the original article published by the ISI/BS in Bernoulli, 2009, Vol. 15, No. 2, 357-379. This reprint differs from the original in pagination and typographic detail.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "mother quake model, the maximum likelihood estimations (MLE's) are \u03b3 = 0.1070, \u03bb = 1.3274, \u03b5 = 0.0126, d = 0.0070, p = 0.2035. The log-likelihood is log(L) = \u221221604.We are interested in \u03b8(y)(\u00b7, \u00b7) = 1 {y is a quake in a cluster} and D t (\u00b7, \u00b7) = 1 {a cluster is active at time t} .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Histograms of (a) aftershock probabilities under ETAS model and (b) conditional probabilities to be in the cluster process in mother quake model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Histogram of p2 \u2212 p1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Time-space plots of the 1500 likely clustered earthquakes under different models: (a) under ETAS model; (b) under the mother quake model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Under the mother quake assumption: (a) the conditional probability that the cluster was alive vs. time; (b) histogram of the conditional probability.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Let \u03bd be a finite measure and, on (\u2126, F , Q), let N and C be independent Poisson random measures with mean measures \u03bd(du) ds; let D satisfy (2.1). Assume all processes are compatible with {F t }. Define L N and L C by solving",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "A \u03bb(u, s, D s , \u03b7 s )\u03bd(du) ds < \u221e for each t > 0,",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Note that, conditional on observations O, all the possible partitions of O = C + N are equally likely under Q. The conditional distribution of L N L C is completely determined by {\u03c1 i } and {I k }. {I k } is independent of {\u03c1 i } since it is independent of N and C. Hence the computation of E Q [L|F O ] is reduced to calculating the expectation of L({\u03c1 i }, {I k }) with i.i.d. Bernoulli(1/2) {\u03c1 i } and i.i.d. Bernoulli(p) {I k }. This expectation can be expressed as a weighted sum over all possible values of {\u03c1 i }, {I k }.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "until time \u03c4 j is L j = exp{\u2212 E\u00d7[0,\u03c4j] \u03b3(u, s)\u03bd(du) ds} j i=0 1 d=0 l j (d, i).Even when there is a moderate number of observations, the scale of the likelihood often exceeds what a computer can handle. Therefore, we compute the log(L) instead. The trick is normalizing l j at each step, thus we have the forward algorithm:1. Computing the normalizing constant:c j\u22121 = j\u22121 (d, i). 2. Normalization: l j\u22121 (d, i) = l j\u22121 (d, i)/c j\u22121 . 3. Updating l j (d,i) according to (A.5) and (A.6). 4. log(L) = n j=1 log(c j ) \u2212 E\u00d7[0,\u03c4n] \u03b3(u, s)\u03bd(du) ds.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "O(E,s) (u)O(du \u00d7 ds) and under the reference measure, the {\u03c1 k (\u00b7), k = 1, 2, . . .} are independent with Q{\u03c1 k(u) = 1} = 1 \u2212 Q{\u03c1 k (u) = 0} = \u03bb Q (u, s)/[\u03bb Q (u, s) + \u03b3(u,s)] and are independent of O.Averaging out the random variables that are independent of O under Q, the equation for the unnormalized conditional distribution becomes",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "0 (x)\u03b1[\u03bb(u, s, x) \u2212 \u03b5(u, s)], s) + \u03b5(u, s) \u03bd(du) ds= \u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k ) \u2212 [\u03c4 k ,t] x\u2208O(s) \u03c0(\u03b8 0 (y)\u03b8 0 (x)\u03b1f, s) u, s, x) \u2212 \u03b5(u, s)\u03bd(du) + \u03b5(s) ds = \u03c0(\u03b8 0 (y)\u03b1f, \u03c4 k ) \u2212 [\u03c4 k ,t] \u03c0(\u03b8 0 (y)\u03b1f, s)a(y, s)1 {y\u2208O(\u03c4 k )} ds + [\u03c4 k ,t] \u03c0(\u03b8 0 (y)\u03b1f, s) \u00d7 x\u2208Y (\u03c4 k ) \u03c0(\u03b8 0 (x)\u03b1, s)[a(x, s) \u2212 \u03b5(s)] + \u03b5(s) ds. j , t) \u2212 \u03b5(s)]\u03c0(\u03b8 0 (y j ), t) \u03c0(\u03b8 0 (y i )\u03b1f, t), (B.2) i = 1, 2, . . . , kComparing (B.2) and (B.1), we conclude:",
            "latex": null,
            "type": "figure"
        },
        "FIGREF14": {
            "text": "(x)\u03b1f \u03bb(u, s, x) + (1 \u2212 \u03b1)\u03b5(u, s)f, s \u03bd(du) ds + E\u00d7[\u03c4 k ,\u03c4 k+1 ] \u03c0(f, s) x\u2208O(s) \u03c0(\u03b8 0 (x)\u03b1\u03bb(u, s, x), s) + (1 \u2212 \u03b1)\u03b5(u, s) \u03bd(du) ds = \u03c0(f, \u03c4 k ) \u2212 [\u03c4 k ,\u03c4 k+1 ] x\u2208O(s) \u03c0(\u03b8 0 (x)\u03b1f, s)[a(x, s) \u2212 \u03b5(s)] 0 (x)\u03b1, s)[a(x, s) \u2212 \u03b5(s)] ds,where a(x, s) = E \u03bb(u, s, x)\u03bd(du), \u03b5(s) = E \u03b5(u, s)\u03bd(du). 0 (x)\u03b1f, t)[a(x, t) \u2212 \u03b5(s)]",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "This material is based upon work supported by, or in part by, the U.S. Army Research Laboratory and the U.S. Army Research Office under contract, grant number DAAD19-01-1-0502, and by NSF Grant DMS 05-03983.I am deeply grateful to my adviser Tom Kurtz for very helpful discussions and comments. He taught me the filtering idea and directed my work on its applications. We thank Jiancang Zhuang who kindly sent us the data set and his numerical results, which were plotted in [8] . We also thank Feng-Chang Lin for pointing us to Zhuang's paper.",
            "cite_spans": [
                {
                    "start": 475,
                    "end": 478,
                    "text": "[8]",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Acknowledgements"
        },
        {
            "text": "Proof of Lemma 2.2.By Theorem III.20 of Protter [13] M ( ",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 52,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Appendix B: Proofs"
        }
    ]
}