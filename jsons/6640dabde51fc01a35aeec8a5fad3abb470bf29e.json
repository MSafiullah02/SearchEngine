{
    "paper_id": "6640dabde51fc01a35aeec8a5fad3abb470bf29e",
    "metadata": {
        "title": "A Novel Approach to Retrieval of Similar Patterns in Biological Images",
        "authors": [
            {
                "first": "Andrzej",
                "middle": [],
                "last": "Sluzek",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Khalifa University",
                    "location": {
                        "settlement": "Abu Dhabi"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Novel descriptors of keypoints are proposed for matching (primarily) biological images. The descriptors incorporate characteristics of limited-size neighborhoods of keypoints. Descriptors are quantized into small vocabularies representing photometry of images (SIFT words) and geometry of their neighborhoods, so that significant distortions can be tolerated. In order to keep precision at a high level, Harris-Affine and Hessian-Affine detectors are independently applied. The retrieval results are accepted only if confirmed by both techniques. Using several test datasets, we preliminarily show that the method can retrieve semantically meaningful data from unknown and unpredictable images without any training or supervision. Low computational complexity of the method makes it a good candidate for scalable analysis of biological (e.g. zoological or botanical) visual databases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Visual inspection of random collections of images is one of the most tedious tasks in biological and biomedical practice. Algorithms performing automatic analysis of images are continuously developed for selected applications, where the objective is to identify well-defined objects/phenomena, e.g. cell counting, MRI segmentation, etc. However, in case of vaguely defined problems (e.g. searching for similarly looking components in images of plants, detection for similar phenomena in microscopic images of bacteria, etc.) images might be too diversified or too unpredictable for such specialized algorithms. The tasks are particularly difficult if the correspondence between semantics of image contents and their pictorial representations is not straightforward. A prospective solution for the above challenges is to apply data mining, which in the visual domain is often referred to as content-based visual information retrieval (CBVIR).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Reported CBVIR applications generally focus on three areas, i.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "i. Retrieval of near-duplicate images (including partial near-duplicates, e.g. the same objects). ii. Retrieval of (semantically) the same category objects. iii. Retrieval of (semantically) the same category scene. In (i), the expected result is a collection of image pairs depicting physically identical scenes or objects (subject to deformations caused by the viewpoint change, camera quality, visibility conditions, partial occlusions, etc.). No training or supervision (regarding image semantics) is usually required. In (ii) and (iii), however, training is indispensable to generalize visual characteristics of semantic categories/objects, because individual images from the same category may be (visually) very different. In all the above areas, keypoint-based approaches are one the most fundamental tool (in particular visual words, BoW, and other word-based techniques, e.g. semantic topics built upon visual words).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Unfortunately, many types of biological images do not fit any of these areas. First, biological objects (even if considered identical) usually have diversified forms (and, subsequently, their images are only approximately near-duplicate). Secondly, in many problems the numbers and types of the semantic categories are not specified (e.g. the contents of images might not be fully classified).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The objective of this paper is to partially fill the gap between (i) and (ii)/(iii). We propose a scheme (based on the general concept of keypoint description and matching) to preliminarily identify images (e.g. biological images) which may contain meaningfully similar contents in spite of a wide range of deformations within these contents. No training (or pre-existing knowledge about the images) is assumed so that the scheme can quickly switch from on application to another. Moreover, the scheme is computationally efficient; we use a novel affine-invariant keypoint description where individual matches indicate the presence of visually similar fragments. The main difference, compared to typical descriptors like SIFT or SURF, is that our descriptions represent not only keypoints but also their limited-size neighborhoods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The method is proposed as a tool for preliminary search and analysis in collections of (primarily) biological images with random and unknown contents (although the general scope of these collections should be known for easier interpretation of the results). As proof-of-concept examples, we use three small (but diversified) datasets containing images of butterflies, leaves and viruses.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In Section 2 of the paper we overview pre-existing tools and techniques contributing to the proposed method. Its description is provided in Section 3. Results for the selected datasets are presented in Section 4, while Section 5 summarizes the paper",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Affine-invariant keypoint detectors are the low-level tools of the method. Two popular detectors, i.e. Harris-Affine and Hessian-Affine, [8] , are used because of their mutually supplementing characteristics. Harris-Affine highlights corner-like saliencies, while Hessian-Affine returns saliencies corresponding to blobs. Keypoints are represented by SIFT descriptor, [7] , quantized into visual words. A relatively small vocabulary SV of 2000 words is used. Such a vocabulary is able to accept significant photometric distortions of image contents, but the discriminative power of individual words is very low (e.g. comments in [10] , [13] ). Nevertheless, as shown in Section 3, we actually use a 3D Cartesian product of vocabularies so that the practical resolution of descriptions is much higher.",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 140,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 368,
                    "end": 371,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 629,
                    "end": 633,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 636,
                    "end": 640,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Typical Keypoint-Based Tools and Techniques"
        },
        {
            "text": "In general, keypoints can be matched using either O2O (typically mutual nearest neighbor) or M2M (typically the same visual word) schemes. However, regardless the scheme (and regardless the vocabulary size) none of the schemes based on individual keypoint correspondences can reliably distinguish between actually similar image fragments and random locally similar contents. Fig. 1 shows examples of matching in a pair of images sharing the same object and in an unrelated pair. For all schemes, there is no qualitative difference between the results for both pairs.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 375,
                    "end": 381,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Typical Keypoint-Based Tools and Techniques"
        },
        {
            "text": ". Partial near-duplicates are usually detected by the verification of configuration constraints for unspecified groups of preliminarily matched keypoints (e.g. [1, [3] , [9] , [14] ). However, this is a computation-intensive operation which is not fully scalable to large databases. In particular, if numerous matches are found using a small vocabulary (e.g. Fig. 1B ) the configuration verification can be prohibitively costly. Moreover, it is generally believed (e.g. [13] ) that small vocabularies excessively reduce precision of retrieved results.",
            "cite_spans": [
                {
                    "start": 164,
                    "end": 167,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 170,
                    "end": 173,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 176,
                    "end": 180,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 470,
                    "end": 474,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 359,
                    "end": 366,
                    "text": "Fig. 1B",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Typical Keypoint-Based Tools and Techniques"
        },
        {
            "text": "Unfortunately, in biological images we need both small vocabularies and relaxed configuration constraints (because of highly diversified appearances of nominally the same objects -e.g. individual butterflies of the same species). Thus, the available methods and techniques are generally unable to handle such flexibility, unless they are trained using sufficiently diversified positive and (sometimes) negative examples to classify/recognize images or objects of predefined categories, e.g. [2] , [5] , [15] .",
            "cite_spans": [
                {
                    "start": 491,
                    "end": 494,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 497,
                    "end": 500,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 503,
                    "end": 507,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Typical Keypoint-Based Tools and Techniques"
        },
        {
            "text": "Our objective is to develop a method which can overcome the difficulties highlighted in Section 2 for a wide range of biological images of diversified contents. In particular, our intensions are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Principles of the Method"
        },
        {
            "text": "1. To bypass the configuration analysis for preliminarily matched keypoints by incorporating affine-invariant descriptions of keypoint neighborhood geometry into descriptors of the keypoints themselves. Descriptors of geometry are quantized into another small-size vocabulary so that significant geometric distortions can be tolerated. 2. To improve precision of image retrieval (when small vocabularies are used) by combining independently obtained results for Harris-Affine and Hessian-Affine keypoints. Only images retrieved in both operations are accepted (subject to additional details discussed below).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Principles of the Method"
        },
        {
            "text": "To represent geometry of keypoint neighborhoods, we propose to use a modification of the method discussed in [12] . First, we build limited-size neighborhoods of extracted keypoints. The neighborhoods consist of a limited number (not more than 20) keypoints of similar sizes (between 50% and 150% of the area of the central keypoint) and within a limited distance (between 70% and 200% of the Mahalanobis distance defined by the ellipse of the central keypoint) from the keypoint of interest; see Fig ",
            "cite_spans": [
                {
                    "start": 109,
                    "end": 113,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [
                {
                    "start": 497,
                    "end": 500,
                    "text": "Fig",
                    "ref_id": null
                }
            ],
            "section": "Extended Description of Keypoints"
        },
        {
            "text": "is used to characterize each of the trapezoids.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extended Description of Keypoints"
        },
        {
            "text": "The range of Inv invariant is actually quantized into 12 values (words) so that geometry of the whole triplet of ellipses is described by a small vocabulary GV of 12 3 = 1728 words. Altogether, a triplet of keypoints can be represented by SIFT words from a vocabulary of 2000 words (visual properties of individual keypoints) and one word from 1728 words of GV vocabulary (geometry of the whole triplet).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extended Description of Keypoints"
        },
        {
            "text": "Given a keypoint K and its neighbors { } ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extended Description of Keypoints"
        },
        {
            "text": "from the Cartesian product SV SV GV \u00d7 \u00d7 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extended Description of Keypoints"
        },
        {
            "text": "Using descriptions proposed in Section 2.1, matching keypoints (i.e. actually matching their neighborhoods as well) is straightforward. Two keypoints K and L match if",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matching Keypoints and Images"
        },
        {
            "text": "i.e. the keypoints are visually similar and their neighborhoods are similar (at least partially) both visually and geometrically. Compared to the correspondences obtained by matching only the keypoints themselves, i.e.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matching Keypoints and Images"
        },
        {
            "text": "( ) ( ) SV K SV L = (see the second row of Fig. 1 ) the results are more meaningful. The results in Fig. 3 show relatively few correspondences (and many of them are correct in the global context). Note that no verification of configuration constraints is needed; the results are obtained by simple keypoint matching.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 43,
                    "end": 49,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 100,
                    "end": 106,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Matching Keypoints and Images"
        },
        {
            "text": "Nevertheless, some correspondences are between locations which are semantically rather different. This is primarily because small SV and GV vocabularies can tolerate significant photometric and geometric distortions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matching Keypoints and Images"
        },
        {
            "text": "Thus, as the final verification step, we combine Harris-Affine and Hessian-Affine correspondences. A pair of matched Harris-Affine keypoints (K HA , L HA ) is accepted only if there is another pair of Hessian-Affine keypoint (K HE , L HE ) overlapping it (and another way around). Formally, \"overlapping\" means that the Mahalanobis distances (defined by the shapes of keypoint ellipses) between the corresponding keypoints are below the threshold. Fig. 4 provides an illustrative example (using 140% of the corresponding Mahalanobis unit distances as the threshold; this value is used in the paper).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 448,
                    "end": 454,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Matching Keypoints and Images"
        },
        {
            "text": "Eventually, a pair of images is retrieved (i.e. is it assumed that both images contain partial near-duplicate of noticeable size) if at least one pair of keypoint correspondences is retained after the final verification. The coordinates of those keypoints indicate the approximate locations of the partial near-duplicates in the matched images. It should be noted that each pair of keypoint correspondences additionally indicates an unspecified number of matches between keypoints from both neighborhoods.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Matching Keypoints and Images"
        },
        {
            "text": "The results after the final verification for exemplary images from Figs 1 and 3 are given in Fig. 5 . The same roadsign is correctly detected as a partial near-duplicate in Fig. 5A . However, one of the keyboard keys is also sufficiently similar (because of small vocabularies) to the roadsign and recognized as a partial near-duplicate as well (Fig. 5B ). This can be considered a disadvantage, but in biological or biomedical images such relatively weak similarities should be usually retrieved as potential candidates for semantically meaningful partial near-duplicates.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 93,
                    "end": 99,
                    "text": "Fig. 5",
                    "ref_id": null
                },
                {
                    "start": 173,
                    "end": 180,
                    "text": "Fig. 5A",
                    "ref_id": null
                },
                {
                    "start": 345,
                    "end": 353,
                    "text": "(Fig. 5B",
                    "ref_id": null
                }
            ],
            "section": "Matching Keypoints and Images"
        },
        {
            "text": "The proposed approach is currently being tested on diversified databases of biological and biomedical images. The objective is to evaluate the practical significance of the results (relevance to the human interpretation of the images, retrieval of unspecified \"hidden\" semantics, etc.). Three small-scale examples are discussed in this paper.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In all examples only greylevel images are used (so that the visual analysis is more difficult).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "A part of the Ponce Research Group butterfly dataset 1 has be used. The same dataset was analyzed in [6] with similar concepts of keypoint matching. However, both training and geometric consistency verification were used. Other reported works on automatic recognition of butterflies (e.g. [4] , [11] ) also heavily rely on the specific properties of butterfly images. The presented method achieves comparable results without any preliminary knowledge about the image domain and by using only individual keypoint matches (note that complexity of the final verification illustrated in Fig. 4 is negligible) .",
            "cite_spans": [
                {
                    "start": 101,
                    "end": 104,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 289,
                    "end": 292,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 295,
                    "end": 299,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 583,
                    "end": 604,
                    "text": "Fig. 4 is negligible)",
                    "ref_id": null
                }
            ],
            "section": "Images of Butterflies"
        },
        {
            "text": "The Altogether, recall of the method (in retrieval of image pairs containing butterflies of the same species) is 65.2%, while precision is nominally 47.1%. However the actual precision is 94.2% because both similarly looking wing fragments and similarly looking background plants are also semantically correct partial near-duplicates (especially because the domain of images is not specified and we should not distinguish between partial near-duplicities within different types of objects).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Images of Butterflies"
        },
        {
            "text": "Additionally, we have built similarity graphs between the dataset images. It was found that its K-connected sub-graphs consist of nodes representing images of the same species butterflies. We do not discuss details of this issue in the paper. However, it is again mentioned in the following Section 4.2. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Images of Butterflies"
        },
        {
            "text": "The other two datasets contain images of plant leaves 2 and viruses (low-resolution images collected from the web). The leaf images are of rather low quality (see examples in Fig. 7 ). The total number of image pairs to be matched (images showing only contours of leaves were deleted) is 2,775. However, the ground truth is not available (even though preliminary grouping is provided).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 175,
                    "end": 181,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Other Images"
        },
        {
            "text": "The algorithm has retrieved 83 pairs of images containing partial near-duplicates. Surprisingly, the similarity graph of the matched images is strongly connected (the similarity graph is 5-connected) so that the retrieved pairs of images -with only 14 images contributing to these pairs -can prospectively define some properties of the retrieved images. Seven of those images are given in Fig. 8 , and it can be clearly seen that all of them have jagged (at least partially) borders. Thus, we can conclude that this is the (semantic) property of leaves automatically identified by the algorithm. In the third test, 528 pairs of images (some are given in Fig. 9A) showing viruses of various diseases have been matched. Eventually, partial near-duplicates have been found in only two pairs of images (Lassa fever virus sharing partial near-duplicates with rubella virus, and SARS virus with West Nile fever virus) -see Fig. 9B .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 389,
                    "end": 395,
                    "text": "Fig. 8",
                    "ref_id": null
                },
                {
                    "start": 654,
                    "end": 662,
                    "text": "Fig. 9A)",
                    "ref_id": null
                },
                {
                    "start": 917,
                    "end": 924,
                    "text": "Fig. 9B",
                    "ref_id": null
                }
            ],
            "section": "Other Images"
        },
        {
            "text": "In general the numbers of images in these two datasets are too small to reliably evaluate performances (e.g. recall and precision) especially because the ground truth classification is missing for some images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Other Images"
        },
        {
            "text": "Nevertheless, the visual inspection of the results confirms that the retrieved visual partial near-duplicates actually have some semantic significance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Other Images"
        },
        {
            "text": "We show that very simple and general tools, i.e. (1) keypoint matching using small vocabularies representing visual and geometric properties of keypoints and their neighborhoods and (2) superposition of results obtained for Harris-Affine and Hessain-Affine keypoints, can be often used to retrieve meaningful similarities (approximated by partial near-duplicates) from visual databases semantically. No training or domain-specific approaches are required. Because of small vocabularies used, the method can tolerate significant photometric and geometric distortions so that it is particularly suitable for processing biological/biomedical images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        },
        {
            "text": "The presented experiments preliminary confirm the practicality of the method. Currently, experiments are conducted on diversified and much larger databases of biomedical images.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Geometric min-hashing: Finding a (thick) needle in a haystack",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chum",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Perdoch",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Matas",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proc. IEEE Conf. CVPR 2009",
            "volume": "",
            "issn": "",
            "pages": "17--24",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Object class recognition by unsupervised scaleinvariant learning",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Fergus",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perona",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "Proc. IEEE Conf. CVPR 2003",
            "volume": "2",
            "issn": "",
            "pages": "264--271",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Improving bag-of-features for large scale image search",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Jegou",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Douze",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Schmid",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Int. J. Comp. Vision",
            "volume": "87",
            "issn": "",
            "pages": "316--336",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Butterly species identification by branch length similarity entropy",
            "authors": [
                {
                    "first": "S.-H",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Jeon",
                    "suffix": ""
                },
                {
                    "first": "S.-H",
                    "middle": [],
                    "last": "Lee",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "J. Asia-Pacific Entomology",
            "volume": "15",
            "issn": "",
            "pages": "437--441",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Learning to detect unseen object classes by between-class attribute transfer",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Lampert",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Nickisch",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Harmeling",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proc. IEEE Conf. CVPR 2009",
            "volume": "",
            "issn": "",
            "pages": "951--958",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Semi-local affine parts for object recognition",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lazebnik",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Schmid",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ponce",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Proc. BMVC",
            "volume": "",
            "issn": "",
            "pages": "779--788",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Distinctive image features from scale-invariant keypoints",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Lowe",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Int. J. Com. Vision",
            "volume": "60",
            "issn": "",
            "pages": "91--110",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Scale and affine invariant interest point detectors",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mikolajczyk",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Schmid",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "Int. J. Comp. Vision",
            "volume": "60",
            "issn": "",
            "pages": "63--86",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Object retrieval with large vocabularies and fast spatial matching",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Philbin",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chum",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Isard",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proc. IEEE Conf. CVPR 2007",
            "volume": "",
            "issn": "",
            "pages": "1--8",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Robust feature bundling",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Romberg",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "August",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "X"
                    ],
                    "last": "Ries",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Lienhart",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Ho",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kankanhalli",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "PCM 2012",
            "volume": "7674",
            "issn": "",
            "pages": "45--56",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Automatic recognition and measurement of butterfly eyespot patterns",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Silveira",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Monteiro",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Biosystems",
            "volume": "95",
            "issn": "",
            "pages": "130--136",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Large vocabularies for keypoint-based representation and matching of image patches",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "\u015aluzek",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ECCV 2012 Ws/Demos, Part I. LNCS",
            "volume": "7583",
            "issn": "",
            "pages": "229--238",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Size matters: Exhaustive geometric verification for image retrieval accepted for ECCV 2012",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Stew\u00e9nius",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "H"
                    ],
                    "last": "Gunderson",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pilet",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Fitzgibbon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lazebnik",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Perona",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sato",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "ECCV 2012, Part II",
            "volume": "7573",
            "issn": "",
            "pages": "674--687",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Bundling features for large scale partial-duplicate web image search",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Isard",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proc. IEEE Conf. CVPR 2009",
            "volume": "",
            "issn": "",
            "pages": "25--32",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Assemble new object detector with few examples",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X.-S",
                    "middle": [],
                    "last": "Hua",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "H.-J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "IEEE Im. Proc",
            "volume": "20",
            "issn": "",
            "pages": "3341--3349",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Exemplary keypoint matching results using: (A) O2O scheme over Harris-Affine keypoints, (B) M2M (2000 words) over Hessian-Affine keypoints, (C) M2M (2 20 words) over Harris-Affine keypoints",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "A keypoint and its neighbors (A); the size of ellipses for two neighbors is shown as an illustration. Trapezoids built within a triplet of elliptic keypoints (B).Subsequently, triplets of the keypoints are formed and their ellipses are represented by trapezoids shown inFig. 2B. Then, the least complex affine-",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Keypoint correspondences obtained by the proposed method for Harris-Affine keypoints (A) and Hessian-Affine keypoints (B). The same images are used inFig. 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "A pair of matched Harris-Affine keypoints (blue) and Hessian-Affine keypoints (redKeypoint correspondences after the final verification. The same images are used in Figs 1 and 3.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "total number of matched image pairs is 7,140 of which 1,140 are the semantic ground truth (i.e. showing butterflies of the same species). Our method retrieved 1,578 image pairs from which: \uf0fc 743 pairs contain butterflies of the same species (examples in Fig. 6A); \uf0fc 696 pairs show butterflies with similar fragments of wings (see Fig. 6B); \uf0fc 47 pairs indicate similarities between background plants (examples in Fig. 6C); \uf0fc 87 pairs indicate similarities between butterflies and the background plants (good camouflage?); a n d \uf0fc 5 pairs show random similar fragment.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Exemplary matches between the same species butterflies (A), between similar fragments of butterfly wings (B) and between plants in the image backgrounds (C).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Exemplary images from the dataset of leaves",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Seven examples of leaves with (partially) jagged borders A B Exemplary images of viruses (A) and two pairs of images retrieved by the system (B)",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": []
}